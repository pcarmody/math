Undergraduate Texts in Mathematics

Peter J. Olver

Introduction to
Partial Differential Equations

Undergraduate Texts in Mathematics

Undergraduate Texts in Mathematics

Series Editors:
Sheldon Axler
San Francisco State University, San Francisco, CA, USA
Kenneth Ribet
University of California, Berkeley, CA, USA

Advisory Board:
Colin Adams, Williams College, Williamstown, MA, USA
Alejandro Adem, University of British Columbia, Vancouver, BC, Canada
Ruth Charney, Brandeis University, Waltham, MA, USA
Irene M. Gamba, The University of Texas at Austin, Austin, TX, USA
Roger E. Howe, Yale University, New Haven, CT, USA
David Jerison, Massachusetts Institute of Technology, Cambridge, MA, USA
Jeffrey C. Lagarias, University of Michigan, Ann Arbor, MI, USA
Jill Pipher, Brown University, Providence, RI, USA
Fadil Santosa, University of Minnesota, Minneapolis, MN, USA
Amie Wilkinson, University of Chicago, Chicago, IL, USA

Undergraduate Texts in Mathematics are generally aimed at third- and fourth-year undergraduate
mathematics students at North American universities. These texts strive to provide students and teachers
with new perspectives and novel approaches. The books include motivation that guides the reader to an
appreciation of interrelations among different aspects of the subject. They feature examples that illustrate
key concepts as well as exercises that strengthen understanding.

For further volumes:
http://www.springer.com/series/666

Peter J. Olver

Introduction to
Partial Differential Equations

Peter J. Olver
School of Mathematics
University of Minnesota
Minneapolis, MN
USA

ISSN 0172-6056
ISSN 2197-5604 (electronic)
ISBN 978-3-319-02098-3
ISBN 978-3-319-02099-0 (eBook)
DOI 10.1007/978-3-319-02099-0
Springer Cham Heidelberg New York Dordrecht London
Library of Congress Control Number: 2013954394
Mathematics Subject Classification: 35-01, 42-01, 65-01

© Springer International Publishing Switzerland 2014, corrected publication 2020
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is
concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on
microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed. Exempted from this legal reservation are
brief excerpts in connection with reviews or scholarly analysis or material supplied specifically for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of this publication or parts
thereof is permitted only under the provisions of the Copyright Law of the Publisher’s location, in its current version, and
permission for use must always be obtained from Springer. Permissions for use may be obtained through RightsLink at the
Copyright Clearance Center. Violations are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even
in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and
therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors
nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher
makes no warranty, express or implied, with respect to the material contained herein.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

To the memory of my father, Frank W.J. Olver (1924-2013) and mother, Grace E. Olver
(née Smith, 1927-1980), whose love, patience, and guidance formed the heart of it all.

Preface

The momentous revolution in science precipitated by Isaac Newton’s calculus soon revealed the central role of partial diﬀerential equations throughout mathematics and its
manifold applications. Notable examples of fundamental physical phenomena modeled
by partial diﬀerential equations, most of which are named after their discoverers or early
proponents, include quantum mechanics (Schrödinger, Dirac), relativity (Einstein), electromagnetism (Maxwell), optics (eikonal, Maxwell–Bloch, nonlinear Schrödinger), ﬂuid mechanics (Euler, Navier–Stokes, Korteweg–de Vries, Kadomstev–Petviashvili), superconductivity (Ginzburg–Landau), plasmas (Vlasov), magneto-hydrodynamics (Navier–Stokes +
Maxwell), elasticity (Lamé, von Karman), thermodynamics (heat), chemical reactions
(Kolmogorov–Petrovsky–Piskounov), ﬁnance (Black–Scholes), neuroscience (FitzHugh–
Nagumo), and many, many more. The challenge is that, while their derivation as physical models — classical, quantum, and relativistic — is, for the most part, well established,
[57, 69], most of the resulting partial diﬀerential equations are notoriously diﬃcult to solve,
and only a small handful can be deemed to be completely understood. In many cases, the
only means of calculating and understanding their solutions is through the design of sophisticated numerical approximation schemes, an important and active subject in its own
right. However, one cannot make serious progress on their numerical aspects without a
deep understanding of the underlying analytical properties, and thus the analytical and
numerical approaches to the subject are inextricably intertwined.
This textbook is designed for a one-year course covering the fundamentals of partial
diﬀerential equations, geared towards advanced undergraduates and beginning graduate
students in mathematics, science, and engineering. No previous experience with the subject
is assumed, while the mathematical prerequisites for embarking on this course of study
will be listed below. For many years, I have been teaching such a course to students
from mathematics, physics, engineering, statistics, chemistry, and, more recently, biology,
ﬁnance, economics, and elsewhere. Over time, I realized that there is a genuine need for
a well-written, systematic, modern introduction to the basic theory, solution techniques,
qualitative properties, and numerical approximation schemes for the principal varieties of
partial diﬀerential equations that one encounters in both mathematics and applications. It
is my hope that this book will ﬁll this need, and thus help to educate and inspire the next
generation of students, researchers, and practitioners.
While the classical topics of separation of variables, Fourier analysis, Green’s functions,
and special functions continue to form the core of an introductory course, the inclusion
of nonlinear equations, shock wave dynamics, dispersion, symmetry and similarity methods, the Maximum Principle, Huygens’ Principle, quantum mechanics and the Schrödinger
equation, and mathematical ﬁnance makes this book more in tune with recent developments
and trends. Numerical approximation schemes should also play an essential role in an introductory course, and this text covers the two most basic approaches: ﬁnite diﬀerences
and ﬁnite elements.
vii

viii

Preface

On the other hand, modeling and the derivation of equations from physical phenomena
and principles, while not entirely absent, has been downplayed, not because it is unimportant, but because time constraints limit what one can reasonably cover in an academic
year’s course. My own belief is that the primary purpose of a course in partial diﬀerential
equations is to learn the principal solution techniques and to understand the underlying
mathematical analysis. Thus, time devoted to modeling eﬀectively lessens what can be adequately covered in the remainder of the course. For this reason, modeling is better left to
a separate course that covers a wider range of mathematics, albeit at a more cursory level.
(Modeling texts worth consulting include [57, 69].) Nevertheless, this book continually
makes contact with the physical applications that spawn the partial diﬀerential equations
under consideration, and appeals to physical intuition and familiar phenomena to motivate,
predict, and understand their mathematical properties, solutions, and applications. Nor
do I attempt to cover stochastic diﬀerential equations — see [83] for this increasingly important area — although I do work through one important by-product: the Black–Scholes
equation, which underlies the modern ﬁnancial industry. I have tried throughout to balance rigor and intuition, thus giving the instructor ﬂexibility with their relative emphasis
and time to devote to solution techniques versus theoretical developments.
The course material has now been developed, tested, and revised over the past six years
here at the University of Minnesota, and has also been used by several other universities in
both the United States and abroad. It consists of twelve chapters along with two appendices
that review basic complex numbers and some essential linear algebra. See below for further
details on chapter contents and dependencies, and suggestions for possible semester and
year-long courses that can be taught from the book.

Prerequisites
The initial prerequisite is a reasonable level of mathematical sophistication, which includes
the ability to assimilate abstract constructions and apply them in concrete situations.
Some physical insight and familiarity with basic mechanics, continuum physics, elementary thermodynamics, and, occasionally, quantum mechanics is also very helpful, but not
essential.
Since partial diﬀerential equations involve the partial derivatives of functions, the most
fundamental prerequisite is calculus — both univariate and multivariate. Fluency in the
basics of diﬀerentiation, integration, and vector analysis is absolutely essential. Thus, the
student should be at ease with limits, including one-sided limits, continuity, diﬀerentiation,
integration, and the Fundamental Theorem. Key techniques include the chain rule, product
rule, and quotient rule for diﬀerentiation, integration by parts, and change of variables in
integrals. In addition, I assume some basic understanding of the convergence of sequences
and series, including the standard tests — ratio, root, integral — along with Taylor’s
theorem and elementary properties of power series. (On the other hand, Fourier series will
be developed from scratch.)
When dealing with several space dimensions, some familiarity with the key constructions and results from two- and three-dimensional vector calculus is helpful: rectangular
(Cartesian), polar, cylindrical, and spherical coordinates; dot and cross products; partial
derivatives; the multivariate chain rule; gradient, divergence, and curl; parametrized curves
and surfaces; double and triple integrals; line and surface integrals, culminating in Green’s
Theorem and the Divergence Theorem — as well as very basic point set topology: notions of

Preface

ix

open, closed, bounded, and compact subsets of Euclidean space; the boundary of a domain
and its normal direction; etc. However, all the required concepts and results will be quickly
reviewed in the text at the appropriate juncture: Section 6.3 covers the two-dimensional
material, while Section 12.1 deals with the three-dimensional counterpart.
Many solution techniques for partial diﬀerential equations, e.g., separation of variables
and symmetry methods, rely on reducing them to one or more ordinary diﬀerential equations. In order to make progress, the student should therefore already know how to ﬁnd
the general solution to ﬁrst-order linear equations, both homogeneous and inhomogeneous,
along with separable nonlinear ﬁrst-order equations, linear constant-coeﬃcient equations,
particularly those of second order, and ﬁrst-order linear systems with constant-coeﬃcient
matrices, in particular the role of eigenvalues and the construction of a basis of solutions.
The student should also be familiar with initial value problems, including statements of
the basic existence and uniqueness theorems, but not necessarily their proofs. Basic references include [18, 20, 23], while more advanced topics can be found in [52, 54, 59]. On
the other hand, while boundary value problems for ordinary diﬀerential equations play a
central role in the analysis of partial diﬀerential equations, the book does not assume any
prior experience, and will develop solution techniques from the beginning.
Students should also be familiar with the basics of complex numbers, including real
and imaginary parts; modulus and phase (or argument); and complex exponentials and
Euler’s formula. These are reviewed in Appendix A. In the numerical chapters, some
familiarity with basic computer arithmetic, i.e., ﬂoating-point and round-oﬀ errors, is assumed. Also, on occasion, basic numerical root ﬁnding algorithms, e.g., Newton’s Method;
numerical linear algebra, e.g., Gaussian Elimination and basic iterative methods; and numerical solution schemes for ordinary diﬀerential equations, e.g., Runge–Kutta Methods,
are mentioned. Students who have forgotten the details can consult a basic numerical
analysis textbook, e.g., [24, 60], or reference volume, e.g., [94].
Finally, knowledge of the basic results and conceptual framework provided by modern
linear algebra will be essential throughout the text. Students should already be on familiar
terms with the fundamental concepts of vector space, both ﬁnite- and inﬁnite-dimensional,
linear independence, span, and basis, inner products, orthogonality, norms, and Cauchy–
Schwarz and triangle inequalities, eigenvalues and eigenvectors, determinants, and linear
systems. These are all covered in Appendix B; a more comprehensive and recommended
reference is my previous textbook, [89], coauthored with my wife, Cheri Shakiban, which
provides a ﬁrm grounding in the key ideas, results, and methods of modern applied linear
algebra. Indeed, Chapter 9 here can be viewed as the next stage in the general linear
algebraic framework that has proven to be so indispensable for the modern analysis and
numerics of not just linear partial diﬀerential equations but, indeed, all of contemporary
pure and applied mathematics.
While applications and solution techniques are paramount, the text does not shy away
from precise statements of theorems and their proofs, especially when these help shed
light on the applications and development of the subject. On the other hand, the more
advanced results that require analytical sophistication beyond what can be reasonably
assumed at this level are deferred to a subsequent, graduate-level course. In particular,
the book does not assume that the student has taken a course in real analysis, and hence,
while the basic ideas underlying Hilbert space are explained in the context of Fourier
analysis, knowledge of measure theory and Lebesgue integration is neither assumed nor
used. Consequently, the precise deﬁnitions of Hilbert space and generalized functions
(distributions) are necessarily left somewhat vague, with the level of detail being similar

x

Preface

to that found in a basic physics course on quantum mechanics. Indeed, one of the goals of
the course is to inspire mathematics students (and others) to take a rigorous real analysis
course, because it is so indispensable to the more advanced theory and applications of
partial diﬀerential equations that build on the material presented here.

Outline of Chapters
The ﬁrst chapter is brief and serves to set the stage, introducing some basic notation
and describing what is meant by a partial diﬀerential equation and a (classical) solution
thereof. It then describes the basic structure and properties of linear problems in a general
sense, appealing to the underlying framework of linear algebra that is summarized in Appendix B. In particular, the fundamental superposition principles for both homogeneous
and inhomogeneous linear equations and systems are employed throughout.
The ﬁrst three sections of Chapter 2 are devoted to ﬁrst-order partial diﬀerential equations in two variables — time and a single space coordinate — starting with simple linear
cases. Constant-coeﬃcient equations are easily solved, leading to the important concepts
of characteristic and traveling wave. The method of characteristics is then extended, initially to linear ﬁrst-order equations with variable coeﬃcients, and then to the nonlinear
case, where most solutions break down into discontinuous shock waves, whose subsequent
dynamics relies on the underlying physics. The material on shocks may be at a slightly
higher level of diﬃculty than the instructor wishes to deal with this early in the course,
and hence may be downplayed or even omitted, perhaps returned to at a later stage, e.g.,
when studying Burgers’ equation in Section 8.4, or when the concept of weak solution
is introduced in Chapter 10. The ﬁnal section of Chapter 2 is essential, and shows how
the second-order wave equation can be reduced to a pair of ﬁrst-order partial diﬀerential
equations, thereby producing the celebrated solution formula of d’Alembert.
Chapter 3 covers the essentials of Fourier series, which is the most important tool in
our analytical arsenal. After motivating the subject by adapting the eigenvalue method for
solving linear systems of ordinary diﬀerential equations to the heat equation, the remainder
of the chapter develops basic Fourier series analysis, in both real and complex forms. The
ﬁnal section investigates the various modes of convergence of Fourier series: pointwise,
uniform, in norm. Along the way, Hilbert space and completeness are introduced, at
an appropriate level of rigor. Although more theoretical than most of the material, this
section is nevertheless strongly recommended, even for applications-oriented students, and
can serve as a launching pad for higher-level analysis.
Chapter 4 immediately delves into the application of Fourier techniques to construct
solutions to the three paradigmatic second-order partial diﬀerential equations in two independent variables — the heat, wave, and Laplace/Poisson equations — via the method
of separation of variables. For dynamical problems, the separation of variables approach
reinforces the importance of eigenfunctions. In the case of the Laplace equation, separation
is performed in both rectangular and polar coordinates, thereby establishing the averaging
property of solutions and, consequently, the Maximum Principle as important by-products.
The chapter concludes with a short discussion of the classiﬁcation of second-order partial
diﬀerential equations, in two independent variables, into parabolic, hyperbolic, and elliptic
categories, emphasizing their disparate natures and the role of characteristics.
Chapter 5 is the ﬁrst devoted to numerical approximation techniques for partial
diﬀerential equations. Here the emphasis is on ﬁnite diﬀerence methods. All of the

Preface

xi

preceding cases are discussed: heat equation, transport equations, wave equation, and
Laplace/Poisson equation. The student learns that, in contrast to the ﬁeld of ordinary
diﬀerential equations, numerical methods must be specially adapted to the particularities
of the partial diﬀerential equation under investigation, and may well not converge unless
certain stability constraints are satisﬁed.
Chapter 6 introduces a second important solution method, founded on the notion of a
Green’s function. Our development relies on the use of distributions (generalized functions),
concentrating on the extremely useful “delta function”, which is characterized both as an
unconventional limit of ordinary functions and, more rigorously but more abstractly, by
duality in function space. While, as with Hilbert space, we do not assume familiarity
with the analysis tools required to develop the fully rigorous theory of such generalized
functions, the aim is for the student to assimilate the basic ideas and comfortably work
with them in the context of practical examples. With this in hand, the Green’s function
approach is then ﬁrst developed in the context of boundary value problems for ordinary
diﬀerential equations, followed by consideration of elliptic boundary value problems for the
Poisson equation in the plane.
Chapter 7 returns to Fourier analysis, now over the entire real line, resulting in the
Fourier transform. Applications to boundary value problems are followed by a further
development of Hilbert space and its role in modern quantum mechanics. Our discussion
culminates with the Heisenberg Uncertainty Principle, which is viewed as a mathematical
property of the Fourier transform. Space and time considerations persuaded me not to
press on to develop the Laplace transform, which is a special case of the Fourier transform,
although it can be proﬁtably employed to study initial value problems for both ordinary
and partial diﬀerential equations.
Chapter 8 integrates and further develops several diﬀerent themes that arise in the
analysis of dynamical evolution equations, both linear and nonlinear. The ﬁrst section
introduces the fundamental solution for the heat equation, and describes applications in
mathematical ﬁnance through the celebrated Black–Scholes equation. The second section
is a brief discussion of symmetry methods for partial diﬀerential equations, a favorite topic
of the author and the subject of his graduate-level monograph [87]. Section 8.3 introduces
the Maximum Principle for the heat equation, an important tool, inspired by physics, in
the advanced analysis of parabolic problems. The last two sections study two basic higherorder nonlinear equations. Burgers’ equation combines dissipative and nonlinear eﬀects,
and can be regarded as a simpliﬁed model of viscous ﬂuid mechanics. Interestingly, Burgers’ equation can be explicitly solved by transforming it into the linear heat equation. The
convergence of its solutions to the shock-wave solutions of the limiting nonlinear transport
equation underlies the modern analytic method of viscosity solutions. The ﬁnal section
treats basic third-order linear and nonlinear evolution equations arising, for example, in
the modeling of surface waves. The linear equation serves to introduce the phenomenon of
dispersion, in which diﬀerent Fourier modes move at diﬀerent velocities, producing common physical eﬀects observed in, for instance, water waves. We also highlight the recently
discovered and fascinating Talbot eﬀect of dispersive quantization and fractalization on
periodic domains. The nonlinear Korteweg–de Vries equation has many remarkable properties, including localized soliton solutions, ﬁrst discovered in the 1960s, that result from
its status as a completely integrable system.
Before proceeding further, Chapter 9 takes time to formulate a general abstract framework that underlies much of the more advanced analysis of linear partial diﬀerential equations. The material is at a slightly higher level of abstraction (although amply illustrated

xii

Preface

by concrete examples), so the more computationally oriented reader may wish to skip
ahead to the last two chapters, referring back to the relevant concepts and general results in particular contexts as needed. Nevertheless, I strongly recommend covering at
least some of this chapter, both because the framework is important to understanding the
commonalities among various concrete instantiations, and because it demonstrates the pervasive power of mathematical analysis, even for those whose ultimate goal is applications.
The development commences with the adjoint of a linear operator between inner product
spaces — a powerful and far-ranging generalization of the matrix transpose — which naturally leads to consideration of self-adjoint and positive deﬁnite operators, all illustrated
by ﬁnite-dimensional linear algebraic systems and boundary value problems governed by
ordinary and partial diﬀerential equations. A particularly important construction, forming
the foundation of the ﬁnite element numerical method, is the characterization of solutions
to positive deﬁnite boundary value problems via minimization principles. Next, general
results concerning eigenvalues and eigenfunctions of self-adjoint and positive deﬁnite operators are established, which serve to explain the key features of reality, orthogonality,
and completeness that underlie Fourier and more general eigenfunction series expansions.
A general characterization of complete eigenfunction systems based on properties of the
Green’s function nicely ties together two of the principal themes of the text.
Chapter 10 returns to the numerical analysis of partial diﬀerential equations, introducing the powerful ﬁnite element method. After outlining the general construction based
on the preceding abstract minimization principle, we present its practical implementation,
ﬁrst for one-dimensional boundary value problems governed by ordinary diﬀerential equations and then for elliptic boundary value problems governed by the Laplace and Poisson
equations in the plane. The ﬁnal section develops an alternative approach, based on the
idea of a weak solution to a partial diﬀerential equation, a concept of independent interest. Indeed, the nonclassical shock-wave solutions encountered in Section 2.3 are properly
characterized as weak solutions.
The ﬁnal two Chapters, 11 and 12, survey the analysis of partial diﬀerential equations
in, respectively, two and three space dimensions, concentrating, as before, on the Laplace,
heat, and wave equations. Much of the analysis relies on separation of variables, which, in
curvilinear coordinates, leads to new classes of special functions that arise as solutions to
certain linear second-order non-constant-coeﬃcient ordinary diﬀerential equations. Since
we are not assuming familiarity with this subject, the method of power series solutions to
ordinary diﬀerential equations is developed in some detail. We also present the methods
of Green’s functions and fundamental solutions, including their qualitative properties and
various applications. The material has been arranged according to spatial dimension rather
than equation type; thus Chapter 11 deals with the planar heat and wave equations (the
planar Laplace and Poisson equations having been treated earlier, in Chapters 4 and 6),
while Chapter 12 covers all their three-dimensional counterparts. This arrangement allows
a more orderly treatment of the required classes of special functions; thus, Bessel functions
play the leading role in Chapter 11, while spherical harmonics, Legendre/Ferrers functions,
and Laguerre polynomials star in Chapter 12. The last chapter also presents the Kirchhoﬀ
formula that solves the wave equation in three-dimensional space, an important consequence being the validity of Huygens’ Principle concerning the localization of disturbances
in space, which, surprisingly, does not hold in a two-dimensional universe. The book culminates with an analysis of the Schrödinger equation for the hydrogen atom, whose bound
states are the atomic energy levels underlying the periodic table, atomic spectroscopy, and
molecular chemistry.

Preface

xiii

Course Outlines and Chapter Dependencies
With suﬃcient planning and a suitably prepared and engaged class, most of the material
in the text can be covered in a year. The typical single-semester course will ﬁnish with
Chapter 6. Some pedagogical suggestions:
Chapter 1: Go through quickly, the main take-away being linearity and superposition.
Chapter 2: Most is worth covering and needed later, although Section 2.3, on shock waves,
is optional, or can be deferred until later in the course.
Chapter 3: Students that have already taken a basic course in Fourier analysis can move
directly ahead to the next chapter. The last section, on convergence, is
important, but could be shortened or omitted in a more applied course.
Chapter 4: The heart of the ﬁrst semester’s course. Some of the material at the end of
Section 4.1 — Robin boundary conditions and the root cellar problem — is
optional, as is the very last subsection, on characteristics.
Chapter 5: A course that includes numerics (as I strongly recommend) should start with
Section 5.1 and then cover at least a couple of the following sections, the
selection depending upon the interests of the students and instructor.
Chapter 6: The material on distributions and the delta function is important for a student’s
general mathematical education, both pure and applied, and, in particular,
for their role in the design of Green’s functions. The proof of Green’s representation formula (6.107) might be heavy going for some, and can be omitted
by just covering the preceding less-rigorous justiﬁcation of the logarithmic
formula for the free-space Green’s function.
Chapter 7: Sections 7.1 and 7.2 are essential, and convolution in Section 7.3 is also important. Section 7.4, on Hilbert space and quantum mechanics, can easily be
omitted.
Chapter 8: All ﬁve sections are more or less independent of each other and, except for the
fundamental solution and maximum principle for the heat equation, not used
subsequently. Thus, the instructor can pick and choose according to interest
and time alotted.
Chapter 9: This chapter is at a more abstract level than the bulk of the text, and can
be skipped entirely (referring back when required), although if one intends
to cover the ﬁnite element method, the material in the ﬁrst three sections
leading to minimization principles is required. Chapters 11 and 12 can, if
desired, be launched into straight after Chapter 8, or even Chapter 7 plus
the material on the heat equation in Chapter 8.
Chapter 10: Again, for a course that includes numerics, ﬁnite elements is extremely important and well worth covering. The ﬁnal Section 10.4, on weak solutions,
is optional, particularly the revisiting of shock waves, although if this was
skipped in the early part of the course, now might be a good time to revisit
Section 2.3.
Chapters 11 and 12: These constitute another essential component of the classical partial
diﬀerential equations course. The detour into series solutions of ordinary

xiv

Preface

diﬀerential equations is worth following, unless this is done elsewhere in the
curriculum. I recommend trying to cover as much as possible, although one
may well run out of time before reaching the end, in which case, consider
omitting the end of Section 11.6, on Chladni ﬁgures and nodal curves, Section 12.6, on Kirchhoﬀ’s formula and Huygens’ Principle, and Section 12.7,
on the hydrogen atom. Of course, if Chapter 6, on Green’s functions, and
Section 8.1, on fundamental solutions, were omitted, those aspects will also
presumably be omitted here; even if they were covered, there is not a compelling reason to revisit these topics in higher dimensions, and one may prefer
to jump ahead to the more novel material appearing in the ﬁnal sections.

Exercises and Software
Exercises appear at the end of almost every subsection, and come in a variety of genres.
Most sets start with some straightforward computational problems to develop and reinforce
the principal new techniques and ideas. Ability to solve these basic problems is a minimal
requirement for successfully assimilating the material. More advanced exercises appear
later on. Some are routine, but others involve challenging computations, computer-based
projects, additional practical and theoretical developments, etc. Some will challenge even
the most advanced reader. A number of straightforward technical proofs, as well as interesting and useful extensions of the material, particularly in the later chapters, have been
relegated to the exercises to help maintain continuity of the narrative.
Don’t be afraid to assign only a few parts of a multi-part exercise. I have found
the True/False exercises to be particularly useful for testing of a student’s level of understanding. A full answer is not merely a T or F, but must include a detailed explanation
of the reason, e.g., a proof or a counterexample, or a reference to a result in the text.
Many computer projects are included, particularly in the numerical chapters, where they
are essential for learning the practical techniques. However, computer-based exercises are
not tied to any speciﬁc choice of language or software; in my own course, Matlab is the
preferred programming platform. Some exercises could be streamlined or enhanced by the
use of computer algebra systems, such as Mathematica and Maple, but, in general, I
have avoided assuming access to any symbolic software.
As a rough guide, some of the exercises are marked with special signs:
♦ indicates an exercise that is referred to in the body of the text, or is important for
further development or applications of the subject. These include theoretical details,
omitted proofs, or new directions of importance.
♥ indicates a project — usually a longer exercise with multiple interdependent parts.
♠ indicates an exercise that requires (or at least strongly recommends) use of a computer.
The student could be asked either to write their own computer code in, say, Matlab,
Maple, or Mathematica, or to make use of pre-existing packages.
♣ = ♠ + ♥ indicates a more extensive computer project.

Movies
In the course of writing this book, I have made a number of movies to illustrate the
dynamical behavior of solutions and their numerical approximations. I have found that

Preface

xv

they are an extremely eﬀective pedagogical tool and strongly recommend showing them
in the classroom with appropriate commentary and discussion. They are an ideal medium
for fostering a student’s deep understanding and insight into the phenomena exhibited by
the at times indigestible analytical formulas — much better than the individual snapshots
that appear in the ﬁgures in the printed book.
While it is clearly impossible to include the movies directly in the printed text, the
electronic e-book version will contain direct links. In addition, I have posted all the movies
on my own web site, along with the Mathematica code used to generate them:
http://www.math.umn.edu/∼olver/mov.html

When a movie is available, the sign
appears in the ﬁgure caption.

Conventions and Notation
A complete list of symbols employed can be found in the Symbol Index that appears at
the end of the book.
Equations are numbered consecutively within chapters, so that, for example, (3.12)
refers to the 12th equation in Chapter 3, irrespecive of which section it appears in.
Theorems, lemmas, propositions, deﬁnitions, and examples are also numbered consecutively within each chapter, using a single scheme. Thus, in Chapter 1, Deﬁnition 1.2
follows Example 1.1, and precedes Proposition 1.3 and Theorem 1.4. I ﬁnd this numbering
system to be the most helpful for speedy navigation through the book.
References (books, papers, etc.) are listed alphabetically at the end of the text, and
are referred to by number. Thus, [89] is the 89th listed reference, namely my Applied
Linear Algebra text.
Q.E.D. signiﬁes the end of a proof, an acronym for “quod erat demonstrandum”, which
is Latin for “which was to be demonstrated”.
The variables that appear throughout will be subject to consistent notational conventions. Thus t always denotes time, while x, y, z represent (Cartesian) space coordinates.
Polar coordinates r, θ, cylindrical coordinates r, θ, z, and spherical coordinates r, θ, ϕ, will
also be used when needed, and our conventions appear at the appropriate places in the
exposition; be especially careful with the last case, since the angular variables θ, ϕ are
subject to two contradictory conventions in the literature. The above are almost always
independent variables in the partial diﬀerential equations under study; the dependent variables or unknowns will mostly be denoted by u, v, w, while f, g, h and F, G, H represent
known functions, appearing as forcing terms or in boundary data. See Chapter 4 for our
convention, used in diﬀerential geometry, used to denote functions in diﬀerent coordinate
systems, i.e., u(x, y) versus u(r, θ).
In accordance with standard contemporary mathematical notation, the “blackboard
bold” letter R denotes the real number line, C denotes the ﬁeld of complex numbers, Z
denotes the set of integers, both positive and negative, while N denotes the natural numbers,
i.e., the nonnegative integers, including 0. Similarly, R n and C n denote the corresponding
n-dimensional real and complex vector spaces consisting of n–tuples of elements of R and
C, respectively. The zero vector in each is denoted by 0.
Boldface lowercase letters, e.g., v, x, a, usually denote vectors (almost always column
vectors), whose entries are indicated by subscripts: v1 , xi , etc. Matrices are denoted by
ordinary capital letters, e.g., A, C, K, M — but not all such letters refer to matrices; for

xvi

Preface

instance, V often refers to a vector space, while F is typically a forcing function. The entries
of a matrix, say A, are indicated by the corresponding subscripted lowercase letters: aij ,
with i the row index and j the column index.
Angles are always measured in radians, although occasionally degrees will be mentioned in descriptive sentences. All trigonometric functions are evaluated on radian angles.
Following the conventions advocated in [85, 86], we use ph z to denote the phase of a
complex number z ∈ C, which is more commonly called the argument and denoted by
arg z. Among the many reasons to prefer “phase” are to avoid potential confusion with
the argument x of a function f (x), as well as to be in accordance with the “Method of
Stationary Phase” mentioned in Chapter 8.
We use { f | C } to denote a set, where f gives the formula for the members of the
set and C is a (possibly empty) list of conditions. For example, { x | 0 ≤ x ≤ 1 } means
the closed unit interval from 0 to 1, also written [ 0, 1 ], while { a x2 + b x + c | a, b, c ∈ R }
is the set of real quadratic polynomials, and {0} is the set consisting only of the number
0. We use x ∈ S to indicate that x is an element of the set S, while y ∈ S says that y
is not an element. Set theoretic union and intersection are denoted by S ∪ T and S ∩ T ,
respectively. The subset sign S ⊂ U includes the possibility that the sets S and U might
be equal, although for emphasis we sometimes write S ⊆ U . On the other hand, S  U
speciﬁcally implies that the two sets are not equal. We use U \ S = { x | x ∈ U, x ∈ S } to
denote the set-theoretic diﬀerence, meaning all elements of U that do not belong to S. We
use the abbreviations max and min to denote the maximum and minimum elements of a
set of real numbers, or of a real-valued function.
The symbol ≡ is used to emphasize when two functions are identically equal, so f (x) ≡
1 means that f is the constant function, equal to 1 at all values of x. It is also occasionally
used in modular arithmetic, whereby i ≡ j mod n means i − j is divisible by n. The symbol
:= will deﬁne a quantity, e.g., f (x) := x2 − 1. An arrow is used in two senses: ﬁrst, to
indicate convergence of a sequence, e.g., xn → x as n → ∞, or, alternatively, to indicate
a function, so f : X → Y means that the function f maps the domain set X to the image
or target set Y , with formula y = f (x). Composition of functions is denoted by f ◦ g, while
f −1 indicates the inverse function. Similarly, A−1 denotes the inverse of a matrix A.
By an elementary function we mean a combination of rational, algebraic, trigonometric, exponential, logarithmic, and hyperbolic functions. Familiarity with their basic
properties is assumed. We always use log x for the natural (base e) logarithm — avoiding
the ugly modern notation ln x. On the other hand, the required properties of the various
special functions — the error and complementary error functions, the gamma function, Airy
functions, Bessel and spherical Bessel functions, Legendre and Ferrers functions, Laguerre
functions, spherical harmonics, etc. — will be developed as needed.
n

Summation notation is used throughout, so
ai denotes the ﬁnite sum a1 + a2 +
i=1

· · · + an or, if the upper limit is n = ∞, an inﬁnite series. Of course, the lower limit need
not be 1; if it is − ∞ and the upper limit is + ∞, the result is a doubly inﬁnite series,
e.g., the complex Fourier series in Chapter 3. We use lim an to denote the usual limit
n→∞

of a sequence an . Similarly, lim f (x) denotes the limit of the function f (x) at a point a,
x→a

while f (x− ) = lim f (x) and f (x+ ) = lim f (x) are the one-sided (left- and right-hand,
x → a−

x → a+

respectively) limits, which agree if and only if lim f (x) exists.
x→a

We will employ a variety of standard notations for derivatives. In the case of ordinary

Preface

xvii

du
for the derivative of u with respect to
dx
∂u ∂u ∂ 2 u ∂ 3 u
,
, and the
,
,
x. As for partial derivatives, both the full Lebiniz notation
∂t ∂x ∂x2 ∂t ∂x2
more compact subscript notation ut , ux , uxx , utxx , etc. will be interchangeably employed
throughout; see also Chapter 1. Unless speciﬁcally mentioned, all functions are assumed to
be suﬃciently smooth that any indicated derivatives exist and the relevant mixed partial
derivatives are equal. Ordinary derivatives can also be indicated by the Newtonian notation
du
d2 u
dn u
(n)
th order derivative
u instead of
,
while
u
denotes
the
n
. If the
and u for
dx
dx2
dxn
 
variable is time, t, instead of space, x, then we may employ dots, u, u, instead of primes.

 b
f (x) dx, while
f (x) dx is the corresponding
Deﬁnite integrals are denoted by
derivatives, the most basic is the Leibniz notation

a

indeﬁnite integral or anti-derivative. We assume familiarity only with the Riemann theory
of integration, although students who have learned Lebesgue integration may wish to take
advantage of that on occasion, e.g., during the discussion of Hilbert space.

Historical Matters
Mathematics is both a historical and a social activity, and many notable algorithms, theorems, and formulas are named after famous (and, on occasion, not-so-famous) mathematicians, scientists, and engineers — usually, but not necessarily, the discoverer(s). The
text includes a succinct description of many of the named contributors. Readers who are
interested in more extensive historical details, complete biographies, and, when available,
portraits or photos, are urged to consult the informative University of St. Andrews Mactutor web site:
http://www-history.mcs.st-andrews.ac.uk/history/index.html
Early prominent contributors to the subject include the Bernoulli family, Euler, d’Alembert,
Lagrange, Laplace, and, particularly, Fourier, whose remarkable methods in part sparked
the nineteenth century’s rigorization of mathematical analysis and then mathematics in
general, as pursued by Cauchy, Riemann, Cantor, Weierstrass, and Hilbert. In the twentieth century, the subject of partial diﬀerential equations reached maturity, producing an
ever-increasing number of research papers, both theoretical and applied. Nevertheless, it
remains one of the most challenging and active areas of mathematical research, and, in
some sense, we have only scratched the surface of this deep and fascinating subject.
Textbooks devoted to partial diﬀerential equations began to appear long ago. Of particular note, Courant and Hilbert’s monumental two-volume treatise, [34, 35], played a
central role in the development of applied mathematics in general, and partial diﬀerential equations in particular. Indeed, it is not an exaggeration to state that all modern
treatments, including this one, as well as large swaths of research, have been directly inﬂuenced by this magniﬁcent text. Modern undergraduate textbooks worth consulting include
[50, 91, 92, 114, 120], which are more or less at the same mathematical level but have a variety of points of view and selection of topics. The graduate-level texts [38, 44, 61, 70, 99]
are recommended starting points for the more advanced reader and beginning researcher.
More specialized monographs and papers will be referred to at the appropriate junctures.
This book began life in 1999 as a part of a planned comprehensive introduction to
applied math, inspired in large part by Gilbert Strang’s wonderful text, [112]. After some

xviii

Preface

time and much eﬀort, it was realized that the original vision was much too ambitious a
goal, so my wife, Cheri Shakiban, and I recast the ﬁrst part as our applied linear algebra
textbook, [89]. I later decided that a large fraction of the remainder could be reworked
into an introduction to partial diﬀerential equations, which, after some time and classroom
testing, resulted in the book you are now reading.

Some Final Remarks
To the student: You are about to delve into the vast and important ﬁeld of partial
diﬀerential equations. I hope you enjoy the experience and proﬁt from it in your future
studies and career, wherever they may take you. Please send me your comments. Did you
ﬁnd the explanations helpful or confusing? Were enough examples included? Were the
exercises of suﬃcient variety and appropriate level to enable you to learn the material? Do
you have suggestions for improvements to be incorporated into a new edition?
To the instructor : Thank you for adopting this text! I hope you enjoy teaching from
it as much as I enjoyed writing it. Whatever your experience, I want to hear from you. Let
me know which parts you liked and which you didn’t. Which sections worked and which
were less successful. Which parts your students enjoyed, which parts they struggled with,
and which parts they disliked. How can it be improved?
To all readers: Like every author, I sincerely hope that I have eliminated all errors in
the text. But, more realistically, I know that no matter how many times one proofreads,
mistakes still manage to squeeze through (or, worse, be generated during the editing process). Please email me your questions, typos, mathematical errors, comments, suggestions,
and so on. The book’s dedicated web site
http://www.math.umn.edu/∼olver/pde.html
will actively maintain a comprehensive list of known corrections, commentary, feedback,
and resources, as well as links to the movies and Mathematica code mentioned above.

Acknowledgments
I have immensely proﬁted from the many comments, corrections, suggestions, and remarks
by students and mathematicians over the years. I would like to particularly thank my
current and former colleagues at the University of Minnesota — Markus Keel, Svitlana
Mayboroda, Willard Miller, Jr., Fadil Santosa, Guillermo Sapiro, Hans Weinberger, and
the late James Serrin — for their invaluable advice and help. Over the past few years,
Ariel Barton, Ellen Bao, Stefanella Boatto, Ming Chen, Bernard Deconinck, Greg Pierce,
Thomas Scoﬁeld, and Steven Taylor all taught from these notes, and alerted me to a number
of errors, made valuable suggestions, and shared their experiences in the classroom. I
would like to thank Kendall Atkinson, Constantine Dafermos, Mark Dunster, and Gil
Strang, for references and answering questions. Others who sent me commentary and
corrections are Steven Brown, Bruno Carballo, Gong Chen, Neil Datta, René Gonin, Zeng
Jianxin, Ben Jordan, Charles Lu, Anders Markvardsen, Cristina Santa Marta, Carmen
Putrino, Troy Rockwood, Hullas Sehgal, Lubos Spacek, Rob Thompson, Douglas Wright,

Preface

xix

and Shangrong Yang. The following students caught typos during various classes: Dan
Brinkman, Haoran Chen, Justin Hausauer, Matt Holzer, Jeﬀ Gassmann, Keith Jackson,
Binh Lieu, Dan Ouellette, Jessica Senou, Mark Stier, Hullas Seghan, David Toyli, Tom
Trogdon, and Fei Zheng. While I didn’t always agree with or follow their suggestions, I
particularly want to thank the many reviewers of the book for their insightful comments
on earlier drafts and valuable suggestions.
I would like to thank Achi Dosanjh for encouraging me to publish this book with
Springer and for her enthusiastic encouragement and help during the production process.
I am grateful to David Kramer for his thorough job copyediting the manuscript. While
I did not always follow his suggested changes (and, somethimes, chose to deliberately go
against certain grammatical and stylistic conventions in the interests of clarity), they were
all seriously considered and the result is a much-improved exposition.
And last, but far from least, my mathematical family — my wife, Cheri Shakiban, my
father, Frank W.J. Olver, and my son, Sheehan Olver — had a profound impact with their
many comments, help, and advice over the years. Sadly, my father passed away at age 88
on April 23, 2013, and so never got to see the ﬁnal printed version. I am dedicating this
book to him and to my mother, Grace, who died in 1980, for their amazing inﬂuence on
my life.
Peter J. Olver
September 2013

First Corrected Printing (2016)
This is the corrected printing of the book that ﬁxes all currently known typos and errors. Some minor improvements of the exposition, typesetting, and ﬁgures have also been
incorporated. Again, I thank the staﬀ at Springer for all their help and understanding during the production process. I would also like to thank to Adrian Fellhauer, Samuel Fleischer, Ulrich Gerlach, Christopher Grant, Joost Hulshof, Qunli Ji, Ted Kroon, Christoph
Leuenberger, Artem Novozhilov, Ercüment Ortaçgil, Paul Georg Papatzacos, and Mikhail
Shvartsman for their helpful comments on and corrections to the ﬁrst printing.

Second Corrected Printing (2020)
Further corrections and improvements to the exposition have been incorporated into
this new printing. I would particularly like to thank Lawrence Baker for his detailed
reading of both the full text and the Solutions Manual, and thereby spotting many of the
required corrections in both. I would also like to thank Henry Boateng, Joseph Feneuil,
Adam Kay, Manuel Mañas, Svitlana Mayboroda, Bruno Poggi, Ma Shi-Zhuang, Radu
Slobodeanu, James Stowe, and John Zweck for their suggestions and corrections. Finally,
I thank Loretta Bartolini at Springer for all her help navigating the production process.
Peter J. Olver
University of Minnesota
olver@umn.edu
http://www.math.umn.edu/∼olver
July, 2020
The original version of this book was revised. The correction is available at
https://doi.org/10.1007/978-3-319-02099-0_13

Table of Contents
Preface

. . . . . . . . . . . . . . . . . . . . . . . . . . . . vii

Chapter 1. What Are Partial Diﬀerential Equations? . . . . . . . .

1

Classical Solutions . . . . . . . . . . . . . . . . . . . . .
Initial Conditions and Boundary Conditions . . . . . . . . . .
Linear and Nonlinear Equations . . . . . . . . . . . . . . .

4
6
8

Chapter 2. Linear and Nonlinear Waves

. . . . . . . . . . . . . 15

2.1. Stationary Waves . . . . . . . . . . .
2.2. Transport and Traveling Waves . . . . .
Uniform Transport . . . . . . . .
Transport with Decay . . . . . . .
Nonuniform Transport . . . . . . .
2.3. Nonlinear Transport and Shocks . . . . .
Shock Dynamics . . . . . . . . .
More General Wave Speeds . . . . .
2.4. The Wave Equation: d’Alembert’s Formula
d’Alembert’s Solution . . . . . . .
External Forcing and Resonance . .

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

. 16
. 19
. 19
. 22
. 24
. 31
. 37
. 46
. 49
. 50
. 56

Chapter 3. Fourier Series . . . . . . . . . . . . . . . . . . . . 63
3.1. Eigensolutions of Linear Evolution Equations
The Heated Ring . . . . . . . . . .
3.2. Fourier Series . . . . . . . . . . . . . .
Periodic Extensions . . . . . . . . .
Piecewise Continuous Functions . . . .
The Convergence Theorem . . . . . .
Even and Odd Functions . . . . . . .
Complex Fourier Series . . . . . . .
3.3. Diﬀerentiation and Integration . . . . . . .
Integration of Fourier Series . . . . .
Diﬀerentiation of Fourier Series . . . .
3.4. Change of Scale . . . . . . . . . . . . .
3.5. Convergence of Fourier Series . . . . . . .
Pointwise and Uniform Convergence . .
Smoothness and Decay . . . . . . .
Hilbert Space . . . . . . . . . . . .
Convergence in Norm . . . . . . . .
Completeness . . . . . . . . . . . .
Pointwise Convergence . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

. 64
. 69
. 72
. 77
. 79
. 82
. 85
. 88
. 92
. 92
. 94
. 95
. 98
. 99
104
106
109
112
115
xxi

xxii

Table of Contents

Chapter 4. Separation of Variables . . . . . . . . . . . . . . .
4.1. The Diﬀusion and Heat Equations . . . . . . . . . . .
The Heat Equation . . . . . . . . . . . . . . .
Smoothing and Long Time Behavior . . . . . . . .
The Heated Ring Redux . . . . . . . . . . . . .
Inhomogeneous Boundary Conditions . . . . . . .
Robin Boundary Conditions . . . . . . . . . . .
The Root Cellar Problem . . . . . . . . . . . .
4.2. The Wave Equation . . . . . . . . . . . . . . . . .
Separation of Variables and Fourier Series Solutions .
The d’Alembert Formula for Bounded Intervals . . .
4.3. The Planar Laplace and Poisson Equations . . . . . . .
Separation of Variables . . . . . . . . . . . . .
Polar Coordinates . . . . . . . . . . . . . . . .
Averaging, the Maximum Principle, and Analyticity .
4.4. Classiﬁcation of Linear Partial Diﬀerential Equations . . .
Characteristics and the Cauchy Problem . . . . . .

Chapter 5. Finite Diﬀerences

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

121
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

. . . . . . . . . . . . . . . . .

5.1. Finite Diﬀerence Approximations . . . . . . . . . . . . . . .
5.2. Numerical Algorithms for the Heat Equation . . . . . . . . . .
Stability Analysis . . . . . . . . . . . . . . . . . . . .
Implicit and Crank–Nicolson Methods . . . . . . . . . . .
5.3. Numerical Algorithms for First Order Partial Diﬀerential Equations
The CFL Condition . . . . . . . . . . . . . . . . . . .
Upwind and Lax–Wendroﬀ Schemes . . . . . . . . . . . .
5.4. Numerical Algorithms for the Wave Equation . . . . . . . . . .
5.5. Finite Diﬀerence Algorithms for the Laplace and Poisson Equations
Solution Strategies . . . . . . . . . . . . . . . . . . .

181
.
.
.
.
.
.
.
.
.
.

Chapter 6. Generalized Functions and Green’s Functions . . . . .
6.1. Generalized Functions . . . . . . . . . . . . . . . . . . . .
The Delta Function . . . . . . . . . . . . . . . . . . .
Calculus of Generalized Functions . . . . . . . . . . . . .
The Fourier Series of the Delta Function . . . . . . . . . .
6.2. Green’s Functions for One–Dimensional Boundary Value Problems .
6.3. Green’s Functions for the Planar Poisson Equation . . . . . . . .
Calculus in the Plane . . . . . . . . . . . . . . . . . .
The Two–Dimensional Delta Function . . . . . . . . . . .
The Green’s Function . . . . . . . . . . . . . . . . . .
The Method of Images . . . . . . . . . . . . . . . . . .

122
124
126
130
133
134
136
140
140
146
152
155
160
167
172
175

182
186
188
190
195
196
198
201
207
211

215
.
.
.
.
.
.
.
.
.
.

216
217
221
229
234
242
242
246
248
256

Table of Contents

xxiii

Chapter 7. Fourier Transforms . . . . . . . . . . . . . . . . .

263

7.1. The Fourier Transform . . . . . . . . . . . . . . .
Concise Table of Fourier Transforms . . . . . . .
7.2. Derivatives and Integrals . . . . . . . . . . . . . .
Diﬀerentiation . . . . . . . . . . . . . . . .
Integration . . . . . . . . . . . . . . . . . .
7.3. Green’s Functions and Convolution . . . . . . . . .
Solution of Boundary Value Problems . . . . . .
Convolution . . . . . . . . . . . . . . . . .
7.4. The Fourier Transform on Hilbert Space . . . . . . .
Quantum Mechanics and the Uncertainty Principle

Chapter 8. Linear and Nonlinear Evolution Equations

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

. . . . . .

8.1. The Fundamental Solution to the Heat Equation . . . . . .
The Forced Heat Equation and Duhamel’s Principle . .
The Black–Scholes Equation and Mathematical Finance
8.2. Symmetry and Similarity . . . . . . . . . . . . . . . .
Similarity Solutions . . . . . . . . . . . . . . . .
8.3. The Maximum Principle . . . . . . . . . . . . . . . .
8.4. Nonlinear Diﬀusion . . . . . . . . . . . . . . . . . .
Burgers’ Equation . . . . . . . . . . . . . . . . .
The Hopf–Cole Transformation . . . . . . . . . . .
8.5. Dispersion and Solitons . . . . . . . . . . . . . . . . .
Linear Dispersion . . . . . . . . . . . . . . . . .
The Dispersion Relation . . . . . . . . . . . . . .
The Korteweg–de Vries Equation . . . . . . . . . .

Chapter 9. A General Framework for
Linear Partial Diﬀerential Equations
9.1. Adjoints . . . . . . . . . . . . . . . . . . .
Diﬀerential Operators . . . . . . . . . . .
Higher–Dimensional Operators . . . . . . .
The Fredholm Alternative . . . . . . . . .
9.2. Self–Adjoint and Positive Deﬁnite Linear Functions
Self–Adjointness . . . . . . . . . . . . .
Positive Deﬁniteness . . . . . . . . . . . .
Two–Dimensional Boundary Value Problems .
9.3. Minimization Principles . . . . . . . . . . . . .
Sturm–Liouville Boundary Value Problems . .
The Dirichlet Principle . . . . . . . . . .
9.4. Eigenvalues and Eigenfunctions . . . . . . . . .
Self–Adjoint Operators . . . . . . . . . .
The Rayleigh Quotient . . . . . . . . . .
Eigenfunction Series . . . . . . . . . . . .
Green’s Functions and Completeness . . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

291
.
.
.
.
.
.
.
.
.
.
.
.
.

. . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

263
272
275
275
276
278
278
281
284
286

292
296
299
305
308
312
315
315
317
323
324
330
333

339
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

340
342
345
350
353
354
355
359
362
363
368
371
371
375
378
379

xxiv

Table of Contents

9.5. A General Framework for Dynamics
Evolution Equations . . . . .
Vibration Equations . . . . .
Forcing and Resonance . . .
The Schrödinger Equation . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

Chapter 10. Finite Elements and Weak Solutions

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

. . . . . . . .

10.1. Minimization and Finite Elements . . . . . . . . . .
10.2. Finite Elements for Ordinary Diﬀerential Equations . . .
10.3. Finite Elements in Two Dimensions . . . . . . . . . .
Triangulation . . . . . . . . . . . . . . . . . .
The Finite Element Equations . . . . . . . . . .
Assembling the Elements . . . . . . . . . . . .
The Coeﬃcient Vector and the Boundary Conditions
Inhomogeneous Boundary Conditions . . . . . . .
10.4. Weak Solutions . . . . . . . . . . . . . . . . . . .
Weak Formulations of Linear Systems . . . . . . .
Finite Elements Based on Weak Solutions . . . . .
Shock Waves as Weak Solutions . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

399
.
.
.
.
.
.
.
.
.
.
.
.

Chapter 11. Dynamics of Planar Media . . . . . . . . . . . . .
11.1. Diﬀusion in Planar Media . . . . . . . . . . . . . .
Derivation of the Diﬀusion and Heat Equations . . .
Separation of Variables . . . . . . . . . . . . .
Qualitative Properties . . . . . . . . . . . . . .
Inhomogeneous Boundary Conditions and Forcing . .
The Maximum Principle . . . . . . . . . . . . .
11.2. Explicit Solutions of the Heat Equation . . . . . . . .
Heating of a Rectangle . . . . . . . . . . . . .
Heating of a Disk — Preliminaries . . . . . . . .
11.3. Series Solutions of Ordinary Diﬀerential Equations . . .
The Gamma Function . . . . . . . . . . . . . .
Regular Points . . . . . . . . . . . . . . . . .
The Airy Equation . . . . . . . . . . . . . . .
Regular Singular Points . . . . . . . . . . . . .
Bessel’s Equation . . . . . . . . . . . . . . . .
11.4. The Heat Equation in a Disk, Continued . . . . . . . .
11.5. The Fundamental Solution to the Planar Heat Equation .
11.6. The Planar Wave Equation . . . . . . . . . . . . .
Separation of Variables . . . . . . . . . . . . .
Vibration of a Rectangular Drum . . . . . . . . .
Vibration of a Circular Drum . . . . . . . . . . .
Scaling and Symmetry . . . . . . . . . . . . . .
Chladni Figures and Nodal Curves . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

385
386
388
389
394

400
403
410
411
416
418
422
424
427
428
430
431

435
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

435
436
439
440
442
443
445
446
450
453
453
456
459
463
467
474
481
486
487
488
490
494
497

Table of Contents

xxv

Chapter 12. Partial Diﬀerential Equations in Space

. . . . . . .

12.1. The Three–Dimensional Laplace and Poisson Equations . . .
Self–Adjoint Formulation and Minimum Principle . . . .
12.2. Separation of Variables for the Laplace Equation . . . . . .
Laplace’s Equation in a Ball . . . . . . . . . . . . .
The Legendre Equation and Ferrers Functions . . . . .
Spherical Harmonics . . . . . . . . . . . . . . . . .
Harmonic Polynomials . . . . . . . . . . . . . . . .
Averaging, the Maximum Principle, and Analyticity . . .
12.3. Green’s Functions for the Poisson Equation . . . . . . . .
The Free–Space Green’s Function . . . . . . . . . . .
Bounded Domains and the Method of Images . . . . . .
12.4. The Heat Equation for Three–Dimensional Media . . . . . .
Heating of a Ball . . . . . . . . . . . . . . . . . .
Spherical Bessel Functions . . . . . . . . . . . . . .
The Fundamental Solution to the Heat Equation in Space
12.5. The Wave Equation for Three–Dimensional Media . . . . .
Vibration of Balls and Spheres . . . . . . . . . . . .
12.6. Spherical Waves and Huygens’ Principle . . . . . . . . . .
Spherical Waves . . . . . . . . . . . . . . . . . .
Kirchhoﬀ’s Formula and Huygens’ Principle . . . . . .
Descent to Two Dimensions . . . . . . . . . . . . .
12.7. The Hydrogen Atom . . . . . . . . . . . . . . . . . .
Bound States . . . . . . . . . . . . . . . . . . . .
Atomic Eigenstates and Quantum Numbers . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

503
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

504
505
506
507
510
517
519
521
527
528
531
535
537
538
543
545
547
551
551
558
561
564
565
567

Correction to: Introduction to Partial Differential Equations . . . . . .

C1

Appendix A. Complex Numbers . . . . . . . . . . . . . . . .

571

Appendix B. Linear Algebra . . . . . . . . . . . . . . . . . .

575

B.1. Vector Spaces and Subspaces .
B.2. Bases and Dimension . . . .
B.3. Inner Products and Norms . .
B.4. Orthogonality . . . . . . . .
B.5. Eigenvalues and Eigenvectors .
B.6. Linear Iteration . . . . . . .
B.7. Linear Functions and Systems .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

575
576
578
581
582
583
585

References . . . . . . . . . . . . . . . . . . . . . . . . . .

589

Symbol Index

. . . . . . . . . . . . . . . . . . . . . . . .

595

Author Index

. . . . . . . . . . . . . . . . . . . . . . . .

603

Subject Index

. . . . . . . . . . . . . . . . . . . . . . . .

607

Chapter 1

What Are Partial Diﬀerential Equations?

Let us begin by delineating our ﬁeld of study. A diﬀerential equation is an equation that
relates the derivatives of a (scalar) function depending on one or more variables. For
example,
d4 u d2 u
+ 2 + u2 = cos x
(1.1)
dx4
dx
is a diﬀerential equation for the function u(x) depending on a single variable x, while
∂u
∂2u ∂2u
+ 2 −u
=
∂t
∂x2
∂y

(1.2)

is a diﬀerential equation involving a function u(t, x, y) of three variables.
A diﬀerential equation is called ordinary if the function u depends on only a single
variable, and partial if it depends on more than one variable. Usually (but not quite always)
the dependence of u can be inferred from the derivatives that appear in the diﬀerential
equation. The order of a diﬀerential equation is that of the highest-order derivative that
appears in the equation. Thus, (1.1) is a fourth-order ordinary diﬀerential equation, while
(1.2) is a second-order partial diﬀerential equation.
Remark : A diﬀerential equation has order 0 if it contains no derivatives of the function
u. These are more properly treated as algebraic equations,† which, while of great interest
in their own right, are not the subject of this text. To be a bona ﬁde diﬀerential equation,
it must contain at least one derivative of u, and hence have order ≥ 1.
There are two common notations for partial derivatives, and we shall employ them
interchangeably. The ﬁrst, used in (1.1) and (1.2), is the familiar Leibniz notation that
employs a d to denote ordinary derivatives of functions of a single variable, and the ∂
symbol (usually also pronounced “dee”) for partial derivatives of functions of more than
one variable. An alternative, more compact notation employs subscripts to indicate partial derivatives. For example, ut represents ∂u/∂t, while uxx is used for ∂ 2 u/∂x2 , and
∂ 3 u/∂x2 ∂y for uxxy . Thus, in subscript notation, the partial diﬀerential equation (1.2) is
written
ut = uxx + uyy − u.
(1.3)
†
Here, the term “algebraic equation” is used only to distinguish such equations from true
“diﬀerential equations”. It does not mean that the deﬁning functions are necessarily algebraic,
e.g., polynomials. For example, the transcendental equation tan u = u, which appears later in
(4.50), is still regarded as an algebraic equation in this book.

P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_1, © Springer International Publishing Switzerland 2014

1

2

1 What Are Partial Diﬀerential Equations?

We will similarly abbreviate partial diﬀerential operators, sometimes writing ∂/∂x as ∂x ,
while ∂ 2 /∂x2 can be written as either ∂x2 or ∂xx , and ∂ 3 /∂x2 ∂y becomes ∂xxy = ∂x2 ∂y .
It is worth pointing out that the preponderance of diﬀerential equations arising in
applications, in science, in engineering, and within mathematics itself are of either ﬁrst
or second order, with the latter being by far the most prevalent. Third-order equations
arise when modeling waves in dispersive media, e.g., water waves or plasma waves. Fourthorder equations show up in elasticity, particularly plate and beam mechanics, and in image
processing. Equations of order ≥ 5 are very rare.
A basic prerequisite for studying this text is the ability to solve simple ordinary diﬀerential equations: ﬁrst-order equations; linear constant-coeﬃcient equations, both homogeneous and inhomogeneous; and linear systems. In addition, we shall assume some familiarity with the basic theorems concerning the existence and uniqueness of solutions to initial
value problems. There are many good introductory texts, including [18, 20, 23]. More
advanced treatises include [31, 52, 54, 59]. Partial diﬀerential equations are considerably
more demanding, and can challenge the analytical skills of even the most accomplished
mathematician. Many of the most eﬀective solution strategies rely on reducing the partial
diﬀerential equation to one or more ordinary diﬀerential equations. Thus, in the course of
our study of partial diﬀerential equations, we will need to develop, ab initio, some of the
more advanced aspects of the theory of ordinary diﬀerential equations, including boundary
value problems, eigenvalue problems, series solutions, singular points, and special functions.
Following the introductory remarks in the present chapter, the exposition begins in
earnest with simple ﬁrst-order equations, concentrating on those that arise as models of
wave phenomena. Most of the remainder of the text will be devoted to understanding and
solving the three essential linear second-order partial diﬀerential equations in one, two,
and three space dimensions:† the heat equation, modeling thermodynamics in a continuous
medium, as well as diﬀusion of animal populations and chemical pollutants; the wave
equation, modeling vibrations of bars, strings, plates, and solid bodies, as well as acoustic,
ﬂuid, and electromagnetic vibrations; and the Laplace equation and its inhomogeneous
counterpart, the Poisson equation, governing the mechanical and thermal equilibria of
bodies, as well as ﬂuid-mechanical and electromagnetic potentials.
Each increase in dimension requires an increase in mathematical sophistication, as
well as the development of additional analytic tools — although the key ideas will have
all appeared once we reach our physical, three-dimensional universe. The three starring
examples — heat, wave, and Laplace/Poisson — are not only essential to a wide range
of applications, but also serve as instructive paradigms for the three principal classes of
linear partial diﬀerential equations — parabolic, hyperbolic, and elliptic. Some interesting
nonlinear partial diﬀerential equations, including ﬁrst-order transport equations modeling
shock waves, the second-order Burgers’ equation governing simple nonlinear diﬀusion processes, and the third-order Korteweg–de Vries equation governing dispersive waves, will
also be discussed. But, in such an introductory text, the further reaches of the vast realm
of nonlinear partial diﬀerential equations must remain unexplored, awaiting the reader’s
more advanced mathematical excursions.
More generally, a system of diﬀerential equations is a collection of one or more equations relating the derivatives of one or more functions. It is essential that all the functions
†
For us, dimension always refers to the number of space dimensions. Time, although theoretically also a dimension, plays a very diﬀerent physical role, and therefore (at least in nonrelativistic
systems) is to be treated on a separate footing.

1 What Are Partial Diﬀerential Equations?

3

occurring in the system depend on the same set of variables. The symbols representing
these functions are known as the dependent variables, while the variables that they depend
on are called the independent variables. Systems of diﬀerential equations are called ordinary or partial according to whether there are one or more independent variables. The
order of the system is the highest-order derivative occurring in any of its equations.
For example, the three-dimensional Navier–Stokes equations
 2

∂u
∂u
∂u
∂u
∂p
∂ u ∂2u ∂2u
+u
+v
+w
=−
+ν
+ 2 + 2 ,
∂t
∂x
∂y
∂z
∂x
∂x2
∂y
∂z

 2
∂2v
∂2v
∂v
∂v
∂v
∂p
∂ v
∂v
+ 2+ 2 ,
+u
+v
+w
=−
+ν
∂t
∂x
∂y
∂z
∂y
∂x2
∂y
∂z
(1.4)

 2
2
∂w
∂w
∂w
∂w
∂p
∂ w ∂ w ∂2w
+
+
,
+u
+v
+w
=−
+ν
∂t
∂x
∂y
∂z
∂z
∂x2
∂y 2
∂z 2
∂w
∂u ∂v
+
+
= 0,
∂x ∂y
∂z
is a second-order system of diﬀerential equations that involves four functions, u(t, x, y, z),
v(t, x, y, z), w(t, x, y, z), p(t, x, y, z), each depending on four variables, while ν ≥ 0 is a
ﬁxed constant. (The function p necessarily depends on t, even though no t derivative of
it appears in the system.) The independent variables are t, representing time, and x, y, z,
representing space coordinates. The dependent variables are u, v, w, p, with v = (u, v, w)
representing the velocity vector ﬁeld of an incompressible ﬂuid ﬂow, e.g., water, and p the
accompanying pressure. The parameter ν measures the viscosity of the ﬂuid. The Navier–
Stokes equations are fundamental in ﬂuid mechanics, [12], and are notoriously diﬃcult to
solve, either analytically or numerically. Indeed, establishing the existence or nonexistence
of solutions for all future times remains a major unsolved problem in mathematics, whose
resolution will earn you a $1,000,000 prize; see http://www.claymath.org for details. The
Navier–Stokes equations ﬁrst appeared in the early 1800s in works of the French applied
mathematician/engineer Claude-Louis Navier and, later, the British applied mathematician George Stokes, whom you already know from his eponymous multivariable calculus
theorem.† The inviscid case, ν = 0, is known as the Euler equations in honor of their discoverer, the incomparably inﬂuential eighteenth-century Swiss mathematician Leonhard
Euler.
We shall be employing a few basic notational conventions regarding the variables that
appear in our diﬀerential equations. We always use t to denote time, while x, y, z will represent (Cartesian) space coordinates. Polar coordinates r, θ, cylindrical coordinates r, θ, z,
and spherical coordinates‡ r, θ, ϕ, will also be used when needed. An equilibrium equation
models an unchanging physical system, and so involves only the space variable(s). The
time variable appears when modeling dynamical , meaning time-varying, processes. Both
time and space coordinates are (usually) independent variables. The dependent variables
will mostly be denoted by u, v, w, although occasionally — particularly in representing
†

Interestingly, Stokes’ Theorem was taken from an 1850 letter that Lord Kelvin wrote to
Stokes, who turned it into an undergraduate exam question for the Smith Prize at Cambridge
University in England. However, unbeknownst to either, the result had, in fact, been discovered
earlier by George Green, the father of Green’s Theorem and also the Green’s function, which will
be the subject of Chapter 6.
‡

See Section 12.2 for our notational convention.

4

1 What Are Partial Diﬀerential Equations?

particular physical quantities — other letters may be employed, e.g., the pressure p in
(1.4). On the other hand, the letters f, g, h typically represent speciﬁed functions of the
independent variables, e.g., forcing or boundary or initial conditions.
In this introductory text, we must conﬁne our attention to the most basic analytic
and numerical solution techniques for a select few of the most important partial diﬀerential
equations. More advanced topics, including all systems of partial diﬀerential equations,
must be deferred to graduate and research-level texts, e.g., [35, 38, 44, 61, 99]. In fact,
many important issues remain incompletely resolved and/or poorly understood, making
partial diﬀerential equations one of the most active and exciting ﬁelds of contemporary
mathematical research. One of my goals is that, by reading this book, you will be both
inspired and equipped to venture much further into this fascinating and essential area of
mathematics and/or its remarkable range of applications throughout science, engineering,
economics, biology, and beyond.

Exercises
1.1. Classify each of the following diﬀerential equations as ordinary or partial, and equilibrium
du
∂u
∂u
or dynamic; then write down its order. (a)
+ x u = 1, (b)
+u
= x,
dx
∂t
∂x
2
2
2
∂u
∂u
∂ u
∂ u
∂ u
+
−
= x2 + y 2 ,
(c) utt = 9 uxx , (d)
=
, (e) −
∂t
∂x2
∂x
∂x2
∂y 2
d2 u
(f )
+ 3 u = sin t, (g) uxx + uyy + uzz + (x2 + y 2 + z 2 )u = 0, (h) uxx = x + u2 ,
dt2
∂u
∂2u
∂u
∂3u
∂2u
+u
+
(i)
+
= 0, (j)
= u, (k) utt = uxxxx + 2 uxxyy + uyyyy .
3
2
∂t
∂x
∂x
∂x
∂y ∂z
1.2. In two space dimensions, the Laplacian is deﬁned as the second-order partial diﬀerential
operator Δ = ∂x2 + ∂y2 . Write out the following partial diﬀerential equations in (i ) Leibniz
notation; (ii ) subscript notation: (a) the Laplace equation Δu = 0; (b) the Poisson equation − Δu = f ; (c) the two-dimensional heat equation ∂t u = Δu; (d) the von Karman
plate equation Δ2 u = 0.
1.3. Answer Exercise 1.2 for the three-dimensional Laplacian Δ = ∂x2 + ∂y2 + ∂z2 .
1.4. Identify the independent variables, the dependent variables, and the order of the following
∂v
∂u
∂v
∂u
=
,
=−
;
systems of partial diﬀerential equations: (a)
∂x
∂y
∂y
∂x
2
∂u
∂2u
∂v
∂ v
=
;
(b) uxx + vyy = cos(x + y), ux vy − uy vx = 1; (c)
=
,
2
∂t
∂x
∂t
∂x2
(d) ut + u ux + v uy = px , vt + u vx + v vy = py , ux + vy = 0;
(e) ut = vxxx + v(1 − v), vt = uxxy + v w, wt = ux + vy .

Classical Solutions
Let us now focus our attention on a single diﬀerential equation involving a single, scalarvalued function u that depends on one or more independent variables. The function u

1 What Are Partial Diﬀerential Equations?

5

is usually real-valued, although complex-valued functions can, and do, play a role in the
analysis. Everything that we say in this section will, when suitably adapted, apply to
systems of diﬀerential equations.
By a solution we mean a suﬃciently smooth function u of the independent variables
that satisﬁes the diﬀerential equation at every point of its domain of deﬁnition. We do not
necessarily require that the solution be deﬁned for all possible values of the independent
variables. Indeed, usually the diﬀerential equation is imposed on some domain D contained
in the space of independent variables, and we seek a solution deﬁned only on D. In
general, the domain D will be an open subset, usually connected, and hence pathwise
connected , meaning any two points can be connected by a curve C ⊂ D, and, particularly
in equilibrium equations, often bounded, with a reasonably nice boundary, denoted by ∂D.
We will call a function smooth if it can be diﬀerentiated suﬃciently often, at least
so that all of the derivatives appearing in the equation are well deﬁned on the domain
of interest D. More speciﬁcally, if the diﬀerential equation has order n, then we require
that the solution u be of class Cn , which means that it and all its derivatives of order
≤ n are continuous functions in D, and such that the diﬀerential equation that relates the
derivatives of u holds throughout D. However, on occasion, e.g., when dealing with shock
waves, we will consider more general types of solutions. The most important such class
consists of the so-called “weak solutions” to be introduced in Section 10.4. To emphasize
the distinction, the smooth solutions described above are often referred to as classical
solutions. In this book, the term “solution” without extra qualiﬁcation will usually mean
“classical solution”.
Example 1.1. A classical solution to the heat equation
∂u
∂2u
=
(1.5)
∂t
∂x2
is a function u(t, x), deﬁned on a domain D ⊂ R 2 , such that all of the functions
∂u
∂u
∂2u
∂2u
∂2u
∂2u
u(t, x),
(t, x),
(t, x),
(t, x) =
(t, x),
(t, x),
(t, x),
2
∂t
∂x
∂t
∂t ∂x
∂x ∂t
∂x2
are well deﬁned and continuous† at every point (t, x) ∈ D, so that u ∈ C2 (D), and,
moreover, (1.5) holds at every (t, x) ∈ D. Observe that, even though only ut and uxx
explicitly appear in the heat equation, we require continuity of all the partial derivatives
of order ≤ 2 in order that u qualify as a classical solution. For example,
u(t, x) = t + 21 x2

(1.6)
2

is a solution to the heat equation that is deﬁned on the full domain D = R because it is‡
C2 , and, moreover,
∂u
∂2u
=1=
.
∂t
∂x2
Another, more complicated but extremely important, solution is
2

e− x /(4 t)
√
.
u(t, x) =
2 πt
†

(1.7)

The equality of the mixed partial derivatives follows from a general theorem in multivariable
calculus, [ 8, 97, 108 ]. Classical solutions automatically enjoy equality of all their relevant mixed
partial derivatives.
‡

In fact, the function (1.6) is C∞ , meaning inﬁnitely diﬀerentiable, on all of R 2 .

6

1 What Are Partial Diﬀerential Equations?

One easily veriﬁes that u ∈ C2 and, moreover, solves the heat equation on the domain
D = { (t, x) | t > 0 } ⊂ R 2 . The reader is invited to verify this by computing ∂u/∂t and
√
∂ 2 u/∂x2 , and then checking that they are equal. Finally, with i = −1 denoting the
imaginary unit, we note that
u(t, x) = e− t+ i x = e− t cos x + i e− t sin x,

(1.8)

the second expression following from Euler’s formula (A.11), deﬁnes a complex-valued
solution to the heat equation. This can be veriﬁed directly, since the rules for diﬀerentiating
complex exponentials are identical to those for their real counterparts:
∂u
= − e− t+ i x ,
∂t

∂u
= i e− t+ i x ,
∂x

and so

∂2u
∂u
.
= − e− t+ i x =
2
∂x
∂t

It is worth pointing out that both the real part, e− t cos x, and the imaginary part, e− t sin x,
of the complex solution (1.8) are individual real solutions, which is indicative of a fairly
general property.
Incidentally, most partial diﬀerential equations arising in physical applications are real,
and, although complex solutions often facilitate their analysis, at the end of the day we
require real, physically meaningful solutions. A notable exception is quantum mechanics,
which is an inherently complex-valued physical theory. For example, the one-dimensional
Schrödinger equation
∂u
2 ∂ 2 u
i
=−
+ V (x) u,
(1.9)
∂t
2 m ∂x2
with  denoting Planck’s constant, which is real, governs the dynamical evolution of the
complex-valued wave function u(t, x) describing the probabilistic distribution of a quantum
particle of mass m, e.g., an electron, moving in the force ﬁeld prescribed by the (real)
potential function V (x). While the solution u is complex-valued, the independent variables
t, x, representing time and space, remain real.

Initial Conditions and Boundary Conditions
How many solutions does a partial diﬀerential equation have? In general, lots. Even
ordinary diﬀerential equations have inﬁnitely many solutions. Indeed, the general solution
to a single nth order ordinary diﬀerential equation depends on n arbitrary constants. The
solutions to partial diﬀerential equations are yet more numerous, in that they depend
on arbitrary functions. Very roughly, we can expect the solution to an nth order partial
diﬀerential equation involving m independent variables to depend on n arbitrary functions
of m − 1 variables. But this must be taken with a large grain of salt — only in a few special
instances will we actually be able to express the solution in terms of arbitrary functions.
The solutions to dynamical ordinary diﬀerential equations are singled out by the imposition of initial conditions, resulting in an initial value problem. On the other hand,
equations modeling equilibrium phenomena require boundary conditions to specify their
solutions uniquely, resulting in a boundary value problem. We assume that the reader is
already familiar with the basics of initial value problems for ordinary diﬀerential equations.
But we will take time to develop the perhaps less familiar case of boundary value problems
for ordinary diﬀerential equations in Chapter 6.

1 What Are Partial Diﬀerential Equations?

7

A similar speciﬁcation of auxiliary conditions applies to partial diﬀerential equations.
Equations modeling equilibrium phenomena are supplemented by boundary conditions imposed on the boundary of the domain of interest. In favorable circumstances, the boundary
conditions serve to single out a unique solution. For example, the equilibrium temperature
of a body is uniquely speciﬁed by its boundary behavior. If the domain is unbounded,
one must also restrict the nature of the solution at large distances, e.g., by asking that it
remain bounded. The combination of a partial diﬀerential equation along with suitable
boundary conditions is referred to as a boundary value problem.
There are three principal types of boundary value problems that arise in most applications. Specifying the value of the solution along the boundary of the domain is called a
Dirichlet boundary condition, to honor the nineteenth-century analyst Johann Peter Gustav Lejeune Dirichlet. Specifying the normal derivative of the solution along the boundary
results in a Neumann boundary condition, named after his contemporary Carl Gottfried
Neumann. Prescribing the function along part of the boundary and the normal derivative
along the remainder results in a mixed boundary value problem. For example, in thermal
equilibrium, the Dirichlet boundary value problem speciﬁes the temperature of a body
along its boundary, and our task is to ﬁnd the interior temperature distribution by solving an appropriate partial diﬀerential equation. Similarly, the Neumann boundary value
problem prescribes the heat ﬂux through the boundary. In particular, an insulated boundary has no heat ﬂux, and hence the normal derivative of the temperature is zero on the
boundary. The mixed boundary value problem prescribes the temperature along part of
the boundary and the heat ﬂux along the remainder. Again, our task is to determine the
interior temperature of the body.
For partial diﬀerential equations modeling dynamical processes, in which time is one of
the independent variables, the solution is to be speciﬁed by one or more initial conditions.
The number of initial conditions required depends on the highest-order time derivative
that appears in the equation. For example, in thermodynamics, which involves only the
ﬁrst-order time derivative of the temperature, the initial condition requires specifying the
temperature of the body at the initial time. Newtonian mechanics describes the acceleration or second-order time derivative of the motion, and so requires two initial conditions:
the initial position and initial velocity of the system. On bounded domains, one must also
impose suitable boundary conditions in order to uniquely characterize the solution and
hence the subsequent dynamical behavior of the physical system. The combination of the
partial diﬀerential equation, the initial conditions, and the boundary conditions leads to an
initial-boundary value problem. We will encounter, and solve, many important examples
of such problems during the course of this text.
Remark : An additional consideration is that, besides any smoothness required by the
partial diﬀerential equation within the domain, the solution and any of its derivatives
speciﬁed in any initial or boundary condition should also be continuous at the initial
or boundary point where the condition is imposed. For example, if the initial condition
speciﬁes the function value u(0, x) for a < x < b, while the boundary conditions specify the
∂u
∂u
derivatives
(t, a) and
(t, b) for t > 0, then, in addition to any smoothness required
∂x
∂x
inside the domain { a < x < b, t > 0 }, we also require that u be continuous at all initial
∂u
points (0, x), and that its derivative
be continuous at all boundary points (t, a) and
∂x
(t, b), in order that u(t, x) qualify as a classical solution to the initial-boundary value
problem.

8

1 What Are Partial Diﬀerential Equations?

Exercises
1.5. Show that the following functions u(x, y) deﬁne classical solutions to the two-dimensional
∂2u
∂2u
+
= 0. Be careful to specify an appropriate domain.
Laplace equation
2
∂x
∂y 2
x
x
2
2
.
(a) e cos y, (b) 1+x −y , (c) x3 −3 x y 2 , (d) log(x2 +y 2 ), (e) tan−1 (y/x), (f ) 2
x + y2
1.6. Find all solutions u = f (r) of the two-dimensional Laplace equation uxx + uyy = 0 that


depend only on the radial coordinate r =

x2 + y 2 .

1.7. Find all (real) solutions to the two-dimensional Laplace equation uxx + uyy = 0 of the form
u = log p(x, y), where p(x, y) is a quadratic polynomial.
1.8. (a) Find all quadratic polynomial solutions of the three-dimensional Laplace equation
∂2u
∂2u
∂2u
+
+
= 0. (b) Find all the homogeneous cubic polynomial solutions.
∂x2
∂y 2
∂z 2
1.9. Find all polynomial solutions p(t, x) of the heat equation ut = uxx with deg p ≤ 3.
1.10. Show that each of the following functions u(t, x) is a solution to the wave equation
2
utt = 4 uxx : (a) 4 t2 + x2 ; (b) cos(x + 2 t); (c) sin 2 t cos x; (d) e−(x−2 t) .
1.11. Find all polynomial solutions p(t, x) of the wave equation utt = uxx with
(a) deg p ≤ 2, (b) deg p = 3.
1.12. Suppose u(t, x) and v(t, x) are C2 functions deﬁned on R 2 that satisfy the ﬁrst-order system of partial diﬀerential equations ut = vx , vt = ux .
(a) Show that both u and v are classical solutions to the wave equation utt = uxx . Which
result from multivariable calculus do you need to justify the conclusion?
(b) Conversely, given a classical solution u(t, x) to the wave equation, can you construct a
function v(t, x) such that u(t, x), v(t, x) form a solution to the ﬁrst-order system?
1.13. Find all solutions u = f (r) of the three-dimensional Laplace equation 
uxx + uyy + uzz = 0 that depend only on the radial coordinate r = x2 + y 2 + z 2 .
1.14. Let u(x, y) be deﬁned on a domain D ⊂ R 2 . Suppose you know that all its second-order
partial derivatives, uxx , uxy , uyx , uyy , are deﬁned and continuous on all of D. Can you conclude that u ∈ C2 (D)?
1.15. Write down a partial diﬀerential equation that has
(a) no real solutions; (b) exactly one real solution; (c) exactly two real solutions.
x2 − y 2
for (x, y) = (0, 0), while u(0, 0) = 0. Prove that
x2 + y 2
∂2u
∂2u
(0, 0) = 1 = −1 =
(0, 0).
∂x ∂y
∂y ∂x
Explain why this example does not contradict the theorem on the equality of mixed partials.

1.16. Let u(x, y) = x y

Linear and Nonlinear Equations
As with algebraic equations and ordinary diﬀerential equations, there is a crucial distinction

1 What Are Partial Diﬀerential Equations?

9

between linear and nonlinear partial diﬀerential equations, and one must have a ﬁrm grasp
of the linear theory before venturing into the nonlinear wilderness. While linear algebraic
equations are (modulo numerical diﬃculties) eminently solvable by a variety of techniques,
linear ordinary diﬀerential equations, of order ≥ 2, already present a challenge, as most
cannot be solved in terms of elementary functions. Indeed, as we will learn in Chapter 11,
solving many of those equations that arise in applications requires introducing new types
of “special functions” that are typically not encountered in a basic calculus course. Linear
partial diﬀerential equations are of a yet higher level of diﬃculty, and only a small handful
of speciﬁc equations can be completely solved. Moreover, explicit solutions tend to be
expressible only in the form of inﬁnite series, requiring subtle analytic tools to understand
their convergence and properties. For the vast majority of partial diﬀerential equations, the
only feasible means of producing general solutions is through numerical approximation. In
this book, we will study the two most basic numerical schemes: ﬁnite diﬀerences and ﬁnite
elements. Keep in mind that, in order to develop and understand numerics for partial
diﬀerential equations, one must already have a good understanding of their analytical
properties.
The distinguishing feature of linearity is that it enables one to straightforwardly combine solutions to form new solutions, through a general Superposition Principle. Linear
superposition is universally applicable to all linear equations and systems, including linear
algebraic systems, linear ordinary diﬀerential equations, linear partial diﬀerential equations, linear initial and boundary value problems, as well as linear integral equations,
linear control systems, and so on. Let us introduce the basic idea in the context of a single
diﬀerential equation.
A diﬀerential equation is called homogeneous linear if both sides are sums of terms,
each of which involves the dependent variable u or one of its derivatives to the ﬁrst power;
on the other hand, there is no restriction on how the terms involve the independent variables. Thus,
d2 u
u
+
=0
2
dx
1 + x2
is a homogeneous linear second-order ordinary diﬀerential equation. Examples of homogeneous linear partial diﬀerential equations include the heat equation (1.5), the partial
diﬀerential equation (1.2), and the equation
∂2u
∂u
= ex 2 + cos(x − t) u.
∂t
∂x
On the other hand, Burgers’ equation
∂u
∂u
∂2u
+u
=
∂t
∂x
∂x2

(1.10)

is not linear, since the second term involves the product of u and its derivative ux . A
similar terminology is applied to systems of partial diﬀerential equations. For example, the
Navier–Stokes system (1.4) is not linear because of the terms u ux , v uy , etc. — although
its ﬁnal constituent equation is linear.
A more precise deﬁnition of a homogeneous linear diﬀerential equation begins with the
concept of a linear diﬀerential operator L. Such operators are assembled by summing the
basic partial derivative operators, with either constant coeﬃcients or, more generally, coefﬁcients depending on the independent variables. The operator acts on suﬃciently smooth

10

1 What Are Partial Diﬀerential Equations?

functions depending on the relevant independent variables. According to Deﬁnition B.32,
linearity imposes two key requirements:
L[ u + v ] = L[ u ] + L[ v ],

L[ c u ] = c L[ u ],

(1.11)

for any two (suﬃciently smooth) functions u, v, and any constant c.
Deﬁnition 1.2. A homogeneous linear diﬀerential equation has the form
L[ u ] = 0,

(1.12)

where L is a linear diﬀerential operator.
As a simple example, consider the second-order diﬀerential operator
L=

∂2
,
∂x2

whereby

L[ u ] =

∂2u
∂x2

for any C2 function u(x, y). The linearity requirements (1.11) follow immediately from
basic properties of diﬀerentiation:
∂2u ∂2v
∂2
(u
+
v)
=
+ 2 = L[ u ] + L[ v ],
∂x2
∂x2
∂x
2
2
∂ u
∂
(c u) = c
= c L[ u ],
L[ c u ] =
∂x2
∂x2

L[ u + v ] =

which are valid for any C2 functions u, v and any constant c. The corresponding homogeneous linear diﬀerential equation L[ u ] = 0 is
∂2u
= 0.
∂x2
The heat equation (1.5) is based on the linear partial diﬀerential operator
L = ∂t − ∂x2 ,

with

L[ u ] = ∂t u − ∂x2 u = ut − uxx = 0.

(1.13)

Linearity follows as above:
L[ u + v ] = ∂t (u + v) − ∂x2 (u + v) = (∂t u − ∂x2 u) + (∂t v − ∂x2 v) = L[ u ] + L[ v ],
L[ c u ] = ∂t (c u) − ∂x2 (c u) = c (∂t u − ∂x2 u) = c L[ u ].
Similarly, the linear diﬀerential operator
L = ∂t2 − ∂x κ(x) ∂x = ∂t2 − κ(x) ∂x2 − κ (x) ∂x ,
where κ(x) is a prescribed C1 function of x alone, deﬁnes the homogeneous linear partial
diﬀerential equation
L[ u ] = ∂t2 u − ∂x (κ(x) ∂x u) = utt − ∂x (κ(x) ux ) = utt − κ(x) uxx − κ (x) ux = 0,
which is used to model vibrations in a nonuniform one-dimensional medium.
The deﬁning attributes of linear operators (1.11) imply the key properties shared by
all homogeneous linear (diﬀerential) equations.
Proposition 1.3. The sum of two solutions to a homogeneous linear diﬀerential
equation is again a solution, as is the product of a solution with any constant.

1 What Are Partial Diﬀerential Equations?

11

Proof : Let u1 , u2 be solutions, meaning that L[ u1 ] = 0 and L[ u2 ] = 0. Then, thanks
to linearity,
L[ u1 + u2 ] = L[ u1 ] + L[ u2 ] = 0,
and hence their sum u1 + u2 is a solution. Similarly, if c is any constant and u any solution,
then
L[ c u ] = c L[ u ] = c 0 = 0,
and so the constant multiple c u is also a solution.

Q.E.D.

As a result, starting with a handful of solutions to a homogeneous linear diﬀerential
equation, by repeating these operations of adding solutions and multiplying by constants,
we are able to build up large families of solutions. In the case of the heat equation (1.5),
we are already in possession of two solutions, namely (1.6) and (1.7). Multiplying each by
a constant produces two inﬁnite families of solutions:
2

u(t, x) = c1 (t + 12 x2 )

and

c e− x /(4 t)
,
u(t, x) = 2 √
2 πt

where c1 , c2 are arbitrary constants. Moreover, one can add the latter solutions together,
producing a two-parameter family of solutions
2
c2 e− x /(4 t)
1 2
√
,
u(t, x) = c1 (t + 2 x ) +

2 πt

valid for any choice of the constants c1 , c2 .
The preceding construction is a special case of the general Superposition Principle for
homogeneous linear equations:
Theorem 1.4. If u1 , . . . , uk are solutions to a common homogeneous linear equation
L[ u ] = 0, then the linear combination, or superposition, u = c1 u1 + · · · + ck uk is a solution
for any choice of constants c1 , . . . , ck .
Proof : Repeatedly applying the linearity requirements (1.11), we ﬁnd
L[ u ] = L[ c1 u1 + · · · + ck uk ] = L[ c1 u1 + · · · + ck−1 uk−1 ] + L[ ck uk ]
= · · · = L[ c1 u1 ] + · · · + L[ ck uk ] = c1 L[ u1 ] + · · · + ck L[ uk ].

(1.14)

In particular, if the functions are solutions, so L[ u1 ] = 0, . . . , L[ uk ] = 0, then the righthand side of (1.14) vanishes, proving that u also solves the equation L[ u ] = 0.
Q.E.D.
In the linear algebraic language of Appendix B, Theorem 1.4 tells us that the solutions to a homogeneous linear partial diﬀerential equation form a vector space. The same
holds true for linear algebraic equations, [89], and linear ordinary diﬀerential equations,
[18, 20, 23, 52]. In the latter two situations, once one ﬁnds a suﬃcient number of independent solutions, the general solution is obtained as a linear combination thereof. In
the language of linear algebra, the solution space is ﬁnite-dimensional. In contrast, most
linear systems of partial diﬀerential equations admit an inﬁnite number of independent
solutions, meaning that the solution space is inﬁnite-dimensional, and, as a consequence,
one cannot hope to build the general solution by taking ﬁnite linear combinations. Instead,
one requires the far more delicate operation of forming inﬁnite series involving the basic
solutions. Such considerations will soon lead us into the heart of Fourier analysis, and
require spending an entire chapter developing the required analytic tools.

12

1 What Are Partial Diﬀerential Equations?

Deﬁnition 1.5. An inhomogeneous linear diﬀerential equation has the form
L[ v ] = f,

(1.15)

where L is a linear diﬀerential operator, v is the unknown function, and f is a prescribed
nonzero function of the independent variables alone.
For example, the inhomogeneous form of the heat equation (1.13) is
L[ v ] = ∂t v − ∂x2 v = vt − vxx = f (t, x),

(1.16)

where f (t, x) is a speciﬁed function. This equation models the thermodynamics of a onedimensional medium subject to an external heat source.
You already learned the basic technique for solving inhomogeneous linear equations
in your study of elementary ordinary diﬀerential equations. Step one is to determine the
general solution to the homogeneous equation. Step two is to ﬁnd a particular solution to
the inhomogeneous version. The general solution to the inhomogeneous equation is then
obtained by adding the two together. Here is the general version of this procedure:
Theorem 1.6. Let v be a particular solution to the inhomogeneous linear equation
L[ v ] = f . Then the general solution to L[ v ] = f is given by v = v + u, where u is the
general solution to the corresponding homogeneous equation L[ u ] = 0.
Proof : Let us ﬁrst show that v = v + u is also a solution whenever L[ u ] = 0. By
linearity,
L[ v ] = L[ v + u ] = L[ v ] + L[ u ] = f + 0 = f.
To show that every solution to the inhomogeneous equation can be expressed in this manner, suppose v satisﬁes L[ v ] = f . Set u = v − v . Then, by linearity,
L[ u ] = L[ v − v ] = L[ v ] − L[ v ] = 0,
and hence u is a solution to the homogeneous diﬀerential equation. Thus, v = v + u has
the required form.
Q.E.D.
In physical applications, one can interpret the particular solution v as a response of
the system to the external forcing function. The solution u to the homogeneous equation
represents the system’s internal, unforced behavior. The general solution to the inhomogeneous linear equation is thus a combination, v = v + u, of the external and internal
responses.
Finally, the Superposition Principle for inhomogeneous linear equations allows one to
combine the responses of the system to diﬀerent external forcing functions. The proof of
this result is left to the reader as Exercise 1.26.
Theorem 1.7. Let v1 , . . . , vk be solutions to the inhomogeneous linear systems
L[ v1 ] = f1 , . . . , L[ vk ] = fk , involving the same linear operator L. Then, given any
constants c1 , . . . , ck , the linear combination v = c1 v1 + · · · + ck vk solves the inhomogeneous
system L[ v ] = f for the combined forcing function f = c1 f1 + · · · + ck fk .
The two general Superposition Principles furnish us with powerful tools for solving
linear partial diﬀerential equations, which we shall repeatedly exploit throughout this text.
In contrast, nonlinear partial diﬀerential equations are much tougher, and, typically, knowledge of several solutions is of scant help in constructing others. Indeed, ﬁnding even one
solution to a nonlinear partial diﬀerential equation can be quite a challenge. While this text

1 What Are Partial Diﬀerential Equations?

13

will primarily concentrate on analyzing the solutions and their properties to some of the
most basic and most important linear partial diﬀerential equations, we will have occasion
to brieﬂy venture into the nonlinear realm, introducing some striking recent developments
in this fascinating arena of contemporary research.

Exercises
1.17. Classify the following diﬀerential equations as either
(i ) homogeneous linear; (ii ) inhomogeneous linear; or (iii ) nonlinear:
(a) ut = x2 uxx + 2 x ux , (b) − uxx − uyy = sin u; (c) uxx + 2 y uyy = 3;
(d) ut + u ux = 3 u; (e) ey ux = ex uy ; (f ) ut = 5 uxxx + x2 u + x.
1.18. Write down all possible solutions to the Laplace equation you can construct from the various solutions provided in Exercise 1.5 using linear superposition.
1.19. (a) Show that the following functions are solutions to the wave equation utt = 4 uxx :
(i ) cos(x − 2 t),
(ii ) ex+2 t ;
(iii ) x2 + 2 x t + 4t2 .
(b) Write down at least four other solutions to the wave equation.
1.20. The displacement u(t, x) of a forced violin string is modeled by the partial diﬀerential
equation utt = 4 uxx +F (t, x). When the string is subjected to the external forcing F (t, x) =
cos x, the solution is u(t, x) = cos(x − 2 t) + 14 cos x, while when F (t, x) = sin x, the solution
is u(t, x) = sin(x − 2 t) + 14 sin x. Find a solution when the forcing function F (t, x) is
(a) cos x − 5 sin x,
(b) sin(x − 3).
∂f
∂f
and ∂y [ f ] =
both deﬁne linear
∂x
∂y
operators on the space of continuously diﬀerentiable functions f (x, y). (b) For which values
∂f
∂f
of a, b, c, d is the diﬀerential operator L[ f ] = a
+b
+ c f + d linear?
∂x
∂y

1.21. (a) Show that the partial derivatives ∂x [ f ] =

1.22. (a) Prove that the Laplacian Δ = ∂x2 + ∂y2 deﬁnes a linear diﬀerential operator.
(b) Write out the Laplace equation Δ[ u ] = 0 and the Poisson equation − Δ[ u ] = f .
1.23. Prove that, on R 3 , the gradient, curl, and divergence all deﬁne linear operators.
1.24. Let L and M be linear partial diﬀerential operators. Prove that the following are also
linear partial diﬀerential operators: (a) L − M , (b) 3 L, (c) f L, where f is an arbitrary
function of the independent variables; (d) L ◦ M .
1.25. Suppose L and M are linear diﬀerential operators and let N = L + M .
(a) Prove that N is a linear operator. (b) True or false: If u solves L[ u ] = f and v solves
M [ v ] = g, then w = u + v solves N[ w ] = f + g.
♦ 1.26. Prove Theorem 1.7.
1.27. Solve the following inhomogeneous linear ordinary diﬀerential equations:
(a) u − 4 u = x − 3, (b) 5 u − 4 u + 4 u = ex cos x, (c) u − 3 u = e3 x .
1.28. Use superposition to solve the following inhomogeneous ordinary diﬀerential equations:
(a) u + 2 u = 1 + cos x, (b) u − 9 u = x + sin x, (c) 9 u − 18 u + 10 u = 1 + ex cos x,
(d) u + u − 2 u = sinh x, where sinh x = 12 (ex − e− x ), (e) u + 9 u = 1 + e3 x .

Chapter 2

Linear and Nonlinear Waves

Our initial foray into the vast mathematical continent that comprises partial diﬀerential
equations will begin with some basic ﬁrst-order equations. In applications, ﬁrst-order
partial diﬀerential equations are most commonly used to describe dynamical processes,
and so time, t, is one of the independent variables. Our discussion will focus on dynamical
models in a single space dimension, bearing in mind that most of the methods we introduce
can be extended to higher-dimensional situations. First-order partial diﬀerential equations
and systems model a wide variety of wave phenomena, including transport of pollutants in
ﬂuids, ﬂood waves, acoustics, gas dynamics, glacier motion, chromatography, traﬃc ﬂow,
and various biological and ecological systems.
A basic solution technique relies on an inspired change of variables, which comes
from rewriting the equation in a moving coordinate frame. This naturally leads to the
fundamental concept of characteristic curve, along which signals and physical disturbances
propagate. The resulting method of characteristics is able to solve a ﬁrst-order linear
partial diﬀerential equation by reducing it to one or more ﬁrst-order nonlinear ordinary
diﬀerential equations.
Proceeding to the nonlinear regime, the most important new phenomenon is the possible breakdown of solutions in ﬁnite time, resulting in the formation of discontinuous
shock waves. A familiar example is the supersonic boom produced by an airplane that
breaks the sound barrier. Signals continue to propagate along characteristic curves, but
now the curves may cross each other, precipitating the onset of a shock discontinuity. The
ensuing shock dynamics is not uniquely speciﬁed by the partial diﬀerential equation, but
relies on additional physical properties, to be speciﬁed by an appropriate conservation law
along with a causality condition. A full-ﬂedged analysis of shock dynamics becomes quite
challenging, and only the basics will be developed here.
Having attained a basic understanding of ﬁrst-order wave dynamics, we then focus
our attention on the ﬁrst of three paradigmatic second-order partial diﬀerential equations,
known as the wave equation, which is used to model waves and vibrations in an elastic
bar, a violin string, or a column of air in a wind instrument. Its multi-dimensional versions
serve to model vibrations of membranes, solid bodies, water waves, electromagnetic waves,
including light, radio waves, microwaves, acoustic waves, and many other physical phenomena. The one-dimensional wave equation is one of a small handful of physically relevant
partial diﬀerential equations that has an explicit solution formula, originally discovered by
the eighteenth-century French mathematician (and encyclopedist) Jean d’Alembert. His
solution is the result of being able to “factorize” the second-order wave equation into a
pair of ﬁrst-order partial diﬀerential equations, of a type solved in the ﬁrst part of this
P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_2, © Springer International Publishing Switzerland 2014

15

16

2 Linear and Nonlinear Waves

u

u

u

x
t=0

x
t=1

Figure 2.1.

Stationary wave.

x


t=2

chapter. We investigate the consequences of d’Alembert’s solution formula for the initial
value problem on the entire real line; solutions on bounded intervals will be deferred until
Chapter 4. Unfortunately, d’Alembert’s method is of rather limited scope, and does not
extend beyond the one-dimensional case, nor to equations modeling vibrations of nonuniform media. The analysis of the wave equation in more than one space dimension can be
found in Chapters 11 and 12.

2.1 Stationary Waves
When entering a new mathematical subject — in our case, partial diﬀerential equations —
one should ﬁrst analyze and fully understand the very simplest examples. Indeed, mathematics is, at its core, a bootstrapping enterprise, in which one builds on one’s knowledge
of and experience with elementary topics — in the present case, ordinary diﬀerential equations — to make progress, ﬁrst with the simpler types of partial diﬀerential equations, and
then, by developing and applying each newly gained insight and technique, to more and
more complicated situations.
The simplest partial diﬀerential equation, for a function u(t, x) of two variables, is
∂u
= 0.
∂t

(2.1)

It is a ﬁrst-order, homogeneous, linear equation. If (2.1) were an ordinary diﬀerential
equation† for a function u(t) of t alone, the solution would be obvious: u(t) = c must be
constant. A proof of this basic fact proceeds by integrating both sides with respect to t
and then appealing to the Fundamental Theorem of Calculus. To solve (2.1) as a partial
diﬀerential equation for u(t, x), let us similarly integrate both sides of the equation from,
say, 0 to t, producing
 t
∂u
(s, x) ds = u(t, x) − u(0, x).
0=
0 ∂t
Therefore, the solution takes the form
u(t, x) = f (x),

where

f (x) = u(0, x),

(2.2)

and hence is a function of the space variable x alone. The only requirement is that f (x)
be continuously diﬀerentiable, so f ∈ C1 , in order that u(t, x) be a bona ﬁde classical
†

Of course, in this situation, we would write the equation as du/dt = 0.

2.1 Stationary Waves

17

x

D
a
Da
t

Figure 2.2.

Domain for stationary-wave solution.

solution of the ﬁrst-order partial diﬀerential equation (2.1). The solution (2.2) represents
a stationary wave, meaning that it does not change in time. The initial proﬁle stays frozen
in place, and the system remains in equilibrium. Figure 2.1 plots a representative solution
as a function of x at three successive times.
The preceding analysis seems very straightforward and perhaps even a little boring.
But, to be completely rigorous, we need to take a bit more care. In our derivation, we
implicitly assumed that the solution u(t, x) was deﬁned everywhere on R 2 . And, in fact,
the solution formula (2.2) is not completely valid as stated if the solution u(t, x) is deﬁned
only on a subdomain D ⊂ R 2 .
Indeed, a solution u(t) to the corresponding ordinary diﬀerential equation du/dt = 0 is
constant, provided it is deﬁned on a connected subinterval I ⊂ R. A solution that is deﬁned
on a disconnected subset D ⊂ R need only be constant on each connected subinterval
I ⊂ D. For instance, the nonconstant function

1,
t > 0,
du
u(t) =
satisﬁes
=0
dt
−1,
t < 0,
everywhere on its domain of deﬁnition, that is, D = { t = 0 }, but is constant only on the
connected positive and negative half-lines.
Similar counterexamples can be constructed in the case of the partial diﬀerential equation (2.1). If the domain of deﬁnition is disconnected, then we do not expect u(t, x) to
depend only on x if we move from one connected component of D to another. Even that
is not the full story. For example, the function
⎧
x > 0,
⎨ 0,
x ≤ 0, t > 0,
x2 ,
(2.3)
u(t, x) =
⎩
2
x ≤ 0, t < 0,
−x ,
is continuously diﬀerentiable† on its domain of deﬁnition, namely D = R 2 \{ (0, x) | x ≤ 0 },
satisﬁes ∂u/∂t = 0 everywhere in D, but, nevertheless, is not a function of x alone, because,
for example, u(1, x) = x2 = u(−1, x) = − x2 .
†

You are asked to rigorously prove diﬀerentiability in Exercise 2.1.10.

18

2 Linear and Nonlinear Waves

A completely correct formulation can be stated as follows: If u(t, x) is a classical
solution to (2.1), deﬁned on a domain D ⊂ R 2 whose intersection with any horizontal‡ line,
namely Da = D ∩ { (t, a) | t ∈ R }, for each ﬁxed a ∈ R, is either empty or a connected
interval, then u(t, x) = f (x) is a function of x alone. An example of such a domain is
sketched in Figure 2.2. In Exercise 2.1.9, you are asked to justify these statements.
We are thus slightly chastened in our dismissal of (2.1) as a complete triviality. The
lesson is that, in future, one must always be careful when interpreting such “general”
solution formulas — since they often rely on unstated assumptions on their underlying
domain of deﬁnition.

Exercises
∂u
= x for u(t, x).
∂t
∂2u
= 0 for u(t, x).
2.1.2. Solve the partial diﬀerential equation
∂t2
2.1.1. Solve the partial diﬀerential equation

2.1.3. Find the general solution u(t, x) to the following partial diﬀerential equations:
(a) ux = 0, (b) ut = 1, (c) ut = x−t, (d) ut +3 u = 0, (e) ux +t u = 0, (f ) utt +4 u = 1.
2.1.4. Suppose u(t, x) is deﬁned for all (t, x) ∈ R 2 and solves ∂u/∂t + 2 u = 0. Prove that
lim u(t, x) = 0 for all x.
t→∞

2.1.5. Write down the general solution to the partial diﬀerential equation ∂u/∂t = 0 for a function of three variables u(t, x, y). What assumptions should be made on the domain of deﬁnition for your solution formula to be valid?
∂2u
2.1.6. Solve the partial diﬀerential equation
= 0 for u(x, y).
∂x ∂y
2.1.7. Answer Exercise 2.1.6 when u(x, y, z) depends on the three independent variables x, y, z.
∂u
+ u2 = 0, u(0, x) = f (x), where f (x) is a
∂t
bounded C1 function of x ∈ R. (a) Show that if f (x) ≥ 0 for all x, then u(t, x) is deﬁned
for all t > 0, and lim u(t, x) = 0. (b) On the other hand, if f (x) < 0, then the solution

♥ 2.1.8. Let u(t, x) solve the initial value problem
t→∞

u(t, x) is not deﬁned for all t > 0, but in fact, lim− u(t, x) = − ∞ for some 0 < τ < ∞.
t→τ

Given x, what is the corresponding value of τ ? (c) Given f (x) as in part (b), what is the
longest time interval 0 < t < t on which u(t, x) is deﬁned for all x ∈ R?
♦ 2.1.9. Justify the claim in the text that if u(t, x) is a solution of ∂u/∂t = 0 that is deﬁned on
a domain D ⊂ R 2 with the property that Da = D ∩ { (t, a) | t ∈ R } is either empty or a
connected interval, then u(t, x) = v(x) depends only on x ∈ D.
♦ 2.1.10. Prove that the function in (2.3) is continuously diﬀerentiable at all points (t, x) in its
domain of deﬁnition.

‡
Important: We will adopt the (slightly unusual) convention of displaying the (t, x)–plane
with time t along the horizontal axis and space x along the vertical axis — which also conforms
with our convention of writing t before x in expressions like u(t, x). Later developments will amply
vindicate our adoption of this convention.

2.2 Transport and Traveling Waves

19

2.2 Transport and Traveling Waves
In many respects, the stationary-wave equation (2.1) does not quite qualify as a partial
diﬀerential equation. Indeed, the spatial variable x enters only parametrically in the solution to what is, in essence (ignoring technical diﬃculties with domains), a very simple
ordinary diﬀerential equation.
Let us then turn to a more “genuine” example. Consider the linear, homogeneous
ﬁrst-order partial diﬀerential equation
∂u
∂u
+c
= 0,
∂t
∂x

(2.4)

for a function u(t, x), in which c is a ﬁxed, nonzero constant, known as the wave speed for
reasons that will soon become apparent. We will refer to (2.4) as the transport equation,
because it models the transport of a substance, e.g., a pollutant, in a uniform ﬂuid ﬂow that
is moving with velocity c. In this model, the solution u(t, x) represents the concentration of
the pollutant at time t and spatial position x. Other common names for (2.4) are the ﬁrstorder or unidirectional wave equation. But for brevity, as well as to avoid any confusion
with the second-order, bidirectional wave equation discussed extensively later on, we will
stick with the designation “transport equation” here. Solving the transport equation is
slightly more challenging, but, as we will see, not diﬃcult.
Since the transport equation involves time, its solutions are distinguished by their
initial values. As a ﬁrst-order equation, we need only specify the value of the solution at
an initial time t0 , leading to the initial value problem
u(t0 , x) = f (x)

for all

x ∈ R.

(2.5)

As we will show, as long as f ∈ C1 , i.e., is continuously diﬀerentiable, the initial conditions
serve to specify a unique classical solution. Also, by replacing the time variable t by t − t0 ,
we can, without loss of generality, set t0 = 0.
Uniform Transport
Let us begin by assuming that the wave speed c is constant. In general, when one is
confronted with a new equation, one solution strategy is to try to convert it into an equation
that you already know how to solve. In this case, we will introduce a simple change of
variables that eﬀectively rewrites the equation in a moving coordinate system, inspired by
the interpretation of c as the overall transport speed.
If x represents the position of an object in a ﬁxed coordinate frame, then
ξ = x − ct

(2.6)

represents the object’s position relative to an observer who is uniformly moving with velocity c. Think of a passenger in a moving train to whom stationary objects appear to
be moving backwards at the train’s speed c. To formulate a physical process in the reference frame of the passenger, we replace the stationary space-time coordinates (t, x) by the
moving coordinates (t, ξ).
Remark : These are the same changes of reference frame that underlie Einstein’s special theory of relativity. However, unlike Einstein, we are working in a purely classical,

20

2 Linear and Nonlinear Waves

u

u

u

x

x

t=0

t=1
Figure 2.3.

Traveling wave with c > 0.

x


t=2

nonrelativistic universe here. Such changes to moving coordinates are, in fact, of a much
older vintage, and named Galilean boosts in honor of Galileo Galilei, who was the ﬁrst to
champion such “relativistic” moving coordinate systems.
Let us see what happens when we re-express the transport equation in terms of the
moving coordinate frame. We rewrite
u(t, x) = v(t, x − c t) = v(t, ξ)

(2.7)

in terms of the characteristic variable ξ = x − c t, along with the time t. To write out
the diﬀerential equation satisﬁed by v(t, ξ), we apply the chain rule from multivariable
calculus, [8, 108], to express the derivatives of u in terms of those of v:
∂v
∂v
∂u
=
−c
,
∂t
∂t
∂ξ

∂u
∂v
=
.
∂x
∂ξ

Therefore,
∂u
∂u
∂v
∂v
∂v
∂v
+c
=
−c
+c
=
.
(2.8)
∂t
∂x
∂t
∂ξ
∂ξ
∂t
We deduce that u(t, x) solves the transport equation (2.4) if and only if v(t, ξ) solves the
stationary-wave equation
∂v
= 0.
(2.9)
∂t
Thus, the eﬀect of using a moving coordinate system is to convert a wave moving with
velocity c into a stationary wave. Think again of the passenger in the train — a second
train moving at the same speed appears as if it were stationary.
According to our earlier discussion, the solution v = v(ξ) to the stationary-wave
equation (2.9) is a function of the characteristic variable alone. (For simplicity, we assume
that v(t, ξ) has an appropriate domain of deﬁnition, e.g., it is deﬁned everywhere on R 2 .)
Recalling (2.7), we conclude that the solution
u = v(ξ) = v(x − c t)
to the transport equation must be a function of the characteristic variable only. We have
therefore proved the following result:
Proposition 2.1. If u(t, x) is a solution to the partial diﬀerential equation
ut + c ux = 0,

(2.10)

u(t, x) = v(x − c t),

(2.11)

which is deﬁned on all of R 2 , then
where v(ξ) is a C function of the characteristic variable ξ = x − c t.
1

2.2 Transport and Traveling Waves

21

x

(t, x)

(0, y)
t

Figure 2.4.

Characteristic line.

In other words, any (reasonable) function of the characteristic variable, e.g., ξ 2 + 1,
or cos ξ, or eξ , will produce a corresponding solution, (x − c t)2 + 1, or cos(x − c t), or
ex−c t , to the transport equation with constant wave speed c. And, in accordance with
the counting principle formulated on page 6, the general solution to this ﬁrst-order partial
diﬀerential equation in two independent variables depends on one arbitrary function of a
single variable.
To a stationary observer, the solution (2.11) appears as a traveling wave of unchanging
form moving at constant velocity c. When c > 0, the wave translates to the right, as illustrated in Figure 2.3. When c < 0, the wave translates to the left, while c = 0 corresponds
to a stationary wave form that remains ﬁxed at its original location, as in Figure 2.1.
At t = 0, the wave has the initial proﬁle
u(0, x) = v(x),

(2.12)

and so (2.11) provides the (unique) solution to the initial value problem (2.4, 12). For
example, the solution to the particular initial value problem
ut + 2 ux = 0,

u(0, x) =

1
,
1 + x2

is

u(t, x) =

1
.
1 + (x − 2 t)2

Since it depends only on the characteristic variable ξ = x − c t, every solution to the
transport equation is constant on the characteristic lines of slope† c, namely
x = c t + k,

(2.13)

where k is an arbitrary constant. At any given time t, the value of the solution at position x depends only on its original value on the characteristic line passing through (t, x).
†
This makes use of our convention that the t–axis is horizontal and the x–axis is vertical.
Reversing the axes will replace the slope by its reciprocal.

22

2 Linear and Nonlinear Waves

u

u

u

x

x

x

t=0

t=1
Figure 2.5.

Decaying traveling wave.



t=2

This is indicative of a general fact concerning such wave models: Signals propagate along
characteristics. Indeed, a disturbance at an initial point (0, y) only aﬀects the value of the
solution at points (t, x) that lie on the characteristic line x = c t + y emanating therefrom,
as illustrated in Figure 2.4.
Transport with Decay
Let a > 0 be a positive constant, and c an arbitrary constant. The homogeneous linear
ﬁrst-order partial diﬀerential equation
∂u
∂u
+c
+ au = 0
(2.14)
∂t
∂x
models the transport of, say, a radioactively decaying solute in a uniform ﬂuid ﬂow with
wave speed c. The coeﬃcient a governs the rate of decay. We can solve this variant of the
transport equation by the self-same change of variables to a uniformly moving coordinate
system.
Rewriting u(t, x) in terms of the characteristic variable, as in (2.7), and then recalling
our chain rule calculation (2.8), we ﬁnd that v(t, ξ) = u(t, ξ + c t) satisﬁes the partial
diﬀerential equation
∂v
+ a v = 0.
∂t
The result is, eﬀectively, a homogeneous linear ﬁrst-order ordinary diﬀerential equation,
in which the characteristic variable ξ enters only parametrically. The standard solution
technique learned in elementary ordinary diﬀerential equations, [20, 23], tells us to multiply
the equation by the exponential integrating factor ea t , leading to


∂v
∂ at
at
e
+ av =
(e v) = 0.
∂t
∂t
We conclude that w = ea t v solves the stationary-wave equation (2.1). Thus,
w = ea t v = f (ξ),

and hence

v(t, ξ) = f (ξ) e− a t ,

where f (ξ) is an arbitrary function of the characteristic variable. Reverting to physical
coordinates, we produce the solution formula
u(t, x) = f (x − c t) e− a t ,

(2.15)

which solves the initial value problem u(0, x) = f (x). It represents a wave that is moving
along with ﬁxed velocity c while simultaneously decaying at an exponential rate as prescribed by the coeﬃcient a > 0. A typical solution, for c > 0, is plotted at three successive

2.2 Transport and Traveling Waves

23

times in Figure 2.5. While the solution (2.15) is no longer constant on the characteristics, signals continue to propagate along them, since a solution’s initial value at a point
(0, y) will only aﬀect its subsequent (decaying) values on the associated characteristic line
x = c t + y.

Exercises
2.2.1. Find the solution to the initial value problem ut + ux = 0, u(1, x) = x/(1 + x2 ).
2.2.2. Solve the following initial value problems and graph the solutions at times t = 1, 2, and 3:
2
(a) ut − 3 ux = 0, u(0, x) = e− x ; (b) ut + 2 ux = 0, u(−1, x) = x/(1 + x2 );
(c) ut + ux + 12 u = 0, u(0, x) = tan−1 x; (d) ut − 4 ux + u = 0, u(0, x) = 1/(1 + x2 ).
2.2.3. Graph some of the characteristic lines for the following equations, and write down a
formula for the general solution:
(a) ut − 3 ux = 0, (b) ut + 5 ux = 0, (c) ut + ux + 3 u = 0, (d) ut − 4 ux + u = 0.
2

2.2.4. Solve the initial value problem ut + 2 ux = 1, u(0, x) = e− x .
Hint: Use characteristic coordinates.
2.2.5. Answer Exercise 2.2.4 for the initial value problem ut + 2 ux = sin x, u(0, x) = sin x.
♦ 2.2.6. Let c be constant. Suppose that u(t, x) solves the initial value problem ut + c ux = 0,
u(0, x) = f (x). Prove that v(t, x) = u(t − t0 , x) solves the initial value problem vt + c vx = 0,
v(t0 , x) = f (x).
2.2.7. Is Exercise 2.2.6 valid when the transport equation is replaced by the damped transport
equation (2.14)?
2.2.8. Let c = 0. Prove that if the initial data satisﬁes u(0, x) = v(x) → 0 as x → ± ∞, then,
for each ﬁxed x, the solution to the transport equation (2.4) satisﬁes u(t, x) → 0 as t → ∞.
2.2.9. (a) Prove that if the initial data is bounded, | f (x) | ≤ M for all x ∈ R, then the solution to the damped transport equation (2.14) with a > 0 satisﬁes u(t, x) → 0 as t → ∞.
(b) Find a solution to (2.14) that is deﬁned for all (t, x) but does not satisfy u(t, x) → 0
as t → ∞.
2.2.10. Let F (t, x) be a C1 function of (t, x) ∈ R 2 . (a) Write down a formula for the general
solution u(t, x) to the inhomogeneous partial diﬀerential equation ut = F (t, x).
(b) Solve the inhomogeneous transport equation ut + c ux = F (t, x).
♥ 2.2.11. (a) Write down a formula for the general solution to the nonlinear partial diﬀerential
equation ut + ux + u2 = 0. (b) Show that if the initial data is positive and bounded,
0 ≤ u(0, x) = f (x) ≤ M , then the solution exists for all t > 0, and u(t, x) → 0 as t → ∞.
(c) On the other hand, if the initial data is negative somewhere, so f (x) < 0 at some x ∈ R,
then the solution blows up in ﬁnite time: lim− u(t, y) = − ∞ for some τ > 0 and some
t→τ

y ∈ R. (d) Find a formula for the earliest blow-up time τ > 0.
2.2.12. A sensor situated at position x = 1 monitors the concentration of a pollutant u(t, 1) as
a function of t for t ≥ 0. Assuming that the pollutant is transported with wave speed c = 3,
at what locations x can you determine the initial concentration u(0, x)?
2.2.13. Write down a solution to the transport equation ut + 2 ux = 0 that is deﬁned on a
connected domain D ⊂ R 2 and that is not a function of the characteristic variable alone.

24

2 Linear and Nonlinear Waves

2.2.14. Let c > 0. Consider the uniform transport equation ut + c ux = 0 restricted to the
quarter-plane Q = { x > 0, t > 0 } and subject to initial conditions u(0, x) = f (x) for x ≥ 0,
along with boundary conditions u(t, 0) = g(t) for t ≥ 0. (a) For which initial and boundary conditions does a classical solution to this initial-boundary value problem exist? Write
down a formula for the solution. (b) On which regions are the eﬀects of the initial conditions felt? What about the boundary conditions? Is there any interaction between the two?
2.2.15. Answer Exercise 2.2.14 when c < 0.

Nonuniform Transport
Slightly more complicated, but still linear, is the nonuniform transport equation
∂u
∂u
+ c(x)
= 0,
∂t
∂x

(2.16)

where the wave speed c(x) is now allowed to depend on the spatial position. Characteristics
continue to guide the behavior of solutions, but when the wave speed is not constant, we
can no longer expect them to be straight lines. To adapt the method of characteristics,
let us look at how the solution varies along a prescribed curve in the (t, x)–plane. Assume
that the curve is identiﬁed with the graph of a function x = x(t), and let
h(t) = u t, x(t)
be the value of the solution on it. We compute the rate of change in the solution along
the curve by diﬀerentiating h with respect to t. Invoking the multivariable chain rule, we
obtain
d
∂u
∂u
dx
dh
=
u t, x(t) =
t, x(t) +
t, x(t)
.
(2.17)
dt
dt
∂t
∂x
dt
In particular, if x(t) satisﬁes
dx
= c x(t) ,
dt

then

dh
∂u
∂u
=
t, x(t) + c x(t)
t, x(t) = 0,
dt
∂t
∂x

since we are assuming that u(t, x) solves the transport equation (2.16) for all values of
(t, x), including those points t, x(t) on the curve. Since its derivative is zero, h(t) must
be a constant, which motivates the following deﬁnition.
Deﬁnition 2.2. The graph of a solution x(t) to the autonomous ordinary diﬀerential
equation
dx
= c(x)
(2.18)
dt
is called a characteristic curve for the transport equation with wave speed c(x).
In other words, at each point (t, x), the slope of the characteristic curve equals the
wave speed c(x) there. In particular, if c is constant, the characteristic curves are straight
lines of slope c, in accordance with our earlier construction.
Proposition 2.3. Solutions to the linear transport equation (2.16) are constant
along characteristic curves.

2.2 Transport and Traveling Waves

25

x

(t, x)
(0, y)

t

Figure 2.6.

Characteristic curve.

The characteristic curve equation (2.18) is an autonomous ﬁrst-order ordinary diﬀerential equation. As such, it can be immediately solved by separating variables, [20, 23].
Assuming c(x) = 0, we divide both sides of the equation by c(x), and then integrate the
resulting equation:

dx
dx
= dt,
whereby
β(x) :=
= t + k,
(2.19)
c(x)
c(x)
with k denoting the integration constant. For each ﬁxed value of k, (2.19) serves to implicitly deﬁne a characteristic curve, namely,
x(t) = β −1 (t + k),
with β −1 denoting the inverse function. On the other hand, if c(x ) = 0, then x is a
ﬁxed point for the ordinary diﬀerential equation (2.18), and the horizontal line x ≡ x is a
stationary characteristic curve.
Since the solution u(t, x) is constant along the characteristic curves, it must therefore
be a function of the characteristic variable
ξ = β(x) − t

(2.20)

u(t, x) = v β(x) − t ,

(2.21)

alone, and hence of the form
1

where v(ξ) is an arbitrary C function. Indeed, it is easy to check directly that, provided
β(x) is deﬁned by (2.19), u(t, x) solves the partial diﬀerential equation (2.16) for any choice
of C1 function v(ξ). (But keep in mind that the algebraic solution formula (2.21) may fail
to be valid at points where the wave speed vanishes: c(x ) = 0.)
Warning: The deﬁnition of characteristic variable used here is slightly diﬀerent from
that in the constant wave speed case, which, by (2.20), would be ξ = x/c − t = (x − c t)/c.
Clearly, rescaling the characteristic variable by 1/c is an inessential modiﬁcation of our
original deﬁnition.

26

2 Linear and Nonlinear Waves

x

t

Figure 2.7.

Characteristic curves for ut + (x2 + 1)−1 ux = 0.

To ﬁnd the solution that satisﬁes the prescribed initial conditions
u(0, x) = f (x),

(2.22)

we merely substitute the general solution formula (2.21). This leads to the implicit equation
v(β(x)) = f (x) for the function v(ξ) = f ◦ β −1 (ξ). The resulting solution formula
u(t, x) = f ◦ β −1 β(x) − t

(2.23)

is not particularly enlightening, but it does have a simple graphical interpretation: To ﬁnd
the value of the solution u(t, x), we look at the characteristic curve passing through the
point (t, x). If this curve intersects the x–axis at the point (0, y), as in Figure 2.6, then
u(t, x) = u(0, y) = f (y), since the solution must be constant along the curve. On the other
hand, if the characteristic curve through (t, x) doesn’t intersect the x–axis, the solution
value u(t, x) is not prescribed by the initial data.
Example 2.4. Let us solve the nonuniform transport equation
∂u
∂u
1
+ 2
=0
(2.24)
∂t
x + 1 ∂x
by the method of characteristics. According to (2.18), the characteristic curves are the
graphs of solutions to the ﬁrst-order ordinary diﬀerential equation
dx
1
= 2
.
dt
x +1
Separating variables and integrating, we obtain

β(x) =
(x2 + 1) dx = 13 x3 + x = t + k,

(2.25)

where k is the integration constant. Representative curves are plotted in Figure 2.7. (In this
case, inverting the function β, i.e., solving (2.25) for x as a function of t, is not particularly
enlightening.)

2.2 Transport and Traveling Waves

27

t=0

t=2

t=5

t = 12

t = 25

t = 50

Figure 2.8.


1
Solution to ut + 2
ux = 0.
x +1

According to (2.20), the characteristic variable is ξ = 13 x3 + x − t, and hence the
general solution to the equation takes the form
u=v

1 3
3x +x−t

,

(2.26)

where v(ξ) is an arbitrary C1 function. A typical solution, corresponding to initial data
u(0, x) =

1
,
1 + (x + 3)2

(2.27)

is plotted† at the indicated times in Figure 2.8. Although the solution remains constant
along each individual curve, a stationary observer will witness a dynamically changing
proﬁle as the wave moves through the nonuniform medium. In this example, since c(x) > 0
everywhere, the wave always moves from left to right; its speed as it passes through a point
x determined by the magnitude of c(x) = (x2 + 1)−1 , with the consequence that each part
accelerates as it approaches the origin from the left, and then slows back down once it
passes by and c(x) decreases in magnitude. To a stationary observer, the wave spreads out
as it speeds through the origin, and then becomes progressively narrower and slower as it
gradually moves oﬀ to + ∞.
Example 2.5. Consider the nonuniform transport equation
ut + (x2 − 1) ux = 0.
†

(2.28)




The required function v(ξ) in (2.26) is implicitly given by the equation v 13 x3 + x = u(0, x),
and so the explicit formula for u(t, x) is not very instructive or useful. Indeed, to make the plots,
we instead sampled the initial data (2.27) at a collection of uniformly spaced points y1 < y2 <
· · · < yn . Since the solution is constant along the characteristic curve (2.25) passing through each
sample point (0, yi ), we can ﬁnd nonuniformly spaced sample values for u(t, xi ) at any later time.
The smooth solution curve u(t, x) is then approximated using spline interpolation, [ 89; §11.4], on
these sample values.

28

2 Linear and Nonlinear Waves

x

t

Figure 2.9.

Characteristic curves for ut + (x2 − 1)ux = 0.

In this case, the characteristic curves are the solutions to
dx
= x2 − 1,
dt
and so


β(x) =

dx
1
x−1
= t + k.
= log
x2 − 1
2
x+1

(2.29)

One must also include the horizontal lines x = x± = ± 1 corresponding to the roots of
c(x) = x2 − 1. The curves are graphed in Figure 2.9. Note that those curves starting below
x+ = 1 converge to x− = −1 as t → ∞, while those starting above x+ = 1 veer oﬀ to ∞
in ﬁnite time. Owing to the sign of c(x) = x2 − 1, points on the graph of u(0, x) lying over
| x | < 1 will move to the left, while those over | x | > 1 will move to the right.
In Figure 2.10, we graph several snapshots of the solution whose initial value is a
bell-shaped Gaussian proﬁle
2
u(0, x) = e− x .
The initial conditions uniquely prescribe the value of the solution along the characteristic
curves that intersect the x–axis. On the other hand, if
x≤

1 + e2 t
1 − e2 t

for

t > 0,

the characteristic curve through (t, x) does not intersect the x–axis, and hence the value
of the solution at such points, lying in the shaded region in Figure 2.9, is not prescribed
by the initial data. Let us arbitrarily assign the solution to be u(t, x) = 0 at such points.
At other values of (t, x) with t ≥ 0, the solution (2.23) is
 
 
x + 1 + (x − 1) e−2 t 2
u(t, x) = exp −
.
(2.30)
x + 1 − (x − 1) e−2 t

2.2 Transport and Traveling Waves

29

t=0

t = .2

t=2

t=3
Figure 2.10.

t=1

Solution to ut + (x2 − 1)ux = 0.



t=5

(The derivation of this solution formula is left as Exercise 2.2.23.) As t increases, the
solution’s peak becomes more and more concentrated near x− = −1, while the section of
the wave above x > x+ = 1 rapidly spreads out to ∞. In the long term, the solution
converges (albeit nonuniformly) to a step function of height 1/e:

u(t, x) −→ s(x) =

1/e ≈ .367879, x ≥ −1,
0,

x < −1,

as

t −→ ∞.

Let us ﬁnish by making a few general observations concerning the characteristic curves
of transport equations whose wave speed c(x) depends only on the position x. Using the
basic existence and uniqueness theory for such autonomous ordinary diﬀerential equations,
[20, 23, 52], and assuming that c(x) is continuously diﬀerentiable:†
• There is a unique characteristic curve passing through each point (t, x) ∈ R 2 .
• Characteristic curves cannot cross each other.
• If t = β(x) is a characteristic curve, then so are all its horizontal translates:
t = β(x) + k for any k.
• Each non-horizontal characteristic curve is the graph of a strictly monotone function.
Thus, each point on a wave always moves in the same direction, and can never
reverse its direction of propagation.
• As t increases, the characteristic curve either tends to a ﬁxed point, x(t) → x as
t → ∞, with c(x ) = 0, or goes oﬀ to ± ∞ in either ﬁnite or inﬁnite time.
Proofs of these statements are assigned to the reader in Exercise 2.2.25.

†

For those who know about such things, [ 18, 52 ], this assumption can be weakened to just
Lipschitz continuity.

30

2 Linear and Nonlinear Waves

Exercises
2.2.16. (a) Find the general solution to the ﬁrst-order equation ut + 32 ux = 0.
(b) Find a solution satisfying the initial condition u(1, x) = sin x. Is your solution unique?
2.2.17. (a) Solve the initial value problem ut − x ux = 0, u(0, x) = (x2 + 1)−1 .
(b) Graph the solution at times t = 0, 1, 2, 3. (c) What is lim u(t, x)?
t→∞

2.2.18. Suppose the initial data u(0, x) = f (x) of the nonuniform transport equation (2.28) is
continuous and satisﬁes f (x) → 0 as | x | → ∞. What is the limiting solution proﬁle u(t, x)
as (a) t → ∞? (b) t → − ∞?
♥ 2.2.19. (a) Find and graph the characteristic curves for the equation
ut + (sin x) ux = 0.


1 
Suppose you are given initial data (i ) u(0, x) =  cos 2 x , (ii ) u(0, x) = cos 12 πx.
(b) Write down a formula for the solution. (c) Graph your solution at times
t = 0, 1, 2, 3, 5, and 10. (d) What is the limiting solution proﬁle as t → ∞?
2.2.20. Consider the linear transport equation ut + (1 + x2 ) ux = 0. (a) Find and sketch the
characteristic curves. (b) Write down a formula for the general solution. (c) Find the
solution to the initial value problem u(0, x) = f (x) and discuss its behavior as t increases.
2.2.21. Prove that, for t
tional to t−2/3 .

0, the speed of the wave in Example 2.4 is asymptotically propor-

2.2.22. Verify directly that formula (2.21) deﬁnes a solution to the diﬀerential equation (2.16).
♦ 2.2.23. Explain how to derive the solution formula (2.30). Justify that it deﬁnes a solution to
equation (2.28).
2.2.24. Let c(x) be a bounded C1 function, so | c(x) | ≤ c < ∞ for all x. Let f (x) be any C1
function. Prove that the solution u(t, x) to the initial value problem ut + c(x) ux = 0,
u(0, x) = f (x), is uniquely deﬁned for all (t, x) ∈ R 2 .
♥ 2.2.25. Suppose that c(x) ∈ C1 is continuously diﬀerentiable for all x ∈ R. (a) Prove that the
characteristic curves of the transport equation (2.16) cannot cross each other. (b) A point
where c(x ) = 0 is known as a ﬁxed point for the characteristic equation dx/dt = c(x).
Explain why the characteristic curve passing through a ﬁxed point (t, x ) is a horizontal
straight line. (c) Prove that if x = g(t) is a characteristic curve, then so are all the horizontally translated curves x = g(t + δ) for any δ. (d) True or false: Every characteristic curve
has the form x = g(t + δ), for some ﬁxed function g(t). (e) Prove that each non-horizontal
characteristic curve is the graph x = g(t) of a strictly monotone function. (f ) Explain why
a wave cannot reverse its direction. (g) Show that a non-horizontal characteristic curve
starts, in the distant past, t → − ∞, at either a ﬁxed point or at −∞ and ends, as
t → + ∞, at either the next-larger ﬁxed point or at +∞.
∂u
∂u
+ c(t, x)
= 0 with time-varying wave speed.
♥ 2.2.26. Consider the transport equation
∂t
∂x
dx
= c(t, x),
Deﬁne the corresponding characteristic ordinary diﬀerential equation to be
dt
the graphs of whose solutions x(t) are the characteristic curves. (a) Prove that any
solution u(t, x) to the partial diﬀerential equation is constant on each characteristic curve.
(b) Suppose that the general solution to the characteristic equation is written in the form
ξ(t, x) = k, where k is an arbitrary constant. Prove that ξ(t, x) deﬁnes a characteristic
variable, meaning that u(t, x) = f (ξ(t, x)) is a solution to the time-varying transport
equation for any continuously diﬀerentiable scalar function f ∈ C1 .
2.2.27. (a) Apply the method in Exercise 2.2.26 to ﬁnd the characteristic curves for the equa2
tion ut + t2 ux = 0. (b) Find the solution to the initial value problem u(0, x) = e− x ,
and discuss its dynamic behavior.

2.3 Nonlinear Transport and Shocks

31

2.2.28. Solve Exercise 2.2.27 for the equation ut + (x − t) ux = 0.
♥ 2.2.29. Consider the ﬁrst-order partial diﬀerential equation ut + (1 − 2 t) ux = 0. Use Exercise
2.2.26 to: (a) Find and sketch the characteristic curves. (b) Write down the general solu1
. (d) Describe the behavior
tion. (c) Solve the initial value problem with u(0, x) =
1 + x2
of your solution u(t, x) from part (c) as t → ∞. What about t → − ∞?
2.2.30. Discuss which of the conclusions of Exercise 2.2.25 are valid for the characteristic curves
of the transport equation with time-varying wave speed, as analyzed in Exercise 2.2.26.
∂u
∂u
∂u
+ c(x, y)
+ d(x, y)
= 0,
∂t
∂x
∂y
whose solution u(t, x, y) depends on time t and space variables x, y. (a) Deﬁne a characteristic curve, and prove that the solution is constant along it. (b) Apply the method of char-

♦ 2.2.31. Consider the two-dimensional transport equation

2

2

acteristics to solve the initial value problem ut + y ux − x uy , u(0, x, y) = e− (x−1) −(y−1) .
(c) Describe the behavior of your solution.

2.3 Nonlinear Transport and Shocks
The ﬁrst-order nonlinear partial diﬀerential equation
ut + u ux = 0

(2.31)

has the form of a transport equation (2.4), but the wave speed c = u now depends, not
on the position x, but rather on the size of the disturbance u. Larger waves will move
faster, and overtake smaller, slower-moving waves. Waves of elevation, where u > 0, move
to the right, while waves of depression, where u < 0, move to the left. This equation
is considerably more challenging than the linear transport models analyzed above, and
was ﬁrst systematically studied in the early nineteenth century by the inﬂuential French
mathematician Siméon–Denis Poisson and the great German mathematician Bernhard Riemann.† It and its multi-dimensional and multi-component generalizations play a crucial
role in the modeling of gas dynamics, acoustics, shock waves in pipes, ﬂood waves in rivers,
chromatography, chemical reactions, traﬃc ﬂow, and so on. Although we will be able to
write down a solution formula, the complete analysis is far from trivial, and will require us
to confront the possibility of discontinuous shock waves. Motivated readers are referred to
Whitham’s book, [122], for further details.
Fortunately, the method of characteristics that was developed for linear transport
equations also works in the present context and leads to a complete mathematical solution.
Mimicking our previous construction, (2.18), but now with wave speed c = u, let us deﬁne
a characteristic curve of the nonlinear wave equation (2.31) to be the graph of a solution
x(t) to the ordinary diﬀerential equation
dx
= u(t, x).
dt
†

(2.32)

In addition to his fundamental contributions to partial diﬀerential equations, complex analysis, and number theory, Riemann also was the inventor of Riemannian geometry, which turned
out to be absolutely essential for Einstein’s theory of general relativity some 70 years later!

32

2 Linear and Nonlinear Waves

As such, the characteristics depend upon the solution u, which, in turn, is to be speciﬁed
by its characteristics. We appear to be trapped in a circular argument.
The resolution of the conundrum is to argue that, as in the linear case, the solution
u(t, x) remains constant along its characteristics, and this fact will allow us to simultaneously specify both. To prove this claim, suppose that x = x(t) parametrizes a characteristic
curve associated with the given solution u(t, x). Our task is to show that h(t) = u t, x(t) ,
which is obtained by evaluating the solution along the curve, is constant, which, as usual,
is proved by checking that its derivative is identically zero. Repeating our chain rule
computation (2.17), and using (2.32), we deduce that
dh
d
∂u
dx ∂u
∂u
∂u
=
u t, x(t) =
t, x(t) +
t, x(t) =
t, x(t) +u t, x(t)
t, x(t) = 0,
dt
dt
∂t
dt ∂x
∂t
∂x
since u is assumed to solve the nonlinear transport equation (2.31) at all values of (t, x),
including those on the characteristic curve. We conclude that h(t) is constant, and hence
u is indeed constant on the characteristic curve.
Now comes the clincher. We know that the right-hand side of the characteristic ordinary diﬀerential equation (2.32) is a constant whenever x = x(t) deﬁnes a characteristic
curve. This means that the derivative dx/dt is a constant — namely the ﬁxed value of u
on the curve. Therefore, the characteristic curve must be a straight line,
x = u t + k,

(2.33)

whose slope equals the value assumed by the solution u on it.
And, as before, since the solution is constant along each characteristic line, it must be
a function of the characteristic variable
ξ = x − tu

(2.34)

u = f (x − t u),

(2.35)

alone, and so
where f (ξ) is an arbitrary C1 function. Formula (2.35) should be viewed as an algebraic
equation that implicitly deﬁnes the solution u(t, x) as a function of t and x. Veriﬁcation
that the resulting function is indeed a solution to (2.31) is the subject of Exercise 2.3.14.
Example 2.6. Suppose that
f (ξ) = α ξ + β,
with α, β constant. Then (2.35) becomes
u = α(x − t u) + β,

and hence

u(t, x) =

αx + β
1+αt

(2.36)

is the corresponding solution to the nonlinear transport equation. At each ﬁxed t, the graph
of the solution is a straight line. If α > 0, the solution ﬂattens out: u(t, x) → 0 as t → ∞.
On the other hand, if α < 0, the straight line rapidly steepens to vertical as t approaches
the critical time t = − 1/α, at which point the solution ceases to exist. Figure 2.11 graphs
two representative solutions. The top row shows the solution with α = 1, β = .5, plotted
at times t = 0, 1, 5, and 20; the bottom row takes α = −.2, β = .1, and plots the solution
at times t = 0, 3, 4, and 4.9. In the second case, the solution blows up by becoming vertical
as t → 5.

2.3 Nonlinear Transport and Shocks

33

t=0

t=1

t=5

t = 20

t=0

t=3

t=4

t = 4.9

Figure 2.11.

Two solutions to ut + u ux = 0.



Remark : Although (2.36) remains a valid solution formula after the blow-up time,
t > 5, this is not to be viewed as a part of the original solution. With the appearance of
such a singularity, the physical solution has broken down, and we stop tracking it.
To solve the general initial value problem
u(0, x) = f (x),

(2.37)

we note that, at t = 0, the implicit solution formula (2.35) reduces to (2.37), and hence the
function f coincides with the initial data. However, because our solution formula (2.35) is
an implicit equation, it is not immediately evident
(a) whether it can be solved to give a well-deﬁned function u(t, x), and,
(b) even granted this, how to describe the resulting solution’s qualitative features and
dynamical behavior.
A more instructive approach is founded on the following geometrical construction.
Through each point (0, y) on the x–axis, draw the characteristic line
x = t f (y) + y

(2.38)

whose slope, namely f (y) = u(0, y), equals the value of the initial data (2.37) at that point.
According to the preceding discussion, the solution will have the same value on the entire
characteristic line (2.38), and so
u(t, t f (y) + y) = f (y)

for all t.

(2.39)

For example, if f (y) = y, then u(t, x) = y whenever x = t y + y; eliminating y, we ﬁnd
u(t, x) = x/(t + 1), which agrees with one of our straight line solutions (2.36).
Now, the problem with this construction is immediately apparent from Figure 2.12,
which plots the characteristic lines associated with the initial data
u(0, x) = 21 π − tan−1 x.

34

2 Linear and Nonlinear Waves

x

t

Figure 2.12.

Characteristics lines for u(0, x) = 12 π − tan−1 x.

Two characteristic lines that are not parallel must cross each other somewhere. The value
of the solution is supposed to equal the slope of the characteristic line passing through the
point. Hence, at a crossing point, the solution is required to assume two diﬀerent values,
one corresponding to each line. Something is clearly amiss, and we need to resolve this
apparent paradox.
There are three principal scenarios. The ﬁrst, trivial, situation occurs when all the
characteristic lines are parallel, and so the diﬃculty does not arise. In this case, they all
have the same slope, say c, which means that the solution has the same value on each one.
Therefore, u(t, x) ≡ c is a constant solution.
The next-simplest case occurs when the initial data is everywhere nondecreasing, so
f (x) ≤ f (y) whenever x ≤ y, which is assured if its derivative is never negative: f  (x) ≥ 0.
In this case, as sketched in Figure 2.13, the characteristic lines emanating from the x axis
fan out into the right half-plane, and so never cross each other at any future time t > 0.
Each point (t, x) with t ≥ 0 lies on a unique characteristic line, and the value of the
solution at (t, x) is equal to the slope of the line. We conclude that the solution u(t, x)
is well deﬁned at all future times t ≥ 0. Physically, such solutions represent rarefaction
waves, which spread out as time progresses. A typical example, corresponding to initial
data
u(0, x) = 12 π + tan−1 (3 x),
has its characteristic lines plotted in Figure 2.13, while Figure 2.14 graphs some representative solution proﬁles.
The more interesting case occurs when the initial data is a decreasing function, and so
f  (x) < 0. Now, as in Figure 2.12, some of the characteristic lines starting at t = 0 will cross
at some point in the future. If a point (t, x) lies on two or more distinct characteristic lines,
the value of the solution u(t, x), which should equal the characteristic slope, is no longer
uniquely determined. Although, in a purely mathematical context, one might be tempted
to allow such multiply valued solutions, from a physical standpoint this is unacceptable.
The solution u(t, x) is supposed to represent a measurable quantity, e.g., concentration,

2.3 Nonlinear Transport and Shocks

35

x

t

Figure 2.13.

Characteristic lines for a rarefaction wave.

t=0

t=1

t=2
Figure 2.14.

Rarefaction wave.



t=3

velocity, pressure, and must therefore assume a unique value at each point. In eﬀect, the
mathematical model has broken down and no longer conforms to physical reality.
However, before confronting this diﬃculty, let us ﬁrst, from a purely theoretical standpoint, try to understand what happens if we mathematically continue the solution as a
multiply valued function. For speciﬁcity, consider the initial data
u(0, x) = 12 π − tan−1 x,

(2.40)

appearing in the ﬁrst graph in Figure 2.15. The corresponding characteristic lines are
displayed in Figure 2.12. Initially, they do not cross, and the solution remains a welldeﬁned, single-valued function. However, after a while one reaches a critical time, t > 0,
when the ﬁrst two characteristic lines cross each other. Subsequently, a wedge-shaped
region appears in the (t, x)–plane, consisting of points that lie on the intersection of three

36

2 Linear and Nonlinear Waves

t=0

t = .5

t=1

t = 1.5

t=2

t = 2.5

Figure 2.15.

Multiply valued compression wave.



distinct characteristic lines with diﬀerent slopes; at such points, the mathematical solution
achieves three distinct values. Points outside the wedge lie on a single characteristic line,
and the solution remains single-valued there. The boundary of the wedge consists of points
where precisely two characteristic lines cross.
To fully appreciate what is going on, look now at the sequence of pictures of the
multiply valued solution in Figure 2.15, plotted at six successive times. Since the initial
data is positive, f (x) > 0, all the characteristic slopes are positive. As a consequence,
every point on the solution curve moves to the right, at a speed equal to its height. Since
the initial data is a decreasing function, points on the graph lying to the left will move
faster than those to the right and eventually overtake them. At ﬁrst, the solution merely
steepens into a compression wave. At the critical time t when the ﬁrst two characteristic
lines cross, say at position x , so that (t , x ) is the tip of the aforementioned wedge, the
solution graph has become vertical:
∂u
(t, x ) −→ ∞
∂x

as

t −→ t ,

and u(t, x) is no longer a classical solution. Once this occurs, the solution graph ceases to
be a single-valued function, and its overlapping lobes lie over the points (t, x) belonging to
the wedge.
The critical time t can, in fact, be determined from the implicit solution formula (2.35).
Indeed, if we diﬀerentiate with respect to x, we obtain


∂u
∂ξ
∂u
∂
=
f (ξ) = f  (ξ)
= f  (ξ) 1 − t
,
where
ξ = x − t u.
∂x
∂x
∂x
∂x
Solving for
∂u
f  (ξ)
=
,
∂x
1 + t f  (ξ)

2.3 Nonlinear Transport and Shocks

37

we see that the slope blows up:
∂u
−→ ∞
∂x

t −→ −

as

1
.
f  (ξ)

In other words, if the initial data has negative slope at position x, so f  (x) < 0, then the
solution along the characteristic line emanating from the point (0, x) will fail to be smooth
at the time − 1/f  (x). The earliest critical time is, thus,

t := min

−

1
f  (x)

f  (x) < 0


.

(2.41)

If x0 is the value of x that produces the minimum t , then the slope of the solution proﬁle
will ﬁrst become inﬁnite at the location where the characteristic starting at x0 is at time
t , namely
(2.42)
x = x0 + f (x0 ) t .
For instance, for the particular initial conﬁguration (2.40) represented in Figure 2.15,
f (x) =

π
− tan−1 x,
2

f  (x) = −

1
,
1 + x2

and so the critical time is
t = min { 1 + x2 } = 1,

with

x = f (0) t = 12 π,

since the minimum value occurs at x0 = 0.
Now, while mathematically plausible, such a multiply valued solution is physically
untenable. So what really happens after the critical time t ? One needs to decide which
(if any) of the possible solution values is physically appropriate. The mathematical model,
in and of itself, is incapable of resolving this quandary. We must therefore revisit the
underlying physics, and ask what sort of phenomenon we are trying to model.

Shock Dynamics
To be speciﬁc, let us regard the transport equation (2.31) as a model of compressible ﬂuid
ﬂow in a single space variable, e.g., the motion of gas in a long pipe. If we push a piston
into the pipe, then the gas will move ahead of it and thereby be compressed. However, if
the piston moves too rapidly, then the gas piles up on top of itself, and a shock wave forms
and propagates down the pipe. Mathematically, the shock is represented by a discontinuity
where the solution abruptly changes value. The formulas (2.41) and (2.42) determine the
time and position for the onset of the shock-wave discontinuity. Our goal now is to predict
its subsequent behavior, and this will be based on use of a suitable physical conservation
law. Indeed, one expects mass to be conserved – even through a shock discontinuity —
since gas atoms can neither be created nor destroyed. And, as we will see, conservation of
mass (almost) suﬃces to prescribe the subsequent motion of the shock wave.
Before investigating the implications of conservation of mass, let us ﬁrst convince
ourselves of its validity for the nonlinear transport model. (Just because a mathematical
equation models a physical system does not automatically imply that it inherits any of its

38

2 Linear and Nonlinear Waves

physical conservation laws.) If u(t, x) represents density, then, at time t, the total mass
lying in an interval a ≤ x ≤ b is calculated by integration:
 b
u(t, x) dx.
(2.43)
Ma,b (t) =
a

Assuming that u(t, x) is a classical solution to the nonlinear transport equation (2.31), we
can determine the rate of change of mass on this interval by diﬀerentiation:

 b
 b
dMa,b
d b
∂u
∂u
=
(t, x) dx = −
(t, x) dx
u(t, x) dx =
u(t, x)
dt
dt a
∂x
a ∂t
a
(2.44)
 b
b

∂ 1
2
2
2
2
1
1
1
=−
= 2 u(t, a) − 2 u(t, b) .
dx = − 2 u(t, x)
2 u(t, x)
x=a
a ∂x
The ﬁnal expression represents the net mass ﬂux through the endpoints of the interval.
Thus, the only way in which the mass on the interval [ a, b ] changes is through its endpoints;
inside, mass can be neither created nor destroyed, which is the precise meaning of the mass
conservation law in continuum mechanics. In particular, if there is zero net mass ﬂux, then
the total mass is constant, and hence conserved. For example, if the initial data (2.37) has
ﬁnite total mass,

∞

f (x) dx
−∞

< ∞,

(2.45)

which requires that f (x) → 0 reasonably rapidly as | x | → ∞, then the total mass of the
solution — at least up to the formation of a shock discontinuity — remains constant and
equal to its initial value:
 ∞
 ∞
 ∞
u(t, x) dx =
u(0, x) dx =
f (x) dx.
(2.46)
−∞

−∞

−∞

Similarly, if u(t, x) represents the traﬃc density on a highway at time t and position x,
then the integrated conservation law (2.44) tells us that the rate of change in the number
of vehicles on the stretch of road between a and b equals the number of vehicles entering
at point a minus the number leaving at point b — which assumes that there are no other
exits or entrances on this part of the highway. Thus, in the traﬃc model, (2.44) represents
the conservation of vehicles.
The preceding calculation relied on the fact that the integrand can be written as an x
derivative. This is a common feature of physical conservation laws in continuum mechanics,
and motivates the following general deﬁnition.
Deﬁnition 2.7. A conservation law , in one space dimension, is an equation of the
form
∂T
∂X
+
= 0.
(2.47)
∂t
∂x
The function T is known as the conserved density, while X is the associated ﬂux .
In the simplest situations, the conserved density T (t, x, u) and ﬂux X(t, x, u) depend
on the time t, the position x, and the solution u(t, x) to the physical system. (Higher-order
conservation laws, which also depend on derivatives of u, arise in the analysis of integrable
partial diﬀerential equations; see Section 8.5 and [36, 87].) For example, the nonlinear
transport equation (2.31) is itself a conservation law, since it can be written in the form
∂u
∂
+
∂t
∂x

1 2
2u

= 0,

(2.48)

2.3 Nonlinear Transport and Shocks

39

u

x
Figure 2.16.

Equal Area Rule.

and so the conserved density is T = u and the ﬂux is X = 12 u2 . And indeed, it was
this identity that made our computation (2.44) work. The general result, proved by an
analogous computation, justiﬁes calling (2.47) a conservation law.
Proposition 2.8.
a ≤ x ≤ b,

Given a conservation law (2.47), then, on any closed interval
d
dt

 b
T dx = − X
a

b

.

(2.49)

x=a

Proof : The proof is an immediate consequence of the Fundamental Theorem of Calculus — assuming suﬃcient smoothness that allows one to bring the derivative inside the
integral sign:

 b
 b
b
∂T
∂X
d b
dx = −
dx = − X
T dx =
.
Q.E .D.
dt a
x=a
a ∂t
a ∂x
We will refer to (2.49) as the integrated form of the conservation law (2.47). It states
that the rate of change of the total density, integrated over an interval, is equal to the
amount of ﬂux through its two endpoints. In particular, if there is no net ﬂux into or out
of the interval, then the integrated density is conserved , meaning that it remains constant
over time. All physical conservation laws — mass, momentum, energy, and so on — for
systems governed by partial diﬀerential equations are of this form or its multi-dimensional
extensions, [87].
With this in hand, let us return to the physical context of the nonlinear transport
equation. By deﬁnition, a shock is a discontinuity in the solution u(t, x). We will make
the physically plausible assumption that mass (or vehicle) conservation continues to hold
even within the shock. Recall that the total mass, which at time t is the area† under
the curve u(t, x), must be conserved. This continues to hold even when themathematical
solution becomes multiply valued, in which case one employs a line integral

u dx, where
C

C represents the graph of the solution, to compute the mass/area. Thus, to construct a
discontinuous shock solution with the same mass, one replaces part of the multiply valued
†
We are implicitly assuming that the mass is ﬁnite, as in (2.45), although the overall construction does not rely on this restriction.

40

2 Linear and Nonlinear Waves

u
a

b
x
bt
Figure 2.17.

Multiply valued step wave.

at


graph by a vertical shock line in such a way that the resulting function is single-valued and
has the same area under its graph. Referring to Figure 2.16, observe that the region under
the shock graph is obtained from that under the multi-valued solution graph by deleting
the upper shaded lobe and appending the lower shaded lobe. Thus the resulting area will
be the same, provided the shock line is drawn so that the areas of the two shaded lobes are
equal. This construction is known as the Equal Area Rule; it ensures that the total mass
of the shock solution matches that of the multiply valued solution, which in turn is equal
to the initial mass, as required by the physical conservation law.
Example 2.9. An illuminating special case occurs when the initial data has the form
of a step function with a single discontinuity at the origin:

a,
x < 0,
(2.50)
u(0, x) =
b,
x > 0.
If a > b, then the initial data is already in the form of a shock wave. For t > 0, the
mathematical solution constructed by continuing along the characteristic lines is multiply
valued in the region b t < x < a t, where it assumes both values a and b; see Figure 2.17.
Moreover, the initial vertical line of discontinuity has become a tilted line, because each
point (0, u) on it has moved along the associated characteristic a distance u t. The Equal
Area Rule tells us to draw the shock line halfway along, at x = 21 (a + b) t, in order that the
two triangles have the same area. We deduce that the shock moves with speed c = 12 (a+b),
equal to the average of the two speeds at the jump. The resulting shock-wave solution is

a,
x < c t,
a+b
u(t, x) =
.
(2.51)
where
c=
2
b,
x > c t,
A plot of its characteristic lines appears in Figure 2.18. Observe that colliding pairs of
characteristic lines terminate at the shock line, whose slope is the average of their individual
slopes.
The fact that the shock speed equals the average of the solution values on either side
is, in fact, of general validity, and is known as the Rankine–Hugoniot condition, named after the nineteenth-century Scottish physicist William Rankine and French engineer Pierre
Hugoniot, although historically these conditions ﬁrst appeared in a 1849 paper by George
Stokes, [109]. However, intimidated by criticism by his contemporary applied mathematicians Lords Kelvin and Rayleigh, Stokes thought he was mistaken, and even ended up

2.3 Nonlinear Transport and Shocks

41

x

t
Figure 2.18.

Characteristic lines for the step wave shock.

deleting the relevant part when his collected works were published in 1883, [110]. The
missing section was restored in the 1966 reissue, [111].
Proposition 2.10. Let u(t, x) be a solution to the nonlinear transport equation that
has a discontinuity at position x = σ(t), with ﬁnite, unequal left- and right-hand limits
u− (t) = u t, σ(t)− =

lim

x → σ(t)−

u(t, x),

u+ (t) = u t, σ(t)+ =

lim

x → σ(t)+

u(t, x), (2.52)

on either side of the shock discontinuity. Then, to maintain conservation of mass, the speed
of the shock must equal the average of the solution values on either side:
u− (t) + u+ (t)
dσ
=
.
dt
2

(2.53)

Proof : Referring to Figure 2.19, consider a small time interval, from t to t + Δt,
with Δt > 0. During this time, the shock moves from position a = σ(t) to position
b = σ(t + Δt). The total mass contained in the interval [ a, b ] at time t, before the shock
has passed through, is
 b
M (t) =



u(t, x) dx ≈ u+ (t) (b − a) = u+ (t) σ(t + Δt) − σ(t) ,

a

where we assume that Δt  1 is very small, and so the integrand is well approximated by
its limiting value (2.52). Similarly, after the shock has passed, the total mass remaining in
the interval is
 b
M (t + Δt) =
a



u(t + Δt, x) dx ≈ u− (t + Δt) (b − a) = u− (t + Δt) σ(t + Δt) − σ(t) .

42

2 Linear and Nonlinear Waves

x

b = σ(t + Δt)
u+
u−
a = σ(t)

t
t
Figure 2.19.

t + Δt

Conservation of mass near a shock.

Thus, the rate of change in mass across the shock at time t is given by
M (t + Δt) − M (t)
dM
= lim
Δt → 0
dt
Δt
 σ(t + Δt) − σ(t)  −
 dσ
 −
= u (t) − u+ (t)
.
= lim u (t + Δt) − u+ (t)
Δt → 0
Δt
dt
On the other hand, at any t < τ < t + Δt, the mass ﬂux into the interval [ a, b ] through
the endpoints is given by the right-hand side of (2.44):




2
2
1
−→ 12 u− (t)2 − u+ (t)2 ,
since τ → t as Δt → 0.
2 u(τ, a) − u(τ, b)
Conservation of mass requires that the rate of change in mass be equal to the mass ﬂux:

 dσ


dM
= u− (t) − u+ (t)
= 12 u− (t)2 − u+ (t)2 .
dt
dt
Solving for dσ/dt establishes (2.53).
Q.E.D.
Example 2.11. By way of contrast, let us investigate the case when the initial data
is a step function (2.50), but with a < b, so the jump goes upwards. In this case, the
characteristic lines diverge from the initial discontinuity, and the mathematical solution is
not speciﬁed at all in the wedge-shaped region a t < x < b t. Our task is to decide how to
“ﬁll in” the solution values between the two regions where the solution is well deﬁned and
constant.
One possible connection is by a straight line. Indeed, a simple modiﬁcation of the
rational solution (2.36) produces the similarity solution †
x
u(t, x) = ,
t
†

See Section 8.2 for general techniques for constructing similarity (scale-invariant) solutions
to partial diﬀerential equations.

2.3 Nonlinear Transport and Shocks

Figure 2.20.

43

Rarefaction wave.



which not only solves the diﬀerential equation, but also has the required values u(t, a t) = a
and u(t, b t) = b at the two edges of the wedge. This can be used to construct the piecewise
aﬃne rarefaction wave
⎧
x ≤ a t,
⎨ a,
x/t,
a t ≤ x ≤ b t,
u(t, x) =
(2.54)
⎩
b,
x ≥ b t,
which is graphed at four representative times in Figure 2.20.
A second possibility would be to continue the discontinuity as a shock wave, whose
speed is governed by the Rankine-Hugoniot condition, leading to a discontinuous solution
having the same formula as (2.51). Which of the two competing solutions should we
use? The ﬁrst, (2.54), makes better physical sense; indeed, if we were to smooth out the
discontinuity, then the resulting solutions would converge to the rarefaction wave and not
the reverse shock wave; see Exercise 2.3.13. Moreover, the discontinuous solution (2.51)
has characteristic lines emanating from the discontinuity, which means that the shock is
creating new values for the solution as it moves along, and this can, in fact, be done in a
variety of ways. In other words, the discontinuous solution violates causality, meaning that
the solution proﬁle at any given time uniquely prescribes its subsequent motion. Causality
requires that, while characteristics may terminate at a shock discontinuity, they cannot
begin there, because their slopes will not be uniquely prescribed by the shock proﬁle, and
hence the characteristics to the left of the shock must have larger slope (or speed), while
those to the right must have smaller slope. Since the shock speed is the average of the two
characteristic slopes, this requires the Entropy Condition
dσ
u− (t) + u+ (t)
(2.55)
=
> u+ (t).
dt
2
With further analysis, it can be shown, [57], that the rarefaction wave (2.54) is the unique
solution† to the initial value problem satisfying the entropy condition (2.55).
u− (t) >

†

Albeit not a classical solution, but rather a weak solution, as per Section 10.4.

44

2 Linear and Nonlinear Waves

u
(1 + t, 1)

x
1
Figure 2.21.

σ(t)

Equal Area Rule for the triangular wave.



These prototypical solutions epitomize the basic phenomena modeled by the nonlinear
transport equation: rarefaction waves, which emanate from regions where the initial data
satisﬁes f  (x) > 0, causing the solution to spread out as time progresses, and compression
waves, emanting from regions where f  (x) < 0, causing the solution to progressively steepen
and eventually break into a shock discontinuity. Anyone caught in a traﬃc jam recognizes
the compression waves, where the vehicles are bunched together and almost stationary,
while the interspersed rarefaction waves correspond to freely moving traﬃc. (An intelligent
driver will take advantage of the rarefaction waves moving backwards through the jam
to switch lanes!) The familiar, frustrating traﬃc jam phenomenon, even on accident- or
construction-free stretches of highway, is, thus, an intrinsic eﬀect of the nonlinear transport
models that govern traﬃc ﬂow, [122].
Example 2.12. Triangular wave: Suppose the initial data has the triangular proﬁle

x,
0 ≤ x ≤ 1,
u(0, x) = f (x) =
0,
otherwise,
as in the ﬁrst graph in Figure 2.22. The initial discontinuity at x = 1 will propagate as a
shock wave, while the slanted line behaves as a rarefaction wave. To ﬁnd the proﬁle at time
t, we ﬁrst graph the multi-valued solution obtained by moving each point on the graph of
f to the right an amount equal to t times its height. As noted above, this motion preserves
straight lines. Thus, points on the x–axis remain ﬁxed, and the diagonal line now goes
from (0, 0) to (1 + t, 1), which is where the uppermost point (1, 1) on the graph of f has
moved to, and hence has slope (1 + t)−1 , while the initial vertical shock line has become
tilted, going from (1, 0) to (0, 1 + t). We now need to ﬁnd the position σ(t) of the shock
line in order to satisfy the Equal Area Rule, namely so that the areas of the two shaded
regions in Figure 2.21 are identical. The reader is invited to determine this geometrically;
instead, we invoke the Rankine–Hugoniot condition (2.53). At the shock line, x = σ(t),
the left- and right-hand limiting values are, respectively,
u− (t) = u t, σ(t)− =

σ(t)
,
1+t

u+ (t) = u t, σ(t)+ = 0,

and hence (2.53) prescribes the shock speed to be


dσ
1 σ(t)
σ(t)
=
+0 =
.
dt
2 1+t
2 (1 + t)

2.3 Nonlinear Transport and Shocks

45

t=0

t=1
Figure 2.22.

Triangular-wave solution.

t=2



x

t

Figure 2.23.

Characteristic lines for the triangular-wave shock.

The solution to the resulting separable ordinary diﬀerential equation is easily found. Since
the shock starts out at σ(0) = 1, we deduce that
σ(t) =

√

1 + t,

with

dσ
1
.
= √
dt
2 1+t

Further, the strength of the shock, namely its height, is
u− (t) =

σ(t)
1
.
=√
1+t
1+t

We conclude that, as t increases, the solution remains a triangular wave, of steadily decreasing slope, while the shock moves oﬀ to x = + ∞ at a progressively slower speed and smaller
height. Its position follows a parabolic trajectory in the (t, x)–plane. See Figure 2.22 for
representative plots of the triangular wave solution, while Figure 2.23 illustrates the characteristic lines and shock wave trajectory.
In more general situations, continuing on after the initial shock formation, other characteristic lines may start to cross, thereby producing new shocks. The shocks themselves
continue to propagate, often at diﬀerent velocities. When a fast-moving shock catches up

46

2 Linear and Nonlinear Waves

with a slow-moving shock, one must then decide how to merge the shocks so as to retain a
physically meaningful solution. The Rankine–Hugoniot (Equal Area) and Entropy Conditions continue to uniquely specify the dynamics. However, at this point, the mathematical
details have become too intricate for us to pursue any further, and we refer the interested
reader to Whitham’s book, [122]. See also [57] for a proof of the following existence
theorem for shock-wave solutions to the nonlinear transport equation.
Theorem 2.13. If the initial data u(0, x) = f (x) is piecewise † C1 with ﬁnitely many
jump discontinuities, then, for t > 0, there exists a unique (weak) solution to the nonlinear
transport equation (2.31) that also satisﬁes the Rankine–Hugoniot condition (2.53) and
the entropy condition (2.55).
Remark : Our derivation of the Rankine–Hugoniot shock speed condition (2.53) relied
on the fact that we can write the original partial diﬀerential equation in the form of a
conservation law. But there are, in fact, other ways to do this. For instance, multiplying the
nonlinear transport equation (2.31) by u allows us write it in the alternative conservative
form
∂u
∂ 1 2
∂u
∂ 1 3
u
+ u2
=
u +
u = 0.
(2.56)
∂t
∂x
∂t 2
∂x 3
In this formulation, the conserved density is T = 12 u2 , and the associated ﬂux is X = 13 u3 .
The integrated form (2.49) of the conservation law (2.56) is



d b1
u(t, x)2 dx = 13 u(t, a)3 − u(t, b)3 .
(2.57)
2
dt a
In some physical models, the integral on the left-hand side represents the energy within the
interval [ a, b ], and the conservation law tells us that energy can enter the interval as a ﬂux
only through its ends. If we assume that energy is conserved at a shock, then, repeating
our previous argument, we are led to the alternative equation
 − 3

1
+
3
2 u− (t)2 + u− (t) u+ (t) + u+ (t)2
dσ
3  u (t) − u (t) 
=
= 1 − 2
(2.58)
+
2
dt
3
u− (t) + u+ (t)
2 u (t) − u (t)
for the shock speed. Thus, a shock that conserves energy moves at a diﬀerent speed from
one that conserves mass! The evolution of a shock wave depends not just on the underlying
diﬀerential equation, but also on the physical assumptions governing the selection of a
suitable conservation law.

More General Wave Speeds
Let us ﬁnish this section by considering a nonlinear transport equation
ut + c(u) ux = 0,

(2.59)

whose wave speed is a more general function of the disturbance u. (Further extensions,
allowing c to depend also on t and x, are discussed in Exercise 2.3.20.) Most of the
†
Meaning continuous everywhere, and continuously diﬀerentiable except at a discrete set of
points; see Deﬁnition 3.7 below for the precise deﬁnition.

2.3 Nonlinear Transport and Shocks

47

development is directly parallel to the special case (2.31) discussed above, and so the
details are left for the reader to ﬁll in, although the shock dynamics does require some
care.
In this case, the characteristic curve equation is
dx
= c u(t, x) .
(2.60)
dt
As before, the solution u is constant on characteristics, and hence the characteristics are
straight lines, now with slope c(u). Thus, to solve the initial value problem
u(0, x) = f (x),

(2.61)

through each point (0, y) on the x–axis, one draws the characteristic line of slope c(u(0, y)) =
c(f (y)). Until the onset of a shock discontinuity, the solution maintains its initial value
u(0, y) = f (y) along the characteristic line.
A shock forms whenever two characteristic lines cross. As before, the mathematical
equation no longer uniquely speciﬁes the subsequent dynamics, and we need to appeal to
an appropriate conservation law. We write the transport equation in the form

∂
∂u
+
C(u) = 0,
where
C(u) = c(u) du
(2.62)
∂t
∂x
is any convenient anti-derivative of the wave speed. Thus, following the same computation
as in (2.44), we discover that conservation of mass now takes the integrated form

d b
(2.63)
u(t, x) dx = C(u(t, a)) − C(u(t, b)),
dt a
with C(u) playing the role of the mass ﬂux. Requiring the conservation of mass, i.e., of
the area under the graph of the solution, means that the Equal Area Rule remains valid.
However, the Rankine–Hugoniot shock-speed condition must be modiﬁed in accordance
with the new dynamics. Mimicking the preceding argument, but with the modiﬁed mass
ﬂux, we ﬁnd that the shock speed is now given by
dσ
C(u− (t)) − C(u+ (t))
=
.
dt
u− (t) − u+ (t)

(2.64)



Note that if
c(u) = u,

then

C(u) =

u du = 12 u2 ,

and so (2.64) reduces to our earlier formula (2.53). Moreover, in the limit as the shock
magnitude approaches zero, u− (t) − u+ (t) → 0, the right-hand side of (2.64) converges to
the derivative C  (u) = c(u) and hence recovers the wave speed, as it should.

Exercises
2.3.1. Discuss the behavior of the solution to the nonlinear transport equation (2.31) for the
following initial data:



2, x < −1,
−2, x < −1,
1,
x < 1,
(b) u(0, x) =
(c) u(0, x) =
(a) u(0, x) =
1, x > −1;
1,
x > −1;
−2, x > 1.

48

2 Linear and Nonlinear Waves


2.3.2. Solve the following initial value problems:


(b) ut − u ux = 0, u(1, x) =

−1,
3,

x < 0,
x > 0;

(a) ut + 3 u ux = 0, u(0, x) =


(c) ut − 2 u ux = 0, u(0, x) =

2,
0,

x < 1,
x > 1;

1,
0,

x < 1,
x > 1.

2.3.3. Let u(0, x) = (x2 + 1)−1 . Does the resulting solution to the nonlinear transport equation
(2.31) produce a shock wave? If so, ﬁnd the time of onset of the shock, and sketch a graph
of the solution just before and soon after the shock wave. If not, explain what happens to
the solution as t increases.
2.3.4. Solve Exercise 2.3.3 when u(0, x) =

(a) − (x2 + 1)−1 ,

(b) x(x2 + 1)−1 .
2

2.3.5. Consider the initial value problem ut − 2 u ux = 0, u(0, x) = e− x . Does the resulting
solution produce a shock wave? If so, ﬁnd the time of onset of the shock and the position
at which it ﬁrst forms. If not, explain what happens to the solution as t increases.
αx + β
2.3.6. (a) For what values of α, β, γ, δ is u(t, x) =
a solution to (2.31)?
γt +δ
λt + αx + β
a solution to (2.31)?
(b) For what values of α, β, γ, δ, λ, μ is u(t, x) =
γ t + μx + δ
2.3.7. A triangular wave is a 
shock-wave solution to the initial value problem for (2.31) that
m x, 0 ≤ x ≤ ,
Assuming m > 0, write down a formula for
has initial data u(0, x) =
0,
otherwise.
the triangular-wave solution at times t > 0. Discuss what happens to the triangular wave as
time progresses.
2.3.8. Solve Exercise 2.3.7 when m < 0.
2.3.9. Solve (2.31) for t > 0 subject to the following initial conditions, and graph your solution
at some representative
times. In what sense does your solution
conserve mass?


1, 0 < x < 1,
x, −1 < x < 1,
(b) u(0, x) =
(a) u(0, x) =
0, otherwise,
0, otherwise,


(c) u(0, x) =

− x,
0,

−1 < x < 1,
otherwise,



(d) u(0, x) =

1 − | x |,
0,

−1 < x < 1,
otherwise.

2.3.10. An N–wave isa solution to the nonlinear transport equation (2.31) that has initial conm x, − ≤ x ≤ ,
where m > 0. (a) Write down a formula for the
ditions u(0, x) =
0,
otherwise,
N–wave solution at times t > 0. (b) What about when m < 0?
 (t, x) are two solutions to the nonlinear transport equation (2.31)
♦ 2.3.11. Suppose u(t, x) and u
 (t , x) for all x. Do the solutions necsuch that, for some t > 0, they agree: u(t , x) = u

 (0, x)? Use your answer to discuss the
essarily have the same initial conditions: u(0, x) = u
uniqueness of solutions to the nonlinear transport equation.

2.3.12. Suppose that x1 < x2 are such that the characteristic lines of (2.31) through (0, x1 )
and (0, x2 ) cross at a shock at (t, σ(t)) and, moreover, the left- and right-hand shock values
(2.52) are f (x1 ) = u− (t), f (x1 ) = u+ (t). Explain why the signed area of the region between
the graph of f (x) and the secant line connecting (x1 , f (x1 )) to (x2 , f (x2 )) is zero.
♦ 2.3.13. Consider the initial value problem uε (0, x) = 2 + tan−1 (x/ε) for the nonlinear transport equation (2.31). (a) Show that, as ε → 0+ , the initial condition converges to a step
function (2.51). What are the values of a, b? (b) Show that, moreover, the resulting solution uε (0, x) to the nonlinear transport equation converges to the corresponding rarefaction
wave (2.54) resulting from the limiting initial condition.

2.4 The Wave Equation: d’Alembert’s Formula

49

♦ 2.3.14. (a) Under what conditions can equation (2.35) be solved for a single-valued function
u(t, x)? Hint: Use the Implicit Function Theorem. (b) Use implicit diﬀerentiation to prove
that the resulting function u(t, x) is a solution to the nonlinear transport equation.


2.3.15. For what values of α, β, γ, δ, k is u(t, x) =
tion ut + u2 ux = 0?



αx + β k
a solution to the transport equaγt + δ

2.3.16. (a) Solve the initial value problem ut + u2 ux = 0, u(0, x) = f (x), by the method of
characteristics. (b) Discuss the behavior of solutions and compare/contrast with (2.31).
2.3.17. (a) Determine the Rankine–Hugoniot condition, based on conservation of mass, for the
speed of a 
shock for the equation ut + u2 ux = 0. (b) Solve the initial value problem
a, x < 0,
u(0, x) =
when (i ) | a | > | b |, (ii ) | a | < | b |. Hint: Use Exercise 2.3.15
b, x > 0,
to determine the shape of a rarefaction wave.
2.3.18. Solve Exercise 2.3.17 when the wave speed c(u) = (i ) 1 − 2 u, (ii ) u3 , (iii ) sin u.
♦ 2.3.19. Justify the shock-speed formula (2.58).
♦ 2.3.20. Consider the general quasilinear ﬁrst-order partial diﬀerential equation
∂u
∂u
+ c(t, x, u)
= h(t, x, u).
∂t
∂x
Let us deﬁne a lifted characteristic curve to be a solution (t, x(t), u(t)) to the system of ordx
du
= c(t, x, u),
= h(t, x, u). The corresponding characdinary diﬀerential equations
dt
dt


teristic curve t, x(t) is obtained by projecting to the (t, x)–plane. Prove that if u(t, x) is a
solution to the partial diﬀerential equation, and u(t0 , x0 ) = u0 , then the lifted characteristic
curve passing through (t0 , x0 , u0 ) lies on the graph of u(t, x). Conclude that the graph of
of all lifted characteristhe solution to the initial value problem u(t0 , x) = f (x) is the union

tic curves passing through the initial data points t0 , x0 , f (x0 ) .
2.3.21. Let a > 0. (a) Apply the method of Exercise 2.3.20 to solve the initial value problem
for the damped transport equation: ut + u ux + a u = 0, u(0, x) = f (x).
(b) Does the damping eliminate shocks?
2.3.22. Apply the method of Exercise 2.3.20 to solve the initial value problem
1
ut + t ux = u2 ,
u(0, x) =
.
1 + x2

2.4 The Wave Equation: d’Alembert’s Formula
Newton’s Second Law states that force equals mass times acceleration. It forms the bedrock
underlying the derivation of mathematical models describing all of classical dynamics.
When applied to a one-dimensional medium, such as the transverse displacements of a
violin string or the longitudinal motions of an elastic bar, the resulting model governing
small vibrations is the second-order partial diﬀerential equation


∂2u
∂
∂u
(2.65)
ρ(x) 2 =
κ(x)
.
∂t
∂x
∂x
Here u(t, x) represents the displacement of the string or bar at time t and position x,
while ρ(x) > 0 denotes its density and κ(x) > 0 its stiﬀness or tension, both of which are

50

2 Linear and Nonlinear Waves

assumed not to vary with t. The right-hand side of the equation represents the restoring
force due to a (small) displacement of the medium from its equilibrium, whereas the lefthand side is the product of mass per unit length and acceleration. A correct derivation of
the model from ﬁrst principles would require a signiﬁcant detour, and we refer the reader
to [120, 124] for the details.
We will simplify the general model by assuming that the underlying medium is uniform, and so both its density ρ and stiﬀness κ are constant. Then (2.65) reduces to the
one-dimensional wave equation

2
∂2u
κ
2 ∂ u
=
c
,
where
the
constant
c
=
> 0
(2.66)
ρ
∂t2
∂x2
is known as the wave speed , for reasons that will soon become apparent.
In general, to uniquely specify the solution to any dynamical system arising from
Newton’s Second Law, including the wave equation (2.66) and the more general vibration
equation (2.65), one must ﬁx both its initial position and initial velocity. Thus, the initial
conditions take the form
∂u
(2.67)
(0, x) = g(x),
∂t
where, for simplicity, we set the initial time t0 = 0. (See also Exercise 2.4.6.) The initial
value problem seeks the corresponding C2 function u(t, x) that solves the wave equation
(2.66) and has the required initial values (2.67). In this section, we will learn how to
solve the initial value problem on the entire line − ∞ < x < ∞. The analysis of the
wave equation on bounded intervals will be deferred until Chapters 4 and 7. The twoand three-dimensional versions of the wave equation are treated in Chapters 11 and 12,
respectively.
u(0, x) = f (x),

d’Alembert’s Solution
Let us now derive the explicit solution formula for the second-order wave equation (2.66)
ﬁrst found by d’Alembert. The starting point is to write the partial diﬀerential equation
in the suggestive form
 u = (∂t − c ∂x ) u = utt − c uxx = 0.
2

2

2

2

(2.68)

Here
 = ∂t − c ∂x
2

2

2

is a common mathematical notation for the wave operator , which is a linear second-order
partial diﬀerential operator. In analogy with the elementary polynomial factorization
t2 − c2 x2 = (t − c x)(t + c x),
we can factor the wave operator into a product of two ﬁrst-order partial diﬀerential operators:†
2
2 2
(2.69)
 = ∂t − c ∂x = (∂t − c ∂x ) (∂t + c ∂x ).
†
The cross terms cancel, thanks to the equality of mixed partial derivatives: ∂t ∂x u = ∂x ∂t u.
Constancy of the wave speed c is essential here.

2.4 The Wave Equation: d’Alembert’s Formula

51

Now, if the second factor annihilates the function u(t, x), meaning
(∂t + c ∂x ) u = ut + c ux = 0,

(2.70)

then u is automatically a solution to the wave equation, since
 u = (∂t − c ∂x ) (∂t + c ∂x ) u = (∂t − c ∂x ) 0 = 0.
We recognize (2.70) as the ﬁrst-order transport equation (2.4) with constant wave speed c.
Proposition 2.1 tells us that its solutions are traveling waves with wave speed c :
u(t, x) = p(ξ) = p(x − c t),

(2.71)

where p is an arbitrary function of the characteristic variable ξ = x − c t. As long as
p ∈ C2 (i.e., is twice continuously diﬀerentiable), the resulting function u(t, x) is a classical
solution to the wave equation (2.66), as you can easily check.
Now, the factorization (2.69) can equally well be written in the reverse order:
 = ∂t − c ∂x = (∂t + c ∂x ) (∂t − c ∂x ).
2

2

2

(2.72)

The same argument tells us that any solution to the “backwards” transport equation
ut − c ux = 0,

(2.73)

with constant wave speed − c, also provides a solution to the wave equation. Again, by
Proposition 2.1, with c replaced by − c, the general solution to (2.73) has the form
u(t, x) = q(η) = q(x + c t),

(2.74)

where q is an arbitrary function of the alternative characteristic variable η = x + c t. The
solutions (2.74) represent traveling waves moving to the left with constant speed c > 0.
Provided q ∈ C2 , the functions (2.74) will provide a second family of solutions to the wave
equation.
We conclude that, unlike ﬁrst-order transport equations, the wave equation (2.68)
is bidirectional in that it admits both left and right traveling-wave solutions. Moreover,
by linearity the sum of any two solutions is again a solution, and so we can immediately
construct solutions that are superpositions of left and right traveling waves. The remarkable
fact is that every solution to the wave equation can be so represented.
Theorem 2.14.
superposition,

Every solution to the wave equation (2.66) can be written as a
u(t, x) = p(ξ) + q(η) = p(x − c t) + q(x + c t),

(2.75)

of right and left traveling waves. Here p(ξ) and q(η) are arbitrary C2 functions, each
depending on its respective characteristic variable
ξ = x − c t,

η = x + c t.

(2.76)

Proof : As in our treatment of the transport equation, we will simplify the wave equation through an inspired change of variables. In this case, the new independent variables
are the characteristic variables ξ, η deﬁned by (2.76). We set


η−ξ η+ξ
u(t, x) = v(x − c t, x + c t) = v(ξ, η),
whereby
v(ξ, η) = u
,
. (2.77)
2c
2

52

2 Linear and Nonlinear Waves

Then, employing the chain rule to compute the partial derivatives,


∂v
∂v
∂u
∂v
∂v
∂u
=c −
+
,
=
+
,
∂t
∂ξ
∂η
∂x
∂ξ
∂η

(2.78)

and, further,
∂2u
= c2
∂t2



∂2v
∂2v
∂2v
+ 2
−2
2
∂ξ
∂ξ ∂η
∂η

Therefore
u =



∂2v
∂2v
∂2v
∂2u
+ 2.
= 2 +2
2
∂x
∂ξ
∂ξ ∂η
∂η

,

2
2
∂2u
2 ∂ u
2 ∂ v
.
−
c
=
−
4
c
∂t2
∂x2
∂ξ ∂η

(2.79)

We conclude that u(t, x) solves the wave equation  u = 0 if and only if v(ξ, η) solves the
second-order partial diﬀerential equation
∂2v
= 0,
∂ξ ∂η
which we write in the form


∂
∂v
∂w
=
= 0,
∂ξ ∂η
∂ξ

where

w=

∂v
.
∂η

Thus, applying the methods of Section 2.1 (and making the appropriate assumptions on
the domain of deﬁnition of w), we deduce that
w=

∂v
= r(η),
∂η

where r is an arbitrary function of the characteristic variable η. Integrating both sides of
the latter partial diﬀerential equation with respect to η, we ﬁnd

v(ξ, η) = p(ξ) + q(η),
where
q(η) = r(η) dη,
while p(ξ) represents the η integration “constant”. Replacing the characteristic variables
by their formulas in terms of t and x completes the proof.
Q.E.D.
Let us see how the solution formula (2.75) can be used to solve the initial value problem
(2.67). Substituting into the initial conditions, we deduce that
∂u
(2.80)
(0, x) = − c p (x) + c q  (x) = g(x).
∂t
To solve this pair of equations for the functions p and q, we diﬀerentiate the ﬁrst,
u(0, x) = p(x) + q(x) = f (x),

p (x) + q  (x) = f  (x),
and then subtract oﬀ the second equation divided by c; the result is
2 p (x) = f  (x) −
Therefore,
1
1
p(x) = f (x) −
2
2c

1
g(x).
c

 x
g(z) dz + a,
0

2.4 The Wave Equation: d’Alembert’s Formula

t=0

t=1

t=3

t=4
Figure 2.24.

53

t=2

Splitting of waves.



t=5

where a is an integration constant. The ﬁrst equation in (2.80) then yields
 x
1
1
g(z) dz − a.
q(x) = f (x) − p(x) = f (x) +
2
2c 0
Substituting these two expressions back into our solution formula (2.75), we obtain
 ξ
 η
1
f (ξ) + f (η)
1
−
u(t, x)= p(ξ) + q(η) =
g(z) dz +
g(z) dz
2
2c 0
2c 0
 η
1
f (ξ) + f (η)
+
g(z) dz,
=
2
2c ξ
where ξ, η are the characteristic variables (2.76). In this manner, we have arrived at
d’Alembert’s solution to the initial value problem for the wave equation on the real line.
Theorem 2.15. The solution to the initial value problem
∂2u
∂2u
= c2 2 ,
2
∂t
∂x

u(0, x) = f (x),

∂u
(0, x) = g(x),
∂t

is given by
u(t, x) =

1
f (x − c t) + f (x + c t)
+
2
2c

−∞ < x < ∞,

(2.81)

g(z) dz.

(2.82)

 x+c t
x−c t

Remark : In order that (2.82) deﬁne a classical solution to the wave equation, we
need f ∈ C2 and g ∈ C1 . However, the formula itself makes sense for more general
initial conditions. We will continue to treat the resulting functions as solutions, albeit
nonclassical, since they ﬁt under the more general rubric of “weak solution”, to be developed
in Section 10.4.
Example 2.16. Suppose there is no initial velocity, so g(x) ≡ 0, and hence the
motion is purely the result of the initial displacement u(0, x) = f (x). In this case, (2.82)
reduces to
u(t, x) = 12 f (x − c t) + 12 f (x + c t).
(2.83)

54

2 Linear and Nonlinear Waves

t=0

t=1

t=3

t=4

t=2

Interaction of waves.

Figure 2.25.



t=5

The eﬀect is that the initial displacement splits into two waves, one moving to the right
and the other moving to the left, each of constant speed c, and each of exactly the same
shape as f (x), but only half as tall. For example, if the initial displacement is a localized
pulse centered at the origin, say
∂u
(0, x) = 0,
∂t

2

u(0, x) = e− x ,
then the solution

2

2

u(t, x) = 12 e− (x−c t) + 12 e− (x+c t)

consists of two half size pulses running away from the origin with the same speed c, but
in opposite directions. A graph of the solution at several successive times can be seen in
Figure 2.24.
If we take two initially separated pulses, say
2

2

u(0, x) = e− x + 2 e− (x−1) ,

∂u
(0, x) = 0,
∂t

centered at x = 0 and x = 1, then the solution
2

2

2

2

u(t, x) = 12 e− (x−c t) + e− (x−1−c t) + 12 e− (x+c t) + e− (x−1+c t)

will consist of four pulses, two moving to the right and two to the left, all with the same
speed. An important observation is that when a right-moving pulse collides with a leftmoving pulse, they emerge from the collision unchanged, which is a consequence of the
inherent linearity of the wave equation. In Figure 2.25, the ﬁrst picture plots the initial
displacement. In the second and third pictures, the two localized bumps have each split into
two copies moving in opposite directions. In the fourth and ﬁfth, the larger right-moving
bump is in the process of interacting with the smaller left-moving bump. Finally, in the
last picture the interaction is complete, and the individual pairs of left- and right-moving
waves move oﬀ in tandem in opposing directions, experiencing no further collisions.
In general, if the initial displacement is localized, so that | f (x) |  1 for | x |  0, then,
after a ﬁnite time, the left- and right-moving waves will separate, and the observer will see
two half-size replicas running away, with speed c, in opposite directions. If the displacement

2.4 The Wave Equation: d’Alembert’s Formula

55

1

−2

−1

1

2

−1
Figure 2.26.

The error function erf x.

is not localized, then the left and right traveling waves will never fully disengage, and one
might be hard pressed to recognize that a complicated solution pattern is, in reality, just
the superposition of two simple traveling waves. For example, consider the elementary
trigonometric solution

cos c t cos x = 12 cos(x − c t) + 12 cos(x + c t).
(2.84)
In accordance with the left-hand expression, an observer will see a standing cosinusoidal
wave that vibrates up and down with frequency c. However, the d’Alembert form of the
solution on the right-hand side says that this is just the sum of left- and right-traveling
cosine waves! The interactions of their peaks and troughs reproduce the standing wave.
Thus, the same solution can be interpreted in two seemingly incompatible ways. And,
in fact, this paradox lies at the heart of the perplexing wave-particle duality of quantum
physics.
Example 2.17. By way of contrast, suppose there is no initial displacement, so
f (x) ≡ 0, and the motion is purely the result of the initial velocity ut (0, x) = g(x).
Physically, this models a violin string at rest being struck by a “hammer blow” at the
initial time. In this case, the d’Alembert formula (2.82) reduces to
 x+c t
1
(2.85)
u(t, x) =
g(z) dz.
2 c x−c t
2

For example, when u(0, x) = 0, ut (0, x) = e− x , the resulting solution (2.85) is
√
 x+c t

1
π
− x2
erf(x + c t) − erf(x − c t) ,
u(t, x) =
e
dz =
2 c x−c t
4c
where

2
erf x = √
π

 x

2

e− z dz

(2.86)

(2.87)

0

is known as the error function due to its many applications throughout probability and
statistics, [39]. The error function integral cannot be written in terms of elementary
functions; nevertheless, its properties have been well studied and its values tabulated,
[86]. A graph appears in Figure 2.26. The constant in front of the integral (2.87) has been
chosen so that the error function has asymptotic values
lim erf x = 1,

x→∞

lim

x → −∞

erf x = −1,

(2.88)

56

2 Linear and Nonlinear Waves

t=0

t=1

t=3

t=4

Figure 2.27.

t=2

t=5

Error function solution to the wave equation.

which follow from a well-known integration formula to be derived in Exercise 2.4.21.
A graph of the solution (2.86) at successive times is displayed in Figure 2.27. The
ﬁrst graph shows the zero initial displacement. Gradually, the eﬀect of the initial hammer
blow is felt further and further away along the string, as the two wave fronts propagate
away from the origin, both with speed c, but in opposite directions. Thus, unlike the case
of a nonzero initial displacement in Figure 2.24, where the solution eventually returns to
its equilibrium position u = 0 after the wave passes by, a nonzero initial velocity leaves the
string permanently deformed.
In general, the lines of slope ± c, where the respective characteristic variables are
constant,
ξ = x − c t = a,
η = x + c t = b,
(2.89)
are known as the characteristics of the wave equation. Thus, the second-order wave equation has two distinct characteristic lines passing through each point in the (t, x)–plane.
Remark : The characteristic lines are the one-dimensional counterparts of the light
cone in Minkowski space-time, which plays a starring role in special relativity, [70, 75].
See Section 12.5 for further details.
In Figure 2.28, we plot the two characteristics going through a point (0, y) on the x
axis. The wedge-shaped region { y − c t ≤ x ≤ y + c t, t ≥ 0 } lying between them is known
as the domain of inﬂuence of the point (0, y), since, in general, the value of the initial data
at a point will aﬀect the subsequent solution values only in its domain of inﬂuence. Indeed,
the eﬀect of an initial displacement at the point y propagates along the two characteristic
lines, while the eﬀect of an initial velocity there will be felt at every point in the triangular
wedge.
External Forcing and Resonance
When a homogeneous vibrating medium is subjected to external forcing, the wave equation
acquires an additional, inhomogeneous term:
∂2u
∂2u
= c2 2 + F (t, x),
2
∂t
∂x

(2.90)

2.4 The Wave Equation: d’Alembert’s Formula

57

x

(0, y)

t

Figure 2.28.

Characteristic lines and domain of inﬂuence.

in which F (t, x) represents a force imposed at time t and spatial position x. With a bit
more work, d’Alembert’s solution technique can be readily adapted to incorporate the
forcing term.
Let us, for simplicity, assume that the diﬀerential equation is supplemented by homogeneous initial conditions,
u(0, x) = 0,

ut (0, x) = 0,

(2.91)

meaning that there is no initial displacement or velocity. To solve the initial value problem
(2.90–91), we switch to the same characteristic coordinates (2.76), setting


η−ξ η+ξ
,
.
v(ξ, η) = u
2c
2
Invoking the chain rule formulas (2.79), we ﬁnd that the forced equation (2.90) becomes


1
η−ξ η+ξ
∂2v
=− 2 F
,
.
(2.92)
∂ξ ∂η
4c
2c
2
Let us integrate both sides of the equation with respect to η, on the interval ξ ≤ ζ ≤ η:

 η 
∂v
∂v
1
ζ −ξ ζ +ξ
(ξ, η) −
(ξ, ξ) = − 2
,
dζ.
(2.93)
F
∂ξ
∂ξ
4c
2c
2
ξ
But, recalling (2.78),
∂v
1 ∂u
(ξ, η) = −
∂ξ
2 c ∂t



η−ξ η+ξ
,
2c
2



1 ∂u
+
2 ∂x



η−ξ η+ξ
,
2c
2


,

and so, in particular,
∂v
1 ∂u
1 ∂u
(ξ, ξ) = −
(0, ξ) +
(0, ξ) = 0,
∂ξ
2 c ∂t
2 ∂x
which vanishes owing to our choice of homogeneous initial conditions (2.91). Indeed, the
initial velocity condition says that ut (0, x) = 0, while diﬀerentiating the initial displacement

58

2 Linear and Nonlinear Waves

condition u(0, x) = 0 with respect to x implies that ux (0, x) = 0 for all x, including x = ξ.
As a result, (2.93) simpliﬁes to

 η 
∂v
ζ −ξ ζ +ξ
1
F
(ξ, η) = − 2
,
dζ.
∂ξ
4c
2c
2
ξ
We now integrate the latter equation with respect to ξ on the interval ξ ≤ χ ≤ η, producing

 η η 
ζ −χ ζ +χ
1
,
dζ dχ,
F
− v(ξ, η) = v(η, η) − v(ξ, η) = − 2
4c
2c
2
ξ
χ
since v(η, η) = u(0, η) = 0, thanks again to the initial conditions. In this manner, we
have produced an explicit formula for the solution to the characteristic variable version of
the forced wave equation subject to the homogeneous initial conditions. Reverting to the
original physical coordinates, the left-hand side of this equation becomes − u(t, x). As for
the double integral on the right-hand side, it takes place over the triangular region
T (ξ, η) = { (χ, ζ) | ξ ≤ χ ≤ ζ ≤ η } .

(2.94)

Let us introduce “physical” integration variables by setting
χ = y − c s,

ζ = y + c s.

The deﬁning inequalities of the triangle (2.94) become
x − c t ≤ y − c s ≤ y + c s ≤ x + c t,
and so, in the physical coordinates, the triangular integration domain assumes the form
D(t, x) = { (s, y) | x − c (t − s) ≤ y ≤ x + c (t − s), 0 ≤ s ≤ t } ,

(2.95)

which is graphed in Figure 2.29. The change of variables formula for double integrals
requires that we compute the Jacobian determinant




∂χ/∂y ∂χ/∂s
1 −c
= 2 c,
det
= det
1
c
∂ζ/∂y ∂ζ/∂s
and so dχ dζ = 2 c ds dy. Therefore,

 t  x+c (t−s)
1
1
u(t, x) =
F (s, y) ds dy =
F (s, y) dy ds,
2c
2 c 0 x−c (t−s)
D(t,x)

(2.96)

which gives the solution formula for the forced wave equation when subject to homogeneous
initial conditions.
To solve the general initial value problem, we appeal to linear superposition, writing its
solution as a sum of the solution (2.96) to the forced wave equation subject to homogeneous
initial conditions plus the d’Alembert solution (2.82) to the unforced equation subject to
inhomogeneous boundary conditions.
Theorem 2.18. The solution to the general initial value problem
utt = c2 uxx + F (t, x),

u(0, x) = f (x),

ut (0, x) = g(x),

− ∞ < x < ∞,

t > 0,

for the wave equation subject to an external forcing is given by
 x+c t
 t  x+c (t−s)
f (x − c t) + f (x + c t)
1
1
u(t, x) =
g(y) dy +
F (s, y) dy ds.
+
2
2 c x−c t
2 c 0 x−c (t−s)
(2.97)

2.4 The Wave Equation: d’Alembert’s Formula

59

x
(0, x + c t)

(t, x)

t
(0, x − c t)
Figure 2.29.

Domain of dependence.

Observe that the solution is a linear superposition of the respective eﬀects of the initial
displacement, the initial velocity, and the external forcing. The triangular integration
region (2.95), lying between the x–axis and the characteristic lines going backwards from
(t, x), is known as the domain of dependence of the point (t, x). This is because, for any
t > 0, the solution value u(t, x) depends only on the values of the initial data and the
forcing function at points lying within the domain of dependence D(t, x). Indeed, the ﬁrst
term in the solution formula (2.97) requires only the initial displacement at the corners
(0, x + c t), (0, x − c t); the second term requires only the initial velocity at points on the
x–axis lying on the vertical side of D(t, x); while the ﬁnal term requires the value of the
external force on the entire triangular region.
Example 2.19. Let us solve the initial value problem
utt = uxx + sin ω t sin x,

u(0, x) = 0,

ut (0, x) = 0,

for the wave equation with unit wave speed subject to a sinusoidal forcing function whose
amplitude varies periodically in time with frequency ω > 0. According to formula (2.96),
the solution is
 t  x+t−s
1
sin ωs sin y dy ds
u(t, x) =
2 0 x−t+s
 t


1
sin ωs cos(x − t + s) − cos(x + t − s) ds
=

2
⎧ 0
sin ω t − ω sin t
⎪
⎪
sin x,
0 < ω = 1,
⎨
1 − ω2
=
⎪
⎪
⎩ sin t − t cos t sin x,
ω = 1.
2
Notice that, when ω = 1, the solution is bounded, being a combination of two vibrational
modes: an externally induced mode at frequency ω along with an internal mode, at frequency 1. If ω = p/q = 1 is a rational number, then the solution varies periodically in

60

2 Linear and Nonlinear Waves

cos t + cos 73 t

√
cos t + cos 5 t
Figure 2.30.

Periodic and quasiperiodic functions.

time. On the other hand, if ω is irrational, then the solution is only quasiperiodic, and never
exactly repeats itself. Finally, if ω = 1, the solution grows without limit as t increases,
indicating that this is a resonant frequency. We will investigate external forcing and the
mechanisms leading to resonance in dynamical partial diﬀerential equations in more detail
in Chapters 4 and 6.
Example 2.20. To appreciate the diﬀerence between periodic and quasiperiodic
vibrations, consider the elementary trigonometric function
u(t) = cos t + cos ω t,
which is a linear combination of two simple periodic vibrations, of frequencies 1 and ω. If
ω = p/q is a rational number, then u(t) is a periodic function of period 2 πq, so u(t+2 πq) =
u(t). However, if ω is an irrational number, then u(t) is not periodic, and never repeats.
You are encouraged to inspect the graphs in Figure 2.30. The ﬁrst is periodic — can you
spot where it begins to repeat? — whereas the second is only quasiperiodic. The only
quasiperiodic functions we will encounter in this text are linear combinations of periodic
trigonometric functions whose frequencies are not all rational multiples of each other. To
the uninitiated, such quasiperiodic motions may appear to be random, even though they are
built from a few simple periodic constituents. While ostensibly complicated, quasiperiodic
motion is not true chaos, which is is an inherently nonlinear phenomenon, [77].

Exercises
2

2.4.1. Solve the initial value problem utt = c2 uxx , u(0, x) = e− x , ut (0, x) = sin x.
2.4.2. (a) Solve the
wave equation utt = uxx when the initial displacement is the box function

1,
1 < x < 2,
u(0, x) =
while the initial velocity is 0.
0,
otherwise,
(b) Sketch the resulting solution at several representative times.

2.4 The Wave Equation: d’Alembert’s Formula

61

2.4.3. Answer Exercise 2.4.2 when the initial velocity is the box function, while the initial displacement is zero.
2.4.4. Write the following solutions to the wave equation utt = uxx in d’Alembert form (2.82).
Hint: What is the appropriate initial data?
(a) cos x cos t, (b) cos 2 x sin 2 t, (c) ex+t , (d) t2 + x2 , (e) t3 + 3 t x2 .
♥ 2.4.5. (a) Solve the dam break 
problem, that is, the wave equation when the initial displacement
1, x > 0,
is a step function σ(x) =
and there is no initial velocity. (b) Analyze the
0, x < 0,
case in which there is no initial displacement, while the initial velocity is a step function.
(c) Are your solutions classical solutions? Explain your answer. (d) Prove that the step
1
1
function is the limit, as n → ∞, of the functions fn (x) =
tan−1 n x + . (e) Show that,
π
2
in both cases, the step function solution can be realized as the limit, as n → ∞, of solutions
to the initial value problems with the functions fn (x) as initial displacement or velocity.
♦ 2.4.6. Suppose u(t, x) solves the initial value problem u(0, x) = f (x), ut (0, x) = g(x), for the
wave equation (2.66). Prove that the solution to the initial value problem u(t0 , x) = f (x),
ut (t0 , x) = g(x), is u(t − t0 , x).
2.4.7. Find all resonant frequencies for the wave equation with wave speed c when subject to
the external forcing function F (t, x) = sin ω t sin k x for ﬁxed ω, k > 0.
2.4.8. Consider the initial value problem utt = 4 uxx + F (t, x), u(0, x) = f (x), ut (0, x) = g(x).
Determine (a) the domain of inﬂuence of the point (0, 2); (b) the domain of dependence of
the point (3, −1); (c) the domain of inﬂuence of the point (3, −1).
2.4.9. (a) A solution to the wave equation utt = 2 uxx is generated by a displacement concentrated at position x0 = 1 and time t0 = 0, but no initial velocity. At what time will an
observer at position x1 = 5 feel the eﬀect of this displacement? Will the observer continue
to feel an eﬀect in the future? (b) Answer part (a) when there is an initial velocity concentrated at position x0 = 1 and time t0 = 0, but no initial displacement.
2.4.10. Suppose u(t, x) solves the initial value problem utt = 4 uxx + sin ω t cos x, u(0, x) = 0,
ut (0, x) = 0. Is h(t) = u(t, 0) a periodic function?
♥ 2.4.11. (a) Write down an explicit formula for the solution to the initial value problem
∂2u
∂2u
− 4 2 = 0,
2
∂t
∂x

u(0, x) = sin x,

∂u
(0, x) = cos x,
∂t

− ∞ < x < ∞,

t ≥ 0.

(b) True or false: The solution is a periodic function of t.
(c) Now solve the forced initial value problem
∂2u
∂2u
− 4 2 = cos 2 t,
2
∂t
∂x

u(0, x) = sin x,

∂u
(0, x) = cos x,
∂t

− ∞ < x < ∞,

t ≥ 0.

(d) True or false: The forced equation exhibits resonance. Explain.
(e) Does the answer to part (d) change if the forcing function is sin 2 t?
2.4.12. Given a classical solution u(t, x) of the wave equation, let E = 12 (u2t + c2 u2x ) be the
associated energy density and P = ut ux the momentum density.
(a) Prove that ∂P/∂t = ∂E/∂x and ∂E/∂t = c2 ∂P/∂x. Explain why both E and P are
conserved densities for the wave equation.
(b) Show that E(t, x) and P (t, x) both satisfy the wave equation.
(c) Suppose that both ut (t, x) → 0 and ux (t, x) → 0 as | x | → ∞
suﬃciently rapidly in

order that the integrals deﬁning the total momentum P(t) =
total energy E(t) =

 ∞

−∞

∞

−∞

P (t, x) dx and the

E(t, x) dx are deﬁned and ﬁnite for each t ∈ R. Show that P(t)

and E(t) are conserved quantities, i.e., they are constants, independent of the time t.

62

2 Linear and Nonlinear Waves

♦ 2.4.13. Let u(t, x) be a classical solution to the wave equation utt = c2 uxx . The total energy




 ∞
1
∂u 2
∂u 2
E(t) =
+ c2
dx
(2.98)
−∞ 2
∂t
∂x
represents the sum of kinetic and potential energies of the displacement u(t, x) at time t.
Suppose that ∇u → 0 suﬃciently rapidly as x → ±∞; more precisely, one can ﬁnd α > 12
and C(t) > 0 such that | ut (t, x) |, | ux (t, x) | ≤ C(t)/| x |α for each ﬁxed t and all suﬃciently
large | x |
0. For such solutions, establish the Law of Conservation of Energy by showing
that E(t) is ﬁnite and constant. Hint: You do not need the formula for the solution.
♦ 2.4.14. (a) Use Exercise 2.4.13 to prove that the only classical solution to the initial-boundary
value problem utt = c2 uxx , u(0, x) = 0, ut (0, x) = 0, satisfying the indicated decay assumptions is the trivial solution u(t, x) ≡ 0. (b) Establish the following Uniqueness Theorem for
the wave equation: there is at most one such solution to the initial-boundary value problem
utt = c2 uxx , u(0, x) = f (x), ut (0, x) = g(x).
2.4.15. The telegrapher’s equation utt + a ut = c2 uxx , with a > 0, models the vibration of
a string under frictional damping. (a) Show that, under the decay assumptions of Exercise 2.4.13, the wave energy (2.98) of a classical solution is a nonincreasing function of t.
(b) Prove uniqueness of such solutions to the initial value problem for the telegrapher’s
equation.
2.4.16. What happens to the proof of Theorem 2.14 if c = 0?
2.4.17. (a) Explain why the d’Alembert factorization method doesn’t work when the wave speed
c(x) depends on the spatial variable x.
(b) Does it work when c(t) depends only on the time t?
∂ 2 u ∂ 2 u 2 ∂u
−
−
= 0. Solve the initial value problem
∂t2 ∂x2 x ∂x
u(0, x) = 0, ut (0, x) = g(x), where g(x) = g(− x) is an even function. Hint: Set w = x u.

2.4.18. The Poisson–Darboux equation is

♥ 2.4.19. (a) Solve the initial value problem utt − 2 utx − 3 uxx = 0, u(0, x) = x2 , ut (0, x) = ex .
Hint: Factor the associated linear diﬀerential operator. (b) Determine the domain of inﬂuence of a point (0, x). (c) Determine the domain of dependence of a point (t, x) with t > 0.
♦ 2.4.20. (a) Use polar coordinates to prove that, for any a > 0,

2
2
π
e− a (x +y ) dx dy = .
a
R2
(b) Explain why
 ∞
2
π
e− a x dx =
.
a
−∞
♦ 2.4.21. Use Exercise 2.4.20 to prove the error function formulae (2.88).

(2.99)

(2.100)

Chapter 3

Fourier Series

Just before 1800, the French mathematician/physicist/engineer Jean Baptiste Joseph
Fourier made an astonishing discovery, [42]. Through his deep analytical investigations
into the partial diﬀerential equations modeling heat propagation in bodies, Fourier was
led to claim that “every” function could be represented as an inﬁnite series of elementary
trigonometric functions: sines and cosines. For example, consider the sound produced by
a musical instrument, e.g., piano, violin, trumpet, or drum. Decomposing the signal into
its trigonometric constituents reveals the fundamental frequencies (tones, overtones, etc.)
that combine to produce the instrument’s distinctive timbre. This Fourier decomposition
lies at the heart of modern electronic music; a synthesizer combines pure sine and cosine
tones to reproduce the diverse sounds of instruments, both natural and artiﬁcial, according
to Fourier’s general prescription.
Fourier’s claim was so remarkable and counterintuitive that most of the leading mathematicians of the time did not believe him. Nevertheless, it was not long before scientists
came to appreciate the power and far-ranging applicability of Fourier’s method, thereby
opening up vast new realms of mathematics, physics, engineering, and beyond. Indeed,
Fourier’s discovery easily ranks in the “top ten” mathematical advances of all time, a list
that would also include Newton’s invention of the calculus, and Gauss and Riemann’s
diﬀerential geometry, which, 70 years later, became the foundation of Einstein’s general
relativity. Fourier analysis is an essential component of much of modern applied (and pure)
mathematics. It forms an exceptionally powerful analytic tool for solving a broad range of
linear partial diﬀerential equations. Applications in physics, engineering, biology, ﬁnance,
etc., are almost too numerous to catalogue: typing the word “Fourier” in the subject index
of a modern science library will dramatically demonstrate just how ubiquitous these methods are. Fourier analysis lies at the heart of signal processing, including audio, speech,
images, videos, seismic data, radio transmissions, and so on. Many modern technological advances, including television, music CDs and DVDs, cell phones, movies, computer
graphics, image processing, and ﬁngerprint analysis and storage, are, in one way or another,
founded on the many ramiﬁcations of Fourier theory. In your career as a mathematician,
scientist, or engineer, you will ﬁnd that Fourier theory, like calculus and linear algebra, is
one of the most basic weapons in your mathematical arsenal. Mastery of the subject is
essential.
Furthermore, a surprisingly large fraction of modern mathematics rests on subsequent
attempts to place Fourier series on a ﬁrm mathematical foundation. Thus, many of modern
analysis’ most basic concepts, including the deﬁnition of a function, the ε–δ deﬁnition
of limit and continuity, convergence properties in function space, the modern theory of
integration and measure, generalized functions such as the delta function, and many others,
P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_3, © Springer International Publishing Switzerland 2014

63

64

3 Fourier Series

all owe a profound debt to the prolonged struggle to establish a rigorous framework for
Fourier analysis. Even more remarkably, modern set theory, and, thus, the foundations
of modern mathematics and logic, can be traced directly back to the nineteenth-century
German mathematician Georg Cantor’s attempts to understand the sets on which Fourier
series converge!
We begin our development of Fourier methods by explaining why Fourier series naturally appear when we try to solve the one-dimensional heat equation. The reader uninterested in such motivations can safely omit this initial section, since the same material
reappears in Chapter 4, where we apply Fourier methods to solve several important linear
partial diﬀerential equations. Beginning in Section 3.2, we shall introduce the most basic
computational techniques for Fourier series. The ﬁnal section is an abbreviated introduction to the analytic background required to develop a rigorous foundation for Fourier series
methods. While this section is a bit more mathematically sophisticated than what has appeared so far, the student is strongly encouraged to delve into it to gain additional insight
and see further developments, including some of direct importance in applications.

3.1 Eigensolutions of Linear Evolution Equations
Following our studies of ﬁrst-order partial diﬀerential equations in Chapter 2, the next
important example to merit investigation is the second-order linear equation
∂u
∂2u
=
,
∂t
∂x2

(3.1)

known as the heat equation, since it models (among other diﬀusion processes) heat ﬂow
in a one-dimensional medium, e.g., a metal bar. For simplicity, we have set the physical
parameters equal to 1 in order to focus on the solution techniques. A more complete
discussion, including a brief derivation from physical principles, will appear in Chapter 4.
Unlike the wave equation considered in Chapter 2, there is no comparably elementary
formula for the general solution to the heat equation. Instead, we will write solutions
as inﬁnite series in certain simple, explicit solutions. This solution method, pioneered by
Fourier, will lead us immediately to the deﬁnition of a Fourier series. The remainder of this
chapter will be devoted to developing the basic properties and calculus of Fourier series.
Once we have mastered these essential mathematical techniques, we will start applying
them to partial diﬀerential equations in Chapter 4.
Let us begin by writing the heat equation (3.1) in a more abstract, but suggestive,
linear evolutionary form
∂u
= L[ u ],
(3.2)
∂t
in which
L[ u ] =

∂2u
∂x2

(3.3)

is a linear second-order diﬀerential operator. Recall, (1.11), that linearity imposes two
requirements on the operator L:
L[ u + v ] = L[ u ] + L[ v ],

L[ c u ] = c L[ u ],

(3.4)

3.1 Eigensolutions of Linear Evolution Equations

65

for any functions† u, v and any constant c. Moreover, since L involves diﬀerentiation only
with respect to x, it also satisﬁes
L[ c(t) u ] = c(t) L[ u ]

(3.5)

for any function c(t) that does not depend on x.
Of course, there are many other possible linear diﬀerential operators, and so our abstract linear evolution equation (3.2) can represent a wide range of linear partial diﬀerential
equations. For example, if
∂u
L[ u ] = − c(x)
,
(3.6)
∂x
where c(x) is a function representing the wave speed in a nonuniform medium, then (3.2)
becomes the transport equation
∂u
∂u
= − c(x)
(3.7)
∂t
∂x
that we studied in Chapter 2. If
∂
1
L[ u ] =
σ(x) ∂x



∂u
κ(x)
,
∂x

(3.8)

where σ(x) > 0 represents heat capacity and κ(x) > 0 thermal conductivity, then (3.2)
becomes the generalized heat equation


1
∂
∂u
∂u
=
κ(x)
,
(3.9)
∂t
σ(x) ∂x
∂x
governing the diﬀusion of heat in a nonuniform bar. If
L[ u ] =

∂2u
− γ u,
∂x2

(3.10)

where γ > 0 is a positive constant, then (3.2) becomes the damped heat equation
∂2u
∂u
=
− γ u,
∂t
∂x2

(3.11)

which models the temperature of a bar that is cooling oﬀ due to radiation of heat energy.
We can even take u to be a function of more than one space variable, e.g., u(t, x, y) or
u(t, x, y, z), in which case (3.2) includes higher-dimensional versions of the heat equation
for plates and solid bodies, which we will study in due course. In all cases, the key
requirements on the operator L are (a) linearity, and (b) only diﬀerentiation with respect
to the spatial variables is allowed.
Fourier’s inspired idea for solving such linear evolution equations is a direct adaptation
of the eigensolution method for ﬁrst-order linear systems of ordinary diﬀerential equations,
[20, 23, 89], which we now recall. The starting point is the elementary scalar ordinary
diﬀerential equation
du
= λ u.
(3.12)
dt
†
We assume throughout that the functions are suﬃciently smooth so that the indicated
derivatives are well deﬁned.

66

3 Fourier Series

The general solution is an exponential function
u(t) = c eλ t ,

(3.13)

whose coeﬃcient c is an arbitrary constant. This elementary observation motivates the
solution method for a ﬁrst-order homogeneous linear system of ordinary diﬀerential equations
du
= A u,
(3.14)
dt
in which A is a constant n × n matrix. Working by analogy, we will seek solutions of
exponential form
(3.15)
u(t) = eλ t v,
where v ∈ R n is a constant vector. We substitute this ansatz † into the equation. First,
du
d
=
eλ t v = λ eλ t v.
dt
dt
On the other hand, since eλ t is a scalar, it commutes with matrix multiplication, and so
A u = A eλ t v = eλ t A v.
Therefore, u(t) will solve the system (3.14) if and only if v satisﬁes
A v = λ v.

(3.16)

We recognize this as the eigenequation that determines the eigenvalues of the matrix A.
Namely, (3.16) has a nonzero solution v = 0 if and only if λ is an eigenvalue and v a
corresponding eigenvector . Each eigenvalue λ and eigenvector v produces a nonzero, exponentially varying eigensolution (3.15) to the linear system of ordinary diﬀerential equations.
 = c v, for c = 0, is autoRemark : Any nonzero scalar multiple of an eigenvector v
matically another eigenvector for the same eigenvalue λ. However, the only eﬀect is to
multiply the eigensolution by the scalar c. Thus, to obtain a complete system of independent solutions, we need only the independent eigenvectors.
For simplicity — and also because all of the linear partial diﬀerential equations we
will treat will have the analogous property — suppose that the n × n matrix A has a
complete system of real eigenvalues λ1 , . . . , λn and corresponding real, linearly independent
eigenvectors v1 , . . . , vn , which therefore form an eigenvector basis of the underlying space
R n . (We allow the possibility of repeated eigenvalues, but require that all eigenvectors be
independent to avoid superﬂuous solutions.) For example, according to Theorem B.26 (see
also [89; Theorem 8.20]), all real, symmetric matrices, A = AT , are complete. Complex
eigenvalues lead to complex exponential solutions, whose real and imaginary parts can be
used to construct the associated real solutions. Incomplete matrices, having an insuﬃcient
number of eigenvectors, are trickier, and the solution to the corresponding linear system
†
The German word ansatz refers to the method of ﬁnding a solution to a complicated equation
by postulating that it is of a special form. Usually, an ansatz will depend on one or more free
parameters — in this case, the entries of the vector v along with the scalar λ — that, with some
luck, can be adjusted to fulﬁll the requirements imposed by the equation. Thus, a reasonable
English translation of “ansatz” is “inspired guess”.

3.1 Eigensolutions of Linear Evolution Equations

67

requires use of the Jordan canonical form, [89; Section 8.6]. Fortunately, we do not have
to deal with the latter, technically annoying, cases here.
Using our completeness assumption, we can produce n independent real exponential
eigensolutions
u1 (t) = eλ1 t v1 ,
...
un (t) = eλn t vn ,
to the linear system (3.14). The Linear Superposition Principle of Theorem 1.4 tells us
that, for any choice of scalars c1 , . . . , cn , the linear combination
c1 u1 (t) + · · · + cn un (t) = c1 eλ1 t v1 + · · · + cn eλn t vn

(3.17)

is also a solution. The basic Existence and Uniqueness Theorems for ﬁrst-order systems of
ordinary diﬀerential equations, [18, 23, 52], imply that (3.17) forms the general solution
to the original linear system, and so the eigensolutions form a basis for the solution space.
Let us now adapt this seminal idea to construct exponentially varying solutions to the
heat equation (3.1) or, for that matter, any linear evolution equation in the form (3.2). To
this end, we introduce an analogous exponential ansatz:
u(t, x) = eλ t v(x),

(3.18)

in which we replace the vector v in (3.15) by a function v(x). We substitute the expression
(3.18) into the dynamical equations (3.2). First, the time derivative of such a function is

∂  λt
∂u
=
e v(x) = λ eλ t v(x).
∂t
∂t
On the other hand, in view of (3.5),


L[ u ] = L eλ t v(x) = eλ t L[ v ].
Equating these two expressions and canceling the common exponential factor, we conclude
that v(x) must satisfy the eigenequation
L[ v ] = λ v

(3.19)

for the linear diﬀerential operator L, in which λ is the eigenvalue, while v(x) is the corresponding eigenfunction. Each eigenvalue and eigenfunction pair will produce an exponentially varying eigensolution (3.18) to the partial diﬀerential equation (3.2). We will then
appeal to Linear Superposition to combine the resulting eigensolutions to form additional
solutions. The key complication is that partial diﬀerential equations admit an inﬁnite
number of independent eigensolutions, and thus one cannot hope to write the general solution as a ﬁnite linear combination thereof. Rather, one is led to try constructing solutions
as inﬁnite series in the eigensolutions. However, justifying such series solution formulas
requires additional analytical skills and sophistication. Not every inﬁnite series converges
to a bona ﬁde function. Moreover, a convergent series of diﬀerentiable functions need not
converge to a diﬀerentiable function, and hence the series may not represent a (classical)
solution to the partial diﬀerential equation. We are being reminded, yet again, that partial
diﬀerential equations are much wilder creatures than their relatively tame cousins, ordinary
diﬀerential equations.
Let us, for speciﬁcity, focus our attention on the heat equation, for which the linear
operator L is given by (3.3). If v(x) is a function of x alone, then
L[ v ] = v  (x).

68

3 Fourier Series

Thus, our eigenequation (3.19) becomes
v  = λ v.

(3.20)

This is a linear second-order ordinary diﬀerential equation for v(x), and so has two linearly
independent solutions. The explicit solution formulas depend on the sign of the eigenvalue
λ, and can be found in any basic text on ordinary diﬀerential equations, e.g., [20, 23]. The
following table summarizes the results for real eigenvalues λ; the case of complex λ is left
as Exercise 3.1.3 for the reader. The resulting exponential eigensolutions are also referred
to as separable solutions to indicate that they are the product of a function of t alone and a
function of x alone. The general method of separation of variables will be one of our main
tools for solving linear partial diﬀerential equations, to be developed in detail starting in
Chapter 4.
Real Eigensolutions of the Heat Equation

λ

Eigenfunctions v(x)

Eigensolutions u(t, x) = eλ t v(x)

λ = − ω2 < 0

cos ω x, sin ω x

e− ω t cos ω x, e− ω t sin ω x

λ=0

1, x

λ = ω2 > 0

e− ω x , eω x

2

2

1, x
2

2

eω t−ω x , eω t+ω x

Remark : Thus, in the absence of boundary conditions, each real number λ qualiﬁes as
an eigenvalue of the linear diﬀerential operator (3.3), possessing two linearly independent
eigenfunctions, and thus two linearly independent eigensolutions to the heat equation. As
with eigenvectors, any (nonzero) linear combination of eigenfunctions (eigensolutions) with
the same eigenvalue is also an eigenfunction (eigensolution). Thus, the preceding table lists
only independent eigenfunctions and eigensolutions.
As noted above, any ﬁnite linear combination of these basic eigensolutions is automatically a solution. Thus, for example,
u(t, x) = c1 e− t cos x + c2 e− 4t sin 2 x + c3 x + c4
is a solution to the heat equation for any choice of constants c1 , c2 , c3 , c4 , as you can easily
check. But, since there are inﬁnitely many independent eigensolutions, we cannot expect
to be able to represent every solution to the heat equation as a ﬁnite linear combination
of eigensolutions. And so, we must learn how to deal with inﬁnite series of eigensolutions.
Remark : Eigensolutions in the ﬁrst class, where λ < 0, are exponentially decaying,
which is in accord with our physical intuition as to how the temperature of a body should
behave. Those in the second class are constant in time — also physically reasonable. However, those in the third class, corresponding to positive eigenvalues λ > 0, are exponentially
growing in time. In the absence of external heat sources, physical bodies should approach
some sort of thermal equilibrium, and certainly not exhibit an exponentially growing temperature! However, notice that the latter eigensolutions (as well as the solution x) are not
bounded in space, and so include an inﬁnite amount of heat energy being supplied to the

3.1 Eigensolutions of Linear Evolution Equations

69

system from inﬁnity. As we will soon come to appreciate, physically relevant boundary
conditions — posed either on a bounded interval or by specifying the asymptotics of the
solutions at large distances — will separate out the physically reasonable solutions from
the mathematically valid but physically irrelevant ones.

The Heated Ring
So far, we have not paid any attention to boundary conditions. As noted above, these will
eliminate nonphysical eigensolutions and thereby reduce the collection to a manageable,
albeit still inﬁnite, number. In this subsection, we will discuss a particularly important
case, which, following Fourier’s line of reasoning, leads us directly into the heart of Fourier
series.
Consider the heat equation on the interval − π ≤ x ≤ π, subject to the periodic
boundary conditions
∂u
∂2u
∂u
∂u
(3.21)
=
(t, − π) =
(t, π).
,
u(t, − π) = u(t, π),
2
∂t
∂x
∂x
∂x
The physical problem being modeled is the thermodynamic behavior of an insulated circular
ring, in which x represents the angular coordinate. The boundary conditions ensure that
the temperature remains continuously diﬀerentiable at the junction point where the angle
switches over from − π to π. Given the ring’s initial temperature distribution
− π ≤ x ≤ π,

u(0, x) = f (x),

(3.22)

our task is to determine the temperature of the ring u(t, x) at each subsequent time t > 0.
Let us ﬁnd out which of the preceding eigensolutions respect the boundary conditions.
Substituting our exponential ansatz (3.18) into the diﬀerential equation and boundary
conditions (3.21), we ﬁnd that the eigenfunction v(x) must satisfy the periodic boundary
value problem
v  = λ v,

v(− π) = v(π),

v  (− π) = v  (π).

(3.23)

Our task is to ﬁnd those values of λ for which (3.23) has a nonzero solution v(x) ≡ 0.
These are the eigenvalues and eigenfunctions.
As noted above, there are three cases, depending on the sign of λ. First, suppose
λ = ω 2 > 0. Then the general solution to the ordinary diﬀerential equation is
v(x) = a eω x + b e− ω x ,
where a, b are arbitrary constants. Substituting into the boundary conditions, we ﬁnd that
a, b must satisfy the pair of linear equations
a e− ω π + b eω π = a eω π + b e− ω π ,

a ω e− ω π − b ω eω π = a ω eω π − b ω e− ω π .

Since ω = 0, the ﬁrst equation implies that a = b, while the second requires a = − b. So,
the only way to satisfy both boundary conditions is to take a = b = 0, and so v(x) ≡ 0 is
a trivial solution. We conclude that there are no positive eigenvalues.
Second, if λ = 0, then the ordinary diﬀerential equation reduces to v  = 0, with
solution
v(x) = a + b x.

70

3 Fourier Series

Substituting into the boundary conditions requires
a − b π = a + b π,

b = b.

The ﬁrst equation implies that b = 0, but this is the only condition. Therefore, any constant
function, v(x) ≡ a, solves the boundary value problem, and hence λ = 0 is an eigenvalue.
We take v0 (x) ≡ 1 as the unique independent eigenfunction, bearing in mind that any
constant multiple of an eigenfunction is automatically also an eigenfunction. We will call
1 a null eigenfunction, indicating that it is associated with the zero eigenvalue λ = 0. The
corresponding eigensolution (3.18) is u(t, x) = e0 t v0 (x) = 1, a constant solution to the
heat equation.
Finally, we must deal with the case λ = − ω 2 < 0. Now, the general solution to the
diﬀerential equation in (3.23) is a trigonometric function:
v(x) = a cos ω x + b sin ω x.
Since

(3.24)

v  (x) = − a ω sin ω x + b ω cos ω x,

when we substitute into the boundary conditions, we obtain
a cos ω π − b sin ω π = a cos ω π + b sin ω π,
a sin ω π + b cos ω π = − a sin ω π + b cos ω π,
where we canceled out a common factor of ω in the second equation. These simplify to
2 b sin ω π = 0,

2 a sin ω π = 0.

If sin ω π = 0, then a = b = 0, and so we have only the trivial solution v(x) ≡ 0. Thus, to
obtain a nonzero eigenfunction, we must have
sin ω π = 0,
which requires that ω = 1, 2, 3, . . . be a positive integer. For such ωk = k, every solution
v(x) = a cos k x + b sin k x,

k = 1, 2, 3, . . . ,

satisﬁes both boundary conditions, and hence (unless identically zero) qualiﬁes as an eigenfunction of the boundary value problem. Thus, the eigenvalue λk = − k 2 admits a twodimensional space of eigenfunctions, with basis vk (x) = cos k x and vk (x) = sin k x.
Consequently, the basic trigonometric functions
1,

cos x,

sin x,

cos 2 x,

sin 2 x,

cos 3 x,

...

(3.25)

form a system of independent eigenfunctions for the periodic boundary value problem
(3.23). The corresponding exponentially varying eigensolutions are
2

uk (x) = e− k t cos k x,

2

u
k (x) = e− k t sin k x,

k = 0, 1, 2, 3, . . . ,

(3.26)

each of which, by design, is a solution to the heat equation (3.21) and satisﬁes the periodic
boundary conditions. Note that we subsumed the case λ0 = 0 into (3.26), keeping in mind
that, when k = 0, the sine function is trivial, and hence u
0 (x) ≡ 0 is not needed. So the null
eigenvalue λ0 = 0 provides (up to a constant multiple) only one eigensolution, whereas the
strictly negative eigenvalues λk = − k 2 < 0 each provide two independent eigensolutions.

3.1 Eigensolutions of Linear Evolution Equations

71

Remark : For completeness, one should also consider the possibility of complex eigenvalues. If λ = ω 2 = 0, where ω is now allowed to be complex, then all solutions to the
diﬀerential equation (3.23) are of the form
v(x) = a eω x + b e− ω x .
The periodic boundary conditions require
a e− ω π + b eω π = a eω π + b e− ω π ,

a ω e− ω π − b ω eω π = a ω eω π − b ω e− ω π .

If eω π = e− ω π , or, equivalently, e2 ω π = 1, then the ﬁrst condition implies a = b, but then
the second implies a = b = 0, and so λ = ω 2 is not an eigenvalue. Thus, the eigenvalues
only occur when e2 ω π = 1. This implies ω = k i , where k is an integer, and so λ = − k 2 ,
leading back to the known trigonometric solutions. Later, in Section 9.5, we will learn that
the “self-adjoint” structure of the underlying boundary value problem implies, a priori, that
all its eigenvalues are necessarily real and nonpositive. So a good part of the preceding
analysis was, in fact, superﬂuous.
We conclude that there is an inﬁnite number of independent eigensolutions (3.26) to
the periodic heat equation (3.21). Linear Superposition, as described in Theorem 1.4, tells
us that any ﬁnite linear combination of the eigensolutions is automatically a solution to
the periodic heat equation. However, only solutions whose initial data u(0, x) = f (x) happens to be a ﬁnite linear combination of the trigonometric eigenfunctions (a trigonometric
polynomial) can be so represented. Fourier’s brilliant idea was to propose taking inﬁnite
“linear combinations” of the eigensolutions in an attempt to solve the general initial value
problem. Thus, we try representing a general solution to the periodic heat equation as an
inﬁnite series of the form†
u(t, x) =

∞

2
2
a0  
+
ak e− k t cos k x + bk e− k t sin k x .
2

(3.27)

k=1

The coeﬃcients a0 , a1 , a2 , . . . , b1 , b2 , . . . , are constants, to be ﬁxed by the initial condition.
Indeed, substituting our proposed solution formula (3.27) into (3.22), we obtain
f (x) = u(0, x) =

∞

a0  
+
ak cos k x + bk sin k x .
2

(3.28)

k=1

Thus, we must represent the initial temperature distribution f (x) as an inﬁnite Fourier
series in the elementary trigonometric eigenfunctions. Once we have prescribed the Fourier
coeﬃcients a0 , a1 , a2 , . . . , b1 , b2 , . . . , we expect that the corresponding eigensolution series
(3.27) will provide an explicit formula for the solution to the periodic initial-boundary
value problem for the heat equation.
However, inﬁnite series are much more delicate than ﬁnite sums, and so this formal
construction requires some serious mathematical analysis to place it on a rigorous foundation. The key questions are:
• When does an inﬁnite trigonometric Fourier series converge?
• What kinds of functions f (x) can be represented by a convergent Fourier series?
†
For technical reasons, one takes the basic null eigenfunction to be 12 instead of 1. The reason
for this choice will be revealed in the following section.

72

3 Fourier Series

• Given such a function, how do we determine its Fourier coeﬃcients ak , bk ?
• Are we allowed to diﬀerentiate a Fourier series?
• Does the result actually form a solution to the initial-boundary value problem for the
heat equation?
These are the basic issues in Fourier analysis, which must be properly addressed before we
can make any serious progress towards actually solving the heat equation. Thus, we will
leave partial diﬀerential equations aside for the time being, and start a detailed investigation
into the mathematics of Fourier series.

Exercises
3.1.1. For each of the following diﬀerential operators, (i ) prove linearity; (ii ) prove (3.5);
(iii ) write down the corresponding linear evolution equation (3.2):
∂
∂2
∂
∂
∂2
∂ x ∂
∂2
(a)
+
3
+
.
, (b)
+ 1, (c)
,
(d)
e
,
(e)
∂x
∂x
∂x2
∂x
∂x
∂x
∂x2
∂y 2
3.1.2. Find all separable eigensolutions to the heat equation ut = uxx on the interval 0 ≤ x ≤ π
subject to (a) homogeneous Dirichlet boundary conditions u(t, 0) = 0, u(t, π) = 0;
(b) mixed boundary conditions u(t, 0) = 0, ux (t, π) = 0;
(c) Neumann boundary conditions ux (t, 0) = 0, ux (t, π) = 0.
♦ 3.1.3. Complete the table of eigensolutions to the heat equation, in the absence of boundary
conditions, by allowing the eigenvalue λ to be complex.
3.1.4. Find all separable eigensolutions to the following partial diﬀerential equations:
(a) ut = ux , (b) ut = ux − u, (c) ut = x ux .
3.1.5. (a) Find the real eigensolutions to the damped heat equation ut = uxx − u. (b) Which
solutions satisfy the periodic boundary conditions u(t, − π) = u(t, π), ux (t, − π) = ux (t, π)?
3.1.6. Answer Exercise 3.1.5 for the diﬀusive transport equation ut + c ux = uxx modeling the
combined diﬀusion and transport of a solute in a uniform ﬂow with constant wave speed c.
♥ 3.1.7. (a) Find the real eigensolutions to the diﬀusion equation ut = (x2 ux )x modeling diﬀusion
in an inhomogeneous medium on the half-line x > 0.
(b) Which solutions satisfy the Dirichlet boundary conditions u(t, 1) = u(t, 2) = 0?

3.2 Fourier Series
The preceding section served to motivate the development of Fourier series as a tool for
solving partial diﬀerential equations. Our immediate goal is to represent a given function
f (x) as a convergent series in the elementary trigonometric functions:
∞

f (x) =


a0
+
[ ak cos k x + bk sin k x ] .
2

(3.29)

k=1

The ﬁrst order of business is to determine the formulae for the Fourier coeﬃcients ak , bk ;
only then will we deal with convergence issues.

3.2 Fourier Series

73

The key that unlocks the Fourier treasure chest is orthogonality. Recall that two vectors in Euclidean space are called orthogonal if they meet at a right angle. More explicitly,
v, w are orthogonal if and only if their dot product is zero: v · w = 0. Orthogonality,
and particularly orthogonal bases, has profound consequences that underpin many modern computational algorithms. See Section B.4 for the basics, and [89] for full details on
ﬁnite-dimensional developments. In inﬁnite-dimensional function space, were it not for orthogonality, Fourier theory would be vastly more complicated, if not completely impractical
for applications.
The starting point is the introduction of a suitable inner product on function space, to
assume the role played by the dot product in the ﬁnite-dimensional context. For classical
Fourier series, we use the rescaled L2 inner product

1 π
f ,g =
f (x) g(x) dx
(3.30)
π −π
on the space of continuous functions deﬁned on the interval† [ − π, π ]. It is not hard to
show that (3.30) satisﬁes the basic inner product axioms listed in Deﬁnition B.10. The
associated norm is
 

1 π
f (x)2 dx .
(3.31)
f  = f ,f  =
π −π
Lemma 3.1. Under the rescaled L2 inner product (3.30), the trigonometric functions
1, cos x, sin x, cos 2 x, sin 2 x, . . . , satisfy the following orthogonality relations:
 cos k x , cos l x  =  sin k x , sin l x  = 0,
1 =

√

 cos k x , sin l x  = 0,
2,

 cos k x  =  sin k x  = 1,

for

k = l,

for all k, l,
for
k = 0,

(3.32)

where k and l indicate nonnegative integers.
Proof : The formulas follow immediately from the elementary integration identities

 π
0, k = l,
⎧
sin k x sin l x dx =
k = l,
 π
⎨ 0,
π, k = l = 0,
−π
cos k x cos l x dx =
2 π, k = l = 0,
 π
⎩
−π
π,
k = l = 0,
cos k x sin l x dx = 0,
(3.33)
−π

which are valid for all nonnegative integers k, l ≥ 0.

Q.E.D.

Lemma 3.1 implies that the elementary trigonometric functions form an orthogonal
system, meaning that any distinct pair are orthogonal under the chosen inner product. If
we were to replace the constant function 1 by √12 , then the resulting functions would form
an orthonormal
system meaning that, in addition, they all have norm 1. However, the
√
extra 2 is utterly annoying, and best omitted.
†
We have chosen to use the interval [ − π, π ] for convenience. A common alternative is to
develop Fourier series on the interval [ 0, 2 π ]. In fact, since the basic trigonometric functions are
2 π–periodic, any interval of length 2 π will serve equally well. Adapting Fourier series to other
intervals will be discussed in Section 3.4.

74

3 Fourier Series

Remark : As with all essential mathematical facts, the orthogonality of the trigonometric functions is not an accident, but indicates that something deeper is going on. Indeed,
orthogonality is a consequence of the fact that the trigonometric functions are the eigenfunctions for the “self-adjoint” boundary value problem (3.23), which is the function space
counterpart to the orthogonality of eigenvectors of symmetric matrices, cf. Theorem B.26.
The general framework will be developed in detail in Section 9.5, and then applied to the
more complicated systems of eigenfunctions we will encounter when dealing with higherdimensional partial diﬀerential equations.
If we ignore convergence issues, then the trigonometric orthogonality relations serve
to prescribe the Fourier coeﬃcients: Taking the inner product of both sides of (3.29) with
cos l x for l > 0, and invoking linearity of the inner product, yields
∞


a
 f , cos l x  = 0  1 , cos l x  +
[ ak  cos k x , cos l x  + bk  sin k x , cos l x  ]
2
k=1

= al  cos l x , cos l x  = al ,
since, by the orthogonality relations (3.32), all terms but the lth vanish. This serves to prescribe the Fourier coeﬃcient al . A similar manipulation with sin l x ﬁxes bl =  f , sin l x ,
while taking the inner product with the constant function 1 gives
∞

f ,1 =


a
a0
[ ak  cos k x , 1  + bk  sin k x , 1  ] = 0  1 2 = a0 ,
1,1 +
2
2
k=1

which agrees with the preceding formula for al when l = 0, and explains why we include
the extra factor 12 in the constant term. Thus, if the Fourier series converges to the
function f (x), then its coeﬃcients are prescribed by taking inner products with the basic
trigonometric functions.
Deﬁnition 3.2. The Fourier series of a function f (x) deﬁned on − π ≤ x ≤ π is
∞

f (x) ∼


a0
[ ak cos k x + bk sin k x ] ,
+
2

(3.34)

k=1

whose coeﬃcients are given by the inner product formulae

1 π
ak =  f , cos k x  =
f (x) cos k x dx,
π −π

1 π
f (x) sin k x dx,
bk =  f , sin k x  =
π −π

k = 0, 1, 2, 3, . . . ,
(3.35)
k = 1, 2, 3, . . . .

The function f (x) cannot be completely arbitrary, since, at the very least, the integrals
in the coeﬃcient formulae must be well deﬁned and ﬁnite. Even if the coeﬃcients (3.35)
are ﬁnite, there is no guarantee that the resulting inﬁnite series converges, and, even if it
converges, no guarantee that it converges to the original function f (x). For these reasons,
we will tend to use the ∼ symbol instead of an equal sign when writing down a Fourier
series. Before tackling these critical issues, let us work through an elementary example.

3.2 Fourier Series

75

Example 3.3. Consider the function f (x) = x. We may compute its Fourier coeﬃcients directly, employing integration by parts to evaluate the integrals:

 π


1 π
1 π
1 x sin k x cos k x
+
x dx = 0, ak =
x cos k x dx =
= 0,
a0 =
π −π
π −π
π
k
k2
x = −π

 π

x cos k x sin k x
1 π
1
2
−
+
x sin k x dx =
= (−1)k+1 .
(3.36)
bk =
π −π
π
k
k2
k
x = −π
The resulting Fourier series is


sin 3 x
sin 4 x
sin 2 x
+
−
+ ··· .
x ∼ 2 sin x −
2
3
4

(3.37)

Establishing convergence of this inﬁnite series is far from elementary. Standard calculus
criteria, including the ratio and root tests, are inconclusive. Even if we know that the series
converges (which it does — for all x), it is certainly not obvious what function it converges
to. Indeed, it cannot converge to the function f (x) = x everywhere! For instance, if x = π,
then every term in the Fourier series is zero, and so it converges to 0 — which is not the
same as f (π) = π.
Recall that the convergence of an inﬁnite series is predicated on the convergence of its
sequence of partial sums, which, in this case, are

a
sn (x) = 0 +
[ ak cos k x + bk sin k x ] .
2
n

(3.38)

k=1

By deﬁnition, the Fourier series converges at a point x if and only if its partial sums have
a limit:
lim sn (x) = f(x),
(3.39)
n→∞

which may or may not equal the value of the original function f (x). Thus, a key requirement
is to ﬁnd conditions on the function f (x) that guarantee that the Fourier series converges,
and, even more importantly, that the limiting sum reproduces the original function: f(x) =
f (x). This will all be done in detail below.
Remark : A ﬁnite Fourier sum, of the form (3.38), is also known as a trigonometric
polynomial . This is because, by trigonometric identities, it can be re-expressed as a polynomial P (cos x, sin x) in the cosine and sine functions; vice versa, every such polynomial
can be uniquely written as such a sum; see [89] for details.
The passage from trigonometric polynomials to Fourier series might be viewed as
analogous to the passage from polynomials to power series. Recall that the Taylor series
of an inﬁnitely diﬀerentiable function f (x) at the point x = 0 is
f (x) ∼ c0 + c1 x + · · · + cn xn + · · · =

∞


c k xk ,

k=0
(k)

f (0)
are expressed in terms
where, according to Taylor’s formula, the coeﬃcients ck =
k!
of its derivatives at the origin, not by an inner product. The partial sums
sn (x) = c0 + c1 x + · · · + cn xn =

n

k=0

c k xk

76

3 Fourier Series

of a power series are ordinary polynomials, and the same basic convergence issues arise.
Although superﬁcially similar, in actuality the two theories are profoundly diﬀerent.
Indeed, while the theory of power series was well established in the early days of the
calculus, there remain, to this day, unresolved foundational issues in Fourier theory. A
power series in a real variable x either converges everywhere, or on an interval centered
at 0, or nowhere except at 0. On the other hand, a Fourier series can converge on quite
bizarre sets. Secondly, when a power series converges, it converges to an analytic function,
whose derivatives are represented by the diﬀerentiated power series. Fourier series may
converge, not only to continuous functions, but also to a wide variety of discontinuous
functions and even more general objects. Therefore, term-wise diﬀerentiation of a Fourier
series is a nontrivial issue.
Once one appreciates how radically diﬀerent the two subjects are, one begins to understand why Fourier’s astonishing claims were initially widely disbelieved. Before that
time, all functions were taken to be analytic. The fact that Fourier series might converge
to a nonanalytic, even discontinuous function was extremely disconcerting, resulting in a
profound re-evaluation of the foundations of function theory and the calculus, culminating
in the modern deﬁnitions of function and convergence that you now learn in your ﬁrst
courses in analysis, [8, 96, 97]. Only through the combined eﬀorts of many of the leading
mathematicians of the nineteenth century was a rigorous theory of Fourier series ﬁrmly
established. Section 3.5 contains the most important details, while more comprehensive
treatments can be found in the advanced texts [37, 68, 128].

Exercises
3.2.1. Find the Fourier series of the following functions:
(c) 3 x − 1, (d) x2 , (e) sin3 x, (f ) sin x cos x,
3.2.2. Find
the Fourier series of the following functions:

1, | x | < 12 π,
1, 12 π < | x | < π,
(b)
(a)
0, otherwise,
0, otherwise,


(d)

x,
0,



| x | < 12 π,

(e)

otherwise,

cos x,
0,

(a) sign x, (b) | x |,
(g) | sin x |, (h) x cos x.


(c)

1,
0,

1
2 π < x < π,

otherwise,

| x | < 12 π,
otherwise.

3.2.3. Find the Fourier series of sin2 x and cos2 x without directly calculating the Fourier coeﬃcients. Hint: Use some standard trigonometric identities.
♦ 3.2.4. Let g(x) = 12 p0 +

n

(pk cos k x + qk sin k x) be a trigonometric polynomial. Explain why
k=1

its Fourier coeﬃcients are ak = pk and bk = qk for k ≤ n, while ak = bk = 0 for k > n.
3.2.5. True or false: (a) The Fourier series for the function 2 f (x) is obtained by multiplying
each term in the Fourier series for f (x) by 2. (b) The Fourier series for the function f (2 x)
is obtained by replacing x by 2 x in the Fourier series for f (x). (c) The Fourier coeﬃcients
of f (x) + g(x) can be found by adding the corresponding Fourier coeﬃcients of f (x) and
g(x). (d) The Fourier coeﬃcients of f (x) g(x) can be found by multiplying the corresponding Fourier coeﬃcients of f (x) and g(x).

3.2 Fourier Series

77

π

−2 π

−π

π

2π

3π

4π

5π

−π
Figure 3.1.

2 π–periodic extension of x.

Periodic Extensions
The trigonometric constituents (3.25) of a Fourier series are all periodic functions of period
2 π. Therefore, if the series converges, the limiting function f(x) must also be periodic of
period 2 π:
f(x + 2 π) = f(x)
for all
x ∈ R.
A Fourier series can converge only to a 2 π–periodic function. So it was unreasonable to
expect the Fourier series (3.37) to converge to the aperiodic function f (x) = x everywhere.
Rather, it should converge to its “periodic extension”, which we now deﬁne.
Lemma 3.4. If f (x) is any function deﬁned for − π < x ≤ π, then there is a unique
2 π–periodic function f, known as the 2 π–periodic extension of f , that satisﬁes f(x) = f (x)
for all − π < x ≤ π.
Proof : Pictorially, the graph of the periodic extension of a function f (x) is obtained
by repeatedly copying the part of its graph between − π and π to adjacent intervals of
length 2 π; Figure 3.1 shows a simple example. More formally, given x ∈ R, there is a
unique integer m such that (2 m − 1) π < x ≤ (2 m + 1) π. Periodicity of f leads us to deﬁne
f(x) = f(x − 2 m π) = f (x − 2 m π).

(3.40)

In particular, if − π < x ≤ π, then m = 0, and hence f(x) = f (x) for such x. The proof
that the resulting function f is 2 π–periodic is left as Exercise 3.2.8.
Q.E.D.
Remark : The construction of the periodic extension in Lemma 3.4 uses the value f (π)
at the right endpoint and requires f(− π) = f(π) = f (π). One could, alternatively, require
f(π) = f(− π) = f (− π), which, if f (− π) = f (π), leads to a slightly diﬀerent 2 π–periodic
extension of the function. There is no a priori reason to prefer one over the other. In fact,
as we shall discover, the preferred Fourier periodic extension f(x) takes the average of the
two values:


f(π) = f(− π) = 12 f (π) + f (− π) ,
(3.41)
which then ﬁxes its values at the odd multiples of π.

78

3 Fourier Series

Example 3.5. The 2 π–periodic extension of f (x) = x is the “sawtooth” function

f (x) graphed in Figure 3.1. It agrees with x between − π and π. Since f (π) = π, f (− π) =
− π, the Fourier extension (3.41) sets f(k π) = 0 for any odd integer k. Explicitly,

x − 2 m π, (2 m − 1) π < x < (2 m + 1) π,

f (x) =
where m is any integer.
0,
x = (2 m − 1) π,
With this convention, it can be proved that the Fourier series (3.37) converges everywhere
to the 2 π–periodic extension f(x). In particular,
2

∞


(−1)

k+1 sin k x

k

k=1


=

x,

− π < x < π,

0,

x = ± π.

(3.42)

Even this very simple example has remarkable and nontrivial consequences. For instance, if we substitute x = 12 π in (3.42) and divide by 2, we obtain Gregory’s series
1
1
1
1
π
= 1 −
+
−
+
− ··· .
4
3
5
7
9

(3.43)

While this striking formula predates Fourier theory — it was, in fact, ﬁrst discovered by
Leibniz — a direct proof is not easy.
Remark : While numerologically fascinating, Gregory’s series is of scant practical use
for actually computing π, since its rate of convergence is painfully slow. The reader may
wish to try adding up terms to see how far out one needs to go to accurately compute
even the ﬁrst two decimal digits of π. Round-oﬀ errors will eventually interfere with any
attempt to numerically compute the summation with any reasonable degree of accuracy.

Exercises
3.2.6. Graph the 2 π–periodic extension of each of the following functions. Which extensions
are continuous? Diﬀerentiable?
(a) x2 , (b) (x2 − π 2 )2 , (c) ex , (d) e− | x | ,
1
1
.
(e) sinh x, (f ) 1 + cos2 x; (g) sin 12 πx, (h)
, (i)
x
1 + x2
3.2.7. Sketch a graph of the 2 π–periodic extension of each of the functions in Exercise 3.2.2.
♦ 3.2.8. Complete the proof of Lemma 3.4 by showing that f (x) is 2 π periodic.
♦ 3.2.9. Suppose f (x) is periodic with period
(a)

a+
a

f (x) dx =

0

and integrable. Prove that, for any a,

f (x) dx,

(b)

0

f (x + a) dx =

0

f (x) dx.

♥ 3.2.10. Let f (x) be a suﬃciently nice 2 π–periodic function. (a) Prove that f  (x) is 2 π–periodic.
(b) Show that if f (x) has mean zero, so

π

−π

f (x) dx = 0, then g(x) =

x

0

f (y) dy is 2 π–

periodic; (c) Does the result in part (b) rely on the fact that the lower limit in the integral
π
1
f (x) dx, then
for g(x) is 0? (d) More generally, prove that if f (x) has mean m =
2 π −π
x
the function g(x) =
f (y) dy − m x is 2 π–periodic.
0

3.2 Fourier Series

79

Figure 3.2.

Piecewise continuous function.

♦ 3.2.11. Given a function f (x) deﬁned for 0 ≤ x < , prove that there is a unique periodic
function of period that agrees with f on the interval [ 0, ). If = 2 π, is this the same
periodic extension as we constructed in the text? Explain your answer. Try the case f (x) =
x as an illustrative example.
3.2.12. Use the method in Exercise 3.2.11 to construct and graph the 1–periodic
extensions of

1, | x | < 12 π,
2
−x
the following functions:
(a) x , (b) e , (c) cos π x, (d)
0, otherwise.
♠ 3.2.13. (a) How many terms in Gregory’s series (3.43) are required to compute the ﬁrst two
decimal digits of π? (b) The ﬁrst 10 decimal digits? Hint: Use the fact that it is an alternating series. (c) For part (a), try summing up the required number of terms on your
computer, and check whether you obtain an accurate result.

Piecewise Continuous Functions
As we shall see, all continuously diﬀerentiable 2 π–periodic functions can be represented
as convergent Fourier series. More generally, we can allow functions that have simple
discontinuities.
Deﬁnition 3.6. A function f (x) is said to be piecewise continuous on an interval
[ a, b ] if it is deﬁned and continuous except possibly at a ﬁnite number of points a ≤ x1 <
x2 < · · · < xn ≤ b. Furthermore, at each point of discontinuity, we require that the leftand right-hand limits
f (x−
k ) = lim f (x),
x → x−
k

f (x+
k ) = lim f (x),
x → x+
k

(3.44)

exist. (At the endpoints a, b, existence of only one of the limits, namely f (a+ ) and f (b− )
is required.) Note that we do not require that f (x) be deﬁned at xk . Even if f (xk ) is
deﬁned, it does not necessarily equal either the left- or the right-hand limit.
A representative graph of a piecewise continuous function appears in Figure 3.2. The

80

3 Fourier Series

Figure 3.3.

The unit step function.

points xk are known as jump discontinuities of f (x), and the diﬀerence
−
βk = f (x+
k ) − f (xk ) = lim f (x) − lim f (x)
x → x+
k

x → x−
k

(3.45)

between the left- and right-hand limits is the magnitude of the jump. Note the value of
the function at the discontinuity, namely f (xk ) — which may not even be deﬁned — plays
no role in the speciﬁcation of the jump magnitude. The jump magnitude is positive if
the function jumps up (when moving from left to right) at xk and negative if it jumps
down. If the jump magnitude vanishes, βk = 0, the left- and right-hand limits agree,
−
and the discontinuity is removable, since redeﬁning f (xk ) = f (x+
k ) = f (xk ) makes f (x)
continuous at x = xk . Since removable discontinuities have no eﬀect in either the theory
or applications, they can always be removed without penalty.
The simplest example of a piecewise continuous function is the unit step function

1,
x > 0,
(3.46)
σ(x) =
0,
x < 0,
graphed in Figure 3.3. It has a single jump discontinuity at x = 0 of magnitude 1:
σ(0+ ) − σ(0− ) = 1 − 0 = 1,
and is continuous — indeed, locally constant — everywhere else. If we translate and scale
the step function, we obtain a function

β,
x > ξ,
(3.47)
h(x) = β σ(x − ξ) =
0,
x < ξ,
with a single jump discontinuity of magnitude β at the point x = ξ.
If f (x) is any piecewise continuous function on [ − π, π ], then its Fourier coeﬃcients
are well deﬁned — the integrals (3.35) exist and are ﬁnite. Continuity, however, is not
enough to ensure convergence of the associated Fourier series.
Deﬁnition 3.7. A function f (x) is called piecewise C1 on an interval [ a, b ] if it is
deﬁned, continuous, and continuously diﬀerentiable except at a ﬁnite number of points

3.2 Fourier Series

81

Figure 3.4.

Piecewise C1 function.

a ≤ x1 < x2 < · · · < xn ≤ b. At each exceptional point, the left- and right-hand limits† of
both the function and its derivative exist:
f (x−
k ) = lim− f (x),
x→xk

f



(x−
k)=



lim f (x),

x→x−
k

f (x+
k ) = lim+ f (x),
x→xk

f



(x+
k)=

lim f  (x).

x→x+
k

See Figure 3.4 for a representative graph. For a piecewise C1 function, an exceptional
point xk is either
• a jump discontinuity where the left- and right-hand derivatives exist, or
+
• a corner , meaning a point where f is continuous, so f (x−
k ) = f (xk ), but has diﬀerent
 −
 +
left- and right-hand derivatives: f (xk ) = f (xk ).
Thus, at each point, including jump discontinuities, the graph of f (x) has well-deﬁned
right and left tangent lines. For example, the function f (x) = | x | is piecewise C1 , since it
is continuous everywhere and has a corner at x = 0, with f  (0+ ) = + 1, f  (0− ) = − 1.
There is an analogous deﬁnition of piecewise Cn functions. One requires that the
function have n continuous derivatives, except at a ﬁnite number of points. Moreover,
at every point, the function must have well deﬁned left- and right-hand limits of all its
derivatives up to order n.
Finally, a function f (x) deﬁned for all x ∈ R is piecewise continuous (or C1 or Cn )
provided it is piecewise continuous (or C1 or Cn ) on any bounded interval. Thus, a piecewise
continuous function on R can have an inﬁnite number of discontinuities, but they are not
allowed to accumulate at any ﬁnite limit point. In particular, a 2 π–periodic function f(x)
is piecewise continuous if and only if it is piecewise continuous on the interval [ − π, π ].

Exercises
3.2.14. Find the discontinuities and the jump magnitudes for the following piecewise continuous functions:
†

As before, at the endpoints we require only the appropriate one-sided limits, namely f (a+ ),
f (a ), and f (b− ), f  (b− ), to exist.


+

82

3 Fourier Series
(a) 2 σ(x) + σ(x + 1) − 3 σ(x − 1), (b) sign(x2 − 2 x), (c) σ(x2 − 2 x), (d) | x2 − 2 x |,
(e)



| x − 2 | , (f ) σ(sin x), (g) sign(sin x), (h) | sin x |, (i) eσ(x) , (j) σ(ex ), (k) e| x−2 | .

3.2.15. Graph the following piecewise continuous functions. List all discontinuities and jump
⎧
magnitudes.
sin x
⎪
⎪
 x

⎪
, 0 < | x | < 2 π,
⎨
e , 1 < | x | < 2,
sin x, 0 < x < 12 π,
(a)
(c) ⎪ 1, x
(b)
x = 0,
0,
otherwise,
⎪
0,
otherwise,
⎪
⎩
0,
otherwise,
⎧
⎧
1
⎪
⎪ −

−1 < x < 0,
⎪
⎪
,
| x | ≥ 1,
⎨ x,
⎨
x
| x | ≤ 1,
x
sin
x,
0
<
x
<
π,
(e)
(f
)
(d)
2
⎪
⎪
⎩
⎪
x2 , | x | > 1,
⎪
, | x | < 1.
⎩
0,
otherwise,
1 + x2
3.2.16. Are the functions in Exercises 3.2.14 and 3.2.15 piecewise
C1 ? If so, list all corners.
⎧
n
⎪
⎨ (x − ξ)
, x > ξ, is piecewise
3.2.17. Prove that the nth order ramp function ρn (x − ξ) = ⎪
n!
⎩
k
0,
x < ξ,
C for any k ≥ 0.
3.2.18. Is x1/3 piecewise continuous? piecewise C1 ? piecewise C2 ?
3.2.19. Answer Exercise 3.2.18 for

1
1
(a) | x |, (b)
, (c) e− 1/| x | , (d) x3 sin , (e) | x |3 , (f ) | x |3/2 .
x
x
3.2.20. (a) Give an example of a function that is continuous but not piecewise C1 .
(b) Give an example that is piecewise C1 but not piecewise C2 .
3.2.21. (a) Prove that the sum f + g of two piecewise continuous functions is piecewise continuous. (b) Where are the jump discontinuities of f + g? What are the jump magnitudes?
(c) Check your result by summing the functions in parts (a) and (b) of Exercise 3.2.14.
3.2.22. Give an example of two piecewise continuous (but not continuous) functions f, g whose
sum f + g is continuous. Can you characterize all such pairs of functions?
♦ 3.2.23. (a) Prove that if f (x) is piecewise continuous on [ − π, π ], then its 2 π–periodic extension
is piecewise continuous on all of R. Where are its jump discontinuities and what are their
magnitudes? (b) Similarly, prove that if f (x) is piecewise C1 , then its periodic extension is
piecewise C1 . Where are the corners?
3.2.24. True or false: (a) If f (x) is a piecewise continuous function, its absolute value | f (x) | is
piecewise continuous. If true, what are the jumps and their magnitudes?
(b) If f (x) is piecewise C1 , then | f (x) | is piecewise C1 . If true, what are the corners?

The Convergence Theorem
We are now able to state the fundamental convergence theorem for Fourier series. But we
will postpone a discussion of its proof until the end of Section 3.5.
Theorem 3.8. If f(x) is a 2 π–periodic, piecewise C1 function, then, at any x ∈ R,
its Fourier series converges to

1
2

f(x),

f(x+ ) + f(x− ) ,

if f is continuous at x,
if x is a jump discontinuity.

3.2 Fourier Series

83

Figure 3.5.

Splitting the diﬀerence.

Thus, the Fourier series converges, as expected, to f(x) at all points of continuity.
At discontinuities, it apparently can’t decide whether to converge to the left- or righthand limit, and so ends up “splitting the diﬀerence” by converging to their average; see
Figure 3.5. If we redeﬁne f(x) at its jump discontinuities to have the average limiting
value, so


f(x) = 12 f(x+ ) + f(x− )
(3.48)
— an equation that automatically holds at all points of continuity — then Theorem 3.8
would say that the Fourier series converges to the 2 π–periodic piecewise C1 function f(x)
everywhere.
Example 3.9. Let σ(x) denote the unit step function (3.46). Its Fourier coeﬃcients
are easily computed:


1 π
1 π
a0 =
σ(x) dx =
dx = 1,
π −π
π 0


1 π
1 π
σ(x) cos k x dx =
cos k x dx = 0,
ak =
π −π
π 0
⎧


⎨ 2 ,
k = 2 l + 1 odd,
1 π
1 π
kπ
bk =
σ(x) sin k x dx =
sin k x dx =
⎩
π −π
π 0
0,
k = 2 l even.
Therefore, the Fourier series for the step function is


1
2
sin 3 x
sin 5 x
sin 7 x
σ(x) ∼
+
sin x +
+
+
+ ··· .
2
π
3
5
7

(3.49)

According to Theorem 3.8, the Fourier series will converge to its 2 π–periodic extension,
⎧
(2 m − 1) π < x < 2 m π,
⎪
⎨ 0,
1,
2 m π < x < (2 m + 1) π,
σ
(x) =
where m is any integer,
⎪
⎩ 1,
x = m π,
2
which is plotted in Figure 3.6. Observe that, in accordance with Theorem 3.8, σ
(x) takes
the midpoint value 12 at the jump discontinuities 0, ± π, ± 2 π, . . . .

84

3 Fourier Series

1
1
2

−π

π
Figure 3.6.

Figure 3.7.

2π

3π

4π

2 π–periodic step function.

Gibbs phenomenon.

It is instructive to investigate the convergence of this particular Fourier series in some
detail. Figure 3.7 displays a graph of the ﬁrst few partial sums, taking, respectively,
n = 4, 10, and 20 terms. The reader will notice that away from the discontinuities, the
series indeed appears to be converging, albeit slowly. However, near the jumps there is a
consistent overshoot of about 9% of the jump magnitude. The region where the overshoot
occurs becomes narrower and narrower as the number of terms increases, but the actual
amount of overshoot persists no matter how many terms are summed up. This was ﬁrst
noted by the American physicist Josiah Gibbs, and is now known as the Gibbs phenomenon
in his honor. The Gibbs overshoot is a manifestation of the subtle nonuniform convergence
of the Fourier series.

Exercises


sin x,
0 < x ≤ π,
(b) Find its
0,
−π ≤ x < 0.
Fourier series. (c) Graph the ﬁrst ﬁve Fourier sums and compare with the function.
(d) Discuss convergence of the Fourier series.

cos x, 0 < x ≤ π,
3.2.26. Answer Exercise 3.2.25 for the cosine half-wave f (x) =
0,
−π ≤ x < 0.

3.2.25. (a) Sketch the 2 π–periodic half-wave f (x) =

3.2.27. (a) Find the Fourier series for f (x) = ex . (b) For which values of x does the Fourier
series converge? (c) Graph the function it converges to.
♠ 3.2.28. (a) Use a graphing package to investigate the Gibbs phenomenon for the Fourier series
(3.37) of the function x. Determine the amount of overshoot of the partial sums at the discontinuities. (b) How many terms do you need to approximate the function to within two
decimal places at x = 2.0? At x = 3.0?

3.2 Fourier Series

85

3.2.29. Use the Fourier series (3.49) for the step function to rederive Gregory’s series (3.43).
♦ 3.2.30. Suppose ak , bk are the Fourier coeﬃcients of the function f (x). (a) To which function
∞
a
does the Fourier series 0 +
[ ak cos 2 k x + bk sin 2 k x ] converge? Hint: The answer is
2
k=1
not f (2 x). (b) Test your answer with the Fourier series (3.37) for f (x) = x.

Even and Odd Functions
We already noted that the Fourier cosine coeﬃcients of the function f (x) = x are all 0.
This is not an accident, but, rather, a consequence of the fact that x is an odd function.
Recall ﬁrst the basic deﬁnition:
Deﬁnition 3.10. A function is called even if f (− x) = f (x). A function is called odd
if f (−x) = −f (x).
For example, the functions 1, cos k x, and x2 are all even, whereas x, sin k x, and sign x
are odd. Note that an odd function necessarily has f (0) = 0. We require three elementary
lemmas, whose proofs are left to the reader.
Lemma 3.11. The sum, f (x) + g(x), of two even functions is even; the sum of two
odd functions is odd.
Remark : Every function can be represented as the sum of an even and an odd function;
see Exercise 3.2.32.
Lemma 3.12. The product f (x) g(x) of two even functions, or of two odd functions,
is an even function. The product of an even and an odd function is odd.
 a Lemma 3.13. If f (x) is odd and integrable on the
 a symmetric interval
 a [ − a, a ], then
f (x) dx = 0. If f (x) is even and integrable, then
f (x) dx = 2
f (x) dx.
−a

−a

0

The next result is an immediate consequence of applying Lemmas 3.12 and 3.13 to the
Fourier integrals (3.35).
Proposition 3.14. If f (x) is even, then its Fourier sine coeﬃcients all vanish, bk = 0,
and so f (x) can be represented by a Fourier cosine series
∞


a
f (x) ∼ 0 +
ak cos k x ,
2

(3.50)

k=1

where

2
ak =
π

 π
f (x) cos k x dx,

k = 0, 1, 2, 3, . . . .

(3.51)

0

If f (x) is odd, then its Fourier cosine coeﬃcients vanish, ak = 0, and so f (x) can be
represented by a Fourier sine series
f (x) ∼

∞

k=1

bk sin k x ,

(3.52)

86

3 Fourier Series

π

−2 π

−π

π
Figure 3.8.

where

2
bk =
π

2π

3π

4π

5π

2 π–periodic extension of | x |.

 π
f (x) sin k x dx,

k = 1, 2, 3, . . . .

(3.53)

0

Conversely, a convergent Fourier cosine series always represents an even function, while a
convergent sine series always represents an odd function.
Example 3.15. The absolute value f (x) = | x | is an even function, and hence has a
Fourier cosine series. The coeﬃcients are

2 π
a0 =
x dx = π,
(3.54)
π 0
⎧

π

0 = k even,
⎨ 0,
2 π
2 x sin k x cos k x
+
ak =
x cos k x dx =
=
⎩ − 4 ,
π 0
π
k
k2
k odd.
x=0
k2 π
Therefore


π
4
cos 3 x
cos 5 x
cos 7x
|x| ∼
−
cos x +
+
+
+ ··· .
(3.55)
2
π
9
25
49
According to Theorem 3.8, this Fourier cosine series converges to the 2 π–periodic extension
of | x |, the “sawtooth function” graphed in Figure 3.8.
In particular, if we substitute x = 0, we obtain another interesting series:
∞


1
1
1
π2
1
=1 +
+
+
+ ··· =
.
8
9
25
49
(2 j + 1)2

(3.56)

j =0

It converges faster than Gregory’s series (3.43), and, while far from optimal in this regard,
can be used to compute reasonable approximations to π. One can further manipulate this
result to compute the sum of the series
S=

∞

1
1
1
1
1
1
1
+
+
+
+
+
+ ··· .
=1 +
2
k
4
9
16
25
36
49

k=1

We note that
∞

∞

k=1

k=1

 1

1
1
1
1
1
S
=
+
+
+ ··· .
=
= +
4
4 k2
(2 k)2
4
16
36
64
Therefore, by (3.56),
3
S
1
1
1
π2
S=S− =1 +
+
+
+ ··· =
,
4
4
9
25
49
8

3.2 Fourier Series

87

from which we conclude that
S=

∞

1
1
1
1
1
π2
=1 +
+
+
+
+ ··· =
.
2
k
4
9
16
25
6

(3.57)

k=1

Remark : The most famous function in number theory — and the source of the most
outstanding problem in mathematics, the Riemann hypothesis — is the Riemann zeta
function
∞

1
ζ(s) =
.
(3.58)
ks
k=1

Formula (3.57) shows that ζ(2) = 16 π 2 . In fact, the value of the zeta function at any even
positive integer s = 2 j is a rational polynomial in π, [9]. Because of its importance to the
study of prime numbers, locating all the complex zeros of the zeta function will earn you
$1,000,000 — see http://www.claymath.org for details.
Any function f (x) deﬁned on [ 0, π ] has a unique even extension to [ − π, π ], obtained
by setting f (− x) = f (x) for − π ≤ x < 0, and also a unique odd extension, where now
f (− x) = − f (x) and f (0) = 0. These in turn can be periodically extended to the entire real
line. The Fourier cosine series of f (x) is deﬁned by the formulas (3.50–51), and represents
the even, 2 π–periodic extension. Similarly, the formulas (3.52–53) deﬁne the Fourier sine
series of f (x), representing its odd, 2 π–periodic extension.
Example 3.16. Suppose f (x) = sin x. Its Fourier cosine series has coeﬃcients
⎧
4
 π
⎨
,
k even,
2
(1 − k 2 ) π
ak =
sin x cos k x dx =
⎩
π 0
0,
k odd.
The resulting cosine series represents the even, 2 π–periodic extension of sin x, namely
∞

4  cos 2 j x
2
−
.
| sin x | ∼
π
π
4 j2 − 1
j =1

On the other hand, f (x) = sin x is already odd, and so its Fourier sine series coincides with
its ordinary Fourier series, namely sin x, all the other Fourier sine coeﬃcients being zero;
in other words, b1 = 1, while bk = 0 for k > 1.

Exercises
3.2.31. Are the following functions even, odd, or neither?
(a) x2 ,

(b) ex ,

(c) sinh x,

(d) sin πx,

(e)

1
,
x

(f )

1
,
1 + x2

(g) tan−1 x.

♦ 3.2.32. Prove that (a) the sum of two even functions is even; (b) the sum of two odd functions
is odd; (c) every function is the sum of an even and an odd function.
♦ 3.2.33. Prove (a) Lemma 3.12; (b) Lemma 3.13.

88

3 Fourier Series

3.2.34. If f (x) is odd, is f  (x) (i ) even? (ii ) odd? (iii ) neither? (iv ) could be either?
3.2.35. If f  (x) is even, is f (x) (i ) even? (ii ) odd? (iii ) neither? (iv ) could be either?
How do you reconcile your answer with Exercise 3.2.34?
3.2.36. Answer Exercise 3.2.34 for f  (x).
3.2.37. True or false: (a) If f (x) is odd, its 2 π–periodic extension is odd.
(b) If the 2 π–periodic extension of f (x) is odd, then f (x) is odd.
3.2.38. Let f (x) denote the odd, 2 π–periodic Fourier extension of a function f (x) deﬁned on
[ 0, π ]. Explain why f (k π) = 0 for any integer k.
3.2.39. Construct and graph the even and odd 2 π–periodic extensions of the function
f (x) = 1 − x. What are their Fourier series? Discuss convergence of each.
3.2.40. Find the
(a) the box function
⎧ Fourier series and discuss convergence for:

⎨ 1, | x | < 1 π,
1 − | x |, | x | < 1,
2
(b)
the
hat
function
h(x)
=
b(x) = ⎩
0,
1 < | x | < π.
0, 12 π < | x | < π,
3.2.41. Find the Fourier sine and cosine series of the following functions. Then graph the
function to which the series converges. (a) 1, (b) cos x, (c) sin3 x, (d) x(π − x).
3.2.42. Find the Fourier series of the hyperbolic functions cosh m x and sinh m x.
3.2.43. Use the Fourier cosine series of the function | sin x | constructed in Example 3.16 to
evaluate the sums

∞


k=1

(4 k2 − 1)−1 and

∞


k=1

(−1)k−1 (4 k2 − 1)−1 .

3.2.44. True or false: The sum of the Fourier cosine series and the Fourier sine series of the
function f (x) is the Fourier series for f (x). If false, what function is represented by the
combined Fourier series?
3.2.45. (a) Show that if a function is periodic of period π, then its Fourier series contains only
even terms, i.e., ak = bk = 0 whenever k = 2 j + 1 is odd. (b) What if the period is 12 π?
3.2.46. Under what conditions on f (x) does its Fourier sine series contain only even terms, i.e.,
its Fourier sine coeﬃcients bk = 0 whenever k is odd?
♠ 3.2.47. (a) Graph the partial sums s3 (x), s5 (x), s10 (x) of the Fourier series (3.55). Do you notice a Gibbs phenomenon? If so, what is the amount of overshoot? If not, explain why.
(b) Answer the same question for the Fourier series (3.81) for the function sign x.
3.2.48. Explain why, in the case of the step function σ(x), all its Fourier cosine coeﬃcients vanish, ak = 0, except for a0 = 1.
♠ 3.2.49. How many terms do you need to sum in (3.56) to correctly approximate π to two decimal digits? To ten digits?
3.2.50. Prove that

(−1)k−1
1
1
1
1
1
1
π2
= 1− + −
+
−
+
− ··· =
.
2
k
4
9
16
25
36
49
12
k=1
∞


Complex Fourier Series
An alternative, and often more convenient, approach to Fourier series is to use complex
exponentials instead of sines and cosines. Indeed, Euler’s formula
e i k x = cos k x + i sin k x,

e− i k x = cos k x − i sin k x,

(3.59)

3.2 Fourier Series

89

shows how to write the trigonometric functions
e i k x + e− i k x
e i k x − e− i k x
(3.60)
,
sin k x =
,
2
2i
in terms of complex exponentials, and so we can easily go back and forth between the two
representations.
Like their trigonometric antecedents, complex exponentials are also endowed with an
underlying orthogonality. But here, since we are dealing with the vector space of complexvalued functions on the interval [ − π, π ], we need to use the rescaled L2 Hermitian inner
product
 π
1
f ,g =
f (x) g(x) dx ,
(3.61)
2 π −π
cos k x =

in which the second function acquires a complex conjugate, as indicated by the overbar.
This is needed to ensure that the associated L2 Hermitian norm

 π
1
f  =
| f (x) |2 dx
(3.62)
2 π −π
is real and positive for all nonzero complex functions:  f  > 0 when f ≡ 0. Orthonormality of the complex exponentials is proved by direct computation:

 π
1,
k = l,
1
ikx
i lx
i (k−l)x
,e
=
e
dx =
e
2 π −π
0,
k = l,
(3.63)
 π
1
| e i k x |2 dx = 1.
 e i k x 2 =
2 π −π
The complex Fourier series for a (piecewise continuous) real or complex function f is
the doubly inﬁnite series
f (x) ∼

∞


ck e i k x = · · · + c−2 e−2 i x + c−1 e− i x + c0 + c1 e i x + c2 e2 i x + · · · . (3.64)

k = −∞

The orthonormality formulae (3.63) imply that the complex Fourier coeﬃcients are obtained by taking the inner products
 π
1
ikx
=
f (x) e− i k x dx.
(3.65)
ck =  f , e
2 π −π
Pay particular attention to the minus sign appearing in the integrated exponential, which
happens because the second argument in the Hermitian inner product (3.61) requires a
complex conjugate.
It must be emphasized that the real (3.34) and complex (3.64) Fourier formulae are just
two diﬀerent ways of writing the same series! Indeed, if we substitute Euler’s formula (3.59)
into (3.65) and compare the result with the real Fourier formulae (3.35), we ﬁnd that the
real and complex Fourier coeﬃcients are related by
ak = ck + c−k ,

ck = 12 (ak − i bk ),

bk = i (ck − c−k ),

c−k = 12 (ak + i bk ),

k = 0, 1, 2, . . . .

(3.66)

90

3 Fourier Series

eπ

cosh π

−π

π

3π

5π

2 π–periodic extension of ex .

Figure 3.9.

Remark : We already see one advantage of the complex version. The constant function
1 = e0 i x no longer plays an anomalous role — the annoying factor of 12 in the real Fourier
series (3.34) has mysteriously disappeared!
Example 3.17. For the unit step function σ(x) considered in Example 3.9, the
complex Fourier coeﬃcients are
⎧ 1
k = 0,
⎪
2,
⎪
⎪
 π
 π
⎨
1
1
0,
0 = k even,
ck =
σ(x) e− i k x dx =
e− i k x dx =
⎪
2 π −π
2π 0
⎪
⎪
⎩ 1 ,
k odd.
ikπ
Therefore, the step function has the complex Fourier series
σ(x) ∼

1
i
−
2
π

∞

l = −∞

e(2 l+1) i x
.
2l + 1

(3.67)

You should convince yourself that this is exactly the same series as the real Fourier series
(3.49). We are merely rewriting it using complex exponentials instead of real sines and
cosines.
Example 3.18. Let us ﬁnd the Fourier series for the exponential function ea x . It is
much easier to evaluate the integrals for the complex Fourier coeﬃcients, and so
 π
π
1
e(a− i k) x
ax
ikx
(a− i k) x
=
e
dx =
ck =  e , e
2 π −π
2 π(a − i k) x = −π
=

(−1)k (a + i k) sinh a π
ea π − e− a π
e(a− i k) π − e− (a− i k) π
= (−1)k
=
.
2 π(a − i k)
2 π(a − i k)
π(a2 + k 2 )

Therefore, the desired Fourier series is
e

ax

∞
sinh a π 
∼
π

k = −∞

(−1)k (a + i k) i k x
e
.
a2 + k 2

(3.68)

3.2 Fourier Series

91

As an exercise, the reader should try writing this as a real Fourier series, either by breaking
up the complex series into its real and imaginary parts, or by direct evaluation of the real
coeﬃcients via their integral formulae (3.35). According to Theorem 3.8 (which is equally
valid for complex Fourier series), the Fourier series converges to the 2 π–periodic extension
of the exponential function, as graphed in Figure 3.9. In particular, its values at odd
multiples of π is the average of the limiting values there, namely cosh a π = 12 (ea π + e− a π ).

Exercises
3.2.51. Find the complex Fourier series of the following functions: (a) sin x,
(b) sin3 x,

x, x ≥ 0,
(c) x, (d) | x |, (e) | sin x |, (f ) sign x, (g) the ramp function ρ(x) =
0, x ≤ 0.
3.2.52. Let − π < ξ < π. Determine the complex Fourier series for the shifted step function
σ(x − ξ), and graph the function it converges to.
3.2.53. Let a ∈ R. Find the real form of the Fourier series for the exponential function ea x :
(a) by breaking up the complex series (3.68) into its real and imaginary parts;
(b) by direct evaluation of the real coeﬃcients via their integral formulae (3.35).
Make sure that your results agree!




1
1
1
2
1
+
+
+ · · · , where
+
2
2
π
π 1+1
1+2
1 + 32
x
−x
cosh x
e +e
coth x =
is the hyperbolic cotangent function.
= x
sinh x
e − e− x

3.2.54. Prove that coth π =

3.2.55. (a) Find the complex Fourier series for x e i x .
(b) Use your result to write down the real Fourier series for x cos x and x sin x.
♦ 3.2.56. Prove that if f (x) =

n

k=m

rk e i k x is a complex trigonometric polynomial, with


− ∞ < m ≤ n < ∞, then its Fourier coeﬃcients are ck =

rk ,
0,

m ≤ k ≤ n,
otherwise.

3.2.57. True or false: If the complex function f (x) = g(x) + i h(x) has Fourier coeﬃcients ck ,
then g(x) = Re f (x) and h(x) = Im f (x) have, respectively, complex Fourier coeﬃcients
Re ck and Im ck .
♦ 3.2.58. Let f (x) be 2 π–periodic. Explain how to construct the complex Fourier series for
f (x − a) from that of f (x).
♦ 3.2.59. (a) Show that if ck are the complex Fourier coeﬃcients for f (x), then the Fourier coefﬁcients of f (x) = f (x) e i x are ck = ck−1 . (b) Let m be an integer. Which function has
complex Fourier coeﬃcients ck = ck+m ? (c) If ak , bk are the Fourier coeﬃcients of the real
function f (x), what are the Fourier coeﬃcients of f (x) cos x and f (x) sin x?
♦ 3.2.60. Can you recognize whether a function is real by looking at its complex Fourier coeﬃcients?
♦ 3.2.61. Can you characterize the complex Fourier coeﬃcients of an even function?
an odd function?
♦ 3.2.62. What does it mean for a doubly inﬁnite series

∞


k = −∞

ck to converge? Be precise!

92

3 Fourier Series

3.3 Diﬀerentiation and Integration
Under appropriate hypotheses, if a series of functions converges, then one will be able
to integrate or diﬀerentiate it term by term, and the resulting series should converge to
the integral or derivative of the original sum. For example, integration and diﬀerentiation
of power series is always valid within the range of convergence, and is used extensively
in the construction of series solutions of diﬀerential equations, series for integrals of nonelementary functions, and so on. (See Section 11.3 for further details.) The convergence
of Fourier series is considerably more delicate, and so one must exercise due care when
diﬀerentiating or integrating. Nevertheless, in favorable situations, both operations lead to
valid results, and are quite useful for constructing Fourier series of more intricate functions.
Integration of Fourier Series
Integration is a smoothing operation — the integrated function is always nicer than the
original. Therefore, we should anticipate being able to integrate Fourier series without
diﬃculty. There is, however, one complication: the integral of a periodic function is not
necessarily periodic. The simplest example is the constant function 1, which is certainly
periodic, but its integral, namely x, is not. On the other hand, integrals of all the other
periodic sine and cosine functions appearing in the Fourier series are periodic. Thus, only
the constant term
 π
a0
1
=
f (x) dx
(3.69)
2
2 π −π
might cause us diﬃculty when we try to integrate a Fourier series (3.34). Note that (3.69)
is the mean, or average, of the function f (x) over the interval [ − π, π ], and so a function
has no constant term in its Fourier series, i.e., a0 = 0, if and only if it has mean zero. It
is easily shown, cf. Exercise 3.2.10, that the mean-zero functions are precisely those that
remain periodic upon integration. In particular, Lemma 3.13 implies that all odd functions
automatically have mean zero, and hence have periodic integrals.
 x
f (y) dy is 2 π–
Lemma 3.19. If f (x) is 2 π–periodic, then its integral g(x) =
 π
0
f (x) dx = 0, so that f has mean zero on the interval [ − π, π ].
periodic if and only if
−π

In view of the elementary integration formulae


cos k x
sin k x
,
sin k x dx = −
,
cos k x dx =
k
k

(3.70)

termwise integration of a Fourier series without constant term is straightforward.
Theorem 3.20. If f is piecewise continuous and has mean zero on the interval
[ − π, π ], then its Fourier series
∞

[ ak cos k x + bk sin k x ]
f (x) ∼
k=1

can be integrated term by term, to produce the Fourier series

 x
∞ 

ak
bk
sin k x .
− cos k x +
g(x) =
f (y) dy ∼ m +
k
k
0
k=1

(3.71)

3.3 Diﬀerentiation and Integration

93

The constant term
m=

1
2π

 π
g(x) dx

(3.72)

−π

is the mean of the integrated function.
 π
Example 3.21. The function f (x) = x is odd, and so has mean zero:
Let us integrate its Fourier series
x ∼ 2

∞

(−1)k−1
k=1

k

x dx = 0.
−π

sin k x,

(3.73)

which we found in Example 3.3. The result is the Fourier series
∞

 (−1)k−1
1 2
π2
x ∼
− 2
cos k x
2
6
k2
k=1


π2
cos 2 x
cos 3 x
cos 4 x
=
− 2 cos x −
+
−
+ ··· ,
6
4
9
16

(3.74)

whose constant term is the mean of the left-hand side:
 π 2
1
π2
x
dx =
.
2 π −π 2
6
Let us revisit the derivation of the integrated Fourier series from a slightly diﬀerent
standpoint. If we were to integrate each trigonometric summand in a Fourier series (3.34)
from 0 to x, we would obtain
 x
 x
sin k x
1 cos k x
,
whereas
−
.
cos k y dy =
sin k y dy =
k
k
k
0
0
The extra 1/k terms coming from the deﬁnite sine integrals did not appear explicitly in
our previous expression for the integrated Fourier series, (3.71), and so must be hidden in
the constant term m. We deduce that the mean value of the integrated function can be
computed using the Fourier sine coeﬃcients of f via the formula
1
2π

 π
g(x) dx = m =
−π

∞

b

k

k=1

k

.

(3.75)

For example, integrating both sides of the Fourier series (3.73) for f (x) = x from 0 to x
produces
∞

x2
(−1)k−1
∼ 2
(1 − cos k x).
2
k2
k=1

The constant terms sum to yield the mean value of the integrated function:

2

1 1
1
1− + −
+ ···
4 9 16


= 2

∞

(−1)k−1
k=1

k2

1
=
2π

which reproduces a formula established in Exercise 3.2.50.

 π

π2
x2
dx =
,
6
−π 2

(3.76)

94

3 Fourier Series

More generally, if f (x) does not have mean zero, its Fourier series contains a nonzero
constant term,
∞

a
[ ak cos k x + bk sin k x ] .
f (x) ∼ 0 +
2
k=1

In this case, the result of integration will be
 x
g(x) =
0

∞


a
f (y) dy ∼ 0 x + m +
2

k=1



a
b
− k cos k x + k sin k x
k
k


,

(3.77)

where m is given in (3.75). The right-hand side is not, strictly speaking, a Fourier series.
There are two ways to interpret this formula within the Fourier framework. We can write
(3.77) as the Fourier series for the diﬀerence
∞


a
g(x) − 0 x ∼ m +
2



k=1

a
b
− k cos k x + k sin k x
k
k


,

(3.78)

which, by Exercise 3.2.10(d), is a 2 π–periodic function. Alternatively, we can replace x
by its Fourier series (3.37), and the result will be the Fourier series for the 2 π–periodic
x

extension of the integral g(x) =

f (y) dy.
0

Diﬀerentiation of Fourier Series
Diﬀerentiation has the opposite eﬀect — it makes a function worse. Therefore, to justify
taking the derivative of a Fourier series, we need to know that the derived function remains
reasonably nice. Since we need the derivative f  (x) to be piecewise C1 for the Convergence
Theorem 3.8 to be applicable, we require that f (x) itself be continuous and piecewise C2 .
Theorem 3.22. If f (x) has a piecewise C2 and continuous 2 π–periodic extension,
then its Fourier series can be diﬀerentiated term by term, to produce the Fourier series for
its derivative


f (x) ∼

∞


k=1



k bk cos k x − k ak sin k x =

∞


i k ck e i k x .

(3.79)

k = −∞

Example 3.23. The derivative (6.31) of the absolute value function f (x) = | x | is
the sign function:

+ 1,
x > 0,
d
| x | = sign x =
(3.80)
dx
− 1,
x < 0.
Therefore, if we diﬀerentiate its Fourier series (3.55), we obtain the Fourier series


4
sin 3 x
sin 5 x
sin 7 x
sign x ∼
sin x +
+
+
+ ··· .
π
3
5
7

(3.81)

Note that sign x = σ(x)−σ(− x) is the diﬀerence of two step functions. Indeed, subtracting
the step function Fourier series (3.49) at x from the same series at − x reproduces (3.81).

3.4 Change of Scale

95

Exercises
3.3.1. Starting with the Fourier series (3.49) for the step function
σ(x), use integration to:

x, x > 0,
(a) Find the Fourier series for the ramp function ρ(x) =
0, x < 0.

1 2
2 x , x > 0,
(b) Then, ﬁnd the Fourier series for the second-order ramp function ρ2 (x) =
0,
x < 0.
3.3.2. Find the Fourier series for the function f (x) = x3 . If you diﬀerentiate your series, do you
recover the Fourier series for f  (x) = 3 x2 ? If not, explain why not.
3.3.3. Answer Exercise 3.3.2 when f (x) = x4 .
3.3.4. Use Theorem 3.20 to construct the Fourier series for (a) x3 , (b) x4 .
3.3.5. Write down the identities obtained by substituting x = 0, 12 π , and 13 π in the Fourier
series (3.74).
♦ 3.3.6. Suppose f (x) is a 2 π–periodic function with complex Fourier coeﬃcients ck , and g(x)
is a 2 π–periodic function with complex Fourier coeﬃcients dk . (a) Find the Fourier coeﬃcients ek of their periodic convolution f (x) ∗ g(x) =

π

−π

f (x − y) g(y) dy.

(b) Find the complex Fourier series for the periodic convolution of cos 3 x and sin 2 x.
(c) Answer part (b) for the functions x and sin 2 x.
♦ 3.3.7. Suppose f is piecewise continuous on [ − π, π ]. Prove that the mean of the integrated
x
1 π 
x
function g(x) =
f (y) dy equals
sign x −
f (x) dx.
0
2 −π
π
3.3.8. Suppose the 2 π–periodic extension of f (x) is continuous and piecewise C1 . Prove directly from the formulas (3.35) that the Fourier coeﬃcients of its derivative f (x) = f  (x)
 = k b and 
are, respectively, a
bk = − k ak , where ak , bk are the Fourier coeﬃcients of f (x).
k
k
♦ 3.3.9. Explain how to integrate a complex Fourier series (3.64). Under what conditions is your
formula valid?
d2 u
du
♥ 3.3.10. The initial value problem 2 + u = f (t), u(0) = 0,
(0) = 0, describes the forced
dt
dt
motion of an initially motionless unit mass attached to a unit spring.
(a) Solve the initial value problem when f (t) = cos k t and f (t) = sin k t for k = 0, 1, . . . .
(b) Assuming that the forcing function f (t) is 2 π–periodic, write out its Fourier series, and
then use your result from part (b) to write out a series for the solution u(t).
(c) Under what conditions is the result a convergent Fourier series, and hence the solution
u(t) remains 2 π–periodic?
(d) Explain why f (t) induces a resonance of the mass-spring system if and only if its Fourier
coeﬃcients of order 1 are not both zero: a21 + b21 = 0.

3.4 Change of Scale
So far, we have dealt only with Fourier series on the standard interval of length 2 π. We
chose [ − π, π ] for convenience, but all of the results and formulas are easily adapted to any
other interval of the same length, e.g., [ 0, 2 π ]. However, since physical objects like bars

96

3 Fourier Series

and strings do not all come in this particular length, we need to understand how to adapt
the formulas to more general intervals.
Any symmetric interval [ −  ,  ] of length 2  can be rescaled (stretched) to the standard
interval [ − π, π ] through the linear change of variables

y,
π

−  ≤ x ≤ .
(3.82)



y lives on
Given a function f (x) deﬁned on [ −  ,  ], the rescaled function F (y) = f
π
[ − π, π ]. Let
∞



a
ak cos k y + bk sin k y
F (y) ∼ 0 +
2
x=

so that

−π≤y≤π

whenever

k=1

be the standard Fourier series for F (y), so that

1 π
F (y) cos k y dy,
ak =
π −π

bk =

1
π

 π
F (y) sin k y dy.

(3.83)

−π

Then, reverting to the unscaled variable x, we deduce that

∞ 

a0
k πx
k πx
f (x) ∼
+
+ bk sin
ak cos
2



(3.84)

k=1

is the Fourier series of f (x) on the interval [ −  ,  ]. The Fourier coeﬃcients ak , bk can,
in fact, be computed directly without appealing to the rescaling. Indeed, replacing the
integration variable in (3.83) by y = πx/, and noting that dy = ( π/ ) dx, we deduce the
rescaled formulae


1
k πx
1
k πx
(3.85)
ak =
f (x) cos
f (x) sin
dx,
bk =
dx,
 −

 −

for the Fourier coeﬃcients of f (x) on the interval [ −  ,  ].
All of the convergence results, integration and diﬀerentiation formulae, etc., that are
valid for the interval [ − π, π ] carry over, essentially unchanged, to Fourier series on nonstandard intervals. In particular, adapting our basic convergence Theorem 3.8, we conclude
that if f (x) is piecewise C1 , then its rescaled Fourier series (3.84) converges to its 2  periodic extension f(x), subject to the proviso that f(x) takes on the midpoint values at all
jump discontinuities.
Example 3.24. Let us compute the Fourier series for the function f (x) = x on the
interval − 1 ≤ x ≤ 1. Since f is odd, only the sine coeﬃcients will be nonzero. We have

1
 1
x cos k πx sin k πx
2(−1)k+1
bk =
x sin k πx dx = −
=
+
.
kπ
(k π)2 x = −1
kπ
−1
The resulting Fourier series is


sin 2 πx
sin 3 πx
2
sin πx −
+
− ··· .
x ∼
π
2
3
The series converges to the 2–periodic extension of the function x, namely

x − 2 m,
2 m − 1 < x < 2 m + 1,
f(x) =
where m ∈ Z is an arbitrary integer,
0,
x = m,
which is plotted in Figure 3.10.

3.4 Change of Scale

97

1

−4

−3

−2

−1

1

2

3

4

−1
Figure 3.10.

2–periodic extension of x.

We can similarly reformulate complex Fourier series on the nonstandard interval
[ −  ,  ]. Using (3.82) to rescale the variables in (3.64), we obtain
f (x) ∼

∞

k = −∞

ck e

i k π x/

,

where

1
ck =
2



f (x) e− i k π x/ dx.

(3.86)

−

Again, this is merely an alternative way of writing the real Fourier series (3.84).
When dealing with a more general interval [ a, b ], there are two possible options. The
ﬁrst is to take a function f (x) deﬁned for a ≤ x ≤ b and periodically extend it to a function
f(x) that agrees with f (x) on [ a, b ] and has period b−a. One can then compute the Fourier
series (3.84) for its periodic extension f(x) on the symmetric interval [ −  ,  ] of width
2  = b − a; the resulting Fourier series will (under the appropriate hypotheses) converge
to f(x) and hence agree with f (x) on the original interval. An alternative approach is to
translate the interval by an amount 12 (a + b) so as to make it symmetric around the origin;
this is accomplished by the change of variables x
 = x − 12 (a + b), followed by an additional
rescaling to convert the interval into [ − π, π ]. The two methods are essentially equivalent,
and details are left to the reader.

Exercises
3.4.1. Let f (x) = x2 for 0 ≤ x ≤ 1. Find its (a) Fourier sine series; (b) Fourier cosine series.
3.4.2. Find the Fourier sine series and the Fourier cosine series of the following functions deﬁned on the interval [ 0, 1 ]; then graph the function to which the series converges:
(a) 1, (b) sin πx, (c) sin3 πx, (d) x(1 − x).
3.4.3. Find the Fourier series for the following functions on the indicated intervals, and graph
the function that the Fourier series converges to.
(a) | x |, −3 ≤ x ≤ 3,
(b) x2 − 4, −2 ≤ x ≤ 2, (c) ex , −10 ≤ x ≤ 10,
(d) sin x, −1 ≤ x ≤ 1, (e) σ(x), −2 ≤ x ≤ 2.
3.4.4. For each of the functions in Exercise 3.4.3, write out the diﬀerentiated Fourier series, and
determine whether it converges to the derivative of the original function.
3.4.5. Find the Fourier series for the integral of each of the functions in Exercise 3.4.3.
♦ 3.4.6. Write down formulas for the Fourier series of both even and odd functions on [ − , ].

98

3 Fourier Series

3.4.7. Let f (x) be a continuous function on [ 0, ].
(a) Under what conditions is its odd 2 –periodic extension also continuous?
(b) Under what conditions is its odd extension also continuously diﬀerentiable?
3.4.8. (a) Write down the formulae for the Fourier series for a function f (x) deﬁned on the interval 0 ≤ x ≤ 2 π. (b) Use your formula in the case f (x) = x. Is the result the same as
(3.37)? Explain, and, if diﬀerent, discuss the connection between the two Fourier series.
3.4.9. Find the Fourier series for the function f (x) = x on the interval 1 ≤ x ≤ 2 using the two
diﬀerent methods described in the last paragraph of this subsection. Are your Fourier series
the same? Explain. Graph the functions that the Fourier series converge to.
3.4.10. Answer Exercise 3.4.9 when f (x) = sin x on the interval π ≤ x ≤ 2 π.

3.5 Convergence of Fourier Series
The goal of this ﬁnal section is to establish some of the most basic convergence results for
Fourier series. This is not a purely theoretical enterprise, since convergence considerations
impinge directly upon applications. One particularly important consequence is the connection between the degree of smoothness of a function and the decay rate of its high-order
Fourier coeﬃcients — a result that is exploited in signal and image denoising and in the
analytic properties of solutions to partial diﬀerential equations.
This section is written at a slightly more theoretically sophisticated level than what you
have read so far. However, an appreciation of the full scope, and limitations, of Fourier
analysis requires some familiarity with the underlying theory. Moreover, the required
techniques and proofs serve as an excellent introduction to some of the most important
tools of modern mathematical analysis, and the eﬀort you expend to assimilate this material
will be more than amply rewarded in both this book and your subsequent mathematical
studies, be they applied or pure.
Unlike power series, which converge to analytic functions on the interval of convergence, and diverge elsewhere (the only tricky point being whether or not the series converges
at the endpoints), the convergence of a Fourier series is a much subtler matter, and still
not completely understood. A large part of the diﬃculty stems from the intricacies of
convergence in inﬁnite-dimensional function spaces. Let us therefore begin with a brief
outline of the key issues.
We assume that you are familiar with the usual calculus deﬁnition of the limit of a
sequence of real numbers: lim an = a . In any ﬁnite-dimensional vector space, e.g.,
n→∞

R m , there is essentially only one way for a sequence of vectors v(0) , v(1) , v(2) , . . . ∈ R m to
converge, as guaranteed by any one of the following equivalent criteria:
• The vectors converge: v(n) → v ∈ R m as n → ∞.
(n)

(n)

(n)
• The individual components of v(n) = (v1 , . . . , vm
) converge, so lim vj = vj for
n→∞
all j = 1, . . . , m.
• The norm of the diﬀerence goes to zero:  v(n) − v  → 0 as n → ∞.
The last requirement, known as convergence in norm, does not, in fact, depend on which
norm is chosen. Indeed, on a ﬁnite-dimensional vector space, all norms are essentially
equivalent, and if one norm goes to zero, so does any other norm, [89; Theorem 3.17].

3.5 Convergence of Fourier Series

99

On the other hand, the analogous convergence criteria are certainly not the same in
inﬁnite-dimensional spaces. There is, in fact, a bewildering variety of convergence mechanisms in function space, including pointwise convergence, uniform convergence, convergence
in norm, weak convergence, and so on. Each plays a signiﬁcant role in advanced mathematical analysis, and hence all are deserving of study. Here, though, we shall cover just
the most basic aspects of convergence of the Fourier series and their applications to partial
diﬀerential equations, leaving the complete development to a more specialized text, e.g.,
[37, 128].
Pointwise and Uniform Convergence
The most familiar convergence mechanism for a sequence of functions vn (x) is pointwise
convergence. This requires that the functions’ values at each individual point converge in
the usual sense:
lim vn (x) = v (x)
for all
x ∈ I,
(3.87)
n→∞

where I ⊂ R denotes an interval contained in their common domain. Even more explicitly,
pointwise convergence requires that, for every ε > 0 and every x ∈ I, there exist an integer
N , depending on ε and x, such that
| vn (x) − v (x) | < ε

for all

n ≥ N.

(3.88)

Pointwise convergence can be viewed as the function space version of the convergence of the
components of a vector. We have already stated the Fundamental Theorem 3.8 regarding
pointwise convergence of Fourier series; the proof will be deferred until the end of this
section.
On the other hand, establishing uniform convergence of a Fourier series is not so
diﬃcult, and so we will begin there. The basic deﬁnition of uniform convergence looks very
similar to that of pointwise convergence, with a subtle, but important, diﬀerence.
Deﬁnition 3.25. A sequence of functions vn (x) is said to converge uniformly to a
function v (x) on a subset I ⊂ R if, for every ε > 0, there exists an integer N , depending
solely on ε, such that
| vn (x) − v (x) | < ε

for all x ∈ I and all n ≥ N .

(3.89)

Clearly, a uniformly convergent sequence of functions converges pointwise, but the
converse does not hold. The key diﬀerence — and the reason for the term “uniform
convergence” — is that the integer N depends only on ε and not on the point x ∈ I.
According to (3.89), the sequence converges uniformly if and only if for every small ε, the
graphs of the functions eventually lie inside a band of width 2 ε centered on the graph of
the limiting function, as in the ﬁrst plot in Figure 3.11. The Gibbs phenomenon shown
in Figure 3.7 is a prototypical example of nonuniform convergence: For a given ε > 0, the
closer x is to the discontinuity, the larger n must be chosen so that the inequality in (3.89)
holds. Hence, there is no uniform choice of N that makes the inequality (3.89) valid for
all x and all n ≥ N .
A key feature of uniform convergence is that it preserves continuity.
Theorem 3.26. If each vn (x) is continuous and vn (x) → v (x) converges uniformly,
then v (x) is also a continuous function.

100

3 Fourier Series

Figure 3.11.

Uniform and nonuniform convergence of functions.

The proof is by contradiction. Intuitively, if v (x) were to have a discontinuity, then, as
sketched in the second plot in Figure 3.11, a suﬃciently small band around its graph would
not connect together, and this prevents the connected graph of any continuous function,
such as vn (x), from remaining entirely within the band. A detailed discussion of these
issues, including the proofs of the basic theorems, can be found in any introductory real
analysis text, [8, 96, 97].
Warning: A sequence of continuous functions can converge nonuniformly to a continuous function. For example, the sequence
vn (x) =

2nx
1 + n2 x2

converges pointwise to v (x) ≡ 0 (why?) but not uniformly, since
max | vn (x) | = vn n1 = 1,
which implies that (3.89) cannot hold when ε < 1.
The convergence (pointwise, uniform, etc.) of an inﬁnite series
deﬁnition, dictated by the convergence of its sequence of partial sums
vn (x) =

n


uk (x).

∞

k = 1 uk (x) is,

by

(3.90)

k=1

The most useful test for uniform convergence of series of functions is known as the Weierstrass M –test, in honor of the nineteenth century German mathematician Karl Weierstrass,
known as the “father of modern analysis”.
Theorem 3.27. Let I ⊂ R. Suppose that, for each k = 1, 2, 3, . . . , the function
uk (x) is bounded:
for all
x ∈ I,
(3.91)
| uk (x) | ≤ mk
where mk ≥ 0 is a nonnegative constant. If the constant series
∞

k=1

mk < ∞

(3.92)

3.5 Convergence of Fourier Series

101

converges, then the function series
∞


uk (x) = f (x)

(3.93)

k=1

converges uniformly and absolutely † to a function f (x) for all x ∈ I. In particular, if the
summands uk (x) are continuous, so is the sum f (x).
Warning: Failure of the M –test strongly indicates, but does not necessarily preclude,
that a pointwise convergent series does not converge uniformly.
With some care, we can manipulate uniformly convergent series just like ﬁnite sums.
Thus, if (3.93) is a uniformly convergent series, so is its term-wise product
∞


g(x) uk (x) = g(x) f (x)

(3.94)

k=1

with any bounded function: | g(x) | ≤ C for all x ∈ I. We can integrate a uniformly
convergent series term by term,‡ and the resulting integrated series

 x
 x
∞
∞  x

uk (y) dy =
uk (y) dy =
f (y) dy
(3.95)
a

k=1

k=1

a

a

is uniformly convergent. Diﬀerentiation is also allowed — but only when the diﬀerentiated
series converges uniformly.
∞

Proposition 3.28. Suppose the series
uk (x) = f (x) converges pointwise. If the
∞
k
=
1

uk (x) = g(x) is uniformly convergent, then the original series is
diﬀerentiated series
k=1

also uniformly convergent, and, moreover, f  (x) = g(x).
We are particularly interested in the convergence of a Fourier series, which, to facilitate
the exposition, we take in its complex form
f (x) ∼

∞


ck e i k x .

(3.96)

k = −∞

Since x is real, e i k x ≤ 1, and hence the individual summands are bounded by
ck e i k x ≤ | ck |

for all x.

Applying the Weierstrass M –test, we immediately deduce the basic result on uniform
convergence of Fourier series.

†

∞

Recall that a series

an = a is said to converge absolutely if

n=1
‡

Assuming that the individual functions are all integrable.

∞
n=1

| an | converges.

102

3 Fourier Series

Theorem 3.29. If the Fourier coeﬃcients ck of a function f (x) satisfy
∞


| ck | < ∞,

(3.97)

k = −∞

then the Fourier series (3.96) converges uniformly to a continuous function f(x) that has
the same Fourier coeﬃcients: ck =  f , e i k x  =  f, e i k x .
Proof : Uniform convergence and continuity of the limiting function follow from Theorem 3.27. To show that the ck actually are the Fourier coeﬃcients of the sum, we multiply
the Fourier series by e− i k x and integrate term by term from − π to π. As in (3.94, 95),
both operations are valid thanks to the uniform convergence of the series.
Q.E.D.
Remark : As with the Weierstrass test, failure of condition (3.97) strongly indicates
that the Fourier series does not converge uniformly, but does not completely rule it out;
nor does it say anything about nonuniform convergence or lack thereof.
The one thing that Theorem 3.29 does not guarantee is that the original function f (x)
used to compute the Fourier coeﬃcients ck is the same as the function f(x) obtained by
summing the resulting Fourier series! Indeed, this may very well not be the case. As we
know, the function that the series converges to is necessarily 2 π–periodic. Thus, at the very
least, f(x) will be the 2 π periodic extension of f (x). But even this may not suﬃce. Indeed,
two functions f (x) and f(x) that have the same values except at a ﬁnite set of points
the same Fourier coeﬃcients. (Why?) For example, the discontinuous
x1 , . . . , xm have 
1, x = 0,
has all zero Fourier coeﬃcients, and hence its Fourier
function f (x) =
0, otherwise,
series converges to the continuous zero function. More generally, two functions that agree
everywhere outside a set of “measure zero” will have identical Fourier coeﬃcients. In this
way, a convergent Fourier series singles out a distinguished representative from a collection
of essentially equivalent 2 π–periodic functions.
Remark : The term “measure” refers to a rigorous generalization of the notion of the
length of an interval to more general subsets S ⊂ R. In particular, S has measure zero if
it can be covered by a collection of intervals of arbitrarily small total length. For example,
any set consisting of ﬁnitely many points, or even countably many points, e.g., the rational
numbers, has measure zero; see Exercise 3.5.19. The proper development of the notion of
measure, and the consequential Lebesgue theory of integration, is properly studied in a
course in real analysis, [96, 98].
As a consequence of Theorem 3.26, a Fourier series cannot converge uniformly when
discontinuities are present. However, it can be proved, [128], that even when the function
is not everywhere continuous, its Fourier series is uniformly convergent on any closed subset
of continuity.
Theorem 3.30. Let f (x) be 2 π–periodic and piecewise C1 . If f (x) is continuous on
the open interval a < x < b, then its Fourier series converges uniformly to f (x) on any
closed subinterval a + δ ≤ x ≤ b − δ for 0 < δ < 12 (b − a).
For example, the Fourier series (3.49) for the unit step function converges uniformly
if we stay away from the discontinuities — for instance, by restriction to a subinterval of
the form [ δ, π − δ ] or [ − π + δ, − δ ] for any 0 < δ < 12 π. This reconﬁrms our observation

3.5 Convergence of Fourier Series

103

that the nonuniform Gibbs behavior becomes progressively more and more localized at the
discontinuities.

Exercises
3.5.1. Consider the following sequence of planar vectors v(n) =





1
1 − n , e− n , n = 1, 2, 3, . . . .

Prove that v(n) converges to v = ( 1, 0 ) as n → ∞ by showing that: (a) the individual
components converge; (b) the Euclidean norms converge:  v(n) − v 2 → 0.
3.5.2. Which
of the following
⎞ sequences of vectors converge as n → ∞? What is the limit?
⎛




2
n
1
cos n sin n
1
1
⎠, (b) ( cos n, sin n ), (c)
,
,
(d)
cos
,
,
sin
(a) ⎝
n
n
n
n ,
1 + n2 1 + 2 n2


(e)
⎛

⎛





log n (log n) (log n)
1 1
1
1
−n
−n
2 −n
, (g) ⎝
,
,
n cos n , n sin n , (f ) e , n e , n e
n
n2
n3
⎞



2

3

⎞
⎠,







1 − n 1 − n 1 − n2 ⎠
1 n
1 −n
,
,
(i)
1
+
,
1
−
,
,
n
n
1 + n 1 + n2 1 + n2
 n


 



e − 1 cos n − 1
1
, (k) n e1/n − 1 , n2 cos n − 1
.
(j)
,
2
n
n

(h) ⎝

3.5.3. Which of the following sequences of functions converge pointwise for x ∈ R as n → ∞?
2
1
x2
,
What is the limit? (a) 1 − 2 , (b) e− n x , (c) e− n x , (d) | x − n |, (e)
n
1 + (x − n)2



x,
| x | < n,
1, x < n,
n2 , n1 < x < n2 , (h)
(g)
(f )
−2
2, x > n,
n x , | x | ≥ n.
0,
otherwise,


3.5.4. Prove that the sequence vn (x) =

1,
0,

0 < x < n1 ,
converges pointwise, but not uniotherwise,

formly, to the zero function.
3.5.5. Which of the following sequences of functions converge pointwise to the zero function for
all x ∈ R? Which converge uniformly?
1
1
x2
(b) e− n | x | ,
(c) x e− n | x | ,
(d)
,
,
(e)
(a) − 2 ,
2)
n
n
(1
+
x
1
+
(x
− n)2



1
1
x/n,
| x | < 1,
n, 0 < | x | < n ,
n , 0 < | x | < n, (h)
(i)
(f ) | x − n |, (g)
1/(n x), | x | ≥ 1.
0, otherwise,
0, otherwise,
2

3.5.6. Does the sequence vn (x) = n x e− n x converge pointwise to the zero function for x ∈ R?
Does it converge uniformly?
3.5.7. Answer Exercise 3.5.6 when


1, n < x < n + 1/n,
0,
otherwise,

√
1/ n, n < x < 2 n,
(e) vn (x) =
0,
otherwise,

(c) vn (x) =



1, n < x < n + 1,
0, otherwise,

1/n, n < x < 2 n,
(d) vn (x) =
0,
otherwise,

2 2
n
x
−
1, − 1/n < x < 1/n,
(f ) vn (x) =
0,
otherwise.
2

(a) vn (x) = x e− n x ,

(b) vn (x) =

3.5.8. (a) What is the limit of the functions vn (x) = tan−1 n x as n → ∞? (b) Is the convergence uniform on all of R? (c) on the interval [ − 1, 1 ]? (d) on the subset { x ≥ 1 }?
3.5.9. True or false: If pn (x) is a sequence of polynomials that converge pointwise to a polynomial p (x), then the convergence is uniform.

104

3 Fourier Series

3.5.10. Suppose vn (x) are continuous functions such that vn → v pointwise on all of R.
vn
True or false: (a) vn − v → 0 pointwise; (b) if v (x) = 0 for all x, then v → 1 pointwise.

3.5.11. Which of the following series satisfy the M –test and hence converge uniformly on the
∞ cos k x
∞ sin k x
∞
∞
k
interval [ 0, 1 ]?
(a)
,
(b)
x
,
(d)
(x/2)k ,
,
(c)
k2
k
k=1
k=1
k=1
k=1
∞

(e)

ek x
,
2
k=1 k

e− k x
,
k2
k=1
∞

(f )

∞

(g)

ex/k − 1
.
k
k=1

∞

3.5.12. Prove that the power series

xk
converges uniformly for −1 ≤ x ≤ 1.
k = 1 k(k + 1)

♦ 3.5.13. (a) Prove the following result: Suppose | g(x) | ≤ M for all x ∈ I. If (3.93) is a uniformly convergent series on I, so is the term-wise product (3.94).
(b) Find a counterexample when g(x) is not uniformly bounded.
♦ 3.5.14. Suppose each uk (x) is continuous, and the series

∞

uk (x) = f (x) converges uniformly
k=1

on the bounded interval a ≤ x ≤ b. Prove that the integrated series (3.95) is uniformly
convergent.
♦ 3.5.15. Prove that if

∞ 

a2k + b2k < ∞, then the real Fourier series (3.34) converges uniformly

k=1

to a continuous 2 π–periodic function.
∞

∞

3.5.16. Suppose
| ak | < ∞ and
| bk | < ∞. Does the conclusion of Exercise 3.5.15 still
k=1
k=1
hold?
3.5.17. Explain why you only need check the inequalities (3.91) for all suﬃciently large k
in order to use the Weierstrass M –test.

0

3.5.18. Suppose we say that a sequence of vectors v(k) ∈ R m converges uniformly to v ∈ R m
(k)
if, for every ε > 0, there is an N, depending only on ε, such that | vi − vi | < ε, for all
k ≥ N and all i = 1, . . . , m. Prove that every convergent sequence of vectors converges
uniformly.
♦ 3.5.19. (a) Let S = { x1 , x2 , x3 , . . . } ⊂ R be a countable set. Prove that S has measure zero by
showing that, for every ε > 0, there exists a collection
, . . . ⊂ R,
 of open intervals I1 , I2 , I3 
with respective lengths 1 , 2 , 3 , . . . , such that S ⊂ Ij , while the total length
j = ε.
(b) Explain why the set of rational numbers Q ⊂ R is dense but nevertheless has measure
zero.

Smoothness and Decay
The criterion (3.97), which guarantees uniform convergence of a Fourier series, requires,
at the very least, that the Fourier coeﬃcients go to zero: ck → 0 as k → ± ∞. And they
cannot decay too slowly. For example, the individual summands of the inﬁnite series
∞

0=k=− ∞

1
| k |α

(3.98)

go to 0 as k → ∞ whenever α > 0, but the series converges only when α > 1. (This is an

3.5 Convergence of Fourier Series

105

immediate consequence of the standard integral convergence test, [8, 97, 108].) Thus, if
we can bound the Fourier coeﬃcients by
| ck | ≤

M
| k |α

for all

| k |  0,

(3.99)

for some exponent α > 1 and some positive constant M > 0, then the Weierstrass M –test
will guarantee that the Fourier series converges uniformly to a continuous function.
An important consequence of the diﬀerentiation formula (3.79) for Fourier series is that
one can detect the degree of smoothness of a function by seeing how rapidly its Fourier
coeﬃcients decay to zero. More rigorously:
Theorem 3.31. Let 0 ≤ n ∈ Z. If the Fourier coeﬃcients of f (x) satisfy
∞


| k |n | ck | < ∞,

(3.100)

k = −∞

then the Fourier series (3.64) converges uniformly to an n–times continuously diﬀerentiable
function f(x) ∈ Cn , which is the 2 π–periodic extension of f (x). Furthermore, for any 0 <
m ≤ n, the m–times diﬀerentiated Fourier series converges uniformly to the corresponding
derivative f(m) (x).
Proof : Iterating (3.79), the Fourier series for the nth derivative of a function is
f (n) (x) ∼

∞


i n k n ck e i k x .

(3.101)

k = −∞

If (3.100) holds, the Weierstrass M –test implies the uniform convergence of the diﬀerentiated series (3.101) to a continuous 2 π–periodic function. Proposition 3.28 guarantees that
Q.E.D.
the limit is the nth derivative of the original Fourier series.
This result enables us to quantify the rule of thumb that, the smaller the highfrequency Fourier coeﬃcients, the smoother the function.
Corollary 3.32. If the Fourier coeﬃcients satisfy (3.99) for some α > n+ 1, then the
Fourier series converges uniformly to an n–times continuously diﬀerentiable 2 π–periodic
function.
If the Fourier coeﬃcients go to zero faster than any power of k, e.g., exponentially
fast, then the function is inﬁnitely diﬀerentiable. Analyticity is more delicate, and we refer
the reader to [128] for details.
Example 3.33. The 2 π–periodic extension of the function | x | is continuous with
piecewise continuous ﬁrst derivative. Its Fourier coeﬃcients (3.54) satisfy the estimate
(3.99) for α = 2, which is not quite fast enough to ensure a continuous second derivative.
On the other hand, the Fourier coeﬃcients (3.36) of the step function σ(x) tend to zero only
as 1/| k |, so α = 1, reﬂecting the fact that its periodic extension is piecewise continuous,
but not continuous.

106

3 Fourier Series

Exercises
∞

1 i kx
e
converges uniformly on
2
k
k=1
the interval [ − π, π ]. (b) Is the sum f (x) continuous? Why or why not?
(c) Is f (x) continuously diﬀerentiable? Why or why not?

3.5.20. (a) Prove that the complex Fourier series f (x) =

3.5.21. First, without explicitly evaluating them, how fast do you expect the Fourier coeﬃcients of the following functions to go to zero as k → ∞? Then prove your claim by evaluating the coeﬃcients.
(a) x − π, (b) | x |, (c) x2 , (d) x4 − 2 π 2 x2 , (e) sin2 x, (f ) | sin x |.
3.5.22. Using the criteria of Theorem 3.31, determine how many continuous derivatives the
functions represented by the following Fourier series have:
∞
∞
∞
∞ ei kx
ei kx
ei kx
i k x−k2
(a)
,
(b)
,
(c)
e
,
(d)
,
4
2
5
k = −∞ 1 + k
k=− ∞ k + k
k = −∞
k=0 k + 1
∞

ei kx
(e)
,
k = −∞ | k | !

∞

k=0


(f )
k=1

1
1 − cos 2
k



e i k x.

♣ 3.5.23. Discuss convergence of each of the following Fourier series. How smooth is the sum?
Graph the partial sums to obtain a reasonable approximation to the graph of the summed
series. How many summands are needed to obtain accuracy in the second decimal digit over
the entire interval? Point out discontinuities, corners, and other features that you observe.
∞
∞ cos k x
∞ sin k x
∞ sin k x
(a)
e− k cos k x, (b)
, (d)
, (c)
.
3
3/2
k=0
k=0 k + 1
k=1 k
k=1 k + k
3.5.24. Prove that if | ak |, | bk | ≤ M k− α for some M > 0 and α > n + 1, then the real Fourier
series (3.34) converges uniformly to an n–times continuously diﬀerentiable 2 π–periodic
function f ∈ Cn .
3.5.25. Give a simple explanation of why, if the Fourier coeﬃcients ak = bk = 0 for all suﬃciently large k
0, then the Fourier series converges to an analytic function.

Hilbert Space
In order to make further progress, we must take a little detour. The proper setting for
the rigorous theory of Fourier series turns out to be the most important function space in
modern analysis and modern physics, known as Hilbert space in honor of the great latenineteenth-/early-twentieth-century German mathematician David Hilbert. The precise
deﬁnition of this inﬁnite-dimensional inner product space is somewhat technical, but a
rough version goes as follows:
Deﬁnition 3.34. A complex-valued function f (x) is called square-integrable on the
interval [ − π, π ] if it has ﬁnite L2 norm:
 π
1
 f 2 =
| f (x) |2 dx < ∞.
(3.102)
2 π −π
The Hilbert space L2 = L2 [ − π, π ] is the vector space consisting of all complex-valued
square-integrable functions.

3.5 Convergence of Fourier Series

107

The triangle inequality
f +g ≤ f + g
implies that if f, g ∈ L2 , so  f ,  g  < ∞, then  f + g  < ∞, and so f + g ∈ L2 .
Moreover, for any complex constant c,
 c f  = | c |  f ,
and so c f ∈ L2 also. Thus, as claimed, Hilbert space is a complex vector space. The
Cauchy–Schwarz inequality
|f ,g| ≤ f  g
implies that the L2 Hermitian inner product
 π
1
f (x) g(x) dx
f ,g =
2 π −π

(3.103)

of two square-integrable functions is well deﬁned and ﬁnite. In particular, the Fourier
coeﬃcients of a function f ∈ L2 are speciﬁed by its inner products
 π
1
ck =  f , e i k x  =
f (x) e− i k x dx
2 π −π
with the complex exponentials (which, by (3.63), are in L2 ), and hence are all well deﬁned
and ﬁnite.
There are some interesting analytic subtleties that arise when one tries to prescribe
precisely which functions are in the Hilbert space. Every piecewise continuous function
belongs to L2 . But some functions with singularities are also members. For example, the
power function | x |− α belongs to L2 for any α < 12 , but not if α ≥ 12 .
Analysis relies on limiting procedures, and it is essential that Hilbert space be “complete” in the sense that appropriately convergent† sequences of functions have a limit. The
completeness requirement is not elementary, and relies on the development of the more
sophisticated Lebesgue theory of integration, which was formalized in the early part of
the twentieth century by the French mathematician Henri Lebesgue. Any function which
is square-integrable in the Lebesgue sense is admitted into L2 . This includes such non1
piecewise-continuous functions as sin and x−1/3 , as well as the strange function
x

1
if x is a rational number,
(3.104)
r(x) =
0
if x is irrational.
Thus, while well behaved in some respects, square-integrable functions can be quite wild
in others.
Remark : The completeness of Hilbert space can be viewed as the inﬁnite-dimensional
analogue of the completeness of the real line R, meaning that every convergent Cauchy
sequence of real numbers has a limit in R. On the other hand, the rational numbers Q are
not complete — since a convergent sequence of rational numbers may well have an irrational
limit — but form a dense subset of R, because every real number can be arbitrarily closely
†

The precise technical requirement is that every Cauchy sequence of functions vk ∈ L2
converges to a function v ∈ L2 ; see [ 37, 96, 98 ] and also Exercise 3.5.42 for details.

108

3 Fourier Series

approximated by rational numbers, e.g., its truncated decimal expansions. Indeed, a fully
rigorous deﬁnition of the real numbers R is somewhat delicate, [96, 97].
Similarly, the space of continuous functions C0 [ − π, π ] is not complete, in that (nonuniformly) convergent sequences of continuous functions are not, in general, continuous, but it
does form a dense subspace of the Hilbert space L2 [ − π, π ], since every L2 function can be
arbitrarily closely approximated (in norm) by continuous functions, e.g., its approximating
trigonometric polynomials. Thus, just as R can be viewed as the completion of Q under
the Euclidean norm, so Hilbert space can be viewed as the completion of the space of continuous functions under the L2 norm, and, just like that of R, its fully rigorous deﬁnition
is rather subtle.
A second complication is that (3.102) does not, strictly speaking, deﬁne a norm once
we allow discontinuous functions into the fold. For example, the piecewise continuous
function

1,
x = 0,
f0 (x) =
(3.105)
0,
x = 0,
has norm zero,  f0  = 0, even though it is not zero everywhere. Indeed, any function
that is zero except on a set of measure zero also has norm zero, including the function
(3.104). Therefore, in order to make (3.102) into a legitimate norm, we must agree to
identify any two functions that have the same values except on a set of measure zero.
Thus, the zero function 0 along with the preceding examples (3.104) and (3.105) are all
viewed as deﬁning the same element of Hilbert space. So, an element of Hilbert space is
not, in fact, a function, but, rather, an equivalence class of functions all diﬀering on a set
of measure zero. All this may strike the applications-oriented reader as becoming much too
abstract and arcane. In practice, you will not lose much by working with the elements of
L2 as if they were ordinary functions, and, even better, assuming that said “functions” are
always piecewise continuous and square-integrable. Nevertheless, the full analytical power
of Hilbert space theory is unleashed only by including completely general square-integrable
functions.
After its invention by pure mathematicians around the turn of the twentieth century,
physicists in the 1920s suddenly realized that Hilbert space was the ideal setting for the
modern theory of quantum mechanics, [66, 72, 115]. A quantum-mechanical wave function
is an element‡ ϕ ∈ L2 that has unit norm:  ϕ  = 1. Thus, the set of wave functions is
merely the “unit sphere” in Hilbert space. Quantum mechanics endows each physical
wave function with a probabilistic interpretation. Suppose the wave function represents
a single subatomic particle — photon, electron, etc. Then the squared modulus of the
wave function, | ϕ(x) |2 , represents the probability density that quantiﬁes the chance of the
particle being located at position x. More precisely, the probability that the particle resides

 b
1
| ϕ(x) |2 dx . In particular, the
in a prescribed interval [ a, b ] ⊂ [ − π, π ] is equal to
2π a
wave function has unit norm,

 π
1
ϕ =
| ϕ(x) |2 dx = 1,
(3.106)
2 π −π
‡
Here we are acting as if the physical universe were represented by the one-dimensional interval
[ − π, π ]. The more apt context of three-dimensional physical space is developed analogously,
replacing the single integral by a triple integral over all of R 3 . See also Section 7.4.

3.5 Convergence of Fourier Series

109

because the particle must certainly, i.e., with probability 1, be somewhere!
Convergence in Norm
We are now in a position to discuss convergence in norm of a Fourier series. We begin with
the basic deﬁnition, which makes sense on any normed vector space.
Deﬁnition 3.35. Let V be a normed vector space. A sequence s1 , s2 , s3 , . . . ∈ V is
said to converge in norm to f ∈ V if  sn − f  → 0 as n → ∞.
As we noted earlier, on ﬁnite-dimensional vector spaces such as R m , convergence in
norm is equivalent to ordinary convergence. On the other hand, on inﬁnite-dimensional
function spaces, convergence in norm diﬀers from pointwise convergence. For instance, it
is possible to construct a sequence of functions that converges in norm to 0, but does not
converge pointwise anywhere! (See Exercise 3.5.43.)
While our immediate interest is in the convergence of the Fourier series of a squareintegrable function f ∈ L2 [ − π, π ], the methods we develop are of very general utility.
Indeed, in later chapters we will require the analogous convergence results for other types
of series solutions to partial diﬀerential equations, including multiple Fourier series as well
as series involving Bessel functions, spherical harmonics, Laguerre polynomials, and so on.
Since it distills the key issues down to their essence, the general, abstract version is, in fact,
easier to digest, and, moreover, will be immediately applicable, not just to basic Fourier
series, but to very general “eigenfunction series”.
Let V be an inﬁnite-dimensional inner product space, e.g., L2 [ − π, π ]. Suppose
ϕ1 , ϕ2 , ϕ3 , . . . , are an orthonormal collection of elements of V , meaning that

1
j = k,
 ϕj , ϕk  =
(3.107)
0,
j = k.
A straightforward argument — see Exercise 3.5.33 — proves that the ϕk are linearly
independent. Given f ∈ V , we form its generalized Fourier series
f ∼

∞


ck ϕk ,

where

ck =  f , ϕk .

(3.108)

k=1

The formula for the coeﬃcient ck is obtained by formally taking the inner product of the
series with ϕk and invoking the orthonormality conditions (3.107). The two main examples
are the real and complex L2 spaces:
2
• V consists of real square-integrable
 π functions deﬁned on [ − π, π ] under the rescaled L
1
inner product  f , g  =
f (x) g(x) dx. The orthonormal system { ϕk } consists
π −π
of the basic trigonometric functions, numbered as follows:
1
ϕ1 = √ , ϕ2 = cos x, ϕ3 = sin x, ϕ4 = cos 2 x, ϕ5 = sin 2 x, ϕ6 = cos 3 x, . . . .
2
• V consists of complex square-integrable functions deﬁned on [ − π, π ] using the Hermitian inner product (3.103). The orthonormal system { ϕk } consists of the complex
exponentials, which we order as follows:
ϕ1 = 1, ϕ2 = e i x ,

ϕ3 = e− i x , ϕ4 = e2 i x ,

ϕ5 = e−2 i x ,

ϕ6 = e3 i x , . . . .

110

3 Fourier Series

In each case, the generalized Fourier series (3.108) reduces to the ordinary Fourier series, with a minor change of indexing. Later, when we extend the separation of variables
technique to partial diﬀerential equations in more than one space dimension, we will encounter a variety of other important examples, in which the ϕk are the eigenfunctions of a
self-adjoint linear operator.
For the remainder of this section, to streamline the ensuing proofs, we will henceforth
assume that V is a real inner product space. However, all results will be formulated so
they are also valid for complex inner product spaces; the slightly more complicated proofs
in the complex case are relegated to the exercises.
By deﬁnition, the generalized Fourier series (3.108) converges in norm to f if the
sequence provided by its partial sums
sn =

n


ck ϕk

(3.109)

k=1

satisﬁes the criterion of Deﬁnition 3.35. Our ﬁrst result states that the partial Fourier
sum (3.109), with ck given by the inner product formula in (3.108), is, in fact, the best
approximation to f ∈ V in the least squares sense, [89].
Theorem 3.36. Let Vn = span {ϕ1 , ϕ2 , . . . , ϕn } ⊂ V be the n-dimensional subspace
spanned by the ﬁrst n elements of the orthonormal system. Then the nth order Fourier
partial sum sn ∈ Vn is the best least squares approximation to f that belongs to the
subspace, meaning that it minimizes  f − pn  among all possible pn ∈ Vn .
Proof : Given any element
pn =

n


dk ϕk ∈ Vn ,

k=1

we have, in view of the orthonormality relations (3.107),
 pn  2 =  pn , pn 

 n
n
n
n




dj ϕj ,
dk ϕk =
dj dk  ϕj , ϕk  =
| dk |2 ,
=
j =1

k=1

j,k = 1

(3.110)

k=1

reproducing the formula (B.27) for the norm with respect to an orthonormal basis. Therefore, by the symmetry property of the real inner product,
 f − pn  2 =  f − pn , f − pn  =  f  2 − 2  f , pn  +  pn  2
n
n
n



=  f 2 − 2
dk  f , ϕk  +  pn 2 =  f 2 − 2
ck dk +
| dk |2
k=1

=  f 2 −

n

k=1

| ck |2 +

k=1
n


k=1

| ck − dk |2 .

k=1

The ﬁnal equality results from adding and subtracting the squared norm of the partial sum
(3.109),
n

 sn  2 =
| ck |2 ,
(3.111)
k=1

3.5 Convergence of Fourier Series

111

which is a particular case of (3.110). We conclude that
 f − pn  2 =  f  2 −  s n  2 +

n


| ck − dk |2 .

(3.112)

k=1

The ﬁrst and second terms on the right-hand side of (3.112) are uniquely determined by
f and hence cannot be altered by the choice of pn ∈ Vn , which aﬀects only the ﬁnal
summation. Since the latter is a sum of nonnegative quantities, it is clearly minimized by
setting all its summands to zero, i.e., setting dk = ck for all k = 1, . . . , n. We conclude
that  f − pn  achieves its minimum value among all pn ∈ Vn if and only if dk = ck , which
implies that pn = sn is the Fourier partial sum (3.109).
Q.E.D.
Example 3.37. Consider the ordinary real Fourier series. The subspace T (n) ⊂ L2
spanned by the trigonometric functions cos k x, sin k x, for 0 ≤ k ≤ n, consists of all
trigonometric polynomials (ﬁnite Fourier sums) of degree ≤ n:

r
pn (x) = 0 +
[ rk cos k x + sk sin k x ] .
2
n

(3.113)

k=1

Theorem 3.36 implies that the nth Fourier partial sum (3.38) is distinguished as the one
that best approximates f (x) in the least squares sense, meaning that it minimizes the L2
norm of the diﬀerence,
 
1 π
| f (x) − pn (x) |2 dx ,
(3.114)
 f − pn  =
π −π
among all such trigonometric polynomials (3.113).
Returning to the general framework, if we set pn = sn , so dk = ck , in (3.112), we
conclude that the minimizing least squares error for the Fourier partial sum is
0 ≤  f − sn  2 =  f  2 −  sn  2 =  f  2 −

n


| ck |2 .

(3.115)

k=1

We conclude that the general Fourier coeﬃcients of the function f must satisfy the inequality
n

| ck |2 ≤  f 2 .
(3.116)
k=1

Let us see what happens in the limit as n → ∞. Since we are summing a sequence of
nonnegative numbers, with uniformly bounded partial sums, the limiting summation must
exist and be subject to the same bound. We have thus established Bessel’s inequality, a
key step on the road to the general theory.
Theorem 3.38. The sum of the squares of the general Fourier coeﬃcients of f ∈ V
is bounded by
∞

| ck |2 ≤  f 2 .
(3.117)
k=1

Now, if a series, such as that on the left-hand side of Bessel’s inequality (3.117), is to
converge, the individual summands must go to zero. Thus, we immediately deduce:

112

3 Fourier Series

Corollary 3.39. The general Fourier coeﬃcients of f ∈ V satisfy ck → 0 as k → ∞.
In the case of the trigonometric Fourier series, Corollary 3.39 yields the following
simpliﬁed form of what is known as the Riemann–Lebesgue Lemma.
Lemma 3.40. If f ∈ L2 [ − π, π ] is square-integrable, then its Fourier coeﬃcients
satisfy

⎫
1 π
⎪
f (x) cos k x dx ⎪
ak =
⎬
π −π
−→ 0
as
k −→ ∞.
(3.118)
 π
⎪
1
⎭
f (x) sin k x dx ⎪
bk =
π −π
Remark : This result is equivalent to the decay of the complex Fourier coeﬃcients
 π
1
f (x) e− i k x dx −→ 0
as
| k | −→ ∞,
(3.119)
ck =
2 π −π
of any complex-valued square-integrable function.
Convergence of the sum (3.117) requires that the coeﬃcients ck not tend to zero too
slowly. For instance, requiring the power bound (3.99) for some α > 12 suﬃces to ensure
∞

| ck |2 < ∞. Thus, as we should have expected, convergence in norm of the
that
k = −∞

Fourier series imposes less-restrictive requirements on the decay of the Fourier coeﬃcients
than uniform convergence — which needed α > 1. Indeed, a Fourier series with slowly
decaying coeﬃcients may very well converge in norm to a discontinuous L2 function, which
is not possible under uniform convergence.
Completeness
Calculations in vector spaces rely on the speciﬁcation of a basis, meaning a set of linearly
independent elements that span the space. The choice of basis serves to introduce a system
of local coordinates on the space, namely, the coeﬃcients in the expression of an element
as a linear combination of basis elements. Orthogonal and orthonormal bases are particularly handy, since the coordinates are immediately calculated by taking inner products,
while general bases require solving linear systems. In ﬁnite-dimensional vector spaces, all
bases contain the same number of elements, which, by deﬁnition, is the dimension of the
space. A vector space is, therefore, inﬁnite-dimensional if it contains an inﬁnite number
of linearly independent elements. However, the question when such a collection forms a
basis for the space is considerably more delicate, and mere counting will no longer suﬃce.
Indeed, omitting a ﬁnite number of elements from an inﬁnite collection would still leave an
inﬁnite number, but the latter will certainly not span the space. Moreover, we cannot, in
general, expect to write a general element of an inﬁnite-dimensional space as a ﬁnite linear
combination of basis elements, and so subtle questions of convergence of inﬁnite series must
also be addressed if we are to properly formulate the concept.
The deﬁnition of a basis of an inﬁnite-dimensional vector space rests on the idea of
completeness. We shall discuss completeness in the general abstract setting, but the key
example is, of course, the Hilbert space L2 [ − π, π ] and the systems of trigonometric or complex exponential functions. For simplicity, we deﬁne completeness in terms of orthonormal

3.5 Convergence of Fourier Series

113

systems here. (Similar arguments will clearly apply to orthogonal systems, but normality
helps to streamline the presentation.)
Deﬁnition 3.41. An orthonormal system ϕ1 , ϕ2 , ϕ3 , . . . ∈ V is called complete if,
for every f ∈ V , its generalized Fourier series (3.108) converges in norm to f :
 f − sn  −→ 0,

as n → ∞,

where

sn =

n


ck ϕk ,

ck =  f , ϕk ,

(3.120)

k=1

is the nth partial sum of the generalized Fourier series (3.108).
Thus, completeness requires that every element of V can be arbitrarily closely approximated (in norm) by a ﬁnite linear combination of the basis elements. A complete
orthonormal system should be viewed as the inﬁnite-dimensional version of an orthonormal basis of a ﬁnite-dimensional vector space. An orthogonal system is called complete
whenever the corresponding orthonormal system obtained by dividing the elements by
their norms is complete. Existence of a complete orthonormal system is directly tied to
completeness of the underlying Hilbert space.
Determining whether a given orthonormal or orthogonal system of functions is complete is a diﬃcult problem, and requires some detailed analysis of their properties. The
key result for classical Fourier series is that the trigonometric functions, or, equivalently,
the complex exponentials, form a complete system; an indication of its proof will appear
below. A general characterization of complete orthonormal eigenfunction systems can be
found in Section 9.4.
Theorem 3.42. The trigonometric functions 1, cos k x, sin k x, k = 1, 2, 3, . . . ,
form a complete orthogonal system in L2 = L2 [ − π, π ]. In other words, if sn (x) denotes
the nth partial sum of the Fourier series of the square-integrable function f (x) ∈ L2 , then
lim  f − sn  = 0.
n→∞

To better comprehend completeness, let us describe some equivalent characterizations
and consequences. One is the inﬁnite-dimensional counterpart of formula (B.27) for the
norm of a vector in terms of its coordinates with respect to an orthonormal basis.
Theorem 3.43. The orthonormal system ϕ1 , ϕ2 , ϕ3 , . . . ∈ V is complete if and only
if Plancherel’s formula
 f 2 =

∞


| ck |2 =

k=1

∞


 f , ϕk 2

(3.121)

k=1

holds for every f ∈ V .
Proof : Theorem 3.43, thus, states that the system of functions is complete if and only
if the Bessel inequality (3.117) is, in fact, an equality. Indeed, letting n → ∞ in (3.115),
we ﬁnd
lim  f − sn 2 =  f 2 − lim

n→∞

n→∞

n

k=1

| ck |2 =  f 2 −

∞


| ck |2 .

k=1

Therefore, the completeness condition (3.120) holds if and only if the right-hand side
vanishes, which is the Plancherel identity (3.121).
Q.E.D.

114

3 Fourier Series

An analogous result holds for the inner product between two elements, which we state
in its general complex form, although the proof given here is for the real version; in Exercise
3.5.35 the reader is asked to supply the slightly more intricate complex proof.
Corollary 3.44. The Fourier coeﬃcients ck =  f , ϕk , dk =  g , ϕk , of any
f, g ∈ V satisfy Parseval’s formula
∞

f ,g =
ck dk .
(3.122)
k=1

Proof : Since, for a real inner product,
 f , g  = 14  f + g 2 −  f − g 2 ,

(3.123)

Parseval’s formula results from applying Plancherel’s formula (3.121) to each term on the
right-hand side:
∞

∞

k=1

k=1

 
1
(ck + dk )2 − (ck − dk )2 =
ck dk ,
f ,g =
4
which agrees with (3.122), since we are assuming that dk = dk are all real.

Q.E.D.

Note that Plancherel’s formula is a special case of Parseval’s formula,† obtained by
setting f = g. In the particular case of the complex exponential basis e i k x of L2 [ − π, π ],
the Plancherel and Parseval formulae take the form
 π
 π
∞
∞


1
1
| f (x) |2 dx =
| ck |2 ,
f (x) g(x) dx =
ck dk ,
(3.124)
2 π −π
2 π −π
k = −∞

k = −∞

, dk =  g , e
 are the ordinary Fourier coeﬃcients of the complexwhere ck =  f , e
valued functions f (x) and g(x). In Exercise 3.5.38, you are asked to write the corresponding
formulas for the real Fourier coeﬃcients.
Completeness also tells us that a function is uniquely determined by its Fourier coefﬁcients.
ikx

ikx

Proposition 3.45. If the orthonormal system ϕ1 , ϕ2 , . . . ∈ V is complete, then the
only element f ∈ V with all zero Fourier coeﬃcients, 0 = c1 = c2 = · · ·, is the zero element:
f = 0. More generally, two elements f, g ∈ V have the same Fourier coeﬃcients if and only
if they are the same: f = g.
Proof : The proof is an immediate consequence of Plancherel’s formula. Indeed, if
ck = 0, then (3.121) implies that  f  = 0 and hence f = 0. The second statement follows
by applying the ﬁrst to their diﬀerence f − g.
Q.E.D.
Another way of stating this result is that the only function that is orthogonal to every
element of a complete orthonormal system is the zero function.‡ In other words, a complete
orthonormal system is maximal in the sense that no further orthonormal elements can be
appended to it.
†
Curiously, Marc-Antoine Parseval des Chênes’ contribution slightly predates Fourier, whereas
Michel Plancherel’s appeared almost a century later.
‡

Or, to be more technically accurate, any function that is zero outside a set of measure zero.

3.5 Convergence of Fourier Series

115

Let us now discuss the completeness of the Fourier trigonometric and complex exponential functions. We shall establish the completeness property only for suﬃciently smooth
functions, leaving the harder general proof to the references, [37, 128].
According to Theorem 3.30, if f (x) is continuous, 2 π periodic, and piecewise C1 , its
Fourier series converges uniformly,
∞


f (x) =

ck e i k x

− π ≤ x ≤ π.

for all

k = −∞

The same holds for its complex conjugate f (x). Therefore,
∞


| f (x) |2 = f (x) f (x) = f (x)

ck e− i k x =

k = −∞

∞


ck f (x) e− i k x ,

k = −∞

which also converges uniformly by (3.94). Formula (3.95) permits us to integrate both
sides from − π to π, yielding
1
f  =
2π

 π

2

| f (x) | dx =
2

−π

∞

k = −∞

ck
2π

 π
f (x) e

−i kx

dx =

−π

∞


ck ck =

k = −∞

∞


| ck |2 .

k = −∞

Therefore, Plancherel’s formula (3.121) holds for any continuous, piecewise C1 function.
With some additional technical work, this result is used to establish the validity of
Plancherel’s formula for all f ∈ L2 , the key step being to suitably approximate f by such
continuous, piecewise C1 functions. With this in hand, completeness is an immediate
consequence of Theorem 3.43.
Q.E.D.

Pointwise Convergence
Let us ﬁnally return to the Pointwise Convergence Theorem 3.8 for the trigonometric
Fourier series. The goal is to prove that, under the appropriate hypotheses on f (x), namely
2 π–periodic and piecewise C1 , the limit of its partial Fourier sums is


lim sn (x) = 12 f (x+ ) + f (x− ) .
(3.125)
n→∞

We begin by substituting the formulae (3.65) for the complex Fourier coeﬃcients into the
formula (3.109) for the nth partial sum:
sn (x) =

n

k = −n

ck e

ikx



 π
n

1
−i ky
=
f (y) e
dy e i k x
2 π −π
k = −n

 n
 π

1
i k(x−y)
dy.
=
f (y)
e
2 π −π
k = −n

To proceed further, we need to calculate the ﬁnal summation
n

k = −n

e i k x = e− i n x + · · · + e− i x + 1 + e i x + · · · + e i n x .

(3.126)

116

3 Fourier Series

This, in fact, has the form of a geometric sum,
m


a rk = a + a r + a r2 + · · · + a rm = a

k=0


 m+1
−1
r
,
r−1

(3.127)

with m + 1 = 2 n + 1 summands, initial term a = e− i nx , and ratio r = e i x . Therefore,

 i (2n+1)x
n

e i (n+1)x − e− i nx
−1
e
=
e i k x = e− i nx
i
x
e −1
eix − 1
k = −n
(3.128)
1
1
sin n + 12 x
e i n+ 2 x − e− i n+ 2 x
=
=
.
sin 12 x
e i x/2 − e− i x/2
In this computation, to pass from the ﬁrst to the second line, we multiplied numerator and
denominator by e− i x/2 , after which we used the formula (3.60) for the sine function in terms
of complex exponentials. Incidentally, (3.128) is equivalent to the intriguing trigonometric
summation formula
1 + 2 cos x + cos 2 x + cos 3 x + · · · + cos n x =

sin n + 12 x
sin 12 x

.

(3.129)

Therefore, substituting back into (3.126), we obtain
 π
sin n + 12 (x − y)
1
dy
sn (x) =
f (y)
2 π −π
sin 12 (x − y)
 π−x
 π
sin n + 12 y
sin n + 12 y
1
1
=
f (x + y )
f
(x
+
y)
d
y

=
dy.
2 π − π−x
2 π −π
sin 12 y
sin 12 y
The second equality is the result of changing the integration variable to y = x − y and
canceling the minus signs in the resulting trigonometric fraction, while the ﬁnal equality
follows since the integrand is 2 π–periodic, and so its integrals over any interval of length
2 π all have the same value; see Exercise 3.2.9.
Thus, to prove (3.125), it suﬃces to show that
 π
sin n + 12 y
1
dy = f (x+ ),
f (x + y)
lim
n→∞ π
sin 12 y
0
(3.130)
 0
sin n + 12 y
1
−
dy = f (x ).
f (x + y)
lim
n→∞ π
sin 12 y
−π
The proofs of the two formulas are identical, and so we concentrate on establishing the
ﬁrst. Using the fact that the integrand is even, and then our summation formula (3.128)
in reverse, yields
 π sin n + 1 y
 π sin n + 1 y
 π 
n
1
1
1
2
2
dy
=
dy
=
e i k y dy = 1,
π 0
2 π −π
2 π −π
sin 12 y
sin 12 y
k = −n

because only the constant term has a nonzero integral. Multiplying this formula by f (x+ )
and then subtracting the result from the ﬁrst formula in (3.130) leads to
 π
1
f (x + y) − f (x+ )
sin n + 12 y dy = 0,
lim
(3.131)
1
n→∞ π
sin
y
0
2

3.5 Convergence of Fourier Series

117

which we now proceed to prove.
We claim that, for each ﬁxed value of x, the function
g(y) =

f (x + y) − f (x+ )
sin 12 y

is piecewise continuous for all 0 ≤ y ≤ π. Owing to our hypotheses on f (x), the only
problematic point is at y = 0, but then, by l’Hôpital’s Rule (for one-sided limits),
f (x + y) − f (x+ )
f  (x + y)
= lim+ 1
= 2 f  (x+ ).
1
1
y→0
sin 2 y
cos
y
2
2

lim+ g(y) = lim+

y→0

y→0

Consequently, (3.131) will be established if we can show that
1
lim
n→∞ π

 π
0

g(y) sin n + 12 y dy = 0

(3.132)

whenever g is piecewise continuous. Were it not for the extra 12 , this would immediately
follow from the simpliﬁed Riemann–Lebesgue Lemma 3.40. More honestly, we can invoke
the addition formula for sin n + 12 y to write
1
π

 π
g(y) sin
0

n + 12

1
y dy =
π

 π
0

g(y) sin 12 y

1
cos n y dy +
π

 π
0

g(y) cos 12 y sin n y dy.

The ﬁrst integral is the nth Fourier cosine coeﬃcient for the piecewise continuous function
g(y) sin 12 y, while the second integral is the nth Fourier sine coeﬃcient for the piecewise
continuous function g(y) cos 12 y. Lemma 3.40 implies that both of these converge to zero
as n → ∞, and hence (3.132) holds. This completes the proof, thus establishing pointwise
convergence of the Fourier series.
Q.E.D.
Remark : An alternative approach to the last part of the proof is to use the general
Riemann–Lebesgue Lemma, whose proof can be found in [37, 128].
Lemma 3.46. Suppose g(x) is piecewise continuous on [ a, b ]. Then
 b
g(x) e i ω x dx

0 = lim

ω→∞

a

 b

= lim

ω→∞

 b
g(x) cos ω x dx + i lim

a

ω→∞

(3.133)
g(x) sin ω x dx.

a

Intuitively, the Riemann–Lebesgue Lemma says that, as the frequency ω gets larger
and larger, the increasingly rapid oscillations of the integrand tend to cancel each other
out.
Remark : While the Fourier series of a merely continuous function need not converge
pointwise everywhere, a deep theorem, proved by the Swedish mathematician Lennart
Carleson in 1966, [28], states that the set of points where it does not converge has measure
zero, and hence the exceptional points form a very small subset.

118

3 Fourier Series

Exercises
3.5.26. Which of the following sequences converge in norm to the zero function on R?

1, n < x < n + 1,
nx
(a) vn (x) =
,
(b)
v
(x)
=
n
2
2
0,
otherwise,
1+n x


1, n < x < n + 1/n,
1/n, n < x < 2 n,
(c) vn (x) =
(d) vn (x) =
0, otherwise,
0,
otherwise,


√
2 2
1/ n, n < x < 2 n,
n
x
−
1, − 1/n < x < 1/n,
(e) vn (x) =
(f ) vn (x) =
0,
otherwise,
0,
otherwise.
3.5.27. Discuss pointwise and L2 convergence of the following sequences on the interval [ 0, 1 ]:

x2
n, 1/n2 < x < 1/n,
(c) e− n x , (d) sin n x.
(a) 1 − 2 , (b)
n
x, otherwise,
3.5.28. Prove, directly from the deﬁnition, the convergence in norm of the Fourier series (3.49)
of the step function.
3.5.29. Let f (x) ∈ L2 [ a, b ] be square integrable. Which constant function g(x) ≡ c best approximates f in the least squares sense?
3.5.30. Suppose the sequence fn (x) converges pointwise to a function f (x) on an interval [ a, b ],
and converges to g (x) in the L2 norm on [ a, b ]. Is f (x) = g (x) at every a ≤ x ≤ b?
3.5.31. Find a formula for the L2 norm of the Fourier series in Exercises 3.5.20 and 3.5.22.
3.5.32. Under what conditions on the function f (x) is the least squares error due to the nth
order Fourier partial sum equal to zero:  f − sn  = 0?
♦ 3.5.33. Let V be an inner product space. Prove that the elements of a (ﬁnite or inﬁnite) orthonormal system ϕ1 , ϕ2 , . . . ∈ V are linearly independent, meaning that any ﬁnite linear
combination vanishes, c1 ϕ1 + · · · + cn ϕn = 0, if and only if the coeﬃcients are all zero:
c1 = · · · = cn = 0.
♦ 3.5.34. Let V be a complex inner product space. Prove that, for all f, g ∈ V ,
(a)  f + g 2 =  f 2 + 2 Re  f , g  +  g 2 ;
(b)  f , g  = 14





 f + g 2 −  f − g 2 + i  f + i g 2 − i  f − i g 2 .

♦ 3.5.35. Let V be an inﬁnite-dimensional complex inner product space, and ϕk ∈ V a complete
orthonormal system. Prove the corresponding Plancherel and Parseval formulas.
Hint: Use the identities in Exercise 3.5.34.
3.5.36. What does Plancherel’s formula (3.121) tell us in a ﬁnite-dimensional vector space?
What about Parseval’s formula (3.122)?
3.5.37. Let f (x) = x, g(x) = sign x. (a) Write out Plancherel’s formula for the complex
Fourier coeﬃcients of f . (b) Write out Plancherel’s formula for the complex Fourier coefﬁcients of g. (c) Write out Parseval’s formula for the complex Fourier coeﬃcients of f, g.
♦ 3.5.38. (a) Prove the real version of the Plancherel formula
∞
1 π
| f (x) |2 dx = 12 a20 +
(a2k + b2k )
π −π
k=1
for the trigonometric Fourier coeﬃcients of a real function f (x).
(b) What is the real version of Parseval’s formula?

(3.134)

3.5.39. Give an alternative proof of formula (3.129) that does not require complex functions by
ﬁrst multiplying through by sin 12 x and then invoking a suitable trigonometric identity for
the product terms.

3.5 Convergence of Fourier Series

119




3.5.40. (a) Prove that the functions ϕn (x) = sin n − 12 x, for n = 1, 2, 3, . . . , form an orthogonal sequence on the interval [ 0, π ] relative to the L2 inner product  f , g  =

π

0

f (x) g(x) dx.

(b) Find the formula for the Fourier coeﬃcients of a function f (x) relative to the orthogonal sequence ϕn (x). (c) State Bessel’s inequality and Plancherel’s formula in this case.
Carefully state any hypotheses that might be required for the validity of your formulas.
♦ 3.5.41. Prove that a sequence of vectors v(n) ∈ R m converges in the Euclidean norm,
 v(n) − v  → 0 as n → ∞, if and only if their individual components converge:
(n)
vi → vi for i = 1, . . . , m.
♦ 3.5.42. Let V be a normed vector space. A sequence vn ∈ V is called a Cauchy sequence if for
every ε > 0 there exists an N such that  vm − vn  < ε whenever both m, n ≥ N. Prove
that a sequence that converges in norm,  vn − v  → 0 as n → ∞, is necessarily a Cauchy
sequence.
Remark : A normed vector space is called complete if every Cauchy sequence
converges in norm. It can be proved, [ 96, 98 ], that any ﬁnite-dimensional normed vector
space is complete, but this is not necessarily the case in inﬁnite dimensions. For example,
the vector spaces consisting of all trigonometric polynomials and of all polynomials are not
complete in the L2 norm. The most important example of a complete inﬁnite-dimensional
vector space is the Hilbert space L2 .


k
1, m
≤ x ≤ k+1
m , where
0, otherwise,
n = 12 m(m + 1) + k and 0 ≤ k ≤ m. Show ﬁrst that m, k are uniquely determined by n.
Then prove that, on the interval [ 0, 1 ], the sequence fn (x) converges in norm to 0 but does
not converge pointwise anywhere!

♦ 3.5.43. For each n = 1, 2, . . . , deﬁne the function fn (x) =

2
∂2u
∂u
2 ∂ u
=
c
, u(0, x) = f (x),
(0, x) = 0,
∂t2
∂x2
∂t
for −∞ < x < ∞, where f (x) → 0 as | x | → ∞. True or false: As t → ∞, the solution
u(t, x) converges to an equilibrium solution (a) pointwise; (b) uniformly; (c) in norm.

♥ 3.5.44. Let u(t, x) solve the initial value problem

∂u
♥ 3.5.45. Answer Exercise 3.5.44 for the initial conditions u(0, x) = 0,
(0, x) = g(x), with
∂t
g(x) → 0 as | x | → ∞.

Chapter 4

Separation of Variables

Three cardinal linear second-order partial diﬀerential equations have collectively driven the
development of the entire subject. The ﬁrst two we have already encountered: The wave
equation describes vibrations and waves in continuous media, including sound waves, water
waves, elastic waves, electromagnetic waves, and so on. The heat equation models diﬀusion
processes, including thermal energy in solids, solutes in liquids, and biological populations.
Third, and in many ways the most important of all, is the Laplace equation and its inhomogeneous counterpart, the Poisson equation, which govern equilibrium mechanics. The
latter two equations arise in an astonishing variety of mathematical and physical contexts,
ranging through elasticity and solid mechanics, ﬂuid mechanics, electromagnetism, potential theory, thermomechanics, geometry, probability, number theory, and many other ﬁelds.
The solutions to the Laplace equation are known as harmonic functions, and the discovery of their many remarkable properties forms one of the most celebrated chapters in the
history of mathematics. All three equations, along with their multi-dimensional kin, will
appear repeatedly throughout this text.
The aim of the current chapter is to develop the method of separation of variables
for solving these key partial diﬀerential equations in their two-independent-variable incarnations. For the wave and heat equations, the variables are time, t, and a single space
coordinate, x, leading to initial-boundary value problems modeling the dynamical behavior of a one-dimensional medium. For the Laplace and Poisson equations, both variables
represent space coordinates, x and y, and the associated boundary value problems model
the equilibrium conﬁguration of a planar body, e.g., the deformations of a membrane. Separation of variables seeks special solutions that can be written as the product of functions
of the individual variables, thereby reducing the partial diﬀerential equation to a pair of
ordinary diﬀerential equations. More-general solutions can then be expressed as inﬁnite
series in the appropriate separable solutions. For the two-variable equations considered
here, this results in a Fourier series representation of the solution. In the case of the wave
equation, separation of variables serves to focus attention on the vibrational character of
the solution, whereas the earlier d’Alembert approach emphasizes its particle-like aspects.
Unfortunately, for the Laplace equation, separation of variables applies only to boundary
value problems in very special geometries, e.g., rectangles and disks. Further development
of the separation of variables method for solving partial diﬀerential equations in three or
more variables can be found in Chapters 11 and 12.
In the ﬁnal section, we take the opportunity to summarize the fundamental tripartite classiﬁcation of planar second-order partial diﬀerential equations. Each of the three
P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_4, © Springer International Publishing Switzerland 2014

121

122

4 Separation of Variables

paradigmatic equations epitomizes one of the classes: hyperbolic, such as the wave equation; parabolic, such as the heat equation; and elliptic, such as the Laplace and Poisson
equations. Each category enjoys its own distinctive properties and features, both analytic
and numeric, and, in eﬀect, forms a separate mathematical subdiscipline.

4.1 The Diﬀusion and Heat Equations
Let us begin with a brief physical derivation of the heat equation from ﬁrst principles.
We consider a bar — meaning a thin, heat-conducting body. “Thin” means that we can
regard the bar as a one-dimensional continuum with no signiﬁcant transverse temperature
variation. We will assume that the bar is fully insulated along its length, and so heat can
enter (or leave) only through its uninsulated endpoints. We use t to represent time, and
a ≤ x ≤ b to denote spatial position along the bar, which occupies the interval [ a, b ]. Our
goal is to ﬁnd the temperature u(t, x) of the bar at position x and time t.
The dynamical equations governing the temperature are based on three fundamental
physical principles. First is the Law of Conservation of Heat Energy. Recalling the general
Deﬁnition 2.7, this particular conservation law takes the form
∂ε ∂w
+
= 0,
∂t
∂x

(4.1)

in which ε(t, x) represents the thermal energy density at time t and position x, while
w(t, x) denotes the heat ﬂux , i.e., the rate of ﬂow of thermal energy along the bar. Our
sign convention is that w(t, x) > 0 at points where the energy ﬂows in the direction of
increasing x (left to right). The integrated form (2.49) of the conservation law, namely
d
dt

 b
ε(t, x) dx = w(t, a) − w(t, b),

(4.2)

a

states that the rate of change in the thermal energy within the bar is equal to the total
heat ﬂux passing through its uninsulated ends. The signs of the boundary terms conﬁrm
that heat ﬂux into the bar results in an increase in temperature.
The second ingredient is a constitutive assumption concerning the bar’s material properties. It has been observed that, under reasonable conditions, thermal energy is proportional to temperature:
ε(t, x) = σ(x) u(t, x).
(4.3)
The factor
σ(x) = ρ(x) χ(x) > 0

(4.4)

is the product of the density ρ of the material and its speciﬁc heat capacity χ, which is
the amount of heat energy required to raise the temperature of a unit mass of the material
by one degree. Note that we are assuming that the medium is not changing in time, and
so physical quantities such as density and speciﬁc heat depend only on position x. We
also assume, perhaps with less physical justiﬁcation, that its material properties do not
depend upon the temperature; otherwise, we would be forced to deal with a much thornier
nonlinear diﬀusion equation, [70, 99].
The third physical principle relates heat ﬂux and temperature. Physical experiments
show that the thermal energy moves from hot to cold at a rate that is in direct proportion to

4.1 The Diﬀusion and Heat Equations

123

the temperature gradient, which, in the one-dimensional case, means its derivative ∂u/∂x.
The resulting relation
∂u
(4.5)
w(t, x) = − κ(x)
∂x
is known as Fourier’s Law of Cooling. The proportionality factor κ(x) > 0 is the thermal
conductivity of the bar at position x, and the minus sign reﬂects the everyday observation
that heat energy moves from hot to cold. A good heat conductor, e.g., silver, will have
high conductivity, while a poor conductor, e.g., glass, will have low conductivity.
Combining the three laws (4.1, 3, 5) produces the linear diﬀusion equation


∂
∂
∂u
σ(x) u =
κ(x)
,
a < x < b,
(4.6)
∂t
∂x
∂x
governing the thermodynamics of a one-dimensional medium. It is also used to model a
wide variety of diﬀusive processes, including chemical diﬀusion, diﬀusion of contaminants
in liquids and gases, population dispersion, and the spread of infectious diseases. If there
is an external heat source along the length of the bar, then the diﬀusion equation acquires
an additional prescribed inhomogeneous term:


∂
∂u
∂
σ(x) u =
κ(x)
+ h(t, x),
a < x < b.
(4.7)
∂t
∂x
∂x
In order to uniquely prescribe the solution u(t, x), we need to specify an initial temperature distribution
u(t0 , x) = f (x),
a ≤ x ≤ b.
(4.8)
In addition, we must impose a suitable boundary condition at each end of the bar. There
are three common types. The ﬁrst is a Dirichlet boundary condition, where the end is held
at a prescribed temperature. For example,
u(t, a) = α(t)

(4.9)

ﬁxes the temperature (possibly time-varying) at the left end. Alternatively, the Neumann
boundary condition
∂u
(4.10)
(t, a) = μ(t)
∂x
prescribes the heat ﬂux w(t, a) = − κ(a)ux (t, a) there. In particular, a homogeneous Neumann condition, ux (t, a) ≡ 0, models an insulated end that prevents thermal energy ﬂowing
in or out. The Robin † boundary condition,
∂u
(4.11)
(t, a) + β(t) u(t, a) = τ (t),
∂x
models the heat exchange resulting from the end of the bar being placed in a heat bath
(thermal reservoir) at temperature τ (t).
Each end of the bar is required to satisfy one of these boundary conditions. For
example, a bar with both ends having prescribed temperatures is governed by the pair of
Dirichlet boundary conditions
u(t, a) = α(t),

u(t, b) = β(t),

(4.12)

†
Since it is named after the nineteenth-century French analyst Victor Gustave Robin, the
pronunciation should be with a French accent.

124

4 Separation of Variables

whereas a bar with two insulated ends requires two homogeneous Neumann boundary
conditions
∂u
∂u
(4.13)
(t, a) = 0,
(t, b) = 0.
∂x
∂x
Mixed boundary conditions, with one end at a ﬁxed temperature and the other insulated,
are similarly formulated, e.g.,
∂u
(t, b) = 0.
∂x

u(t, a) = α(t),

(4.14)

Finally, the periodic boundary conditions
∂u
∂u
(4.15)
(t, a) =
(t, b),
∂x
∂x
correspond to a circular ring obtained by joining the two ends of the bar. As before, we
are assuming that the heat is allowed to ﬂow only around the ring — insulation prevents
the radiation of heat from one side of the ring aﬀecting the other side.
u(t, a) = u(t, b),

The Heat Equation
In this book, we will retain the term “heat equation” to refer to the case in which the
bar is composed of a uniform material, and so its density ρ, conductivity κ, and speciﬁc
heat χ are all positive constants. We also exclude external heat sources (other than at the
endpoints), meaning that the bar remains insulated along its entire length. Under these
assumptions, the general diﬀusion equation (4.6) reduces to the homogeneous heat equation
∂u
∂2u
=γ
∂t
∂x2

(4.16)

for the temperature u(t, x) at time t and position x. The constant
γ=

κ
κ
=
σ
ρχ

(4.17)

is called the thermal diﬀusivity; it incorporates all of the bar’s relevant physical properties.
The solution u(t, x) will be uniquely prescribed once we specify initial conditions (4.8) and
a suitable boundary condition at both of its endpoints.
As we learned in Section 3.1, the separable solutions to the heat equation are based
on the exponential ansatz†
u(t, x) = e− λ t v(x),
(4.18)
where v(x) depends only on the spatial variable. Functions of this form, which “separate”
into a product of a function of t times a function of x, are known as separable solutions.
Substituting (4.18) into (4.16) and canceling the common exponential factors, we ﬁnd that
v(x) must solve the second-order linear ordinary diﬀerential equation
−γ

d2 v
= λ v.
dx2

†
Anticipating the eventual signs of the eigenvalues, and to facilitate later discussions, we now
include a minus sign in the exponential term.

4.1 The Diﬀusion and Heat Equations

125

Each nontrivial solution v(x) ≡ 0 is an eigenfunction, with associated eigenvalue λ, for the
linear diﬀerential operator L[ v ] = − γ v  (x). With the separable eigensolutions (4.18) in
hand, we will then be able to reconstruct the desired solution u(t, x) as a linear combination,
or rather inﬁnite series, thereof.
Let us concentrate on the simplest case: a uniform, insulated bar of length  that is
held at zero temperature at both ends. We specify its initial temperature f (x) at time
t0 = 0, and so the relevant initial and boundary conditions are
u(t, 0) = 0,

u(t, ) = 0,

u(0, x) = f (x),

t ≥ 0,
0 ≤ x ≤ .

(4.19)

The eigensolutions (4.18) are found by solving the Dirichlet boundary value problem
d2 v
(4.20)
+ λ v = 0,
v(0) = 0,
v() = 0.
dx2
By direct calculation (as you are asked to do in Exercises 4.1.19–20), one ﬁnds that if λ
is either complex, or real and nonpositive, then the only solution to the boundary value
problem (4.20) is the trivial solution v(x) ≡ 0. This means that all the eigenvalues must
necessarily be real and positive. In fact, the reality and positivity of the eigenvalues need
not be explicitly checked. Rather, they follow from very general properties of positive
deﬁnite boundary value problems, of which (4.20) is a particular case. See Section 9.5 for
the underlying theory and Theorem 9.34 for the relevant result.
When λ > 0, the general solution to the diﬀerential equation is a trigonometric function

v(x) = a cos ω x + b sin ω x,
where
ω = λ/γ ,
γ

and a and b are arbitrary constants. The ﬁrst boundary condition requires v(0) = a = 0.
This serves to eliminate the cosine term, and then the second boundary condition requires
v() = b sin ω  = 0.
Therefore, since we require b = 0 — otherwise, the solution is trivial and does not qualify
as an eigenfunction — ω  must be an integer multiple of π, and so
2π
3π
π
,
,
,
... .



We conclude that the eigenvalues and eigenfunctions of the boundary value problem (4.20)
are
" n π #2
n πx
(4.21)
λn = γ
,
n = 1, 2, 3, . . . .
,
vn (x) = sin


The corresponding eigensolutions (4.18) are


γ n2 π 2 t
n πx
(4.22)
un (t, x) = exp −
,
n = 1, 2, 3, . . . .
sin
2


ω =

Each represents a trigonometrically oscillating temperature proﬁle that maintains its form
while decaying to zero at an exponentially fast rate.
To solve the general initial value problem, we assemble the eigensolutions into an
inﬁnite series,


∞
∞


γ n2 π 2 t
n πx
bn un (t, x) =
bn exp −
u(t, x) =
,
(4.23)
sin
2


n=1
n=1

126

4 Separation of Variables

whose coeﬃcients bn are to be ﬁxed by the initial conditions. Indeed, assuming that the
series converges, the initial temperature proﬁle is
∞


u(0, x) =

bn sin

n=1

n πx
= f (x).


(4.24)

This has the form of a Fourier sine series (3.52) on the interval [ 0,  ]. Thus, the coeﬃcients
are determined by the Fourier formulae (3.53), and so
2
bn =



f (x) sin
0

n πx
dx,


(4.25)

n = 1, 2, 3, . . . .

The resulting formula (4.23) describes the Fourier sine series for the temperature u(t, x) of
the bar at each later time t ≥ 0.
Example 4.1. Consider the initial temperature proﬁle
⎧
0 ≤ x ≤ 15 ,
⎪
⎨ − x,
1
7
u(0, x) = f (x) =
x − 25 ,
5 ≤ x ≤ 10 ,
⎪
⎩
7
1 − x,
10 ≤ x ≤ 1,

(4.26)

on a bar of length 1, plotted in the ﬁrst graph in Figure 4.1. Using (4.25), the ﬁrst few
Fourier coeﬃcients of f (x) are computed (by either exact or numerical integration) to be
b1 ≈ .0897,

b2 ≈ − .1927,

b3 ≈ − .0289,

b4 = 0,

b5 ≈ − .0162,

b6 ≈ .0132,

b7 ≈ .0104,

b8 = 0,

... .

The resulting Fourier series solution to the heat equation is
u(t, x) =

∞


bn un (t, x) =

n=1

≈ .0897 e

∞


2

2

bn e− γ n π t sin n πx

n=1
− γ π2 t

2

2

sin πx − .1927 e− 4 γ π t sin 2 πx − .0289 e− 9 γ π t sin 3 πx − · · · .

In Figure 4.1, the solution, for γ = 1, is plotted at some representative times. Observe
that the corners in the initial proﬁle are immediately smoothed out. As time progresses,
2
the solution decays, at a fast exponential rate of e− π t ≈ e− 9.87 t , to a uniform, zero temperature, which is the equilibrium temperature distribution for the homogeneous Dirichlet
boundary conditions. As the solution decays to thermal equilibrium, the higher Fourier
modes rapidly disappear, and the solution assumes the progressively more symmetric shape
of a single sine arc, of rapidly decreasing amplitude.

Smoothing and Long–Time Behavior
The fact that we can write the solution to an initial-boundary value problem in the form
of an inﬁnite series (4.23) is progress of a sort. However, because we are unable to sum the
series in closed form, this “solution” is much less satisfying than a direct, explicit formula.
Nevertheless, there are important qualitative and quantitative features of the solution that
can be easily gleaned from such series expansions.

4.1 The Diﬀusion and Heat Equations

127

t=0

t = .001

t = .01

t = .03

t = .05

t = .1

Figure 4.1.

A solution to the heat equation.



If the initial data f (x) is integrable (e.g., piecewise continuous), then its Fourier coefﬁcients are uniformly bounded; indeed, for any n ≥ 1,


n πx
2
2
f (x) sin
| f (x) | dx ≡ M.
(4.27)
dx ≤
| bn | ≤
 0

 0
This property holds even for quite irregular data. Under these conditions, each term in the
series solution (4.23) is bounded by an exponentially decaying function




γ n2 π 2
γ n2 π 2
n πx
≤ M exp −
bn exp −
t sin
t .
2

2
This means that, as soon as t > 0, most of the high-frequency terms, n  0, will be
extremely small. Only the ﬁrst few terms will be at all noticeable, and so the solution
essentially degenerates into a ﬁnite sum over the ﬁrst few Fourier modes. As time increases,
more and more of the Fourier modes will become negligible, and the sum further degenerates
into fewer and fewer signiﬁcant terms. Eventually, as t → ∞, all of the Fourier modes will
decay to zero. Therefore, the solution will converge exponentially fast to a zero temperature
proﬁle: u(t, x) → 0 as t → ∞, representing the bar in its ﬁnal uniform thermal equilibrium.
The fact that its equilibrium temperature is zero is the result of holding both ends of the
bar ﬁxed at zero temperature, whereby any initial thermal energy is eventually dissipated
away through the ends. The small-scale temperature ﬂuctuations tend to rapidly cancel
out through diﬀusion of thermal energy, and the last term to disappear is the one with the
slowest decay, namely



γ π2
πx
2
πx
u(t, x) ≈ b1 exp − 2 t sin
f (x) sin
,
where
b1 =
dx. (4.28)


 0

For generic initial data, the coeﬃcient b1 = 0, and the solution approaches thermal equilibrium at an exponential rate prescribed by the smallest eigenvalue, λ1 = γ π 2 /2 , which is
proportional to the thermal diﬀusivity divided by the square of the length of the bar. The

128

4 Separation of Variables

t=0

t = .00001

t = .00005

t = .0001

t = .001

t = .01

Figure 4.2.

Denoising a signal with the heat equation.



longer the bar, or the smaller the diﬀusivity, the longer it takes for the eﬀect of holding the
ends at zero temperature to propagate along its entire length. Also, again provided b1 = 0,
the asymptotic shape of the temperature proﬁle is a small, exponentially decaying sine arc,
just as we observed in Example 4.1. In exceptional situations, namely when b1 = 0, the
solution decays even faster, at a rate equal to the eigenvalue λk = γ k 2 π 2 /2 corresponding
to the ﬁrst nonzero term, bk = 0, in the Fourier series; its asymptotic shape now oscillates
k times over the interval.
Another, closely related, observation is that, for any ﬁxed time t > 0 after the initial
moment, the coeﬃcients in the Fourier sine series (4.23) decay exponentially fast as n → ∞.
According to Corollary 3.32, this implies that the Fourier series converges to an inﬁnitely
diﬀerentiable function of x at each positive time t, no matter how unsmooth the initial
temperature proﬁle. We have discovered the basic smoothing property of heat ﬂow, which
we state for a general initial time t0 .
Theorem 4.2. If u(t, x) is a solution to the heat equation with piecewise continuous
initial data f (x) = u(t0 , x), or, more generally, initial data satisfying (4.27), then, for any
t > t0 , the solution u(t, x) is an inﬁnitely diﬀerentiable function of x.
In other words, the heat equation instantaneously smoothes out any discontinuities
and corners in the initial temperature proﬁle by fast damping of the high-frequency modes.
The heat equation’s eﬀect on irregular initial data underlies its eﬀectiveness for smoothing
and denoising signals. We take the initial data u(0, x) = f (x) to be a noisy signal, and
then evolve the heat equation forward to a prescribed time t > 0. The resulting function
g(x) = u(t , x) will be a smoothed version of the original signal f (x) in which most of
the high-frequency noise has been eliminated. Of course, if we run the heat ﬂow for too
long, all of the low-frequency features will also be smoothed out and the result will be
a uniform, constant signal. Thus, the choice of stopping time t is crucial to the success

4.1 The Diﬀusion and Heat Equations

129

of this method. Figure 4.2 shows the eﬀect of running the heat equation,† with γ = 1,
on a signal that has been contaminated by random noise. Observe how quickly the noise
is removed. By the ﬁnal time, the overall smoothing eﬀect of the heat ﬂow has caused
signiﬁcant degradation (blurring) of the original signal. The heat equation approach to
denoising has the advantage that no Fourier coeﬃcients need be explicitly computed, nor
does one need to reconstruct the smoothed signal. Basic numerical solution schemes for
the heat equation are to be discussed in Chapter 5.
An important theoretical consequence of the smoothing property is that diﬀusion is a
one-way process — one cannot run time backwards and accurately infer what a temperature
distribution looked like in the past. In particular, if the initial data u(0, x) = f (x) is not
smooth, then the value of u(t, x) for any t < 0 cannot be deﬁned, because if u(t0 , x) were
deﬁned and integrable at some t0 < 0 then, by Theorem 4.2, u(t, x) would be smooth at all
subsequent times t > t0 , including t = 0, in contradiction to our assumption. Moreover, for
most initial data, the Fourier coeﬃcients in the solution formula (4.23) are, at any t < 0,
exponentially growing as n → ∞, indicating that high-frequency noise has completely
overwhelmed the solution, thereby precluding any kind of convergence of the Fourier series.
Mathematically, we can reverse future and past by changing t to − t. In the diﬀerential
equation, this merely reverses the sign of the time-derivative term; the x derivatives are
unaﬀected. Thus, by the above reasoning, the backwards heat equation
∂u
∂2u
= −γ
,
with a negative diﬀusion coeﬃcient
− γ < 0,
(4.29)
∂t
∂x2
is an ill-posed problem in the sense that small changes in the initial data — e.g., a small
perturbation of a high-frequency mode — can produce arbitrarily large changes in the
solution arbitrarily close to the initial time. In other words, the solution does not depend
continuously on the initial data. Even worse, for nonsmooth initial data, the solution is not
even well deﬁned in forwards time t > 0 (although it is well-posed if we run t backwards).
The same holds for more general diﬀusion processes, e.g., (4.6). If, as in all physically
relevant cases, the coeﬃcient of uxx is everywhere positive, then the initial value problem
is well-posed for t > 0, but ill-posed for t < 0. On the other hand, if the coeﬃcient is
everywhere negative, the reverse holds. A coeﬃcient that changes signs would cause the
diﬀerential equation to be ill-posed in both directions.
While theoretically undesirable, the unsmoothing eﬀect of the backwards heat equation has potential beneﬁts in certain contexts. For example, in image processing, diﬀusion
will gradually blur an image by damping out the high-frequency modes. Image enhancement is the reverse process, and can be based on running the heat ﬂow backwards in some
stable manner. In forensics, determining the time of death based on the current temperature of a corpse also requires running the equations governing the dissipation of body
heat backwards in time. One option would be to restrict the backwards evolution to the
ﬁrst few Fourier modes, which prevents the small-scale ﬂuctuations from overwhelming the
computation. Ill-posed problems also arise in the reconstruction of subterranean proﬁles
from seismic data, a central problem of the oil and gas industry. These and other applications are driving contemporary research into how to cleverly circumvent the ill-posedness
of backwards diﬀusion processes.
†

To avoid artifacts at the ends of the interval, we are, in fact, using periodic boundary
conditions in the plots. Away from the ends, running the equation with Dirichlet boundary
conditions leads to almost identical results.

130

4 Separation of Variables

Remark : The irreversibility of the heat equation, along with the irreversibility of nonlinear transport in the presence of shock waves discussed in Section 2.3, highlight a crucial
distinction between partial diﬀerential equations and ordinary diﬀerential equations. Ordinary diﬀerential equations are always reversible — the existence, uniqueness, and continuous dependence properties of solutions are all equally valid in reverse time (although
their detailed qualitative and quantitative properties will, of course, depend upon whether
time is running forwards or backwards). The irreversibility and ill-posedness of partial
diﬀerential equations modeling thermodynamical, biological, and other diﬀusive processes
in our universe may explain why Time’s Arrow points exclusively to the future.

The Heated Ring Redux
Let us next consider the periodic boundary value problem modeling heat ﬂow in an insulated circular ring. We ﬁx the length of the ring to be  = 2 π, with − π ≤ x ≤ π
representing the “angular” coordinate around the ring. For simplicity, we also choose units
in which the thermal diﬀusivity is γ = 1. Thus, we seek to solve the heat equation
∂u
∂2u
=
,
− π < x < π,
∂t
∂x2
subject to periodic boundary conditions

(4.30)

t > 0,

∂u
∂u
(4.31)
(t, − π) =
(t, π),
t ≥ 0,
∂x
∂x
that ensure continuity of the solution when the angular coordinate switches from − π to π.
The initial temperature distribution is
u(t, − π) = u(t, π),

− π < x ≤ π.

u(0, x) = f (x),

(4.32)

The resulting temperature u(t, x) will be a periodic function in x of period 2 π.
Substituting the separable solution ansatz (3.15) into the heat equation and the boundary conditions results in the periodic eigenvalue problem
d2 v
(4.33)
+ λ v = 0,
v(− π) = v(π),
v  (− π) = v  (π).
dx2
As we already noted in Section 3.1, the eigenvalues of this particular boundary value
problem are λn = n2 , where n = 0, 1, 2, . . . is a nonnegative integer; the corresponding
eigenfunctions are the trigonometric functions
vn (x) = sin n x,

vn (x) = cos n x,

n = 0, 1, 2, . . . .

Note that λ0 = 0 is a simple eigenvalue, with constant eigenfunction cos 0 x = 1 — the
sine solution sin 0 x ≡ 0 is trivial — while the positive eigenvalues are, in fact, double, each
possessing two linearly independent eigenfunctions. The corresponding eigensolutions to
the heated ring equation (4.30–31) are
2

2

un (t, x) = e−n t cos n x,

u
n (t, x) = e−n t sin n x,

n = 0, 1, 2, 3, . . . .

The resulting inﬁnite series solution is
u(t, x) = 12 a0 +

∞

n=1

2

2

an e−n t cos n x + bn e−n t sin n x ,



(4.34)

4.1 The Diﬀusion and Heat Equations

131

with as yet unspeciﬁed coeﬃcients an , bn . The initial conditions require
u(0, x) = 21 a0 +

∞

n=1

(an cos n x + bn sin n x) = f (x),

(4.35)

which is precisely the complete Fourier series (3.34) of the initial temperature proﬁle f (x).
Consequently,


1 π
1 π
an =
f (x) cos n x dx,
bn =
f (x) sin n x dx,
(4.36)
π −π
π −π
are its usual Fourier coeﬃcients (3.35).
As in the Dirichlet problem, after the initial instant, the high-frequency terms in the
2
series (4.34) become extremely small, since e−n t  1 for n  0. Therefore, as soon as
t > 0, the solution instantaneously becomes smooth, and quickly degenerates into what is
in essence a ﬁnite sum over the ﬁrst few Fourier modes. Moreover, as t → ∞, all of the
Fourier modes will decay to zero with the exception of the constant mode, associated with
the null eigenvalue λ0 = 0. Consequently, the solution will converge, at an exponential
rate, to a constant-temperature proﬁle,
 π
1
1
u(t, x) −→ 2 a0 =
f (x) dx,
2 π −π
which equals the average of the initial temperature proﬁle. In physical terms, since the
insulation prevents any thermal energy from escaping the ring, it rapidly redistributes itself
so that the ring achieves a uniform constant temperature — its eventual equilibrium state.
Prior to attaining equilibrium, only the very lowest frequency Fourier modes will still
be noticeable, and so the solution will asymptotically look like
u(t, x) ≈ 12 a0 + e− t (a1 cos x + b1 sin x) = 12 a0 + r1 e− t cos(x − δ1 ),
where
1
a1 = r1 cos δ1 =
2π

 π
f (x) cos x dx,
−π

1
b1 = r1 sin δ1 =
2π

(4.37)

 π
f (x) sin x dx.
−π

Thus, for most initial data, the solution approaches thermal equilibrium at an exponential
rate of e− t . The exceptions are when a1 = b1 = 0, for which the rate of convergence is
2
even faster, namely at a rate e− k t , where k is the smallest integer such that at least one
of the k th order Fourier coeﬃcients ak , bk is nonzero.
In fact, once we are convinced that the bar must tend to thermal equilibrium as t → ∞,
we can predict the ﬁnal temperature without knowing the explicit solution formula. Our
derivation in Section 4.1 implies that the heat equation has the form of a conservation law
(4.1), with the conserved density being the temperature u(t, x). As in (4.2), the integrated
form of the conservation law reads

 π 2
 π
d π
∂u
∂ u
(t, x) dx = γ
u(t, x) dx =
(t, x) dx
2
dt −π
−π ∂t
−π ∂x


∂u
∂u
(t, π) −
(t, − π) = 0,
=γ
∂x
∂x
where the ﬂux terms cancel thanks to the periodic boundary conditions (4.31). Physically,
any ﬂux out of one end of the circular bar is immediately fed into the other, abutting end,

132

4 Separation of Variables

and so there is no net loss of thermal energy. We conclude that, for the periodic boundary
value problem, the total thermal energy
 π
u(t, x) dx = constant
(4.38)
E(t) =
−π

remains constant for all time. (In contrast, the thermal energy does not remain constant
for the Dirichlet boundary value problem, decaying steadily to 0 due to the out-ﬂux of heat
through the ends of the bar; see Exercise 4.1.13 for further details.)
Remark : More correctly, according to (4.3), the thermal energy is obtained by multiplying the temperature by the product, σ = ρ χ, of the density and the speciﬁc heat of the
body. For the heat equation, both are constant, and so the physical thermal energy equals
σ E(t). Mathematically, we can safely ignore this extra constant factor, or, equivalently,
work in physical units in which σ =
 1. This does not extend to nonuniform bodies, whose
π

thermal energy is given by E(t) =

σ(x) u(t, x) dx, and whose constancy, under suitable
−π

boundary conditions, follows from the conservation-law form (4.6) of the linear diﬀusion
equation.
In general, a system is in (static) equilibrium if it remains unaltered as time progresses.
Thus, any equilibrium conﬁguration has the form u = u (x), and hence satisﬁes ∂u /∂t = 0.
If, in addition, u (x) is an equilibrium solution to the periodic heat equation (4.30–33),
then it must satisfy
∂ 2 u
∂u
=0=
,
∂t
∂x2

u (− π) = u (π),

∂u
∂u
(− π) =
(π).
∂x
∂x

(4.39)

In other words, u is a solution to the periodic boundary value problem (4.33) for the null
eigenvalue λ = 0. Thus, the null eigenfunctions (including the zero solution) are all the
possible equilibrium solutions. In particular, for the periodic boundary value problem, the
null eigenfunctions are constant, and therefore solutions to the periodic heat equation will
tend to a constant equilibrium temperature.
Now, once we know that the solution tends to a constant, u(t, x) → a as t → ∞, then
its thermal energy tends to
 π
 π
E(t) =
u(t, x) dx −→
a dx = 2 πa
as
t −→ ∞.
−π

−π

On the other hand, as we just demonstrated, the thermal energy is constant, so
 π
 π
u(0, x) dx =
f (x) dx.
E(t) = E(0) =
−π

−π

Combining these two, we conclude that
 π
f (x) dx = 2 πa, and so the equilibrium temperature
−π

1
a=
2π

 π
f (x) dx
−π

equals the average initial temperature. This reconﬁrms our earlier result, but avoids having
to know an explicit series solution formula. As a result, the latter method can be applied
to a much wider range of situations.

4.1 The Diﬀusion and Heat Equations

133

Inhomogeneous Boundary Conditions
So far, we have concentrated our attention on homogeneous boundary conditions. There is
a simple trick that will convert a boundary value problem with inhomogeneous but constant
Dirichlet boundary conditions,
∂u
∂2u
(4.40)
=γ
,
u(t, 0) = α,
u(t, ) = β,
t ≥ 0,
∂t
∂x2
into a homogeneous Dirichlet problem. We begin by solving for the equilibrium temperature
proﬁle. As in (4.39), the equilibrium does not depend on t and hence satisﬁes the boundary
value problem
∂u
∂ 2 u
=0=γ
,
u (0) = α,
u () = β.
∂t
∂x2
Solving the ordinary diﬀerential equation yields u (x) = a+b x, where the constants a, b are
ﬁxed by the boundary conditions. We conclude that the equilibrium solution is a straight
line connecting the boundary values:
u (x) = α +

β−α
x.


(4.41)

The diﬀerence

β−α
x
(4.42)

measures the deviation of the solution from equilibrium. It clearly satisﬁes the homogeneous boundary conditions at both ends:
u
(t, x) = u(t, x) − u (x) = u(t, x) − α −

u
(t, 0) = 0 = u
(t, ).
Moreover, by linearity, since both u(t, x) and u (x) are solutions to the heat equation, so
is u
(t, x). The initial data must be similarly adapted:
β−α
x ≡ f(x).
(4.43)

Solving the resulting homogeneous initial-boundary value problem, we write u
(t, x) in
Fourier series form (4.23), where the Fourier coeﬃcients are speciﬁed by the modiﬁed
initial data f(x) in (4.43). The solution to the inhomogeneous boundary value problem
thus has the series form


∞

γ n2 π 2
n πx
β−α

bn exp −
t sin
x +
,
(4.44)
u(t, x) = α +
2



n=1
u
(0, x) = u(t, x) − u (x) = f (x) − α −

where
b = 2
n




f(x) sin
0

n πx
dx,


n = 1, 2, 3, . . . .

(4.45)

Since u
(t, 0) decays to zero at an exponential rate as t → ∞, the actual temperature proﬁle
(4.44) will asymptotically decay to the equilibrium proﬁle,
β−α
u(t, x) −→ u (x) = α +
x,

at the same exponentially fast rate, governed by the ﬁrst eigenvalue λ1 = π 2 /2 — unless
b = 0, in which case the decay rate is even faster.
1

134

4 Separation of Variables

This method does not work as well when the boundary conditions are time-dependent:
u(t, 0) = α(t),

u(t, ) = β(t).

Attempting to mimic the preceding technique, we discover that the deviation†
β(t) − α(t)
x,
(4.46)
u
(t, x) = u(t, x) − u (t, x),
where
u (t, x) = α(t) +

satisﬁes the homogeneous boundary conditions, but now solves an inhomogeneous or forced
version of the heat equation:
∂
u
∂2u

∂u
β  (t) − α (t)

=
(t,
x)
=
−
α
x. (4.47)
+
h(t,
x),
where
h(t,
x)
=
−
(t)
−
∂t
∂x2
∂t

Solution techniques for the latter partial diﬀerential equation will be discussed in Section 8.1
below.
Robin Boundary Conditions
Consider a bar of unit length and unit thermal diﬀusivity, insulated along its length,
which has one of its ends held at 0◦ and the other put in a heat bath. The resulting
thermodynamics are modeled by the heat equation subject to Dirichlet boundary conditions
at x = 0 and Robin boundary conditions at x = 1:
∂2u
∂u
=
,
∂t
∂x2

u(t, 0) = 0,

∂u
(t, 1) + β u(t, 1) = 0,
∂x

(4.48)

where β = 0 is a constant‡ that measures the rate of transfer of thermal energy, with β > 0
when the bath is cold and so the energy is being extracted from the bar. As before, the
general solution to the resulting initial-boundary value problem can be assembled from
the separable eigensolutions based on our usual exponential ansatz u(t, x) = e− λ t v(x).
Substituting this expression into (4.48), we ﬁnd that the eigenfunction v(x) must satisfy
the boundary value problem
−

d2 v
= λ v,
dx2

v(0) = 0,

v  (1) + β v(1) = 0.

(4.49)

In order to ﬁnd nontrivial solutions v(x) ≡ 0 to (4.49), let us ﬁrst assume λ = ω 2 > 0,
where, without loss of generality, ω > 0. The solution to the ordinary diﬀerential equation
that satisﬁes the Dirichlet boundary condition at x = 0 is a constant multiple of v(x) =
sin ω x. Substituting this function into the Robin boundary condition at x = 1, we ﬁnd
ω cos ω + β sin ω = 0,

or, equivalently,

ω = − β tan ω.

(4.50)

It is not hard to see that there is an inﬁnite number of real, positive solutions 0 < ω1 < ω2 <
ω3 < · · · → ∞ to the latter transcendental equation. Indeed, they can be characterized as
the abscissas ωn > 0 of the intersection points of the graphs of the two functions f (ω) = ω
†

In this case, u (t, x) is not an equilibrium solution. Indeed, we do not expect the bar to go
to equilibrium if the temperature of its endpoints is constantly changing.
‡
The case β = 0 reduces to the mixed boundary value problem, whose analysis is left to the
reader.

4.1 The Diﬀusion and Heat Equations

ω1

ω2

ω0

ω3
λ < 0, β = −.5.

λ > 0, β = 1.
Figure 4.3.

135

λ < 0, β = −2.

Eigenvalue equation for Robin boundary conditions.

and g(ω) = −β tan ω, as shown in the ﬁrst plot in Figure 4.3. Each root ωn deﬁnes a
positive eigenvalue λn = ωn2 > 0 to the boundary value problem (4.49) and hence an
exponentially decaying eigensolution
un (t, x) = e− λn t sin ωn x

(4.51)

to the Robin boundary value problem (4.48). While there is no explicit formula, numerical approximations to the eigenvalues are easily found via a numerical root ﬁnder,
e.g., Newton’s Method, [24, 94]. In particular, for β = 1, the ﬁrst three eigenvalues are
λ1 = ω12 ≈ 4.1159, λ2 = ω22 ≈ 24.1393, λ3 = ω32 ≈ 63.6591.
What about a zero eigenvalue? If λ = 0 in (4.49), then the solution to the ordinary
diﬀerential equation that satisﬁes the Dirichlet boundary condition is a constant multiple
of v(x) = x. This function satisﬁes the Robin boundary condition v  (1) + β v(1) = 0 if and
only if β = −1. In this special conﬁguration, the heat equation admits a time-independent
eigensolution u0 (t, x) = x with eigenvalue λ0 = 0. Physically, the rate of transfer of thermal
energy into the bar through its end in the heat bath is exactly enough to cancel the heat
loss through the Dirichlet end, resulting in a steady-state solution. All other eigenmodes
correspond to positive eigenvalues, and hence are exponentially decaying. The general
solution decays to the steady state, which is a constant multiple of the null eigensolution:
u(t, x) → c x as t → ∞, at an exponential rate prescribed, generically, by the ﬁrst positive
eigenvalue λ1 > 0.
However, in contrast to the more common types of boundary conditions (Dirichlet,
Neumann, mixed, periodic), we cannot automatically rule out the existence of negative
eigenvalues in the Robin case. Suppose λ = − ω 2 < 0 with ω > 0. Now the solution to
(4.49) that satisﬁes the Dirichlet boundary condition at x = 0 is a constant multiple of
the hyperbolic sine function v(x) = sinh ω x. Substituting this expression into the Robin
boundary condition at x = 1 produces
ω cosh ω + β sinh ω = 0,
where

or, equivalently,

ω = − β tanh ω,

(4.52)

sinh ω
eω − e− ω
(4.53)
= ω
cosh ω
e + e− ω
is the hyperbolic tangent. If β > −1, there are no solutions ω > 0 to this transcendental
equation, and in this case all the eigenvalues are strictly positive and all solutions to the
tanh ω =

136

4 Separation of Variables

heat equation are exponentially decaying. On the other hand, if β < −1, there is a single
solution ω0 > 0, which produces a single negative eigenvalue λ0 = − ω02 . Representative
graphs illustrating the two possibilities appear in Figure 4.3; in the ﬁrst, the graph of
f (ω) = ω does not intersect the graph of g(ω) = 12 tanh ω when ω > 0, whereas it intersects
the graph of g(ω) = 2 tanh ω at a single point, with abscissa ω0 ≈ 1.9150, producing the
negative eigenvalue λ0 ≈ − ω02 ≈ − 3.6673. Thus, when β < −1, there is, in addition to all
the exponentially decaying eigenmodes associated with the positive eigenvalues, a single
unstable exponentially growing eigenmode
u0 (t, x) = eλ0 t sinh ω0 x.

(4.54)

Physically, β < −1 implies that thermal energy is entering the Robin end of the bar at a
faster rate than can be removed through the Dirichlet end, and hence the bar experiences
an exponential increase in its overall temperature.
Remark : Even though some Robin boundary conditions admit exponentially growing
solutions, and hence lead to unstable dynamics, the initial-boundary value problem remains
well-posed because the solution exists and is uniquely determined by the initial data, and,
moreover, small changes in the initial conditions induce relatively small changes in the
resulting solution on bounded time intervals.

The Root Cellar Problem
As a ﬁnal example, we discuss a problem that involves analysis of the heat equation on
a semi-inﬁnite interval. The question is this: how deep should you dig a root cellar? In
the prerefrigeration era, a root cellar was used to keep food cool in the summer, but not
freeze in the winter. We assume that the temperature inside the Earth depends only on
the depth and the time of year. Let u(t, x) denote the deviation in the temperature from
its annual mean at depth x > 0 and time t. We shall assume that the temperature at the
Earth’s surface, x = 0, ﬂuctuates in a periodic manner; speciﬁcally, we set
u(t, 0) = a cos ω t,

(4.55)

where the oscillatory frequency
ω=

2π
= 2.0 × 10−7 sec−1
365.25 days

(4.56)

refers to yearly temperature variations. In this model, we shall ignore daily temperature
ﬂuctuations, since their eﬀect is not signiﬁcant below a very thin surface layer. At large
depths the temperature is assumed to be unvarying:
u(t, x) −→ 0

x −→ ∞,

as

(4.57)

where 0 refers to the mean temperature.
Thus, we must solve the heat equation on a semi-inﬁnite bar 0 < x < ∞, with timedependent boundary conditions (4.55, 57) at the ends. The analysis will be simpliﬁed a
little if we replace the cosine by a complex exponential, and so we look for a complex
solution with boundary conditions
u(t, 0) = a e i ω t ,

lim u(t, x) = 0.

x→∞

(4.58)

4.1 The Diﬀusion and Heat Equations

137

Let us try a separable solution of the form
u(t, x) = v(x) e i ω t .

(4.59)

Substituting this expression into the heat equation ut = γ uxx leads to
i ω v(x) e i ω t = γ v  (x) e i ω t .
Canceling the common exponential factors, we conclude that v(x) should solve the boundary value problem
γ v  (x) = i ω v,

v(0) = a,

lim v(x) = 0.

x→∞

The solutions to the ordinary diﬀerential equation are
√
√
√
√
v2 (x) = e− i ω/γ x = e− ω/(2 γ) (1+ i ) x .
v1 (x) = e i ω/γ x = e ω/(2 γ) (1+ i ) x ,
The ﬁrst solution is exponentially growing as x → ∞, and so not germane to our problem. The solution to the boundary value problem must therefore be a multiple of the
exponentially decaying solution:
√
v(x) = a e− ω/(2 γ) (1+ i ) x .
Substituting back into (4.59), we ﬁnd the (complex) solution to the root cellar problem to
be
√
√
u(t, x) = a e− x ω/(2 γ) e i (ω t− ω/(2 γ) x) .
(4.60)
The corresponding real solution is obtained by taking the real part,



√
ω
u(t, x) = a e− x ω/(2 γ) cos ω t −
x .
2γ

(4.61)

The ﬁrst factor in (4.61) is exponentially decaying as a function of the depth. Thus, the
further underground one is, the less noticeable is the eﬀect of the surface temperature
ﬂuctuations. The second factor is periodic in time, with the same annual frequency ω. The
interesting feature is that the temperature variations (4.61) are typically out of phase with
respect to the surface temperature ﬂuctuations, having an overall phase lag of

ω
δ=
x
2γ
that depends linearly on the depth x. In particular, a cellar built at a depth where δ is an
odd multiple of π will be completely out of phase, being hottest in the winter, and coldest
in the summer. Thus, the (shallowest) ideal depth at which to build a root cellar would
take δ = π, corresponding to a depth of

2γ
.
(4.62)
x=π
ω
For typical soils in the Earth, γ ≈ 10−6 meters2 sec−1 , and so, with ω given by (4.56),
x ≈ 9.9 meters. However, at this depth, the relative amplitude of the oscillations is
√
e− x ω/2 γ = e− π = .04,
and hence there is only a 4% temperature ﬂuctuation. In Minneapolis, the temperature
varies, roughly, from − 40◦ C to + 40◦ C, and hence our 10-meter-deep root cellar would

138

4 Separation of Variables

experience only a 3.2◦ C annual temperature deviation from the winter, when it is the
warmest, to the summer, when it is the coldest. Building the cellar twice as deep would
lead to a temperature ﬂuctuation of .2%, now in phase with the surface variations, which
means that the cellar would be, for all practical purposes, at constant temperature year
round.

Exercises
4.1.1. Suppose the ends of a bar of length 1 and thermal diﬀusivity γ = 1 are held ﬁxed at
respective temperatures 0◦ and 10◦ . (a) Determine the equilibrium temperature proﬁle.
(b) Determine the rate at which the equilibrium temperature proﬁle is approached.
(c) What does the temperature proﬁle look like as it nears equilibrium?
4.1.2. A uniform insulated bar 1 meter long is stored at room temperature of 20◦ Celsius. An
experimenter places one end of the bar in boiling water and the other end in ice water.
(a) Set up an initial-boundary value problem that models the temperature in the bar.
(b) Find the equilibrium temperature distribution.
(c) Discuss how your answer depends on the material properties of the bar.
4.1.3. Consider the initial-boundary value problem
u(t, 0) = 0 = u(t, 10),
t > 0,
∂u
∂2u
,
=
∂t
∂x2
u(0, x) = f (x),
0 < x < 10,
for the heat equation where the initial data has the following form:
⎧
x − 1,
⎪
⎪
⎪
⎪
⎪
11
− 5 x,
⎪
⎨

f (x) = ⎪ 5 x − 19,
⎪
⎪
⎪
⎪
⎪ 5 − x,
⎩
0,

1 ≤ x ≤ 2,
2 ≤ x ≤ 3,
3 ≤ x ≤ 4,
4 ≤ x ≤ 5,
otherwise.

Discuss what happens to the solution as t increases. You do not need to write down an explicit formula, but for full credit you must explain (sketches can help) at least three or four
interesting things that happen to the solution as time progresses.
4.1.4. Find a series solution to the initial-boundary value problem for the heat equation
ut = uxx for 0 < x < 1 when one the end of the bar is held at 0◦ and the other is insulated.
Discuss the asymptotic behavior of the solution as t → ∞.
4.1.5. Answer Exercise 4.1.4 when both ends of the bar are insulated.
4.1.6. A metal bar, of length = 1 meter and thermal diﬀusivity γ = 2, is taken out of a 100◦
oven and then fully insulated except for one end, which is ﬁxed to a large ice cube at 0◦ .
(a) Write down an initial-boundary value problem that describes the temperature u(t, x) of
the bar at all subsequent times. (b) Write a series formula for the temperature distribution u(t, x) at time t > 0. (c) What is the equilibrium temperature distribution in the bar,
i.e., for t
0? How fast does the solution go to equilibrium? (d) Just before the temperature distribution reaches equilibrium, what does it look like? Sketch a picture and discuss.
4.1.7. A metal bar of length

= 1 and thermal diﬀusivity γ = 1 is fully
insulated, including its
⎧
⎨ x,
0 ≤ x ≤ 12 ,
ends. Suppose the initial temperature distribution is u(0, x) = ⎩
1
1 − x,
2 ≤ x ≤ 1.
(a) Use Fourier series to write down the temperature distribution at time t > 0.

4.1 The Diﬀusion and Heat Equations

139

(b) What is the equilibrium temperature distribution in the bar, i.e., for t
0?
(c) How fast does the solution go to equilibrium? (d) Just before the temperature
distribution reaches equilibrium, what does it look like? Sketch a picture and discuss.
4.1.8. (a) Find the series solution to the heat equation ut = uxx on − 2 < x < 2, t > 0, when
subject to the
Dirichlet boundary conditions u(t, −2) = u(t, 2) = 0 and the initial condition

x, | x | < 1,
u(0, x) =
(b) Sketch a graph of the solution at some representative
0, otherwise.
times. (c) At what rate does the temperature approach thermal equilibrium?
4.1.9. Solve the heat equation when the right-hand end of a bar of unit length is held at a ﬁxed
constant temperature α while the left-hand end is insulated. Discuss the asymptotic
behavior of the solution.
4.1.10. For each of the following initial temperature distributions, (i ) write out the Fourier
series solution to the heated ring (4.30–32), and (ii ) ﬁnd the resulting
equilibrium

1,
− π < x < 0,
temperature as t → ∞:
(a) cos x, (b) sin3 x, (c) | x |, (d)
0, 0 < x < π.
♦ 4.1.11. Suppose that the temperature u(t, x) of a homogeneous bar satisﬁes the heat equation.
Show that the associated heat ﬂux w(t, x) is also a solution to the same heat equation.
♦ 4.1.12. Show that the time derivative v = ut of any solution to the heat equation is also a
solution. If u(t, x) satisﬁes the initial condition u(0, x) = f (x), what initial condition does
v(t, x) inherit?
♦ 4.1.13. Explain why the thermal energy E(t) =

 
0

u(t, x) dx is not constant for the Dirichlet

initial-boundary value problem for the heat equation on the interval [ 0, ].
♦ 4.1.14. (a) Show that the thermal energy E(t) =

 
0

u(t, x) dx is constant for the Neumann

boundary value problem on the interval [ 0, ]. (b) Use part (a) to prove that the constant
equilibrium solution for the homogeneous Neumann boundary value problem is equal to the
mean initial temperature u(0, x).
4.1.15. Let u(t, x) be any nonconstant solution to the periodic
heat equation (4.30–31). Prove

that the squared L2 norm of the solution, N(t) =

π

−π

u(t, x)2 dx, is a strictly decreasing

function of t. Remark : Interestingly, comparing this result with formula (4.38), we ﬁnd
that, for the periodic boundary value problem, the integral of u is constant, but the
integral of u2 is strictly decreasing. How is this possible?
♥ 4.1.16. The cable equation vt = γ vxx − α v, with γ, α > 0, also known as the lossy heat
equation, was derived by the nineteenth-century Scottish physicist William Thomson to
model propagation of signals in a transatlantic cable. Later, in honor of his work on
thermodynamics, including determining the value of absolute zero temperature, he was
named Lord Kelvin by Queen Victoria. Kelvin’s cable equation was later used to model
the electrical activity of neurons. (a) Show that the general solution to the cable equation
is given by v(t, x) = e− α t u(t, x), where u(t, x) solves the heat equation ut = γ uxx .
(b) Find a Fourier series solution to the Dirichlet initial-boundary value problem
v(0, x) = f (x),
v(t, 0) = 0 = v(t, 1),
0 ≤ x ≤ 1,
t > 0.
vt = γ vxx − α v,
Does your solution approach an equilibrium value? If so, how fast?
(c) Answer part (b) for the Neumann problem
v(0, x) = f (x),
vx (t, 0) = 0 = vx (t, 1),
0 ≤ x ≤ 1,
t > 0.
vt = γ vxx − α v,
♦ 4.1.17. The convection-diﬀusion equation ut + c ux = γ uxx is a simple model for the diﬀusion
of a pollutant in a ﬂuid ﬂow moving with constant speed c. Show that v(t, x) = u(t, x + c t)
solves the heat equation. What is the physical interpretation of this change of variables?
4.1.18. Combine Exercises 4.1.16–17 to solve the lossy convection-diﬀusion equation
ut = γ uxx + c ux − α u.

140

4 Separation of Variables

♦ 4.1.19. Let γ > 0 and λ ≤ 0. (a) Find all solutions to the diﬀerential equation γ v  + λ v = 0.
(b) Prove that the only solution that satisﬁes the boundary conditions v(0) = 0, v( ) = 0,
is the zero solution v(x) ≡ 0.
♦ 4.1.20. Answer Exercise 4.1.19 when λ is a non-real complex number.

4.2 The Wave Equation
Let us return to the one-dimensional wave equation
2
∂2u
2 ∂ u
=
c
,
(4.63)
∂t2
∂x2
with constant wave speed c > 0, used to model the vibrations of bars and strings. In Chapter 2, we learned how to explicitly solve the wave equation by the method of d’Alembert.
Unfortunately, d’Alembert’s approach does not extend to other equations of interest to us,
and so alternative solution techniques, particularly those based on Fourier methods, are
worth developing. Indeed, the resulting series solutions provide valuable insight into wave
dynamics on bounded intervals.

Separation of Variables and Fourier Series Solutions
One of the oldest — and still one of the most widely used — techniques for constructing
explicit analytic solutions to a wide range of linear partial diﬀerential equations is the
method of separation of variables. We have, in fact, already employed a simpliﬁed version
of the method when constructing each eigensolution to the heat equation as an exponential
function of t times a function of x. In general, the separation of variables method seeks
solutions to the partial diﬀerential equation that can be written as the product of functions
of the individual independent variables. For the wave equation, we seek solutions
u(t, x) = w(t) v(x)

(4.64)

that can be written as the product of a function of t alone and a function of x alone.
When the method succeeds (which is not guaranteed in advance), both factors are found
as solutions to certain ordinary diﬀerential equations.
Let us see whether such an expression can possibly solve the wave equation. First of
all,
∂2u
∂2u

=
w
(t)
v(x),
= w(t) v  (x),
∂t2
∂x2
where the primes indicate ordinary derivatives. Substituting these expressions into the
wave equation (4.63), we obtain
w (t) v(x) = c2 w(t) v  (x).
Dividing both sides by w(t) v(x) (which we assume is not identically zero, since otherwise,
the solution would be trivial) yields
w (t)
v  (x)
= c2
,
w(t)
v(x)

4.2 The Wave Equation

141

which eﬀectively “separates” the t and x variables on each side of the equation, whence
the name “separation of variables”.
Now, how could a function of t alone be equal to a function of x alone? A moment’s
reﬂection should convince the reader that this can happen if and only if the two functions
are constant,† so
w (t)
v  (x)
= c2
= λ,
(4.65)
w(t)
v(x)
where we use λ to indicate the common separation constant. Thus, the individual factors
w(t) and v(x) must satisfy ordinary diﬀerential equations
d2 w
d2 v
λ
− λ w = 0,
− 2 v = 0,
2
2
dt
dx
c
as promised. We already know how to solve both of these ordinary diﬀerential equations
by elementary techniques. There are three diﬀerent cases, depending on the sign of the
separation constant λ. As a result, each value of λ leads to four independent separable
solutions to the wave equation, as listed in the accompanying table.
Separable Solutions to the Wave Equation

λ

w(t)

v(x)

λ = −ω < 0

cos ω t, sin ω t

ωx
ωx
cos
, sin
c
c

λ=0

1, t

1, x

λ = ω2 > 0

e− ω t , eω t

e− ω x/c , eω x/c

2

u(t, x) = w(t) v(x)
ωx
ωx
, cos ω t sin
,
c
c
ωx
ωx
sin ω t cos
, sin ω t sin
c
c

cos ω t cos

1, x, t, t x
e− ω (t+x/c) , eω (t−x/c) ,
e− ω (t−x/c) , eω (t+x/c)

So far, we have not taken the boundary conditions into account. Consider ﬁrst the
case of a string of length  with two ﬁxed ends, and thus subject to homogeneous Dirichlet
boundary conditions
u(t, 0) = 0 = u(t, ).
Substituting the separable ansatz (4.65), we ﬁnd that v(x) must satisfy
d2 v
λ
(4.66)
− 2 v = 0,
v(0) = 0 = v().
2
dx
c
The complete system of (nontrivial) solutions to this boundary value problem were found
in (4.21):
" n πc #2
n πx
,
λn = −
,
n = 1, 2, 3, . . . .
vn (x) = sin


†
Technical detail : one should assume that the underlying domain is connected for this to be
valid as stated. In practice, this technicality can be safely ignored.

142

4 Separation of Variables

Hence, according to the table, the corresponding separable solutions are
un (t, x) = cos

n πx
n πc t
sin
,



u
n (t, x) = sin

n πx
n πc t
sin
.



(4.67)

We will now employ these solutions to construct a candidate series solution to the wave
equation subject to the prescribed boundary conditions:
u(t, x) =

∞ 

n=1

n πx
n πx
n πc t
n πc t
sin
+ dn sin
sin
bn cos






.

(4.68)

The solution is thus a linear combination of the natural Fourier modes vibrating with
frequencies

nπ κ
n πc
=
,
n = 1, 2, 3, . . . ,
ωn =
(4.69)


ρ
where the second expression follows from (2.66). Observe that, the longer the length 
of the string, or the higher its density ρ, the slower the vibrations, whereas increasing its
stiﬀness or tension κ speeds them up — in exact accordance with our physical intuition.
The Fourier coeﬃcients bn and dn in (4.68) will be uniquely determined by the initial
conditions
∂u
u(0, x) = f (x),
(0, x) = g(x),
0 < x < .
∂t
Diﬀerentiating the series term by term, we discover that we must represent the initial
displacement and velocity as Fourier sine series
∞


∞

∂u
n πx
n πc
(0, x) =
sin
= g(x).
dn
∂t


n=1

n πx
u(0, x) =
= f (x),
bn sin

n=1
Therefore,
bn =

2



f (x) sin
0

n πx
dx,


n = 1, 2, 3, . . . ,

(4.70)

are the Fourier sine coeﬃcients (3.85) of the initial displacement f (x), while
2
dn =
n πc


g(x) sin
0

n πx
dx,


n = 1, 2, 3, . . . .

(4.71)

are rescaled versions of the Fourier sine coeﬃcients of the initial velocity g(x).
Example 4.3. A string of unit length ﬁxed at both ends is held taut at its center
and then released. Our task is to describe the ensuing vibrations. Let us assume that the
physical units are chosen so that c2 = 1, and so we are asked to solve the initial-boundary
value problem
utt = uxx ,

u(0, x) = f (x),

ut (0, x) = 0,

u(t, 0) = u(t, 1) = 0.

(4.72)

To be speciﬁc, we assume that the center of the string has been moved by half a unit, and
so the initial displacement is

x,
0 ≤ x ≤ 12 ,
f (x) =
1
1 − x,
2 ≤ x ≤ 1.

4.2 The Wave Equation

143

t=0

t = .2

t = .4

t = .6

t = .8

t=1

Plucked string solution of the wave equation.

Figure 4.4.



The vibrational frequencies ωn = n π are the integral multiples of π, and so the natural
modes of vibration are
cos n πt sin n πx

and

sin n πt sin n πx

for

n = 1, 2, . . . .

Consequently, the general solution to the boundary value problem is
∞

u(t, x) =
bn cos n πt sin n πx + dn sin n πt sin n πx ,
n=1

where

⎧
⎨

 1

bn = 2

f (x) sin n πx dx =
0

⎩

 1/2
x sin n π x dx =

4
0

4 (− 1)k
,
(2 k + 1)2 π 2

0,

while dn = 0. Therefore, the solution is the Fourier sine series
∞
4 
cos(2 k + 1) π t sin(2 k + 1) πx
u(t, x) = 2
(− 1)k
,
π
(2 k + 1)2

n = 2 k + 1,
n = 2 k,

(4.73)

k=0

whose proﬁle is depicted in Figure 4.4. At time t = 1, the original displacement is reproduced exactly, but upside down. The subsequent dynamics proceeds as before, but in
mirror-image form. The original displacement reappears at time t = 2, after which time

144

4 Separation of Variables

the motion is periodically repeated. Interestingly, at times tk = .5, 1.5, 2.5, . . . , the displacement is identically zero, u(tk , x) ≡ 0, although the velocity is not, ut (tk , x) ≡ 0. The
solution appears to be piecewise aﬃne, i.e., its graph is a collection of straight lines. This
can, in fact, be proved as a consequence of the d’Alembert formula; see Exercise 4.2.13.
Observe that, unlike the heat equation, the wave equation does not smooth out discontinuities and corners in the initial data. And, although we will loosely refer to such piecewise
C2 functions as “solutions”, they are not, in fact, classical solutions. (Their status as weak
solutions, though, can be established using the methods of Section 10.4.)
While the series form (4.68) of the solution is perhaps less satisfying than a d’Alembertstyle formula, we can still use it to deduce important qualitative properties. First of all,
since each term is periodic in t with period 2 /c, the entire solution is time periodic with
that period: u(t + 2 /c, x) = u(t, x). In fact, after half a period, the solution reduces to
 

∞
∞

n πx
n π( − x)

=−
= − u(0,  − x) = − f ( − x).
(−1)n bn sin
bn sin
,x =
u
c


n=1
n=1
In general,




u t + , x = − u(t,  − x),
c



2
u t+
, x = u(t, x).
c

(4.74)

Therefore, the initial wave form is reproduced, ﬁrst as an upside down mirror image of
itself at time t = /c, and then in its original form at time t = 2 /c. This has the important consequence that vibrations of (homogeneous) one-dimensional media are inherently
periodic, because the fundamental frequencies (4.69) are all integer multiples of the lowest
one: ωn = n ω1 .
Remark : The immediately preceding remark has important musical consequences. To
the human ear, sonic vibrations that are integral multiples of a single frequency, and thus
periodic in time, sound harmonious, whereas those with irrationally related frequencies,
and hence experiencing aperiodic vibrations, sound dissonant. This is why most tonal
instruments rely on vibrations in one dimension, be it a violin or piano string, a column
of air in a wind instrument (ﬂute, clarinet, trumpet, or saxophone), a xylophone bar, or
a triangle. On the other hand, most percussion instruments rely on the vibrations of twodimensional media, e.g., drums and cymbals, or three-dimensional solid bodies, e.g., blocks.
As we shall see in Chapters 11 and 12, the frequency ratios of the latter are irrationally
related, and hence their motion is only quasiperiodic, as in Example 2.20. For some reason,
our appreciation of music is psychologically attuned to the diﬀerences between rationally
related/periodic and irrationally related/quasiperiodic vibrations, [105].
Consider next a string with both ends left free, and so subject to the Neumann boundary conditions
∂u
∂u
(t, 0) = 0 =
(t, ).
(4.75)
∂x
∂x
The solutions of (4.66) satisfying v  (0) = 0 = v  () are now
n πx
n πc
vn (x) = cos
with
ωn =
,
n = 0, 1, 2, 3, . . . .


The resulting solution takes the form of a Fourier cosine series

∞ 

n πc t
n πc t
n πx
n πx
cos
+ cn sin
cos
an cos
u(t, x) = a0 + c0 t +
.
(4.76)




n=1

4.2 The Wave Equation

145

The ﬁrst two terms come from the null eigenfunction v0 (x) = 1 with ω0 = 0. The string
vibrates with the same fundamental frequencies (4.69) as in the ﬁxed-end case, but there
is now an additional unstable mode c0 t that is no longer periodic, but grows linearly in
time. In general, the presence of null eigenfunctions implies that the wave equation admits
unstable modes.
Substituting (4.76) into the initial conditions
∂u
(0, x) = g(x),
0 < x < ,
∂t
we ﬁnd that the Fourier coeﬃcients are prescribed, as before, by the initial displacement
and velocity:


2
n πx
2
n πx
an =
dx,
cn =
dx,
n = 1, 2, 3, . . . .
f (x) cos
g(x) cos
 0

n πc 0

u(0, x) = f (x),

The order-zero coeﬃcients†
a0 =

1



f (x) dx,
0

c0 =

1



g(x) dx,
0

are equal to the average initial displacement and average initial velocity of the string. In
particular, when c0 = 0, there is no net initial velocity, and the unstable mode is not
excited. In this case, the solution is time-periodic, oscillating around the position given by
the average initial displacement. On the other hand, if c0 = 0, the string will move oﬀ with
constant average speed c0 , all the while vibrating at the same fundamental frequencies.
Similar considerations apply to the periodic boundary value problem for the wave
equation on a circular ring. The details are left as Exercise 4.2.6 for the reader.

Exercises
4.2.1. In music, an octave corresponds to doubling the frequency of the sound waves. On my
piano, the middle C string has length .7 meter, while the string for the C an octave higher
has length .6 meter. Assuming that they have the same density, how much tighter does the
shorter string need to be tuned?
4.2.2. How much longer would a piano string have to be to make the same sound when it is
pulled twice as tight?
4.2.3. Write down the solutions to the following initial-boundary value problems for the wave
equation in the form of a Fourier series:
(a) utt = uxx , u(t, 0) = u(t, π) = 0, u(0, x) = 1, ut (0, x) = 0;
(b) utt = 2 uxx , u(t, 0) = u(t, π) = 0, u(0, x) = 0, ut (0, x) = 1;
(c) utt = 3 uxx , u(t, 0) = u(t, π) = 0, u(0, x) = sin3 x, ut (0, x) = 0;
(d) utt = 4 uxx , u(t, 0) = u(t, 1) = 0, u(0, x) = x, ut (0, x) = − x;
(e) utt = uxx , u(t, 0) = ux (t, 1) = 0, u(0, x) = 1, ut (0, x) = 0;
(f ) utt = 2 uxx , ux (t, 0) = ux (t, 2 π) = 0, u(0, x) = −1, ut (0, x) = 1;
(g) utt = uxx , ux (t, 0) = ux (t, 1) = 0, u(0, x) = x(1 − x), ut (0, x) = 0.
†
Note that we have not included the usual 12 factor in the constant terms in the Fourier series
(4.76).

146

4 Separation of Variables

4.2.4. Find all separable solutions to the wave equation utt = uxx on the interval 0 ≤ x ≤ π
subject to (a) mixed boundary conditions u(t, 0) = 0, ux (t, π) = 0;
(b) Neumann boundary conditions ux (t, 0) = 0, ux (t, π) = 0.
4.2.5. (a) Under what conditions is the solution to the Neumann boundary value problem (4.75)
a periodic function of t? What is the period? (b) Establish explicit periodicity formulas of
the form (4.74). (c) Under what conditions is the velocity ∂u/∂t periodic in t?
♥ 4.2.6. (a) Formulate the periodic initial-boundary value problem for the wave equation on the
interval − π ≤ x ≤ π, modeling the vibrations of a circular ring. (b) Write out a formula for
the solution to your problem in the form of a Fourier series. (c) Is the solution a periodic
function of t? If so, what is the period? (d) Suppose the initial displacement coincides with
that in Figure 4.6, while the initial velocity is zero. Describe what happens to the solution
as time evolves.
4.2.7. Show that the time derivative, v = ∂u/∂t, of any solution to the wave equation is also a
solution. If you know the initial conditions of u, what initial conditions does v satisfy?
4.2.8. Find all the separable real solutions to the wave equation subject to a restoring force:
utt = uxx − u. Discuss their long-term behavior.
♥ 4.2.9. Let a, c > 0 be positive constants. The telegrapher’s equation utt + a ut = c2 uxx represents a damped version of the wave equation. Consider the Dirichlet boundary value problem u(t, 0) = u(t, 1) = 0, on the interval 0 ≤ x ≤ 1, with initial conditions u(0, x) = f (x),
ut (0, x) = 0. (a) Find all separable solutions to the telegrapher’s equation that satisfy the
boundary conditions. (b) Write down a series solution for the initial boundary value problem. (c) Discuss the long term behavior of your solution. (d) State a criterion that distinguishes overdamped from underdamped versions of the equation.
4.2.10. The fourth-order partial diﬀerential equation utt = − uxxxx is a simple model for a
vibrating elastic beam. (a) Find all separable real solutions to the beam equation.
(b) Show that any C4 (complex) solution to the Schrödinger equation i ut = uxx solves the
beam equation.
4.2.11. The initial-boundary value problem
u(t, 0) = uxx (t, 0) = u(t, 1) = uxx (t, 1) = 0,
0 < x < 1,
utt = − uxxxx ,
t > 0,
u(0, x) = f (x),
ut (0, x) = 0,
models the vibrations of an elastic beam of unit length with simply supported ends, subject
to a nonzero initial displacement f (x) and zero initial velocity. (a) What are the vibrational frequencies for the beam? (b) Write down the solution to the initial-boundary value
problem as a Fourier series. (c) Does the beam vibrate periodically
(i ) for all initial conditions? (ii ) for some initial conditions? (iii ) for no initial conditions?
4.2.12. Multiple choice: The initial-boundary value problem
u(t, 0) = uxx (t, 0) = u(t, 1) = uxx (t, 1) = 0,
utt = uxxxx ,
u(0, x) = f (x),
ut (0, x) = g(x),

0 < x < 1,
t > 0,

is well-posed for (a) t > 0; (b) t < 0; (c) all t; (d) no t. Explain your answer.

The d’Alembert Formula for Bounded Intervals
In Theorem 2.15, we derived the explicit d’Alembert formula
 x+c t
f (x − c t) + f (x + c t)
1
g(z) dz,
u(t, x) =
+
2
2 c x−c t

(4.77)

4.2 The Wave Equation

147

Figure 4.5.

Odd periodic extension of a concentrated pulse.

for solving the basic initial value problem for the wave equation on an inﬁnite interval:
∂u
∂2u
∂2u
= c2 2 ,
u(0, x) = f (x),
(0, x) = g(x),
−∞ < x < ∞.
2
∂t
∂x
∂t
In this section we explain how to adapt the formula in order to solve initial-boundary value
problems on bounded intervals, thereby eﬀectively summing the Fourier series solution.
The easiest case to deal with is the periodic problem on 0 ≤ x ≤ , with boundary
conditions
u(t, 0) = u(t, ),
ux (t, 0) = ux (t, ).
(4.78)
If we extend the initial displacement f (x) and velocity g(x) to be periodic functions of
period , so f (x+) = f (x) and g(x+) = g(x) for all x ∈ R, then the resulting d’Alembert
solution (4.77) will also be periodic in x, so u(t, x + ) = u(t, x). In particular, it satisﬁes
the boundary conditions (4.78) and so coincides with the desired solution. Details are to
be supplied in Exercises 4.2.27–28.
Next, suppose we have ﬁxed (Dirichlet) boundary conditions
u(t, 0) = 0,

u(t, ) = 0.

(4.79)

The resulting solution can be written as a Fourier sine series (4.68), and hence is both odd
and 2 –periodic in x. Therefore, to write the solution in d’Alembert form (4.77), we extend
the initial displacement f (x) and velocity g(x) to be odd, periodic functions of period 2  :
f (− x) = − f (x),

f (x + 2 ) = f (x),

g(− x) = − g(x),

g(x + 2 ) = g(x).

This will ensure that the d’Alembert solution also remains odd and periodic. As a result,
it satisﬁes the homogeneous Dirichlet boundary conditions (4.79) for all t, cf. Exercise
4.2.31. Keep in mind that, while the solution u(t, x) is deﬁned for all x, the only physically
relevant values occur on the interval 0 ≤ x ≤ . Nevertheless, the eﬀects of displacements
in the unphysical regime will eventually be felt as the propagating waves pass through the
physical interval.
For example, consider an initial displacement that is concentrated near x = ξ for some
0 < ξ < . Its odd 2 –periodic extension consists of two sets of replicas: those of the same
form occurring at positions ξ ± 2 , ξ ± 4 , . . . , and their upside-down mirror images at
the intermediate positions − ξ, − ξ ± 2 , − ξ ± 4 , . . . ; Figure 4.5 shows a representative
example. The resulting solution begins with each of the pulses, both positive and negative,
splitting into two half-size replicas that propagate with speed c in opposite directions.
When a left and right moving pulse meet, they emerge from the interaction unaltered. The
process repeats periodically, with an inﬁnite row of half-size pulses moving to the right
kaleidoscopically interacting with an inﬁnite row moving to the left.
However, only the part of this solution that lies on 0 ≤ x ≤  is actually observed
on the physical string. The eﬀect is as if one were watching the full solution as it passes
by a window of length . Such observers will interpret what they see a bit diﬀerently. To

148

4 Separation of Variables

Figure 4.6.

Solution to wave equation with ﬁxed ends.



wit, the original pulse starting at position 0 < ξ <  splits up into two half-size replicas
that move oﬀ in opposite directions. As each half-size pulse reaches an end of the string,
it meets a mirror-image pulse that has been propagating in the opposite direction from
the nonphysical regime. The pulse is reﬂected at the end of the interval and becomes an
upside-down mirror image moving in the opposite direction. The original positive pulse
has moved oﬀ the end of the string just as its mirror image has moved into the physical
regime. (A common physical realization is a pulse propagating down a jump rope that is
held ﬁxed at its end; the reﬂected pulse returns upside down.) A similar reﬂection occurs
as the other half-size pulse hits the other end of the physical interval, after which the
solution consists of two upside-down half-size pulses moving back towards each other. At
time t = /c they recombine at the point  − ξ to instantaneously form a full-sized, but
upside-down mirror image of the original disturbance — in accordance with (4.74). The
recombined pulse in turn splits apart into two upside-down half-size pulses that, when each
collides with the end, reﬂect and return to their original upright form. At time t = 2 /c,
the pulses recombine to exactly reproduce the original displacement. The process then
repeats, and the solution is periodic in time with period 2 /c.
In Figure 4.6, the ﬁrst picture displays the initial displacement. In the second, it has
split into left- and right-moving half-size clones. In the third picture, the left-moving bump
is in the process of colliding with the left end of the string. In the fourth picture, it has
emerged from the collision, and is now upside down, reﬂected, and moving to the right.
Meanwhile, the right-moving pulse is starting to collide with the right end. In the ﬁfth
picture, both pulses have completed their collisions and are now moving back towards each
other, where, in the last picture, they recombine into an upside-down mirror image of the
original pulse. The process then repeats itself, in mirror image, ﬁnally recombining to the
original pulse, at which point the entire process starts over.
The Neumann (free) boundary value problem
∂u
(t, 0) = 0,
∂x

∂u
(t, ) = 0,
∂x

(4.80)

is handled similarly. Since the solution has the form of a Fourier cosine series in x, we

4.2 The Wave Equation

149

extend the initial conditions to be even 2 –periodic functions
f (− x) = f (x),

f (x + 2 ) = f (x),

g(− x) = g(x),

g(x + 2 ) = g(x).

The resulting d’Alembert solution (4.77) is also even and 2 –periodic in x, and hence
satisﬁes the boundary conditions, cf. Exercise 4.2.31(b). In this case, when a pulse hits
one of the ends, its reﬂection remains upright, but becomes a mirror image of the original;
a familiar physical illustration is a water wave that reﬂects oﬀ a solid wall. Further details
are left to the reader in Exercise 4.2.22
In summary, we have now studied two very diﬀerent ways to solve the one-dimensional
wave equation. The ﬁrst, based on the d’Alembert formula, emphasizes their particle-like
aspects, where individual wave packets collide with each other, or reﬂect at the boundary,
all the while maintaining their overall form, while the second, based on Fourier analysis,
emphasizes the vibrational or wave-like character of the solutions. Some solutions look
like vibrating waves, while others appear much more like interacting particles. But, like
the proverbial blind men describing an elephant, these are merely two facets of the same
solution. The Fourier series formula shows how every particle-like solution can be decomposed into its constituent vibrational modes, while the d’Alembert formula demonstrates
how vibrating solutions combine into moving wave packets.
The coexistence of particle and wave features is reminiscent of the long-running historical debate over the nature of light. Newton and his disciples proposed a particle-based
theory, anticipating the modern concept of photons. However, until the beginning of the
twentieth century, most physicists advocated a wave-like or vibrational viewpoint. Einstein’s explanation of the photoelectric eﬀect served to resurrect the particle interpretation.
Only with the establishment of quantum mechanics was the debate resolved — light, and,
indeed, all subatomic particles manifest both particle and wave features, depending upon
the experiment and the physical situation. But a theoretical basis for the perplexing waveparticle duality could have been found already in Fourier’s and d’Alembert’s competing
solution formulae for the classical wave equation!

Exercises
♦ 4.2.13. (a) Solve the initial-boundary value problem from Example 4.3 using the d’Alembert
method. (b) Verify that your solution coincides with the Fourier series solution derived
above. (c) Justify our earlier observation that, at each time t, the solution u(t, x) is a
piecewise aﬃne function of x.
4.2.14. Sketch the solution of the wave equation utt = uxx and describe its behavior when

1,
1 < x < 2,
the initial displacement is the box function u(0, x) =
while the initial
0,
otherwise,
velocity is 0 in each of the following scenarios: (a) on the entire line − ∞ < x < ∞;
(b) on the half-line 0 ≤ x < ∞, with homogeneous Dirichlet boundary condition at the
end; (c) on the half-line 0 ≤ x < ∞, with homogeneous Neumann boundary condition at
the end; (d) on the bounded interval 0 ≤ x ≤ 5 with homogeneous Dirichlet boundary
conditions; (e) on the bounded interval 0 ≤ x ≤ 5 with homogeneous Neumann boundary
conditions.

150

4 Separation of Variables

4.2.15. Answer Exercise 4.2.14 when the initial velocity is the box function, while the initial
displacement is zero.
4.2.16. Consider the initial-boundary value problem
∂2u
∂2u
=
,
∂t2
∂x2

u(t, 0) = 0 = u(t, 10),
u(0, x) = f (x), ut (0, x) = 0,

t > 0,
0 < x < 10,

for the wave equation, where the initial data has the following form:
⎧
3 x − 7.5,
⎪
⎪
⎪
⎨ 6 − 1.5 x,

f (x) = ⎪

1.5 x − 7.5,
⎪
⎪
⎩
0,

2.5 ≤ x ≤ 3,
3 ≤ x ≤ 4.5,
4.5 ≤ x ≤ 5,
otherwise.

Discuss what happens to the solution. You do not need to write down an explicit formula
for the solution, but for full credit you must explain (sketches can help) at least three or
four interesting things that happen to the solution as time progresses.
4.2.17. Repeat Exercise 4.2.16 for the Neumann boundary conditions.
4.2.18. Suppose the initial displacement of a string of length looks like
the graph to the right. Assuming that the ends of the string are held
ﬁxed, graph the string’s proﬁle at times t = /c and 2 /c.
♣ 4.2.19. Consider the wave equation utt = uxx on the interval 0 ≤ x ≤ 1, with homogeneous
Dirichlet boundary conditions at both ends. (a) Use the d’Alembert formula to explicitly
solve the initial value problem u(0, x) = x − x2 , ut (0, x) = 0. (b) Graph the solution
proﬁle at some representative times, and discuss what you observe. (c) Find the Fourier
series at each t of your solution and compare the two. (d) How many terms do you need
to sum to obtain a reasonable approximation to the exact solution?
♣ 4.2.20. Solve Exercise 4.2.19 for the initial conditions u(0, x) = 0, ut (0, x) = x2 − x.
♣ 4.2.21. Solve (i ) Exercise 4.2.19, (ii ) Exercise 4.2.20, when the solution is subject to homogeneous Neumann boundary conditions.
♦ 4.2.22. Under what conditions is the solution to the Neumann boundary value problem for the
wave equation on a bounded interval [ 0, ] periodic in time? What is the period?
4.2.23. Discuss and sketch the behavior of the solution to the Neumann boundary value problem utt = 4 uxx , 0 < x < 1, ux (t, 0) = 0 = ux (t, 1), u(0, x) = f (x), ut (0, x) = g(x), for

1, .2 < x < .3,
g(x) = 0;
(a) a localized initial displacement: f (x) =
0, otherwise.

1, .2 < x < .3,
(b) a localized initial velocity: f (x) = 0, g(x) =
.
0, otherwise.
4.2.24. (a) Explain how to solve the Neumann initial-boundary value problem
∂2u
∂2u
=
,
2
∂t
∂x2

∂u
∂u
(t, 0) = 0 =
(t, 1),
∂x
∂x

u(0, x) = f (x),

∂u
(0, x) = g(x),
∂t

on the interval 0 ≤ x ≤ 1.
⎧
1
1
1
⎪
⎪
⎨ x − 4,
4 ≤ x ≤ 2,
3
1
3 and g(x) = 0. Sketch the graph of the solution at
(b) Let f (x) = ⎪ − x,
2 ≤ x ≤ 4,
⎪
⎩ 4
0,
otherwise,
a few representative times, and discuss what is happening. Is the solution periodic in
time? If so, what is the period?
(c) Do the same when f (x) = 0 and g(x) = x.

4.2 The Wave Equation

151

4.2.25. (a) Write down a formula for the solution u(t, x) to the initial-boundary value problem
∂2u
∂2u
∂u
∂u
∂u
−
4
= 0, u(0, x) = sin x,
(0, x) =
(t, 0) =
(t, π) = 0, 0 < x < π, t > 0.
2
2
∂t
∂x
∂t
∂x
∂x

period.



π π
2, 2







. (c) Prove that h(t) = u t, π2 is a periodic function of t and ﬁnd its
∂u
have any discontinuities? If so, discuss their behavior.
(d) Does
∂x

(b) Find u

4.2.26. Answer Exercise 4.2.25 for the mixed boundary conditions u(t, 0) = 0 = ux (t, π).
♥ 4.2.27. (a) Explain how to use d’Alembert’s formula (4.77) to solve the periodic initial-boundary
value problem for the wave equation given in Exercise 4.2.6.
(b) Do the d’Alembert and Fourier series formulae represent the same solution? If so, can
you justify it? If not, explain why they are diﬀerent.
♦ 4.2.28. Show that the solution u(t, x) to the wave equation on an interval [ 0, ], subject to periodic boundary conditions u(t, 0) = u(t, ), ux (t, 0) = ux (t, ), is a periodic function of t if
and only if there is no net initial velocity:

0

g(x) dx = 0.

4.2.29. (a) Explain how to solve the wave equation on a half-line x > 0 when subject to Dirichlet boundary conditions u(t, 0) = 0. (b) Assuming c = 1, ﬁnd the solution satisfying
2

u(0, x) = (x − 2) e−5 (x−2.2) , ut (0, x) = 0. (c) Sketch a picture of your solution at some
representative times, and discuss what is happening.
4.2.30. Solve Exercise 4.2.29 for homogeneous Neumann boundary conditions at x = 0.
♦ 4.2.31. (a) Given that f (x) is odd and 2 –periodic, explain why f (0) = 0 = f ( ).
(b) Given that f (x) is even and 2 –periodic, explain why f  (0) = 0 = f  ( ).
♦ 4.2.32. (a) Prove that if f (− x) = − f (x), f (x + 2 ) = f (x), for all x, then
u(t, x) = 12 [ f (x − c t) + f (x + c t) ] satisﬁes the Dirichlet boundary conditions (4.79).
(b) Prove that if g(− x) = − g(x), g(x + 2 ) = g(x) for all x, then
x+c t
1
g(z) dz also satisﬁes the Dirichlet boundary conditions.
u(t, x) =
2 c x−c t
4.2.33. If both u(0, x) = f (x) and ut (0, x) = g(x) are even functions, show that the solution
u(t, x) of the wave equation is even in x for all t.
4.2.34. (a) Prove that the solution u(t, x) to the wave equation for x ∈ R is an even function of
t if and only if its initial velocity, at t = 0, is zero.
(b) Under what conditions is u(t, x) an odd function of t?
♦ 4.2.35. Let u(t, x) be a classical solution to the wave equation utt = c2 uxx on the interval
0 < x < , satisfying homogeneous Dirichlet boundary conditions. The total energy of u at
time t is
⎡
2
2 ⎤

1 ⎣ ∂u
∂u ⎦
2
+c
dx.
(4.81)
E(t) =
0 2
∂t
∂x
Establish the Law of Conservation of Energy by showing that E(t) = E(0) is a constant
function.
♦ 4.2.36. (a) Use Exercise 4.2.35 to prove that the only C2 solution to the initial-boundary value
problem vtt = c2 vxx , v(t, 0) = v(t, ) = 0, v(0, x) = 0, vt (0, x) = 0, is the trivial solution v(t, x) ≡ 0. (b) Establish the following Uniqueness Theorem for the wave equation:
given f (x), g(x) ∈ C2 , there is at most one C2 solution u(t, x) to the initial-boundary value
problem utt = c2 uxx , u(t, 0) = u(t, ) = 0, u(0, x) = f (x), ut (0, x) = g(x).
4.2.37. Referring back to Exercises 4.2.35 and 4.2.36: (a) Does conservation of energy hold for
solutions to the homogeneous Neumann initial-boundary value problem?
(b) Can you establish a uniqueness theorem for the Neumann problem?

152

4 Separation of Variables

4.2.38. Explain how to solve the Dirichlet initial-boundary value problem
utt = c2 uxx + F (t, x),

u(0, x) = f (x),

ut (0, x) = g(x),

u(t, 0) = u(t, ) = 0,

for the wave equation subject to an external forcing on the interval [ 0, ].

4.3 The Planar Laplace and Poisson Equations
The two-dimensional Laplace equation is the second-order linear partial diﬀerential equation
∂2u ∂2u
+ 2 = 0,
(4.82)
∂x2
∂y
named in honor of the inﬂuential eighteenth-century French mathematician Pierre–Simon
Laplace. It, along with its higher-dimensional versions, is arguably the most important
diﬀerential equation in all of mathematics. A real-valued solution u(x, y) to the Laplace
equation is known as a harmonic function. The space of harmonic functions can thus be
identiﬁed as the kernel of the second-order linear partial diﬀerential operator
Δ=

∂2
∂2
+
,
∂x2
∂y 2

(4.83)

known as the Laplace operator , or Laplacian for short. The inhomogeneous or forced
version, namely
∂2u ∂2u
− Δ[ u ] = −
− 2 = f (x, y),
(4.84)
∂x2
∂y
is known as Poisson’s equation, named after Siméon–Denis Poisson, who was taught by
Laplace. The mathematical and physical reasons for including the minus sign will gradually
become clear.
Besides their theoretical importance, the Laplace and Poisson equations arise as the
basic equilibrium equations in a remarkable variety of physical systems. For example, we
may interpret u(x, y) as the displacement of a membrane, e.g., a drum skin; the inhomogeneity f (x, y) in the Poisson equation represents an external forcing over the surface of
the membrane. Another example is in the thermal equilibrium of ﬂat plates; here u(x, y)
represents the temperature and f (x, y) an external heat source. In ﬂuid mechanics, u(x, y)
represents the potential function whose gradient v = ∇u is the velocity vector ﬁeld of a
steady planar ﬂuid ﬂow. Similar considerations apply to two-dimensional electrostatic
and gravitational potentials. The dynamical counterparts to the Laplace equation are the
two-dimensional versions of the heat and wave equations, to be analyzed in Chapter 11.
Since both the Laplace and Poisson equations describe equilibrium conﬁgurations, they
almost always appear in the context of boundary value problems. We seek a solution u(x, y)
to the partial diﬀerential equation deﬁned at points (x, y) belonging to a bounded, open
domain Ω ⊂ R 2 . The solution is required to satisfy suitable conditions on the boundary
of the domain, denoted by ∂Ω, which will consist of one or more simple closed curves, as
illustrated in Figure 4.7. As in one-dimensional boundary value problems, there are several
especially important types of boundary conditions.

4.3 The Planar Laplace and Poisson Equations

153

n
n
Ω

n

n

∂Ω
Figure 4.7.

A planar domain with outward unit normals on its boundary.

The ﬁrst are the ﬁxed or Dirichlet boundary conditions, which specify the value of the
function u on the boundary:
u(x, y) = h(x, y)

for

(x, y) ∈ ∂Ω.

(4.85)

Under mild regularity conditions on the domain Ω, the boundary values h, and the forcing
function f , the Dirichlet conditions (4.85) serve to uniquely specify the solution u(x, y) to
the Laplace or the Poisson equation. Physically, in the case of a free or forced membrane,
the Dirichlet boundary conditions correspond to gluing the edge of the membrane to a
wire at height h(x, y) over each boundary point (x, y) ∈ ∂Ω, as illustrated in Figure 4.8.
A physical realization can be easily obtained by dipping the wire in a soap solution; the
resulting soap ﬁlm spanning the wire forms a minimal surface, which, if the wire is reasonably close to planar shape,† is the solution to the Dirichlet problem prescribed by the wire.
Similarly, in the modeling of thermal equilibrium, a Dirichlet boundary condition represents the imposition of a prescribed temperature distribution, represented by the function
h, along the boundary of the plate.
The second important class consists of the Neumann boundary conditions
∂u
= ∇u · n = k(x, y)
∂n

on

∂Ω,

(4.86)

in which the normal derivative of the solution u on the boundary is prescribed. In general, n
denotes the unit outwards normal to the boundary ∂Ω, i.e., the vector of unit length,  n  =
1, that is orthogonal to the tangent to the boundary and points away from the domain; see
Figure 4.7. For example, in thermomechanics, a Neumann boundary condition speciﬁes
the heat ﬂux out of a plate through its boundary. The “no-ﬂux” or homogeneous Neumann
boundary conditions, where k(x, y) ≡ 0, correspond to a fully insulated boundary. In the
case of a membrane, homogeneous Neumann boundary conditions correspond to a free,
unattached edge of a drum. In ﬂuid mechanics, the Neumann conditions prescribe the
ﬂuid ﬂux through the boundary; in particular, homogeneous Neumann boundary conditions
†

More generally, the minimal surface formed by the soap ﬁlm solves the vastly more complicated nonlinear minimal surface equation (1 + u2x )uxx − 2 ux uy uxy + (1 + u2y )uyy = 0, which, for
surfaces with small variation, i.e., with  ∇u   1, can be approximated by the Laplace equation.

154

4 Separation of Variables

h(x, y)

∂Ω
Figure 4.8.

Dirichlet boundary conditions.

correspond to a solid boundary that the ﬂuid cannot penetrate. More generally, the Robin
boundary conditions
∂u
+ β(x, y) u = k(x, y)
on
∂Ω,
∂n
also known as impedance boundary conditions due to their applications in electromagnetism, are used to model insulated plates in heat baths, or membranes attached to springs.
Finally, one can mix the previous kinds of boundary conditions, imposing, say, Dirichlet conditions on part of the boundary and Neumann conditions on the complementary
part. A typical mixed boundary value problem has the form
− Δu = f

in

Ω,

u=h

on D,

∂u
=k
∂n

on N,

(4.87)

with the boundary ∂Ω = D ∪ N being the disjoint union of a “Dirichlet segment”, denoted
by D, and a “Neumann segment” N . For example, if u represents the equilibrium temperature in a plate, then the Dirichlet segment of the boundary is where the temperature
is ﬁxed, while the Neumann segment is insulated, or, more generally, has prescribed heat
ﬂux. Similarly, when modeling the displacement of a membrane, the Dirichlet segment is
where the edge of the drum is attached to a support, while the homogeneous Neumann
segment is left hanging free.

Exercises
4.3.1. (a) Solve the boundary value problem Δu = 1 for x2 + y 2 < 1 and u(x, y) = 0 for
x2 + y 2 = 1 directly. Hint: The solution is a simple polynomial.
(b) Graph your solution, interpreting it as the equilibrium displacement of a circular drum
under a constant gravitational force.
4.3.2. Set up the boundary value problem corresponding to the equilibrium of a circular
membrane subject to a constant downwards gravitational force, half of whose boundary is
glued to a ﬂat semicircular wire, while the other half is unattached.
4.3.3. Set up the boundary value problem corresponding to the thermal equilibrium of a
rectangular plate that is insulated on two of its sides, has 0◦ at its top edge and 100◦ at the

4.3 The Planar Laplace and Poisson Equations

155

bottom edge. Where do you expect the maximum temperature to be located? What is its
value? Can you ﬁnd a formula for the temperature inside the plate? Hint: The solution is
constant along horizontal lines.
4.3.4. Set up the boundary value problem corresponding to the thermal equilibrium of an
insulated semi-circular plate with unit diameter, whose curved edge is kept at 0◦ and whose
straight edge is at 50◦ .
4.3.5. Explain why the solution to the homogeneous Neumann boundary value problem for the
Laplace equation is not unique.
4.3.6. Write down the Dirichlet boundary value problem for the Laplace equation on the unit
square 0 ≤ x, y ≤ 1 that is satisﬁed by u(x, y) = 1 + x y.
4.3.7. Write down the Neumann boundary value problem for the Poisson equation on the unit
disk x2 + y 2 ≤ 1 that is satisﬁed by u(x, y) = x3 + x y 2 .
♦ 4.3.8. Suppose u(x, y) is a solution to the Laplace equation.
(a) Show that any translate U (x, y) = u(x − a, y − b), where a, b ∈ R, is also a solution.
(b) Show that the rotated function U (x, y) = u(x cos θ + y sin θ, − x sin θ + y cos θ), where
− π < θ ≤ π, is also a solution.
♦ 4.3.9. (a) Show that if u(x, y) solves the Laplace equation, then so does the rescaled function
U (x, y) = c u(α x, α y) for any constants c, α.
(b) Discuss the eﬀect of scaling on the Dirichlet boundary value problem.
(c) What happens if we use diﬀerent scaling factors in x and y?

Separation of Variables
Our ﬁrst approach to solving the Laplace equation
Δu =

∂2u ∂2u
+ 2 =0
∂x2
∂y

(4.88)

will be based on the method of separation of variables. As in (4.64), we seek solutions that
can be written as a product
u(x, y) = v(x) w(y)
(4.89)
of a function of x alone times a function of y alone. We compute
∂2u
= v  (x) w(y),
∂x2

∂2u
= v(x) w (y),
∂y 2

and so
Δu =

∂2u ∂2u
+ 2 = v  (x) w(y) + v(x) w (y) = 0.
∂x2
∂y

We then separate the variables by placing all the terms involving x on one side of the
equation and all the terms involving y on the other; this is accomplished by dividing by
v(x) w(y) and then writing the resulting equation in the separated form
v  (x)
w (y)
=−
= λ.
v(x)
w(y)

(4.90)

156

4 Separation of Variables

As we argued in (4.65), the only way a function of x alone can be equal to a function of y
alone is if both functions are equal to a common separation constant λ. Thus, the factors
v(x) and w(y) must satisfy the elementary ordinary diﬀerential equations
v  − λ v = 0,

w + λ w = 0.

As before, the solution formulas depend on the sign of the separation constant λ. We list
the resulting collection of separable harmonic functions in the following table:
Separable Solutions to Laplace’s Equation
λ

v(x)

w(y)

u(x, y) = v(x) w(y)

λ = − ω2 < 0

cos ω x, sin ω x

e− ω y , eω y ,

λ=0

1, x

1, y

λ = ω2 > 0

e− ω x , eω x

cos ω y, sin ω y

eω y cos ω x,
e

−ω y

eω y sin ω x,

cos ω x, e− ω y sin ω x
1, x, y, x y

eω x cos ω y,

eω x sin ω y,

e− ω x cos ω y, e− ω x sin ω y

Since Laplace’s equation is a homogeneous linear system, any linear combination of
solutions is also a solution. So, we can build more general solutions as ﬁnite linear combinations, or, provided we pay proper attention to convergence issues, inﬁnite series in the
separable solutions. Our goal is to solve boundary value problems, and so we must ensure
that the resulting combination satisﬁes the boundary conditions. But this is not such an
easy task, unless the underlying domain has a rather special geometry.
In fact, the only bounded domains on which we can explicitly solve boundary value
problems using the preceding separable solutions are rectangles. So, we will concentrate
on boundary value problems for Laplace’s equation
Δu = 0

on a rectangle

R = { 0 < x < a, 0 < y < b }.

(4.91)

To make progress, we will allow nonzero boundary values on only one of the four sides of
the rectangle. To illustrate, we will focus on the following Dirichlet boundary conditions:
u(x, 0) = f (x),

u(x, b) = 0,

u(0, y) = 0,

u(a, y) = 0.

(4.92)

Once we know how to solve this type of problem, we can employ linear superposition to
solve the general Dirichlet boundary value problem on a rectangle; see Exercise 4.3.12 for
details. Other boundary conditions can be treated in a similar fashion — with the proviso
that the condition on each side of the rectangle is either entirely Dirichlet or entirely
Neumann or, more generally, entirely Robin with constant transfer coeﬃcient.
To solve the boundary value problem (4.91–92), the ﬁrst step is to narrow down the
separable solutions to only those that respect the three homogeneous boundary conditions.
The separable function u(x, y) = v(x) w(y) will vanish on the top, right, and left sides of
the rectangle, provided
v(0) = v(a) = 0

and

w(b) = 0.

4.3 The Planar Laplace and Poisson Equations

157

Referring to the preceding table, the ﬁrst condition v(0) = 0 requires
⎧
⎪
λ = − ω 2 < 0,
⎨ sin ω x,
v(x) =
x,
λ = 0,
⎪
⎩
sinh ω x,
λ = ω 2 > 0,
where sinh z = 12 (ez − e− z ) is the usual hyperbolic sine function. However, the second
and third cases cannot satisfy the second boundary condition v(a) = 0, and so we discard
them. The ﬁrst case leads to the condition
v(a) = sin ω a = 0,

and hence

ω a = π, 2 π, 3 π, . . . .

The corresponding separation constants and solutions (up to constant multiple) are
λn = − ω 2 = −

n2 π 2
,
a2

vn (x) = sin

n πx
,
a

n = 1, 2, 3, . . . .

(4.93)

Note: So far, we have merely recomputed the known eigenvalues and eigenfunctions
of the familiar boundary value problem v  − λ v = 0, v(0) = v(a) = 0.
Next, since λ = − ω 2 < 0, we have w(y) = c1 eω y + c2 e− ω y for constants c1 , c2 . The
third boundary condition w(b) = 0 then requires that, up to constant multiple,
wn (y) = sinh ω (b − y) = sinh

n π(b − y)
.
a

(4.94)

We conclude that the harmonic functions
n πx
n π(b − y)
(4.95)
sinh
,
n = 1, 2, 3, . . . ,
a
a
provide a complete list of separable solutions that satisfy the three homogeneous boundary
conditions. It remains to analyze the inhomogeneous boundary condition along the bottom
edge of the rectangle. To this end, let us try a linear superposition of the relevant separable
solutions in the form of an inﬁnite series
un (x, y) = sin

u(x, y) =

∞


cn un (x, y) =

n=1

∞

n=1

cn sin

n πx
n π (b − y)
sinh
,
a
a

whose coeﬃcients c1 , c2 , . . . are to be prescribed by the remaining boundary condition. At
the bottom edge, y = 0, we ﬁnd
u(x, 0) =

∞


cn sinh

n πb
n πx
sin
= f (x),
a
a

0 ≤ x ≤ a,

(4.96)

n=1

which takes the form of a Fourier sine series for the function f (x). Let

2 a
n πx
f (x) sin
dx
bn =
a 0
a

(4.97)

be its Fourier sine coeﬃcients, whence cn = bn / sinh(n πb/a). We thus anticipate that the
solution to the boundary value problem can be expressed as the inﬁnite series

u(x, y) =

∞ b sin

n
n=1

n πx
n π(b − y)
sinh
a
a
.
n πb
sinh
a

(4.98)

158

4 Separation of Variables

Figure 4.9.

Square membrane on a wire.

Does this series actually converge to the solution to the boundary value problem?
Fourier analysis says that, under very mild conditions on the boundary function f (x), the
answer is yes. Suppose that its Fourier coeﬃcients are uniformly bounded,
| bn | ≤ M

for all

n ≥ 1,

(4.99)

which, according
to (4.27), is true whenever f (x) is piecewise continuous or, more generally,

a

| f (x) | dx < ∞. In this case, as you are asked to prove in Exercise 4.3.20,

integrable:
0

the coeﬃcients of the Fourier sine series (4.98) go to zero exponentially fast:
n π(b − y)
a
−→ 0
n πb
sinh
a

bn sinh

as n −→ ∞

for all

0 < y ≤ b,

(4.100)

and so, at each point inside the rectangle, the series can be well approximated by partial
summation. Theorem 3.31 tells us that, for each 0 < y ≤ b, the solution u(x, y) is an
inﬁnitely diﬀerentiable function of x. Moreover, by term-wise diﬀerentiation of the series
with respect to y and use of Proposition 3.28, we also establish that the solution is inﬁnitely
diﬀerentiable with respect to y; see Exercise 4.3.21. (In fact, as we shall see, solutions to
the Laplace equation are always analytic functions inside their domain of deﬁnition — even
when their boundary values are rather rough.) Since the individual terms all satisfy the
Laplace equation, we conclude that the series (4.98) is indeed a classical solution to the
boundary value problem.
Example 4.4. A membrane is stretched over a wire in the shape of a unit square
with one side bent in half, as graphed in Figure 4.9. The precise boundary conditions are
⎧
x,
0 ≤ x ≤ 12 ,
y = 0,
⎪
⎪
⎪
1
⎪
⎪
y = 0,
⎨ 1 − x,
2 ≤ x ≤ 1,
u(x, y) =
0,
0 ≤ x ≤ 1,
y = 1,
⎪
⎪
⎪
⎪ 0,
x = 0,
0 ≤ y ≤ 1,
⎪
⎩
0,
x = 1,
0 ≤ y ≤ 1.

4.3 The Planar Laplace and Poisson Equations

159

The Fourier sine series of the inhomogeneous boundary function is readily computed:

x,
0 ≤ x ≤ 12 ,
f (x) =
1
1 − x,
2 ≤ x ≤ 1,


∞
sin 3 πx sin 5 πx
sin(2 j + 1)πx
4 
4
(−1)j
.
+
−··· = 2
= 2 sin πx −
π
9
25
π j =0
(2 j + 1)2
Specializing (4.98) to a = b = 1, we conclude that the solution to the boundary value
problem can be expressed as a Fourier series
∞

4 
sin(2 j + 1) πx sinh(2 j + 1) π(1 − y)
u(x, y) = 2
(−1)j
.
π j =0
(2 j + 1)2 sinh(2 j + 1) π
In Figure 4.9 we plot the sum of the ﬁrst 10 terms in the series. This gives a reasonably good
approximation to the actual solution, except when we are very close to the raised corner
of the boundary wire — which is the point of maximal displacement of the membrane.

Exercises
4.3.10. Solve the following boundary value problems for Laplace’s equation on the square
Ω = { 0 ≤ x ≤ π, 0 ≤ y ≤ π }.
(a) u(x, 0) = sin3 x, u(x, π) = 0, u(0, y) = 0, u(π, y) = 0.
(b) u(x, 0) = 0, u(x, π) = 0, u(0, y) = sin y, u(π, y) = 0.
(c) u(x, 0) = 0, u(x, π) = 1, u(0, y) = 0, u(π, y) = 0.
(d) u(x, 0) = 0, u(x, π) = 0, u(0, y) = 0, u(π, y) = y(π − y).
♦ 4.3.11. (a) Explain how to use linear superposition to solve the boundary value problem
Δu = 0,
u(x, 0) = f (x),
u(x, b) = g(x),
u(0, y) = h(y),
u(a, y) = k(y),
on the rectangle R = { 0 < x < a, 0 < y < b }, by splitting it into four separate boundary
value problems for which each of the solutions vanishes on three sides of the rectangle.
(b) Write down a series formula for the resulting solution.
4.3.12. Solve the following Dirichlet problems for Laplace’s equation on the unit square
S = { 0 < x, y < 1 }. Hint: Use superposition as in Exercise 4.3.11.
(a) u(x, 0) = sin πx, u(x, 1) = 0, u(0, y) = sin πy, u(1, y) = 0;
(b) u(x, 0) = 1, u(x, 1) = 0, u(0, y) = 1, u(1, y) = 0;
(c) u(x, 0) = 1, u(x, 1) = 1, u(0, y) = 0, u(1, y) = 0;
(d) u(x, 0) = x, u(x, 1) = 1 − x, u(0, y) = y, u(1, y) = 1 − y.
4.3.13. Solve the following mixed boundary value problems for Laplace’s equation Δu = 0 on
the square S = { 0 < x, y < π }.
(a) u(x, 0) = sin 12 x, uy (x, π) = 0, u(0, y) = 0, ux (π, y) = 0;
(b) u(x, 0) = sin 12 x, uy (x, π) = 0, ux (0, y) = 0, ux (π, y) = 0;
(c) u(x, 0) = x, u(x, π) = 0, ux (0, y) = 0, ux (π, y) = 0;
(d) u(x, 0) = x, u(x, π) = 0, u(0, y) = 0, ux (π, y) = 0.
4.3.14. Find the solution to the boundary value problem
uy (x, 0) = uy (x, 2) = 0,
Δu = 0,
u(0, y) = 2 cos πy − 1, u(1, y) = 0,

0 < x < 1,
0 < y < 2.

160

4 Separation of Variables

4.3.15. Find the solution to the boundary value problem
u(x, 0) = 2 cos 7 πx − 4, u(x, 1) = 5 cos 3 πx,
Δu = 0,
ux (0, y) = ux (1, y) = 0,

0 < x, y < 1.

4.3.16. Let u(x, y) be the solution to the boundary value problem
Δu = 0, u(x, −1) = f (x), u(x, 1) = 0, u(−1, y) = 0, u(1, y) = 0, −1 < x < 1, −1 < y < 1.
(a) True or false: If f (− x) = − f (x) is odd, then u(0, y) = 0 for all −1 ≤ y ≤ 1.
(b) True or false: If f (0) = 0, then u(0, y) = 0 for all −1 ≤ y ≤ 1.
(c) Under what conditions on f (x) is u(x, 0) = 0 for all −1 ≤ x ≤ 1?
4.3.17. Use separation of variables to solve the following boundary value problem:
uxx + 2 uy + uyy = 0, u(x, 0) = 0, u(x, 1) = f (x), u(0, y) = 0, u(1, y) = 0.
4.3.18. Use separation of variables to solve the Helmholtz boundary value problem Δu = u,
u(x, 0) = 0, u(x, 1) = f (x), u(0, y) = 0, u(1, y) = 0, on the unit square 0 < x, y < 1.
♦ 4.3.19. Provide the details for the derivation of (4.94).
♦ 4.3.20. Justify the statement that if | bn | ≤ M are uniformly bounded, then the coeﬃcients
given in (4.100) go to zero exponentially fast as n → ∞ for any 0 < y ≤ b.
♦ 4.3.21. Let u(x, y) denote the solution to the boundary value problem (4.91–92).
(a) Write down the Fourier sine series for ∂u/∂y . (b) Prove that ∂u/∂y is an inﬁnitely
diﬀerentiable function of x. (c) Justify the same result for the functions ∂ k u/∂y k for each
k ≥ 0. Hint: Don’t forget that u(x, y) solves the Laplace equation.

Polar Coordinates
The method of separation of variables can be successfully exploited in certain other very
special geometries. One particularly important case is a circular disk. To be speciﬁc, let
us take the disk to have radius 1 and be centered at the origin. Consider the Dirichlet
boundary value problem
Δu = 0,

x2 + y 2 < 1,

and

u = h,

x2 + y 2 = 1,

(4.101)

so that the function u(x, y) satisﬁes the Laplace equation on the unit disk and the speciﬁed
Dirichlet boundary conditions on the unit circle. For example, u(x, y) might represent the
displacement of a circular drum that is attached to a wire of height
h(x, y) = h(cos θ, sin θ) ≡ h(θ),

− π < θ ≤ π,

(4.102)

at each point (x, y) = (cos θ, sin θ) on its edge.
The rectangular separable solutions are not particularly helpful in this situation, and
so we look for solutions that are better adapted to a circular geometry. This inspires us to
adopt polar coordinates

y
(4.103)
θ = tan−1 ,
x = r cos θ,
y = r sin θ,
or
r = x2 + y 2 ,
x
and write the solution u(r, θ) as a function thereof.
Warning: We will often retain the same symbol, e.g., u, when rewriting a function
in a diﬀerent coordinate system. This is the convention of tensor analysis, physics, and

4.3 The Planar Laplace and Poisson Equations

161

diﬀerential geometry, [3], that treats the function (scalar ﬁeld) as an intrinsic object, which
is concretely realized through its formula in any chosen coordinate system. For instance,
if u(x, y) = x2 + 2 y in rectangular coordinates, then its expression in polar coordinates
is u(r, θ) = (r cos θ)2 + 2 r sin θ, not r2 + 2 θ. This convention avoids the inconvenience of
having to devise new symbols when changing coordinates.
We need to relate derivatives with respect to x and y to those with respect to r and
θ. Performing a standard multivariate chain rule computation based on (4.103), we obtain
∂
∂
∂
= cos θ
+ sin θ
,
∂r
∂x
∂y
∂
∂
∂
= − r sin θ
+ r cos θ
,
∂θ
∂x
∂y

so

∂
∂
sin θ ∂
= cos θ
−
,
∂x
∂r
r ∂θ
∂
cos θ ∂
∂
= sin θ
+
.
∂y
∂r
r ∂θ

(4.104)

Applying the squares of the latter diﬀerential operators to u(r, θ), we ﬁnd, after a calculation in which many of the terms cancel, the polar coordinate form of the Laplace equation:
Δu =

∂2u ∂2u
1 ∂2u
∂ 2 u 1 ∂u
+
+
=
+
= 0.
∂x2
∂y 2
∂r2
r ∂r
r2 ∂θ2

(4.105)

The boundary conditions are imposed on the unit circle r = 1, and so, by (4.102), take the
form
u(1, θ) = h(θ).
(4.106)
Keep in mind that, in order to be single-valued functions of x, y, the solution u(r, θ) and
its boundary values h(θ) must both be 2 π–periodic functions of the angular coordinate:
u(r, θ + 2 π) = u(r, θ),

h(θ + 2 π) = h(θ).

(4.107)

Polar separation of variables is based on the ansatz
u(r, θ) = v(r) w(θ),

(4.108)

which assumes that the solution is a product of functions of the individual variables. Substituting (4.108) into the polar form (4.105) of Laplace’s equation yields
1 
1
v (r) w(θ) + 2 v(r) w (θ) = 0.
r
r
We now separate variables by moving all the terms involving r onto one side of the equation
and all the terms involving θ onto the other. This is accomplished by ﬁrst multiplying the
equation by r2 / v(r) w(θ) and then moving the ﬁnal term to the right-hand side:
v  (r) w(θ) +

r2 v  (r) + r v  (r)
w (θ)
=−
= λ.
v(r)
w(θ)
As in the rectangular case, a function of r can equal a function of θ if and only if both are
equal to a common separation constant, which we call λ. The partial diﬀerential equation
thus splits into a pair of ordinary diﬀerential equations
r2 v  + r v  − λ v = 0,

w + λ w = 0,

(4.109)

that will prescribe the separable solution (4.108). Observe that both have the form of an
eigenfunction equation in which the separation constant λ plays the role of the eigenvalue.
We are, as always, interested only in nonzero solutions.

162

4 Separation of Variables

We have already solved the eigenvalue problem for w(θ). According to (4.107),
w(θ + 2 π) = w(θ) must be a 2 π–periodic function. Therefore, by our earlier discussion,
this periodic boundary value problem has the nonzero eigenfunctions
1,

sin n θ,

cos n θ,

n = 1, 2, . . . ,

(4.110)

corresponding to the eigenvalues (separation constants)
λ = n2 ,

n = 0, 1, 2, . . . .

With the value of λ ﬁxed, the linear ordinary diﬀerential equation for the radial component,
r2 v  + r v  − n2 v = 0,

(4.111)

does not have constant coeﬃcients. But, fortunately, it has the form of a second-order Euler
ordinary diﬀerential equation, [23, 89], and hence can be readily solved by substituting the
power ansatz v(r) = rk . (See also Exercise 4.3.23.) Note that
v  (r) = k rk−1 ,

v  (r) = k (k − 1) rk−2 ,

and hence, by substituting into the diﬀerential equation,


r2 v  + r v  − n2 v = k (k − 1) + k − n2 rk = (k 2 − n2 )rk .
Thus, rk is a solution if and only if
k 2 − n2 = 0,

and hence

k = ± n.

For n = 0, we have found the two linearly independent solutions:
v2 (r) = r−n ,

v1 (r) = rn ,

n = 1, 2, . . . .

(4.112)

When n = 0, the power ansatz yields only the constant solution. But in this case, the
equation r2 v  + r v  = 0 is eﬀectively of ﬁrst order and linear in v  , and hence readily
integrated. This provides the two independent solutions
v1 (r) = 1,

v2 (r) = log r,

n = 0.

(4.113)

Combining (4.110) and (4.112–113), we produce the complete list of separable polar coordinate solutions to the Laplace equation:
1,

rn cos n θ,

rn sin n θ,

log r,

r−n cos n θ,

r−n sin n θ,

n = 1, 2, 3, . . . .

(4.114)

Now, the solutions in the top row of (4.114) are continuous (in fact analytic) at the origin,
where r = 0, whereas the solutions in the bottom row have singularities as r → 0. The
latter are not of use in the present situation, since we require that the solution remain
bounded and smooth — even at the center of the disk. Thus, we should use only the
nonsingular solutions to concoct a candidate series solution
u(r, θ) =

∞

a0
+
an rn cos n θ + bn rn sin n θ .
2
n=1

(4.115)

4.3 The Planar Laplace and Poisson Equations

163

The coeﬃcients an , bn will be prescribed by the boundary conditions (4.106). Substituting
r = 1, we obtain
∞


a0
u(1, θ) =
+
an cos n θ + bn sin n θ = h(θ).
2
n=1

We recognize this as a standard Fourier series (3.29) (with θ replacing x) for the 2 π periodic
function h(θ). Therefore,
an =

1
π

 π
h(θ) cos n θ dθ,
−π

bn =

1
π

 π
h(θ) sin n θ dθ,

(4.116)

−π

are precisely its Fourier coeﬃcients, cf. (3.35). In this manner, we have produced a series
solution (4.115) to the boundary value problem (4.105–106).
Remark : Introducing the complex variable
z = x + i y = r e i θ = r cos θ + i r sin θ

(4.117)

z n = rn e i n θ = rn cos n θ + i rn sin n θ.

(4.118)

allows us to write

Therefore, the nonsingular separable solutions are the harmonic polynomials
rn cos n θ = Re z n ,

rn sin n θ = Im z n .

(4.119)

The ﬁrst few are listed in the following table:
n

Re z n

Im z n

0
1
2
3
4

1
x
2
x − y2
3
x − 3 x y2
x4 − 6 x2 y 2 + y 4

0
y
2xy
3 x2 y − y 3
4 x3 y − 4 x y 3

Their general expression is obtained using the Binomial Formula:
z n = (x + i y)n

 
 
n n−2
n n−3
2
= x + nx
( i y) +
x
( i y) +
x
( i y)3 + · · · + ( i y)n
2
3
 
 
n n−2 2
n n−3 3
n
n−1
= x + i nx
y−
x
y −i
x
y + ··· ,
2
3
n

where

n−1

 
n
n!
=
k ! (n − k) !
k

(4.120)

164

4 Separation of Variables

Figure 4.10.

Membrane attached to a helical wire.

are the usual binomial coeﬃcients. Separating the real and imaginary terms, we produce
the explicit formulae
 
 
n n−2 2
n n−4 4
rn cos n θ = Re z n = xn −
x
y +
x
y + ··· ,
2
4
 
 
(4.121)
n n−3 3
n n−5 5
n
n
n−1
r sin n θ = Im z = n x
y−
x
y +
x
y + ··· ,
3
5
for the two independent harmonic polynomials of degree n.
Example 4.5. Consider the Dirichlet boundary value problem on the unit disk with
u(1, θ) = θ

for

− π < θ < π.

(4.122)

The boundary data can be interpreted as a wire in the shape of a single turn of a spiral
helix sitting over the unit circle. The wire has a single jump discontinuity, of magnitude
2 π, at the boundary point (−1, 0). The required Fourier series


sin 2 θ sin 3 θ sin 4 θ
h(θ) = θ ∼ 2 sin θ −
+
−
+···
2
3
4
was already computed in Example 3.3. Therefore, invoking our solution formula (4.115–
116), we have


r2 sin 2 θ r3 sin 3 θ r4 sin 4 θ
u(r, θ) = 2 r sin θ −
+
−
+···
(4.123)
2
3
4
is the desired solution, which is plotted in Figure 4.10. In fact, this series can be explicitly
summed. In view of (4.119) and the usual formula (A.13) for the complex logarithm, we
have


z2
z3
z4
u = 2 Im z −
+
−
+ · · · = 2 Im log(1 + z) = 2 ψ,
(4.124)
2
3
4

4.3 The Planar Laplace and Poisson Equations

165

(x, y)
ψ

Figure 4.11.
where

Geometric construction of the solution.

y
1+x
is the angle that the line passing through the two points (x, y) and (−1, 0) makes with the
x-axis, as sketched in Figure 4.11. You should try to convince yourself that, on the unit
circle, 2 ψ = θ has the correct boundary values. Observe that, even though the boundary
values are discontinuous, the solution is an analytic function inside the disk.
ψ = tan−1

In fact, unlike the rectangular series (4.98), the general polar series solution formula (4.115) can, in fact, be summed in closed form! If we substitute the explicit Fourier
formulae (4.116) into (4.115) — remembering to change the integration variable to, say, φ
to avoid a notational conﬂict — we obtain
∞

a0
an rn cos n θ + bn rn sin n θ
+
2
n=1

 π


∞  n

rn sin n θ π
r cos n θ π
1
h(φ) dφ +
h(φ) cos n φ dφ +
h(φ) sin n φ dφ
=
2 π −π
π
π
−π
−π
n=1
%
$

∞

1 π
1
n
dφ
=
h(φ)
r cos n θ cos n φ + sin n θ sin n φ
+
π −π
2
n=1
$
%
(4.125)

∞

1
1 π
n
h(φ)
r cos n (θ − φ) dφ.
=
+
π −π
2
n=1

u(r, θ) =

We next show how to sum the ﬁnal series. Using (4.118), we can write it as the real part
of a geometric series:






∞
∞


1
1
z
1+z
1
n
n
= Re
+
+
+
= Re
r cos n θ = Re
z
2
2 n=1
2 1−z
2 (1 − z)
n=1


1 − | z |2
1 − r2
Re (1 + z − z − | z |2 )
(1 + z)(1 − z)
=
=
=
= Re
,
2 | 1 − z |2
2 | 1 − z |2
2 | 1 − z |2
2 (1 + r2 − 2 r cos θ)
which is known as the Poisson kernel . Substituting back into (4.125) establishes the
important Poisson Integral Formula for the solution to the boundary value problem.

166

4 Separation of Variables

Figure 4.12.

Equilibrium temperature of a disk.

Theorem 4.6. The solution to the Laplace equation in the unit disk subject to
Dirichlet boundary conditions u(1, θ) = h(θ) is
 π
1 − r2
1
dφ.
(4.126)
h(φ)
u(r, θ) =
2 π −π
1 + r2 − 2 r cos(θ − φ)
Example 4.7. A uniform metal disk of unit radius has half of its circular boundary
held at 1◦ , while the other half is held at 0◦ . Our task is to ﬁnd the equilibrium temperature
u(x, y). In other words, we seek the solution to the Dirichlet boundary value problem

1,
x2 + y 2 = 1, y > 0,
u(x, y) =
Δu = 0,
x2 + y 2 < 1,
(4.127)
0,
x2 + y 2 = 1, y < 0.
In polar coordinates, the boundary data is a (periodic) step function

1,
0 < θ < π,
h(θ) =
0, − π < θ < 0.
Therefore, according to the Poisson formula (4.126), the solution is given by†


⎧
1 − r2
1
−1
⎪
⎪
,
0 < θ < π,
tan
1
−
⎪
π
⎪
2 r sin θ
⎪
⎪
⎪
 π
⎨
1 − r2
1
1
u(r, θ) =
dφ
=
,
θ = 0, ± π,
2
⎪
2 π 0 1 + r − 2 r cos(θ − φ)
2
⎪
⎪


⎪
⎪
2
⎪
⎪
⎩ − 1 tan−1 1 − r
,
− π < θ < 0,
π
2 r sin θ
(4.128)

†

The detailed derivation of the ﬁnal expressions is left to the reader as Exercise 4.3.40.

4.3 The Planar Laplace and Poisson Equations

167

where we use the principal branch − 12 π < tan−1 t < 12 π of the inverse tangent. Reverting to rectangular coordinates, we ﬁnd that the equilibrium temperature has the explicit
formula


⎧
1 − x2 − y 2
1
−1
⎪
⎪
1 − tan
,
x2 + y 2 < 1, y > 0,
⎪
π
⎪
2
y
⎪
⎪
⎨
1
u(x, y) =
(4.129)
,
x2 + y 2 < 1, y = 0,
⎪
2
⎪


⎪
⎪
2
2
⎪
⎪
⎩ − 1 tan−1 1 − x − y
,
x2 + y 2 < 1, y < 0.
π
2y
The result is depicted in Figure 4.12.

Averaging, the Maximum Principle, and Analyticity
Let us investigate some important consequences of the Poisson integral formula (4.126).
First, setting r = 0 yields
 π
1
h(φ) dφ.
(4.130)
u(0, θ) =
2 π −π
The left-hand side is the value of u at the origin — the center of the disk — and so
independent of θ; the right-hand side is the average of its boundary values around the unit
circle. This formula is a particular instance of an important general fact.
Theorem 4.8. Let u(x, y) be harmonic inside a disk of radius a centered at a point
(x0 , y0 ) with piecewise continuous (or, more generally, integrable) boundary values on the
circle C = { (x − x0 )2 + (y − y0 )2 = a2 }. Then its value at the center of the disk is equal
to the average of its values on the boundary circle:
&
 π
1
1
u(x0 , y0 ) =
u ds =
u(x0 + a cos θ, y0 + a sin θ) dθ.
(4.131)
2 πa C
2 π −π
Proof : One approach is to use the scaling and translation symmetries of the Laplace
equation, cf. Exercises 4.3.8–9, to map the disk of radius a centered at (x0 , y0 ) to the unit
disk centered at the origin, and then invoke (4.130). Speciﬁcally, we set
U (x, y) = u(x0 + a x, y0 + a y).

(4.132)

An easy chain rule computation proves that U (x, y) also satisﬁes the Laplace equation on
the unit disk x2 + y 2 < 1, with boundary values
h(θ) = U (cos θ, sin θ) = u(x0 + a cos θ, y0 + a sin θ).
Therefore, by (4.130),
1
U (0, 0) =
2π

 π

1
h(θ) dθ =
2π
−π

 π
U (cos θ, sin θ) dθ.
−π

Replacing U by its formula (4.132) produces the desired result for solutions deﬁned by
the Poisson integral formula. However, it is not a priori clear that all solutions, i.e.,
all harmonic functions, are necessarily of this form. This will follow eventually from the
Uniqueness Theorem 4.10; however, its proof relies on formula (4.131), leading to a circular
argument.

168

4 Separation of Variables

A better proof, which does not rely on the solution formula (4.115, 116), is the following. Given the harmonic function u(x, y), consider the scalar function
 π
1
g(a) =
u(x0 + a cos θ, y0 + a sin θ) dθ,
2 π −π
which is well deﬁned for a > 0 suﬃciently small. Since u ∈ C2 , we can calculate the
derivative of g as follows:

 π 
1
∂u
∂u

(x +a cos θ, y0 +a sin θ) + sin θ
(x +a cos θ, y0 +a sin θ) dθ
g (a) =
cos θ
2 π −π
∂x 0
∂y 0
∂u
1
ds,
=
2 πa C ∂n
where n = (cos θ, sin θ) deﬁnes the unit normal to C at the point (x0 + a cos θ, y0 + a sin θ)
and ds = a dθ is the arc length element. Letting D = { (x − x0 )2 + (y − y0 )2 ≤ a2 } denote
the disk of radius a, so C = ∂D is its boundary, the divergence identity (6.89), which is an
easy consequence of Green’s Theorem 6.13, implies that the latter integral equals

∂u
ds =
Δu dx dy = 0
C ∂n
D
because u is harmonic. Thus, g  (a) = 0 for all a > 0 suﬃciently small, which implies
g(a) = c is constant. But g(a) represents the average of u(x, y) on the circle C of radius a
and hence, as a → 0, the average g(a) → u(x0 , y0 ). We conclude that g(a) = u(x0 , y0 ) for
all a > 0 such that u(x, y) is harmonic in the disk of radius a, which establishes (4.131) for
all such harmonic functions.
Q.E.D.
An important consequence of the integral formula (4.131) is the Strong Maximum
Principle for harmonic functions.
Theorem 4.9. Let u be a nonconstant harmonic function deﬁned on a bounded
domain Ω and continuous on ∂Ω. Then u achieves its maximum and minimum values only
at boundary points of the domain. In other words, if
m = min { u(x, y) | (x, y) ∈ ∂Ω } ,

M = max { u(x, y) | (x, y) ∈ ∂Ω },

are, respectively, its maximum and minimum values on the boundary, then
m < u(x, y) < M

at all interior points

(x, y) ∈ Ω.

Proof : Let M  ≥ M be the maximum value of u on all of Ω = Ω ∪ ∂Ω, and assume
u(x0 , y0 ) = M  at some interior point (x0 , y0 ) ∈ Ω. Theorem 4.8 implies that u(x0 , y0 )
equals its average over any circle C centered at (x0 , y0 ) that bounds a closed disk contained
in Ω. Since u is continuous and ≤ M  on C, its average must be strictly less than M 
— except in the trivial case in which it is constant and equal to M  on all of C. Thus,
our assumption implies that u(x, y) = M  = u(x0 , y0 ) for all (x, y) belonging to any
circle C ⊂ Ω centered at (x0 , y0 ). Since Ω is connected, this allows us to conclude† that
u(x, y) = M  is constant throughout Ω, in contradiction to our original assumption.
A similar argument works for the minimum; alternatively, one can interchange maximum and minimum by replacing u by − u.
Q.E.D.
†

You are asked to supply the details in Exercise 4.3.42.

4.3 The Planar Laplace and Poisson Equations

169

Physically, if we interpret u(x, y) as the vertical displacement of a membrane stretched
over a wire, then Theorem 4.9 says that, in the absence of external forcing, the membrane
cannot have any internal bumps — its highest and lowest points are necessarily on the
boundary of the domain. This reconﬁrms our physical intuition: the restoring force exerted
by the stretched membrane will serve to ﬂatten any bump, and hence a membrane with a
local maximum or minimum cannot be in equilibrium. A similar interpretation holds for
heat conduction. A body in thermal equilibrium will achieve its maximum and minimum
temperature only at boundary points. Indeed, thermal energy would ﬂow away from any
internal maximum, or towards any local minimum, and so if the body contained a local
maximum or minimum in its interior, it could not remain in thermal equilibrium.
The Maximum Principle immediately implies the uniqueness of solutions to the Dirichlet boundary value problem for both the Laplace and Poisson equations:
Theorem 4.10. If u and u
 both satisfy the same Poisson equation − Δu = f = − Δ
u
within a bounded domain Ω, and u = u
 on ∂Ω, then u ≡ u
 throughout Ω.
Proof : By linearity, the diﬀerence v = u − u
 satisﬁes the homogeneous boundary value
problem Δv = 0 in Ω and v = 0 on ∂Ω. Our assumption implies that the maximum and
minimum boundary values of v are both 0 = m = M . Theorem 4.9 implies that v(x, y) ≡ 0
at all (x, y) ∈ Ω, and hence u ≡ u
 everywhere in Ω.
Q.E.D.
Finally, let us discuss the analyticity of harmonic functions. In view of (4.119), the
nth order term in the polar series solution (4.115), namely,


an rn cos n θ + bn rn sin n θ = an Re z n + bn Im z n = Re (an − i bn ) z n ,
is, in fact, a homogeneous polynomial in (x, y) of degree n. This means that, when written
in rectangular coordinates x and y, (4.115) is, in fact, a power series for the harmonic
function u(x, y). It is well known, [8, 23, 97], that any convergent power series converges
to an analytic function — in this case u(x, y). Moreover, the power series must, in fact, be
the Taylor series for u(x, y) based at the origin, and so its coeﬃcients are multiples of the
derivatives of u at x = y = 0. Details are worked out in Exercise 4.3.49.
We can adapt this argument to prove analyticity of all solutions to the Laplace equation. Note especially the contrast with the wave equation, which has many non-analytic
solutions.
Theorem 4.11. A harmonic function is analytic at every point in the interior of its
domain of deﬁnition.
Proof : Let u(x, y) be a solution to the Laplace equation on the open domain Ω ⊂ R 2 .
Let x0 = (x0 , y0 ) ∈ Ω, and choose a > 0 such that the closed disk of radius a centered at
x0 is entirely contained within Ω:
Da (x0 ) = {  x − x0  ≤ a } ⊂ Ω,
where  ·  is the usual Euclidean norm. Then the function U (x, y) deﬁned by (4.132) is
harmonic on the unit disk, with well-deﬁned boundary values. Thus, by the preceding
remarks, U (x, y) is analytic at every point inside the unit disk, and hence so is


x − x0 y − y0
,
u(x, y) = U
a
a
at every point (x, y) in the interior of the disk Da (x0 ). Since x0 ∈ Ω was arbitrary, this
establishes the analyticity of u throughout the domain.
Q.E.D.

170

4 Separation of Variables

This concludes our discussion of the method of separation of variables for the planar
Laplace equation and some of its important consequences. The method can be used in a
few other special coordinate systems. See [78, 79] for a complete account, including the
fascinating connections with the underlying symmetry properties of the equation.

Exercises
4.3.22. Solve the following Euler diﬀerential equations by use of the power ansatz:
(a) x2 u + 5 x u − 5 u = 0,

(b) 2 x2 u − x u − 2 u = 0,

(d) x2 u + x u − 3 u = 0,

(e) 3 x2 u − 5 x u − 3 u = 0,

(c) x2 u − u = 0,
d2 u
2 du
= 0.
(f )
+
dx2
x dx

♦ 4.3.23. (i ) Show that if u(x) solves the Euler diﬀerential equation
d2 u
du
+ bx
+ c u = 0,
dx2
dx
then v(y) = u(ey ) solves a linear constant-coeﬃcient diﬀerential equation.
(ii ) Use this technique to solve the Euler diﬀerential equations in Exercise 4.3.22.
a x2

(4.133)

4.3.24. (a) Use the method in Exercise 4.3.23 to solve an Euler equation whose characteristic
equation has a double root r1 = r2 = r. (b) Solve the speciﬁc equations
d2 u
1 du
(ii )
+
(i ) x2 u − x u + u = 0,
= 0.
dx2
x dx
4.3.25. Solve the following boundary value problems:
(a) Δu = 0, x2 + y 2 < 1, u = x3 , x2 + y 2 = 1;
(b) Δu = 0, x2 + y 2 < 2, u = log(x2 + y 2 ), x2 + y 2 = 1;
(c) Δu = 0, x2 + y 2 < 4, u = x4 , x2 + y 2 = 4;
∂u
(d) Δu = 0, x2 + y 2 < 1,
= x, x2 + y 2 = 1.
∂n
4.3.26. Let u(x, y) be the solution to the boundary value problem uxx + uyy = 0, x2 + y 2 < 1,
u(x, y) = x2 , x2 + y 2 = 1. Find u(0, 0).
♥ 4.3.27. (a) Find the equilibrium temperature on a disk of radius 1 when half the boundary is
held at 1◦ and the other half is held at −1◦ . (b) Find the equilibrium temperature on a
half-disk of radius 1 when the temperature is held to 1◦ on the curved edge and 0◦ on the
straight edge. (c) Find the equilibrium temperature on a half disk of radius 1 when the
temperature is held to 0◦ on the curved edge and 1◦ on the straight edge.
4.3.28. Find the solution to Laplace’s equation uxx + uyy = 0 on the semi-disk x2 + y 2 < 1,
y > 0, that satisﬁes the boundary conditions u(x, 0) = 0 for −1 < x < 1 and u(x, y) = y 3
for x2 + y 2 = 1, y > 0.
4.3.29. Find the equilibrium temperature on a half-disk of radius 1 when the temperature is
held to 1◦ on the curved edge, while the straight edge is insulated.
4.3.30. Solve the Dirichlet boundary value problem for the Laplace equation on the pie wedge
W = { 0 < θ < 14 π, 0 < r < 1 }, when the nonzero boundary data u(1, θ) = h(θ) appears
only on the curved portion of its boundary.
4.3.31. Find a harmonic function u(x, y) deﬁned on the annulus 12 < r < 1 subject to the
constant Dirichlet boundary conditions u = a on r = 12 and u = b on r = 1.
4.3.32. Boiling water ﬂows continually through a long circular metal pipe of inner radius 1 cm
and outer radius 1.2 cm placed in an ice water bath. True or false: The temperature at the
midpoint, at radius 1.1 cm, is 50◦ . If false, what is the temperature at this point?

4.3 The Planar Laplace and Poisson Equations

171

4.3.33. Write out the series solution to the boundary value problem u(1, θ) = 0, u(2, θ) = h(θ),
for the Laplace equation on an annulus 1 < r < 2. Hint: Use all of the separable solutions
listed in (4.114).
4.3.34. Solve the following boundary value problems for the Laplace equation on the annulus
1 < r < 2:
(a) u(1, θ) = 0, u(2, θ) = 1, (b) u(1, θ) = 0, u(2, θ) = cos θ,
(c) u(1, θ) = sin 2 θ, u(2, θ) = cos 2 θ, (d) ur (1, θ) = 0, u(2, θ) = 1,
(e) ur (1, θ) = 0, u(2, θ) = sin 2 θ, (f ) ur (1, θ) = 0, ur (2, θ) = 1,
(g) ur (1, θ) = 2, ur (2, θ) = 1.
4.3.35. Solve the following boundary value problems for the Laplace equation on the semiannular domain D = { 1 < x2 + y 2 < 2, y > 0 }:
(a) u(x, y) = 0, x2 + y 2 = 1, u(x, y) = 1, x2 + y 2 = 2, u(x, 0) = 0;
(b) u(x, y) = 0, x2 + y 2 = 1 or 2, u(x, 0) = 0, x > 0, u(x, 0) = 1, x < 0.
4.3.36. Solve the following boundary value problem:
(x2 + y 2 )(uxx + uyy ) + 2 x ux + 2 y uy = 0, x2 + y 2 < 1, u(x, y) = 1 + 3 x, x2 + y 2 = 1.
♦ 4.3.37. Justify the chain rule computation (4.104). Then justify formula (4.105) for the Laplacian in polar coordinates.
4.3.38. Suppose

π
−π

| h(θ) | dθ < ∞. Prove that (4.115) converges uniformly to the solution to

the boundary value problem (4.101) on any smaller disk Dr = { r ≤ r < 1 }  D1 .
4.3.39. Prove directly that (4.124) satisﬁes the boundary conditions (4.122).
♦ 4.3.40. Justify the integration formula in (4.128).
4.3.41. Provide a complete proof that (4.129) is indeed the solution to the boundary value
problem (4.127).
♦ 4.3.42. Complete the proof of Theorem 4.9 by showing that u(x, y) = M  for all (x, y) ∈ Ω.
Hint: Join (x0 , y0 ) to (x, y) by a curve C ⊂ Ω of ﬁnite length, and use the preceding part
of the proof to inductively deduce the existence of a ﬁnite sequence of points (xi , yi ) ∈ C,
i = 0, . . . , n, with (xn , yn ) = (x, y), and such that u(xi , yi ) = M  .
♦ 4.3.43. Derive the analogue of the Poisson integral formula for the solution to the Neumann
boundary value problem Δu = 0, x2 + y 2 < 1, ∂u/∂n = h, x2 + y 2 = 1, on the unit disk.
Pay careful attention to the existence and uniqueness of solutions in your formulation.
4.3.44. Give an example of a solution to Poisson’s equation on the unit disk that achieves its
maximum at an interior point. Interpret your construction physically.
4.3.45. Let p(x, y) be a polynomial (not necessarily harmonic). Suppose u(x, y) is harmonic
and equals p(x, y) on the unit circle x2 + y 2 = 1. Prove that u(x, y) is a harmonic polynomial.
4.3.46. Write down an integral formula for the solution to the Dirichlet boundary value problem on a disk of radius R > 0, namely, Δu = 0, x2 + y 2 < R2 , u = h, x2 + y 2 = R2 .
4.3.47. State and prove a one-dimensional version of Theorem 4.8. Does the analogue of
Theorem 4.9 hold?
4.3.48. A unit area square plate has 100◦ temperature on its top edge and 0◦ on its three other
edges. True or false: The temperature at the center equals the average edge temperature.
♦ 4.3.49. Let u(x, y) be a harmonic function on the unit disk with boundary values h(θ) when
r = 1. Using the fact that (4.115) is the Taylor series for u(x, y) at the origin: (a) Find
integral formulas for its partial derivatives ux (0, 0), uy (0, 0), involving the boundary values
h(θ). (b) Generalize part (a) to the second-order derivatives uxx (0, 0), uxy (0, 0), uyy (0, 0).

172

4 Separation of Variables

4.3.50. Prove that if u(x, y) is a bounded harmonic function deﬁned on all of R 2 , then u is constant. Hint: First generalize Exercise 4.3.49(a) to ﬁnd the value of its gradient, ∇u(x0 , y0 ),
in terms of the values of u on a circle of radius a centered at (x0 , y0 ). Then see what happens when the radius of the circle goes to ∞.

4.4 Classiﬁcation of Linear Partial Diﬀerential Equations
We have, at last, been introduced to the three paradigmatic linear second-order partial
diﬀerential equations for functions of two variables. The homogeneous versions are
(a) The wave equation:

utt − c2 uxx = 0,

hyperbolic,

(b) The heat equation:
(c) Laplace’s equation:

ut − γ uxx = 0,
uxx + uyy = 0,

parabolic,
elliptic.

The last column indicates the equation’s type, in accordance with the standard taxonomy
of partial diﬀerential equations; an explanation will appear momentarily. The wave, heat,
and Laplace equations are the prototypical representatives of these three fundamental genres. Each genre has its own distinctive analytic features, physical manifestations, and even
numerical solution schemes. Equations governing vibrations, such as the wave equation,
are typically hyperbolic. Equations modeling diﬀusion, such as the heat equation, are
parabolic. Hyperbolic and parabolic equations both typically represent dynamical processes, and so one of the independent variables is identiﬁed as time. On the other hand,
equations modeling equilibrium phenomena, including the Laplace and Poisson equations,
are usually elliptic, and involve only spatial variables. Elliptic partial diﬀerential equations
are associated with boundary value problems, whereas parabolic and hyperbolic equations
require initial and initial-boundary value problems.
The classiﬁcation theory of real linear second-order partial diﬀerential equations for a
scalar-valued function u(t, x) depending on two variables† proceeds as follows. The most
general such equation has the form
L[ u ] = A utt + B utx + C uxx + D ut + E ux + F u = G,

(4.134)

where the coeﬃcients A, B, C, D, E, F are all allowed to be functions of (t, x), as is the
inhomogeneity or forcing function G(t, x). The equation is homogeneous if and only if
G ≡ 0. We assume that at least one of the leading coeﬃcients A, B, C is not identically
zero, since otherwise, the equation degenerates to a ﬁrst-order equation.
The key quantity that determines the type of such a partial diﬀerential equation is its
discriminant
Δ = B 2 − 4 A C.
(4.135)
This should (and for good reason) remind the reader of the discriminant of the quadratic
equation
Q(x, y) = A x2 + B x y + C y 2 + D x + E y + F = 0,
(4.136)
whose solutions trace out a plane curve — a conic section. In the nondegenerate cases, the
discriminant (4.135) ﬁxes its geometric type:
†

For equilibrium equations, we identify t with the space variable y.

4.4 Classiﬁcation of Linear Partial Diﬀerential Equations

173

• a hyperbola when Δ > 0,
• a parabola when Δ = 0,
• an ellipse when Δ < 0.
This motivates the choice of terminology used to classify second-order partial diﬀerential
equations.
Deﬁnition 4.12. At a point (t, x), the linear second-order partial diﬀerential equation (4.134) is called
• hyperbolic if Δ(t, x) > 0,
• parabolic if Δ(t, x) = 0, but A2 + B 2 + C 2 = 0,
• elliptic
if Δ(t, x) < 0,
• singular
if A = B = C = 0.
In particular:
• The wave equation utt − uxx = 0 has discriminant Δ = 4, and is hyperbolic.
• The heat equation uxx − ut = 0 has discriminant Δ = 0, and is parabolic.
• The Poisson equation utt + uxx = − f has discriminant Δ = −4, and is elliptic.
Example 4.13. When the coeﬃcients A, B, C vary, the type of the partial diﬀerential
equation may not remain ﬁxed over the entire domain. Equations that change type are
less common, as well as being much harder to analyze and solve, both analytically and
numerically. One example arising in the theory of supersonic aerodynamics, [44], is the
Tricomi equation
∂2u ∂2u
(4.137)
x 2 − 2 = 0.
∂t
∂x
Comparing with (4.134), we ﬁnd that
A = x, B = 0, C = −1,

while

D = E = F = G = 0.

The discriminant in this particular case is
Δ = B 2 − 4 A C = 4 x,
and hence the equation is hyperbolic when x > 0, elliptic when x < 0, and parabolic on the
transition line x = 0. In the physical model, the hyperbolic region corresponds to subsonic
ﬂow, while the supersonic regions are of elliptic type. The transitional parabolic boundary
represents the shock line between the sub- and super-sonic regions — the familiar sonic
boom as an airplane crosses the sound barrier.
While this tripartite classiﬁcation into hyperbolic, parabolic, and elliptic equations
initially appears in the bivariate context, the terminology, underlying properties, and associated physical models carry over to second-order partial diﬀerential equations in higher
dimensions. Most of the partial diﬀerential equations arising in applications fall into one
of these three categories, and it is fair to say that the ﬁeld of partial diﬀerential equations
splits into three distinct subﬁelds. Or rather four subﬁelds, the last containing all the equations, including higher-order equations, that do not ﬁt into the preceding categorization.
(One important example appears in Section 8.5.)

174

4 Separation of Variables

Remark : The classiﬁcation into hyperbolic, parabolic, elliptic, and singular types carries over as stated to quasilinear second-order equations, whose coeﬃcients A, . . . , G are
allowed to depend on u and its ﬁrst-order derivatives, ut , ux . Here the type of the equation
can vary with both the point in the domain and the particular solution being considered.
Even more generally, for a fully nonlinear second-order partial diﬀerential equation
H(t, x, u, ut , ux , utt , utx , uxx ) = 0,

(4.138)

one deﬁnes its discriminant to be

Δ=

∂H
∂utx

2

−4

∂H ∂H
.
∂utt ∂uxx

(4.139)

Its sign determines the type of the equation as above — again depending on the point in
the domain and the solution under consideration.

Exercises
4.4.1. Plot the following conic sections and classify their type:
(a) x2 + 3 y 2 = 1, (b) x y + x + y = 4, (c) x2 − x y + y 2 = x − 2 y,
(d) x2 + 2 x y + y 2 + y = 1, (e) x2 − 2 y 2 = 6 x + 8 y + 1.
4.4.2. Determine the type of the following partial diﬀerential equations:
(a) utt + 3 uxx = 0, (b) utx + ut + ux = u, (c) utt + ut + ux = 0,
(d) utt − utx + uxx = u, (e) utt + 4 utx + 4 uxx = ut , (f ) utx + uxx = 0.
4.4.3. Consider the partial diﬀerential equation x utt + (t + x) uxx = 0. At what points of the
plane is the equation elliptic? hyperbolic? parabolic? degenerate?
4.4.4. Answer Exercise 4.4.3 for the equations
(a) x2 uxx + x ux + uyy = 0, (b) ∂x (x ux ) = ∂y (y uy ), (c) ut = ∂x [ (x + t)ux ],
(d) ∇ · (c(x, y)∇u) = u, where c(x, y) is a given function.
4.4.5. Steady ﬂow of air past an airplane is modeled by the partial diﬀerential equation
(m2 − 1)uxx + uyy = 0, in which x is the ﬂight direction, y the transverse direction, and
m ≥ 0 is the Mach number — the ratio of the airplane’s speed to the speed of sound. Show
that the equation is hyperbolic for subsonic ﬂight, but elliptic for supersonic ﬂight.
4.4.6. Show that the second-order partial diﬀerential equation




∂u
∂
∂u
∂
p(x, y)
−
q(x, y)
+ r(x, y) u = f (x, y)
−
∂x
∂x
∂y
∂y
is elliptic if and only if p(x, y) and q(x, y) are nonzero and have the same sign.
♦ 4.4.7. True or false: The type of a linear second-order partial diﬀerential equation is not affected by a change of independent variables: τ = ϕ(t, x), ξ = ψ(t, x).
4.4.8. Let v(t, x) = a(t, x) u(t, x) + b(t, x), where a, b are ﬁxed functions with a = 0. Suppose u
is a solution to a second-order linear partial diﬀerential equation. Prove that v also solves a
linear partial diﬀerential equation of the same type.
♦ 4.4.9. True or false: The polar coordinate form (4.105) of the Laplace equation is elliptic.
4.4.10. Rewrite the Laplace equation uxx + uyy = 0 in terms of parabolic coordinates ξ, η, as
deﬁned by the equations x = ξ 2 − η 2 , y = 2 ξ η. Is the resulting equation elliptic?

4.4 Classiﬁcation of Linear Partial Diﬀerential Equations

175

♦ 4.4.11. Prove that the complex change of variables x = x, t = i y, maps the Laplace equation
uxx + uyy = 0 to the wave equation utt = uxx . Explain why the type of a partial diﬀerential
equation is not necessarily preserved under a complex change of variables.
♥ 4.4.12. Suppose, against all advice, we pose the elliptic Laplace equation as an initial value
problem, namely
0 < x < 1,
t > 0,
for
utt = − uxx
u(0, x) = f (x),
ut (0, x) = 0,
0 ≤ x ≤ 1,
u(t, 0) = 0 = u(t, 1),
t ≥ 0.
sin n π t cosh n π x
(a) Prove that for any positive integer n > 0, the function un (t, x) =
n
satisﬁes the initial value problem. Determine the initial condition un (0, x) = fn (x).
(b) Prove that, as n → ∞, the initial condition fn (x)
→ 0 becomes vanishingly small,

whereas, at any t > 0, the solution value un t, 12 → ∞.
(c) Explain why this represents an ill-posed problem.
4.4.13. The minimal surface equation (1+u2x )uxx −2 ux uy uxy +(1+u2y )uyy = 0 is (a) hyperbolic,
(b) parabolic, (c) elliptic, (d) singular, (e) of variable type depending on the point in the
domain, or (f ) of variable type depending on the solution and the point in the domain.

Characteristics and the Cauchy Problem
In Chapter 2, we discovered that the characteristic curves guide the behavior of solutions
to ﬁrst-order partial diﬀerential equations. Characteristics play a similarly fundamental
role in the analysis of more general hyperbolic partial diﬀerential equations and systems.
In particular, they provide a mechanism for distinguishing among the various classes of
second-order partial diﬀerential equations.
As above, we will focus our attention on partial diﬀerential equations involving two
independent variables. The starting point is the general initial value problem, also known
as the Cauchy problem, in honor of the proliﬁc nineteenth-century French mathematician Augustin–Louis Cauchy, justly famous for his wide-ranging contributions throughout
mathematics and its applications, including the Cauchy–Schwarz inequality, many of the
fundamental concepts in complex analysis, as well as the foundations of elasticity and
materials science. The general Cauchy problem speciﬁes appropriate initial data along a
smooth curve† Γ ⊂ R 2 and seeks a solution to the partial diﬀerential equation that assumes the given initial data on Γ. In all our examples, the curve in question has been a
straight line, e.g., the x–axis, but one could easily envisage more general situations. If the
partial diﬀerential equation has order n, then the Cauchy data consists of the values of the
dependent variable u along with all its partial diﬀerential equations up to order n − 1 on
the curve Γ. For most curves, there is a unique solution u(t, x) to the partial diﬀerential
equation that achieves the speciﬁed values along Γ. More rigorously, if we are in the analytic category, meaning that the partial diﬀerential equation, the curve, and the Cauchy
data are all speciﬁed by analytic functions, then the fundamental Cauchy–Kovalevskaya
Theorem guarantees the existence of an analytic solution u(t, x) to the Cauchy problem
near any point on the initial curve. The statement of proof of this important theorem, due
to Cauchy and, in general form, the inﬂuential nineteenth-century Russian mathematician
Soﬁa Kovalevskaya, relies on the construction of convergent power series for the desired
†
More generally, for partial diﬀerential equations in m > 2 independent variables, the curve
is replaced by a hypersurface S ⊂ R m of dimension m − 1.

176

4 Separation of Variables

solution and would take us too far aﬁeld. We refer the interested reader to [35, 44]. The
exceptional curves, for which the Cauchy–Kovalevskaya Existence Theorem does not apply,
are called the characteristics of the underlying partial diﬀerential equations.
More prosaically, a curve Γ will be called non-characteristic for the given partial
diﬀerential equation if one can determine the values of all the derivatives of u along Γ
from the speciﬁed Cauchy data. Indeed, the determination of the values of the higherorder derivatives along the curve is a necessary preliminary step towards establishing the
Cauchy–Kovalevskaya existence result. As we will now show, this requirement serves to distinguish the characteristic and non-characteristic curves for the examples we have already
encountered, and hence to lead to their characterization in much more general contexts.
To illustrate the preceding requirement, let us begin with a ﬁrst-order linear partial
diﬀerential equation of the form
∂u
∂u
+ c(t, x)
= f (t, x).
(4.140)
∂t
∂x
Let Γ ⊂ R 2 be a smooth curve parametrized‡ by x(s) = t(s), x(s) T , where smoothness
necessitates that its tangent vector not vanish: x (s) = ( dt/ds, dx/ds )T = 0. Since the
equation is of order n = 1, the Cauchy data requires specifying the values of the dependent
variable u only along Γ — in other words, the function
h(s) = u t(s), x(s) .

(4.141)

The curve will be non-characteristic if we can then determine the values of the derivatives
of u along Γ, starting with
∂u
∂u
(4.142)
t(s), x(s) ,
t(s), x(s) .
∂t
∂x
To this end, let us diﬀerentiate the Cauchy data (4.141): applying the chain rule, we obtain
∂u
dt
∂u
dx
d
u t(s), x(s) =
t(s), x(s)
+
t(s), x(s)
.
(4.143)
ds
∂t
ds ∂x
ds
On the other hand, we are assuming that u(t, x) solves the partial diﬀerential equation
(4.140) at all points in its domain of deﬁnition. In particular, at points on the curve Γ, the
partial diﬀerential equation requires
h (s) =

∂u
∂u
t(s), x(s) + c t(s), x(s)
t(s), x(s) = f t(s), x(s) .
(4.144)
∂t
∂x
We can regard (4.143–144) as a pair of inhomogeneous linear algebraic equations, which
can be uniquely solved for the as yet unknown quantities (4.142), unless the determinant
of their coeﬃcient matrix vanishes:


1
c t(s), x(s)
dx
dt
det
=
− c t(s), x(s)
= 0.
(4.145)
ds
ds
dt/ds
dx/ds
This condition serves to deﬁne a characteristic curve for the ﬁrst-order partial diﬀerential
equation (4.140). In particular, if the curve is parametrized by s = t, i.e., can be identiﬁed
with the graph of a function x = g(t), then the characteristic condition (4.145) reduces to
dx
= c(t, x),
dt
‡

(4.146)

The parameter s could be the arc length, but this is not required. See also Exercise 4.4.20.

4.4 Classiﬁcation of Linear Partial Diﬀerential Equations

177

thus reproducing our original deﬁnition of characteristic curve, as in (2.18) and, more
generally, Exercise 2.2.26. On the other hand, if the determinant (4.145) is nonzero, then
one can solve (4.143–144) for the values of the ﬁrst-order derivatives (4.142) along Γ.
Further diﬀerentiation of these conditions proves that one can, in fact, determine the
values of all the higher-order derivatives of the solution u along the curve, which is hence
non-characteristic.
Next, consider a nonsingular linear second-order partial diﬀerential equation of the
form (4.134). Since the equation has order n = 2, the Cauchy data along a curve Γ
parametrized as above consists of the values of the function and its ﬁrst derivatives:
u t(s), x(s) ,

∂u
t(s), x(s) ,
∂t

∂u
t(s), x(s) .
∂x

(4.147)

However, the latter cannot be speciﬁed independently. Indeed, given the value of the
dependent variable, h(s) = u t(s), x(s) , along Γ, its derivative
h (s) =

∂u
dt
∂u
dx
d
u t(s), x(s) =
t(s), x(s)
+
t(s), x(s)
ds
∂t
ds ∂x
ds

(4.148)

prescribes a particular combination of the two ﬁrst-order derivatives. Thus, once the
value of one derivative of u on Γ is known, the other is automatically ﬁxed by the relation
(4.148). For example, if dx/ds = 0, we can use (4.148) to determine ux t(s), x(s) , knowing
u t(s), x(s) and ut t(s), x(s) . Similarly, if we diﬀerentiate the values of the ﬁrst-order
derivatives with respect to the curve parameter, we can determine two combinations of
second-order derivatives along the curve Γ:
∂2u
∂2u
dt
d ∂u
t(s), x(s) = 2 t(s), x(s)
+
t(s), x(s)
ds ∂t
∂t
ds ∂t ∂x
d ∂u
∂2u
dt
∂2u
t(s), x(s) =
t(s), x(s)
+
t(s), x(s)
ds ∂x
∂t ∂x
ds ∂x2

dx
,
ds
dx
.
ds

(4.149)

On the other hand, the partial diﬀerential equation (4.134) induces yet a third relation
among the second-order partial derivatives utt , utx , uxx . These three linear equations can
be uniquely solved for values of these derivatives on Γ if and only if the determinant of
their coeﬃcient matrix is nonzero:
⎛
⎞
A(t, x) B(t, x) C(t, x)
 2
 2
dx
dt
dt dx
det ⎝ dt/ds dx/ds
0 ⎠ = A(t, x)
+ C(t, x)
− B(t, x)
= 0.
ds
ds ds
ds
0
dt/ds dx/ds
(4.150)
We conclude that a smooth curve x(s) = t(s), x(s) T ⊂ R 2 is a characteristic curve
for the nonsingular linear second-order partial diﬀerential equation (4.134) whenever its
tangent vector x (s) = ( dt/ds, dx/ds )T = 0 satisﬁes the quadratic characteristic equation
(4.150). Conversely, if the curve is non-characteristic, meaning that its tangent does not
satisfy (4.150) anywhere, then one can, with some further work, determine all the higherorder derivatives of the solution u(t, x) along Γ, and then, at least in the analytic category,
prove existence of a solution to the Cauchy problem, [35].
According to Exercise 4.4.20, the status of a curve as characteristic or not does not
depend on the choice of parametrization. In particular, if the curve is given by the graph
of the function x = x(t), which we parametrize by s = t, then the characteristic equation

178

4 Separation of Variables

(4.150) takes the form of a quadratically nonlinear ﬁrst-order ordinary diﬀerential equation
 2
dx
dx
+ C(t, x) = 0,
(4.151)
− B(t, x)
A(t, x)
dt
dt
whose solutions are characteristic curves of the second-order partial diﬀerential equation.
Warning: If A(t, x) = 0, then the partial diﬀerential equation admits characteristic
curves with vertical tangents that cannot be parametrized by s = t. For example, if
A(t, x) ≡ 0, then the vertical lines e.g., t = constant, x = s, are characteristic, satisfying
(4.150), but do not appear as solutions to (4.151).
For example, consider the hyperbolic wave equation
utt − c2 uxx = 0.
According to (4.151), any characteristic curve that is given by the graph of x(t) must solve
 2
dx
dx
= ± c.
− c2 = 0,
which implies that
dt
dt
Thus, in accordance with our previous analysis, the characteristic curves are the straight
lines of slope ± c, and there are two characteristic curves passing through each point of the
(t, x)–plane. On the other hand, the elliptic Laplace equation
utt + uxx = 0
has no (real) characteristic curves, since the characteristic equation (4.150) reduces to
 2  2
dt
dx
+
= 0,
ds
ds
and ts and xs are not allowed to vanish simultaneously. Finally, for the parabolic heat
equation
uxx − ut = 0,
the characteristic curve equation (4.150) is simply
 2
dt
=0
ds
(since the ﬁrst-derivative term plays no role), and so there is only one characteristic curve
passing through each point, namely the vertical line t = constant. Observe that the standard initial value problem u(0, x) = f (x) for the heat equation takes place on a characteristic curve — the x–axis — but does not take the form of a Cauchy problem, which would
also require specifying the ﬁrst-order derivatives ut (0, x), ux(0, x) there. And indeed, the
standard initial value problem is not well-posed near the characteristic x–axis for negative
t < 0.
In general, the number of real solutions to the nondegenerate quadratic characteristic
curve equation (4.150) depends on its discriminant Δ = B 2 − 4 A C: In the hyperbolic
case, Δ > 0, and there are two real characteristic curves passing through each point; in
the parabolic case, Δ = 0, and there is just one real characteristic curve passing through
each point; in the elliptic case, Δ < 0, and there are no real characteristic curves. In this
manner, elliptic, parabolic, and hyperbolic partial diﬀerential equations are distinguished

4.4 Classiﬁcation of Linear Partial Diﬀerential Equations

179

by the number of (real) characteristic curves passing through a point — namely, zero,
one, and two, respectively. First-order partial diﬀerential equations are also viewed as
hyperbolic, since they always admit real characteristic curves.
With further analysis, [35, 70, 122], it can be shown that, as with the wave equation,
signals and disturbances propagate along characteristic curves. Thus, hyperbolic equations share many qualitative properties with the wave equation, with signals moving in
two diﬀerent directions. For example, light rays move along characteristic curves, and are
thereby subject to the optical phenomena of refraction and focusing. Similarly, since the
characteristic curves for the parabolic heat equation are the vertical lines, this indicates
that the eﬀect of a disturbance at a point (t, x) = (t0 , x0 ) is simultaneously felt along the
entire contemporaneous vertical line t = t0 . This has the implication that disturbances in
the heat equation propagate at inﬁnite speed — a counterintuitive fact that will be further
expounded on in Section 8.1. Elliptic equations have no characteristics, and as a consequence, do not support propagating signals; indeed, the eﬀect of a localized disturbance
is immediately felt throughout the domain. For example, even when an external force is
concentrated near a single point, it displaces the entire membrane.

Exercises
4.4.14. Find and graph the real characteristic curves for each of the partial diﬀerential
equations in Exercise 4.4.2.
4.4.15. Graph the characteristic curves for the Tricomi equation (4.137) in its hyperbolic region.
What happens to the characteristics as one approaches the parabolic transition boundary?
4.4.16. True or false: The characteristic curves of the Helmholtz equation uxx + uyy − u = 0
are circles.
4.4.17. (a) At what points of the plane is the partial diﬀerential equation x uxx + y uyy = 0
elliptic? parabolic? hyperbolic? (b) How many characteristics are there through the point
(1, −1)? (c) Find them explicitly.
4.4.18. Consider the partial diﬀerential equation uxx + y uxy = y 2 .
(a) On which regions of the (x, y)–plane is the equation elliptic? parabolic? hyperbolic?
(b) Find the characteristics in the hyperbolic region.
(c) Find the general solution in the hyperbolic region. Hint: Use characteristic coordinates.
4.4.19. Find a partial diﬀerential equation whose characteristic curves are:
(a) the lines x − y = a, x + 2 y = b, where a, b ∈ R are arbitrary constants;
(b) the exponential curves y = c ex for c ∈ R;
(c) the concentric circles x2 + y 2 = a for a ≥ 0, and the rays y = b x.
♦ 4.4.20. Prove that any reparametrization of a characteristic curve for a given second-order
linear partial diﬀerential equation is also a characteristic curve.
4.4.21. True or false: You can uniquely recover a second-order partial diﬀerential equation by
knowing all its characteristic curves.
♦ 4.4.22. Prove that any invertible change of variables, as in Exercise 4.4.7, maps the characteristic curves of the original linear partial diﬀerential equation to the characteristic curves of
the transformed equation. Thus, characteristic curves are intrinsic: they do not depend on
the parametrization, nor on the coordinates used to represent the partial diﬀerential
equation.

Chapter 5

Finite Diﬀerences

As one quickly learns, the diﬀerential equations that can be solved by explicit analytic
formulas are few and far between. Consequently, the development of accurate numerical
approximation schemes is an essential tool for extracting quantitative information as well
as achieving a qualitative understanding of the possible behaviors of solutions to the vast
majority of partial diﬀerential equations. (On the other hand, the successful design of
numerical algorithms necessitates a fairly deep understanding of their basic analytic properties, and so exclusive reliance on numerics is not an option.) Even in cases, such as
the heat and wave equations, in which explicit solution formulas (either in closed form or
inﬁnite series) exist, numerical methods can still be proﬁtably employed. Indeed, one can
accurately test a proposed numerical algorithm by running it on a known solution. As we
will see, the lessons learned in the design and testing of numerical algorithms on simpler
“solved” examples are of inestimable value when confronting more challenging problems.
Many of the basic numerical solution schemes for partial diﬀerential equations can be
ﬁt into two broad themes. The ﬁrst, to be presented in the present chapter, is that of
ﬁnite diﬀerence methods, obtained by replacing the derivatives in the equation by appropriate numerical diﬀerentiation formulae. We thus start with a brief discussion of some
elementary ﬁnite diﬀerence formulas used to numerically approximate ﬁrst- and secondorder derivatives of functions. We then establish and analyze some of the most basic ﬁnite
diﬀerence schemes for the heat equation, ﬁrst-order transport equations, the second-order
wave equation, and the Laplace and Poisson equations. As we will learn, not all ﬁnite difference schemes produce accurate numerical approximations, and one must confront issues
of stability and convergence in order to distinguish reliable from worthless methods. In
fact, inspired by Fourier analysis, the key numerical stability criterion is a consequence of
the scheme’s handling of complex exponentials.
The second category of numerical solution techniques comprises the ﬁnite element
methods, which will be the topic of Chapter 10. These two chapters should be regarded as
but a preliminary excursion into this vast and active area of contemporary research. More
sophisticated variations and extensions, as well as other classes of numerical integration
schemes, e.g., spectral, pseudo-spectral, multigrid, multipole, probabilistic (Monte Carlo,
etc.), geometric, symplectic, and many more, can be found in specialized numerical analysis
texts, including [6, 51, 60, 80, 94], and research papers. Also, the journal Acta Numerica
is an excellent source of survey papers on state-of-the-art numerical methods for a broad
range of disciplines.
P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_5, © Springer International Publishing Switzerland 2014

181

182

5 Finite Diﬀerences

5.1 Finite Diﬀerence Approximations
In general, a ﬁnite diﬀerence approximation to the value of some derivative of a scalar
function u(x) at a point x0 in its domain, say u (x0 ) or u (x0 ), relies on a suitable combination of sampled function values at nearby points. The underlying formalism used to
construct these approximation formulas is known as the calculus of ﬁnite diﬀerences. Its
development has a long and inﬂuential history, dating back to Newton.
We begin with the ﬁrst-order derivative. The simplest ﬁnite diﬀerence approximation
is the ordinary diﬀerence quotient
u(x + h) − u(x)
≈ u (x),
h

(5.1)

which appears in the original calculus deﬁnition of the derivative. Indeed, if u is diﬀerentiable at x, then u (x) is, by deﬁnition, the limit, as h → 0 of the ﬁnite diﬀerence quotients.
Geometrically, the diﬀerence quotient measures the slope of the secant line through the
two points (x, u(x)) and (x + h, u(x + h)) on its graph. For small enough h, this should be
a reasonably good approximation to the slope of the tangent line, u (x), as illustrated in
the ﬁrst picture in Figure 5.1. Throughout our discussion, h, the step size, which may be
either positive or negative, is assumed to be small: | h |  1. When h > 0, (5.1) is referred
to as a forward diﬀerence, while h < 0 yields a backward diﬀerence.
How close an approximation is the diﬀerence quotient? To answer this question, we
assume that u(x) is at least twice continuously diﬀerentiable, and examine its ﬁrst-order
Taylor expansion
u(x + h) = u(x) + u (x) h + 12 u (ξ) h2
(5.2)
at the point x. We have used Lagrange’s formula for the remainder term, [8, 97], in which
ξ, which depends on both x and h, is a point lying between x and x + h. Rearranging
(5.2), we obtain
u(x + h) − u(x)
− u (x) = 12 u (ξ) h.
h
Thus, the error in the ﬁnite diﬀerence approximation (5.1) can be bounded by a multiple
of the step size:
u(x + h) − u(x)
− u (x) ≤ C | h |,
h
where C = max 12 | u (ξ) | depends on the magnitude of the second derivative of the function
over the interval in question. Since the error is proportional to the ﬁrst power of h, we
say that the ﬁnite diﬀerence quotient (5.1) is a ﬁrst-order approximation to the derivative
u (x). When the precise formula for the error is not so important, we will write
u (x) =

u(x + h) − u(x)
+ O(h).
h

(5.3)

The “big Oh” notation O(h) refers to a term that is proportional to h, or, more precisely,
whose absolute value is bounded by a constant multiple of | h | as h → 0.
Example 5.1. Let u(x) = sin x. Let us try to approximate
u (1) = cos 1 = .5403023 . . .

5.1 Finite Diﬀerence Approximations

183

Central diﬀerence

Forward diﬀerence
Figure 5.1.

Finite diﬀerence approximations.

by computing ﬁnite diﬀerence quotients
cos 1 ≈

sin(1 + h) − sin 1
.
h

The result for smaller and smaller (positive) values of h is listed in the following table.
h
approximation
error

.1
.497364
−.042939

.01
.536086
−.004216

.001
.539881
−.000421

.0001
.540260
−.000042

1
reduces the size of the error by
We observe that reducing the step size by a factor of 10
approximately the same factor. Thus, to obtain 10 decimal digits of accuracy, we anticipate
needing a step size of about h = 10−11 . The fact that the error is more or less proportional
to the step size conﬁrms that we are dealing with a ﬁrst-order numerical approximation.

To approximate higher-order derivatives, we need to evaluate the function at more
than two points. In general, an approximation to the nth order derivative u(n) (x) requires
at least n + 1 distinct sample points. For simplicity, we restrict our attention to equally
spaced sample points, although the methods introduced can be readily extended to more
general conﬁgurations.
For example, let us try to approximate u (x) by sampling u at the particular points
x, x + h, and x − h. Which combination of the function values u(x − h), u(x), u(x + h)
should be used? The answer is found by consideration of the relevant Taylor expansions†
h2
h3
+ u (x)
+ O(h4 ),
2
6
h2
h3
− u (x)
+ O(h4 ),
u(x − h) = u(x) − u (x) h + u (x)
2
6
u(x + h) = u(x) + u (x) h + u (x)

(5.4)

where the error terms are proportional to h4 . Adding the two formulas together yields
u(x + h) + u(x − h) = 2 u(x) + u (x) h2 + O(h4 ).
†
Throughout, the function u(x) is assumed to be suﬃciently smooth so that any derivatives
that appear are well deﬁned and the expansion formula is valid.

184

5 Finite Diﬀerences

Dividing by h2 and rearranging terms, we arrive at the centered ﬁnite diﬀerence approximation to the second derivative of a function:
u (x) =

u(x + h) − 2 u(x) + u(x − h)
+ O(h2 ).
h2

(5.5)

Since the error is proportional to h2 , this forms a second-order approximation.
2

2

Example 5.2. Let u(x) = ex , with u (x) = (4 x2 + 2) ex . Let us approximate
u (1) = 6 e = 16.30969097 . . .
using the ﬁnite diﬀerence quotient (5.5):
2

2

e(1+h) − 2 e + e(1−h)
u (1) = 6 e ≈
.
h2


The results are listed in the following table.
h
approximation

.1
16.48289823

.01
16.31141265

.001
16.30970819

.0001
16.30969115

error

.17320726

.00172168

.00001722

.00000018

1
Each reduction in step size by a factor of 10
reduces the size of the error by a factor of
1
about 100 , thereby gaining two new decimal digits of accuracy, which conﬁrms that the
centered ﬁnite diﬀerence approximation is of second order.
However, this prediction is not completely borne out in practice. If we take h = .00001
then the formula produces the approximation 16.3097002570, with an error of .0000092863
— which is less accurate than the approximation with h = .0001. The problem is that
round-oﬀ errors due to the ﬁnite precision of numbers stored in the computer (in the preceding computation we used single-precision ﬂoating-point arithmetic) have now begun to
aﬀect the computation. This highlights the inherent diﬃculty with numerical diﬀerentiation: Finite diﬀerence formulae inevitably require dividing very small quantities, and so
round-oﬀ inaccuracies may produce noticeable numerical errors. Thus, while they typically produce reasonably good approximations to the derivatives for moderately small step
sizes, achieving high accuracy requires switching to higher-precision computer arithmetic.
Indeed, a similar comment applies to the previous computation in Example 5.1. Our expectations about the error were not, in fact, fully justiﬁed, as you may have discovered had
you tried an extremely small step size.

Another way to improve the order of accuracy of ﬁnite diﬀerence approximations is to
employ more sample points. For instance, if the ﬁrst-order approximation (5.3) to u (x)
based on the two points x and x + h is not suﬃciently accurate, one can try combining the
function values at three points, say x, x+h, and x−h. To ﬁnd the appropriate combination
of function values u(x − h), u(x), u(x + h), we return to the Taylor expansions (5.4). To
solve for u (x), we subtract the two formulas, and so
u(x + h) − u(x − h) = 2 u (x) h + O(h3 ).
Rearranging the terms, we are led to the well-known centered diﬀerence formula
u (x) =

u(x + h) − u(x − h)
+ O(h2 ),
2h

(5.6)

5.1 Finite Diﬀerence Approximations

185

which is a second-order approximation to the ﬁrst derivative. Geometrically, the centered
diﬀerence quotient represents the slope of the secant line passing through the two points
(x − h, u(x − h)) and (x + h, u(x + h)) on the graph of u, which are centered symmetrically
about the point x. Figure 5.1 illustrates the two approximations, and the advantage of
the centered diﬀerence version is graphically evident. Higher-order approximations can be
found by evaluating the function at yet more sample points, say, x + 2 h, x − 2 h, etc.
Example 5.3. Return to the function u(x) = sin x considered in Example 5.1. The
centered diﬀerence approximation to its derivative u (1) = cos 1 = .5403023 . . . is
cos 1 ≈

sin(1 + h) − sin(1 − h)
.
2h

The results are tabulated as follows:
h
approximation

.1
.53940225217

.01
.54029330087

.001
.54030221582

.0001
.54030230497

error

−.00090005370

−.00000900499

−.00000009005

−.00000000090

As advertised, the results are much more accurate than the one-sided ﬁnite diﬀerence
approximation used in Example 5.1 at the same step size. Since it is a second-order
1
approximation, each reduction in the step size by a factor of 10
results in two more decimal
places of accuracy — up until the point where the eﬀects of round-oﬀ error kick in.
Many additional ﬁnite diﬀerence approximations can be constructed by similar manipulations of Taylor expansions, but these few very basic formulas, along with a couple
that are derived in the exercises, will suﬃce for our purposes. (For a thorough treatment
of the calculus of ﬁnite diﬀerences, the reader can consult [74].) In the following sections,
we will employ the ﬁnite diﬀerence formulas to devise numerical solution schemes for a variety of partial diﬀerential equations. Applications to the numerical integration of ordinary
diﬀerential equations can be found, for example, in [24, 60, 63].

Exercises
♣ 5.1.1. Use the ﬁnite diﬀerence formula (5.3) with step sizes h = .1, .01, and .001 to approximate
the derivative u (1) of the following functions u(x). Discuss the accuracy of your
1
, (c) log x, (d) cos x, (e) tan−1 x.
approximation.
(a) x4 , (b)
1 + x2
♣ 5.1.2. Repeat Exercise 5.1.1 using the centered diﬀerence formula (5.6). Compare your
approximations with those in the previous exercise — are the values in accordance with the
claimed orders of accuracy?
♣ 5.1.3. Approximate the second derivative u (1) of the functions in Exercise 5.1.1 using the
ﬁnite diﬀerence formula (5.5) with h = .1, .01, and .001. Discuss the accuracy of your
approximations.
5.1.4. Construct ﬁnite diﬀerence approximations to the ﬁrst and second derivatives of a function u(x) using its values at the points x − k, x, x + h, where h, k  1 are of comparable size,
but not necessarily equal. What can you say about the error in the approximation?

186

5 Finite Diﬀerences

♠ 5.1.5. In this exercise, you are asked to derive some basic one-sided ﬁnite diﬀerence formulas,
which are used for approximating derivatives of functions at or near the boundary of their
domain. (a) Construct a ﬁnite diﬀerence formula that approximates the derivative u (x)
using the values of u(x) at the points x, x + h, and x + 2 h. What is the order of your
formula? (b) Find a ﬁnite diﬀerence formula for u (x) that involves the same three function values. What is its order? (c) Test your formulas by computing approximations to the
2
ﬁrst and second derivatives of u(x) = ex at x = 1 using step sizes h = .1, .01, and .001.
What is the error in your numerical approximations? Are the errors compatible with the
theoretical orders of the ﬁnite diﬀerence formulas? Discuss why or why not.
(d) Answer part (c) at the point x = 0.
♣ 5.1.6. (a) Using the function values u(x), u(x + h), u(x + 3 h), construct a numerical approximation to the derivative u (x). (b) What is the order of accuracy of your approximation?
(c) Test your approximation on the function u(x) = cos x at x = 1 using the step sizes
h = .1, .01, and .001. Are the errors consistent with your answer in part (b)?
♣ 5.1.7. Answer Exercise 5.1.6 for the second derivative u (x).
5.1.8. (a) Find the order of the ﬁve-point centered ﬁnite diﬀerence approximation
− u(x + 2 h) + 8 u(x + h) − 8 u(x − h) + u(x − 2 h)
.
u (x) ≈
12 h
(b) Test your result on the function (1 + x2 )−1 at x = 1 using the values h = .1, .01, .001.
5.1.9. (a) Using the formula in Exercise 5.1.8 as a guide, ﬁnd ﬁve-point ﬁnite diﬀerence formulas to approximate (i ) u (x), (ii ) u (x), (iii ) u(iv) (x). What is the order of accuracy?
(b) Test your formulas on the function (1 + x2 )−1 at x = 1 using the values h = .1, .01, .001.

5.2 Numerical Algorithms for the Heat Equation
Consider the heat equation
∂2u
∂u
=γ
,
∂t
∂x2

0 < x < ,

t > 0,

(5.7)

on an interval of length , with constant thermal diﬀusivity γ > 0. We impose timedependent Dirichlet boundary conditions
u(t, 0) = α(t),

u(t, ) = β(t),

t > 0,

(5.8)

ﬁxing the temperature at the ends of the interval, along with the initial conditions
u(0, x) = f (x),

0 ≤ x ≤ ,

(5.9)

specifying the initial temperature distribution. In order to eﬀect a numerical approximation
to the solution to this initial-boundary value problem, we begin by introducing a rectangular
mesh consisting of nodes (tj , xm ) ∈ R 2 with
0 = t0 < t1 < t2 < · · ·

and

0 = x0 < x1 < · · · < xn = .

For simplicity, we maintain a uniform mesh spacing in both directions, with
Δt = tj+1 − tj ,

Δx = xm+1 − xm =


,
n

5.2 Numerical Algorithms for the Heat Equation

187

representing, respectively, the time step size and the spatial mesh size. It will be essential
that we do not a priori require that the two be the same. We shall use the notation
uj,m ≈ u(tj , xm ),

where

tj = j Δt,

xm = m Δx,

(5.10)

to denote the numerical approximation to the solution value at the indicated node.
As a ﬁrst attempt at designing a numerical solution scheme, we shall employ the
simplest ﬁnite diﬀerence approximations to the derivatives appearing in the equation. The
second-order space derivative is approximated by the centered diﬀerence formula (5.5), and
hence
u(tj , xm+1 ) − 2 u(tj , xm ) + u(tj , xm−1 )
∂2u
(t
,
x
)
≈
+ O (Δx)2
j
m
∂x2
(Δx)2
uj,m+1 − 2 uj,m + uj,m−1
+ O (Δx)2 ,
≈
(Δx)2

(5.11)

where the error in the approximation is proportional to (Δx)2 . Similarly, the one-sided
ﬁnite diﬀerence approximation (5.3) is used to approximate the time derivative, and so
u(tj+1 , xm ) − u(tj , xm )
uj+1,m − uj,m
∂u
(tj , xm ) ≈
+ O(Δt) ≈
+ O(Δt),
∂t
Δt
Δt

(5.12)

where the error is proportional to Δt. In general, one should try to ensure that the
approximations have similar orders of accuracy, which leads us to require
Δt ≈ (Δx)2 .

(5.13)

Assuming Δx < 1, this implies that the time steps must be much smaller than the space
mesh size.
Remark : At this stage, the reader might be tempted to replace (5.12) by the secondorder central diﬀerence approximation (5.6). However, this introduces signiﬁcant complications, and the resulting numerical scheme is not practical; see Exercise 5.2.10.
Replacing the derivatives in the heat equation (5.14) by their ﬁnite diﬀerence approximations (5.11, 12) and rearranging terms, we end up with the linear system
uj+1,m = μ uj,m+1 + (1 − 2 μ)uj,m + μ uj,m−1 ,
in which
μ=

j = 0, 1, 2, . . . ,
m = 1, . . . , n − 1,

γ Δt
.
(Δx)2

(5.14)

(5.15)

The resulting scheme is of iterative form, whereby the solution values uj+1,m ≈ u(tj+1 , xm )
at time tj+1 are successively calculated, via (5.14), from those at the preceding time tj .
The initial condition (5.9) indicates that we should initialize our numerical data by
sampling the initial temperature at the nodes:
u0,m = fm = f (xm ),

m = 1, . . . , n − 1.

(5.16)

Similarly, the boundary conditions (5.8) require that
uj,0 = αj = α(tj ),

uj,n = βj = β(tj ),

j = 0, 1, 2, . . . .

(5.17)

188

5 Finite Diﬀerences

For consistency, we should assume that the initial and boundary conditions agree at the
corners of the domain:
f0 = f (0) = u(0, 0) = α(0) = α0 ,

fn = f ( ) = u(0, ) = β(0) = β0 .

The three equations (5.14, 16, 17) completely prescribe the numerical approximation scheme
for the solution to the initial-boundary value problem (5.7–9).
Let us rewrite the preceding equations in a more transparent vectorial form. First, let


T
T
u(j) = uj,1 , uj,2 , . . . , uj,n−1
≈ u(tj , x1 ), u(tj , x2 ), . . . , u(tj , xn−1 )
(5.18)
be the vector whose entries are the numerical approximations to the solution values at time
tj at the interior nodes. We omit the boundary nodes (tj , x0 ), (tj , xn ), since those values
are ﬁxed by the boundary conditions (5.17). Then (5.14) takes the form
u(j+1) = A u(j) + b(j) ,
where

⎞

⎛

1 − 2μ
μ
1 − 2μ
μ
⎜ μ
⎜
μ
1 − 2μ μ
⎜
⎜
..
..
A=⎜
.
.
μ
⎜
⎜
..
..
⎝
.
.
μ

μ
1 − 2μ

⎟
⎟
⎟
⎟
⎟,
⎟
⎟
⎠

(5.19)
⎛

⎞
μ αj
⎜ 0 ⎟
⎜
⎟
⎜ 0 ⎟
⎟
b(j) = ⎜
.
⎜ .. ⎟.
⎜
⎟
⎝ 0 ⎠

(5.20)

μ βj

The (n−1)×(n−1) coeﬃcient matrix A is symmetric and tridiagonal, and only its nonzero
entries are displayed. The contributions (5.17) of the boundary nodes appear in the vector
b(j) ∈ R n−1 . This numerical method is known as an explicit scheme, since each iterate is
computed directly from its predecessor without having to solve any auxiliary equations —
unlike the implicit schemes to be discussed next.
Example 5.4. Let us ﬁx the diﬀusivity γ = 1 and the interval length = 1. For
illustrative purposes, we take a spatial step size of Δx = .1. We work with the initial data
⎧
0 ≤ x ≤ 15 ,
⎪
⎨ − x,
1
7
u(0, x) = f (x) =
x − 25 ,
5 ≤ x ≤ 10 ,
⎪
⎩
7
1 − x,
10 ≤ x ≤ 1,
used earlier in Example 4.1, along with homogeneous Dirichlet boundary conditions, so
u(t, 0) = u(t, 1) = 0. In Figure 5.2 we compare the numerical solutions resulting from
two (slightly) diﬀerent time step sizes. The ﬁrst row uses Δt = (Δx)2 = .01 and plots
the solution at the indicated times. The numerical solution is already showing signs of
instability (the ﬁnal plot does not even ﬁt in the window), and indeed, soon thereafter, it
becomes completely wild. The second row takes Δt = .005. Even though we are employing
a rather coarse mesh, the numerical solution is not too far away from the true solution to
the initial value problem, which can be seen in Figure 4.1.

Stability Analysis
In light of the preceding calculation, we need to understand why our numerical scheme
sometimes gives reasonable answers but sometimes utterly fails. To this end, we investigate

5.2 Numerical Algorithms for the Heat Equation

t=0

189

t = .02

Figure 5.2.

Numerical solutions for the heat equation
based on the explicit scheme.

t = .04


the eﬀect of the numerical scheme on simple functions. As we know, the general solution
to the heat equation can be decomposed into a sum over the various Fourier modes. Thus,
we can concentrate on understanding what the numerical scheme does to an individual
complex exponential,† bearing in mind that we can then reconstruct its eﬀect on more
general initial data by taking suitable linear combinations of exponentials.
To this end, suppose that, at time t = tj , the solution is a sampled exponential
u(tj , x) = e i k x ,

and so

uj,m = u(tj , xm ) = e i k xm ,

(5.21)

where k is a real parameter. Substituting the latter values into our numerical equations
(5.14), we ﬁnd that the updated value at time tj+1 is also a sampled exponential:
uj+1,m = μ uj,m+1 + (1 − 2 μ)uj,m + μ uj,m−1
= μ e i k xm+1 + (1 − 2 μ)e i k xm + μ e i k xm−1
= μ e i k(xm +Δx) + (1 − 2 μ)e i k xm + μ e i k(xm −Δx)

(5.22)

= λ e i k xm ,
where

λ = λ(k) = μ e i k Δx + (1 − 2 μ) + μ e− i k Δx


= 1 − 2 μ 1 − cos(k Δx) = 1 − 4 μ sin2 12 k Δx .

(5.23)

Thus, the eﬀect of a single step is to multiply the complex exponential (5.21) by the
magniﬁcation factor λ :
u(tj+1 , x) = λ e i k x .
(5.24)
†

As usual, complex exponentials are easier to work with than real trigonometric functions.

190

5 Finite Diﬀerences

In other words, e i k x plays the role of an eigenfunction, with the magniﬁcation factor λ(k)
the corresponding eigenvalue, of the linear operator governing each step of the numerical
scheme. Continuing in this fashion, we ﬁnd that the eﬀect of p further iterations of the
scheme is to multiply the exponential by the pth power of the magniﬁcation factor:
u(tj+p , x) = λp e i k x .

(5.25)

As a result, the stability is governed by the size of the magniﬁcation factor: If | λ | > 1,
then λp grows exponentially, and so the numerical solutions (5.25) become unbounded as
p → ∞, which is clearly incompatible with the analytical behavior of solutions to the
heat equation. Therefore, an evident necessary condition for the stability of our numerical
scheme is that its magniﬁcation factor satisfy
| λ | ≤ 1.

(5.26)

This method of stability analysis was developed by the mid-twentieth-century Hungarian/American mathematician — and father of the electronic computer — John von
Neumann. The stability criterion (5.26) eﬀectively distinguishes the stable, and hence
valid, numerical algorithms from the unstable, and hence ineﬀectual, schemes. For the
particular case (5.23), the von Neumann stability criterion (5.26) requires


−1 ≤ 1 − 4 μ sin2 12 k Δx ≤ 1,
or, equivalently,
0 ≤ μ sin2 12 k Δx ≤ 12 .
Since this is required to hold for all possible k, we must have
0≤μ=

1
γ Δt
≤ ,
(Δx)2
2

and hence

Δt ≤

(Δx)2
,
2γ

(5.27)

since γ > 0. Thus, once the space mesh size is ﬁxed, stability of the numerical scheme
places a restriction on the allowable time step size. For instance, if γ = 1, and the space
mesh size Δx = .01, then we must adopt a minuscule time step size Δt ≤ .00005. It
would take an exorbitant number of time steps to compute the value of the solution at
even moderate times, e.g., t = 1. Moreover, the accumulation of round-oﬀ errors might
then cause a signiﬁcant reduction in the overall accuracy of the ﬁnal solution values. Since
not all choices of space and time steps lead to a convergent scheme, the explicit scheme
(5.14) is called conditionally stable.
Implicit and Crank–Nicolson Methods
An unconditionally stable method — one that does not restrict the time step — can be
constructed by replacing the forward diﬀerence formula (5.12) used to approximate the
time derivative by the backwards diﬀerence formula
u(tj , xm ) − u(tj−1 , xm )
∂u
+ O(Δt).
(tj , xm ) ≈
∂t
Δt

(5.28)

Substituting (5.28) and the same centered diﬀerence approximation (5.11) for uxx into the
heat equation, and then replacing j by j + 1, leads to the iterative system
− μ uj+1,m+1 + (1 + 2 μ)uj+1,m − μ uj+1,m−1 = uj,m ,

j = 0, 1, 2, . . . ,
m = 1, . . . , n − 1,

(5.29)

5.2 Numerical Algorithms for the Heat Equation

t = .02

t = .04

Figure 5.3.

Numerical solutions for the heat equation
based on the implicit scheme.

191

t = .06


where the parameter μ = γ Δt/(Δx)2 is as before. The initial and boundary conditions
have the same form (5.16, 17). The latter system can be written in the matrix form
 u(j+1) = u(j) + b(j+1) ,
A

(5.30)

 is obtained from the matrix A in (5.20) by replacing μ by − μ. This serves to
where A
deﬁne an implicit scheme, since we have to solve a linear system of algebraic equations
at each step in order to compute the next iterate u(j+1) . However, since the coeﬃcient
 is tridiagonal, the solution can be computed extremely rapidly, [89], and so its
matrix A
calculation is not an impediment to the practical implementation of this implicit scheme.
Example 5.5. Consider the same initial-boundary value problem considered in
Example 5.4. In Figure 5.3, we plot the numerical solutions obtained using the implicit
scheme. The initial data is not displayed, but we graph the numerical solutions at times
t = .02, .04, .06 with a mesh size of Δx = .1. In the top row, we use a time step of Δt = .01,
while in the bottom row Δt = .005. In contrast to the explicit scheme, there is very little
diﬀerence between the two — indeed, both come much closer to the actual solution than
the explicit scheme. In fact, even signiﬁcantly larger time steps yield reasonable numerical
approximations to the solution.
Let us apply the von Neumann analysis to investigate the stability of the implicit
scheme. Again, we need only look at the eﬀect of the scheme on a complex exponential.
Substituting (5.21, 24) into (5.29) and canceling the common exponential factor leads to
the equation
λ (− μ e i k Δx + 1 + 2 μ − μ e− i k Δx ) = 1.

192

5 Finite Diﬀerences

t = .02

t = .04

Figure 5.4.

Numerical Solutions for the heat equation
based on the Crank–Nicolson scheme.

t = .06


We solve for the magniﬁcation factor
λ=

1
1 + 2 μ 1 − cos(k Δx)

=

1
.
1 + 4 μ sin2 12 k Δx

(5.31)

Since μ > 0, the magniﬁcation factor is always less than 1 in absolute value, and so the
stability criterion (5.26) is satisﬁed for any choice of step sizes. We conclude that the
implicit scheme (5.14) is unconditionally stable.
Another popular numerical scheme for solving the heat equation is the Crank–Nicolson
method , due to the British numerical analysts John Crank and Phyllis Nicolson:
uj+1,m − uj,m = 12 μ (uj+1,m+1 − 2 uj+1,m + uj+1,m−1 + uj,m+1 − 2 uj,m + uj,m−1 ), (5.32)
which can be obtained by averaging the explicit and implicit schemes (5.14) and (5.29).
We can write (5.32) in vectorial form
 u(j+1) = B u(j) + 1 b(j) + b(j+1) ,
B
2
where

⎛

1+μ
⎜ − 12 μ
⎜

B=⎜
⎝

− 12 μ
1+μ
− 12 μ

⎞
− 12 μ
..
.
..
.

⎟
⎟
. . ⎟,
.⎠
..
.

⎛

1−μ
⎜ 12 μ
⎜
B=⎜
⎝

1
2μ

1−μ
1
μ
2

⎞
1
2μ

..

.

..

.

⎟
⎟
. . ⎟,
.⎠
..
.

(5.33)

are both tridiagonal.
Applying the von Neumann analysis as before, we deduce that the magniﬁcation factor
has the form
1 − 2 μ sin2 12 k Δx
(5.34)
.
λ=
1 + 2 μ sin2 12 k Δx

5.2 Numerical Algorithms for the Heat Equation

193

Since μ > 0, we see that | λ | ≤ 1 for all choices of step size, and so the Crank–Nicolson
scheme is also unconditionally stable. A detailed analysis based on a Taylor expansion of
the solution reveals that the errors are of order (Δt)2 and (Δx)2 , and so it is reasonable to
choose the time step to have the same order of magnitude as the space step: Δt ≈ Δx. This
gives the Crank–Nicolson scheme a signiﬁcant advantage over the previous two methods,
in that one can get away with far fewer time steps. However, applying it to the initial
value problem considered above reveals a subtle weakness. The top row in Figure 5.4 has
space and time step sizes Δt = Δx = .01, and does a reasonable job of approximating
the solution except near the corners, where an annoying and incorrect local oscillation
persists as the solution decays. The bottom row uses Δt = Δx = .001, and performs much
better, although a similar oscillatory error can be observed at much smaller times. Indeed,
unlike the implicit scheme, the Crank–Nicolson method fails to rapidly damp out the highfrequency Fourier modes associated with small-scale features such as discontinuities and
corners in the initial data, although it performs quite well in smooth regimes. Thus, when
dealing with irregular initial data, a good strategy is to ﬁrst run the implicit scheme until
the small-scale noise is dissipated away, and then switch to Crank–Nicolson with a much
larger time step to determine the later large scale dynamics.
Finally, we remark that the ﬁnite diﬀerence schemes developed above for the heat
equation can all be readily adapted to more general parabolic partial diﬀerential equations.
The stability criteria and observed behaviors are fairly similar, and a couple of illustrative
examples can be found in the exercises.

Exercises
5.2.1. Suppose we seek to approximate the solution to the initial-boundary value problem
ut = 5 uxx ,
u(t, 0) = u(t, 3) = 0,
u(0, x) = x(x − 1)(x − 3),
0 ≤ x ≤ 3,
by employing the explicit scheme (5.14). (a) Given the spatial mesh size Δx = .1, what
range of time steps Δt can be used to produce an accurate numerical approximation?
(b) Test your prediction by implementing the scheme using one value of Δt in the allowed
range and one value outside.
5.2.2. Solve the following initial-boundary value problem
u(t, 0) = u(t, 1) = 0,
u(0, x) = f (x),
ut = uxx ,
with initial data



⎧
⎪
2  x − 16  − 13 ,
⎪
⎪
⎨

f (x) = ⎪ 0,



⎪
⎪
⎩ 1 − 3  x − 5  ,
2
6

0 ≤ x ≤ 13 ,
1
2
3 ≤ x ≤ 3,
2
3 ≤ x ≤ 1,

0 ≤ x ≤ 1,

using

(i ) the explicit scheme (5.14); (ii ) the implicit scheme (5.29); and (iii ) the Crank–Nicolson
scheme (5.32). Use space step sizes Δx = .1 and .05, and suitably chosen time steps Δt.
Discuss which features of the solution can be observed in your numerical approximations.
5.2.3. Repeat Exercise 5.2.2 for the initial-boundary value problem ut = 3 uxx , u(0, x) = 0,
u(t, −1) = 1, u(t, 1) = −1, using space step sizes Δx = .2 and .1.
5.2.4. (a) Solve the initial-boundary value problem
ut = uxx ,
u(t, −1) = u(t, 1) = 0,
u(0, x) = | x |1/2 − x2 ,
−1 ≤ x ≤ 1,
using (i ) the explicit scheme (5.14); (ii ) the implicit scheme (5.29); (iii ) the Crank–Nicolson
scheme (5.32). Use Δx = .1 and an appropriate time step Δt. Compare your numerical
solutions at times t = 0, .01, , .02, .05, .1, .3, .5, 1.0, and discuss your ﬁndings. (b) Repeat

194

5 Finite Diﬀerences
part (a) for the implicit and Crank-Nicolson schemes with Δx = .01. Why aren’t you being
asked to implement the explicit scheme?

5.2.5. Use the implicit scheme with spatial mesh sizes Δx = .1 and .05 and appropriately chosen values of the time step Δt to investigate the solution to the periodically forced boundary value problem ut = uxx , u(0, x) = 0, u(t, 0) = sin 5 π t, u(t, 1) = cos 5 π t. Is your
solution periodic in time?
♥ 5.2.6. (a) How would you modify (i ) the explicit scheme; (ii ) the implicit scheme; to deal with
Neumann boundary conditions? Hint: Use the one-sided ﬁnite diﬀerence formulae found in
Exercise 5.1.5 to approximate the derivatives at the boundary.
(b) Test your proposals on the boundary value problem
ut = uxx ,
u(0, x) = 12 + cos 2 πx − 12 cos 3 πx,
ux (t, 0) = 0 = ux (t, 1),
using space step sizes Δx = .1 and .01 and appropriate time steps. Compare your numerical solution with the exact solution at times t = .01, .03, .05, and explain any discrepancies.
5.2.7. (a) Design an explicit numerical scheme for approximating the solution to the initialboundary value problem
ut = γ uxx + s(x),
u(t, 0) = u(t, 1) = 0,
u(0, x) = f (x),
0 ≤ x ≤ 1,
for the heat equation with a source term s(x). (b) Test your scheme when
γ = 16 ,

s(x) = x(1 − x)(10 − 22 x),



⎧

1
1
⎪
x − 6  − 3,
2
⎪
⎪
⎨

f (x) = ⎪ 0,



⎪
⎪

⎩ 1
5
2 − 3 x− 6 ,

0 ≤ x ≤ 13 ,
1
2
3 ≤ x ≤ 3,
2
3 ≤ x ≤ 1,

using space step sizes Δx = .1 and .05, and a suitably chosen time step Δt. Are your two
numerical solutions close? (c) What is the long-term behavior of the solution? Can you
ﬁnd a formula for its eventual proﬁle? (d) Design an implicit scheme for the same problem.
Does this aﬀect the behavior of your numerical solution? What are the advantages of the
implicit scheme?
5.2.8. Consider the initial-boundary value problem for the lossy diﬀusion equation
t ≥ 0,
∂2u
∂u
− α u,
u(t, 0) = u(t, 1) = 0,
u(0, x) = f (x),
=
∂t
∂x2
0 ≤ x ≤ 1,
where α > 0 is a positive constant. (a) Devise an explicit ﬁnite diﬀerence method for
computing a numerical approximation to the solution. (b) For what mesh sizes would
you expect your method to provide a good approximation to the solution?
(c) Discuss the case when α < 0.
5.2.9. Consider the initial-boundary value problem for the diﬀusive transport equation
∂u
∂u
∂2u
t ≥ 0,
+2
=
,
u(t, 0) = u(t, 1) = 0,
u(0, x) = x(1 − x),
∂t
∂x2
∂x
0 ≤ x ≤ 1.
(a) Devise an explicit ﬁnite diﬀerence scheme for computing numerical approximations to
the solution. Hint: Make sure your approximations are of comparable order. (b) For what
range of time step sizes would you expect your method to provide a decent approximation
to the solution? (c) Test your answer in part (b) for the spatial step size Δx = .1.
♦ 5.2.10. (a) Show that using the centered diﬀerence approximation (5.6) to approximate the
time derivative leads to Richardson’s method for numerically solving the heat equation:
j = 1, 2, . . . ,
uj+1,m = uj−1,m + 2 μ (uj,m+1 − 2 uj,m + uj,m−1 ),
m = 1, . . . , n − 1,
where μ = γ Δt/(Δx)2 is as in (5.15). (b) Discuss how to start Richardson’s method.
(c) Discuss the stability of Richardson’s method. (d) Test Richardson’s method on the
initial-boundary value problem in Exercise 5.2.2. Does your numerical solution conform
with your expectations from part (b)?

5.3 Numerical Algorithms for First Order Partial Diﬀerential Equations

195

5.3 Numerical Algorithms for
First–Order Partial Diﬀerential Equations
Let us next apply the method of ﬁnite diﬀerences to construct some basic numerical methods for ﬁrst-order partial diﬀerential equations. As noted in Section 4.4, ﬁrst-order partial
diﬀerential equations are prototypes for hyperbolic equations, and so many of the lessons
learned here carry over to the general hyperbolic regime, including the second-order wave
equation, which we analyze in detail in the following section.
Consider the initial value problem for the elementary transport equation
∂u
∂u
(5.35)
+c
= 0,
u(0, x) = f (x),
− ∞ < x < ∞,
∂t
∂x
with constant wave speed c. Of course, as we learned in Section 2.2, the solution is a simple
traveling wave
u(t, x) = f (x − c t)
(5.36)
that is constant along the characteristic lines of slope c in the (t, x)–plane. Although the
analytical solution is completely elementary, there will be valuable lessons to be learned
from our attempt to reproduce it by numerical approximation. Indeed, each of the numerical schemes developed below has an evident adaptation to transport equations with
variable wave speeds c(t, x), and even to nonlinear transport equations whose wave speed
depends on the solution u, and so admit shock-wave solutions.
As before, we restrict our attention to a rectangular mesh (tj , xm ) with uniform time
step size Δt = tj+1 − tj and space mesh size Δx = xm+1 − xm . We use uj,m ≈ u(tj , xm )
to denote our numerical approximation to the solution u(t, x) at the indicated node. The
simplest numerical scheme is obtained by replacing the time and space derivatives by their
ﬁrst-order ﬁnite diﬀerence approximations (5.1):
uj+1,m − uj,m
∂u
(tj , xm ) ≈
+ O(Δt),
∂t
Δt

uj,m+1 − uj,m
∂u
(tj , xm ) ≈
+ O(Δx).
∂x
Δx
(5.37)
Substituting these expressions into the transport equation (5.35) leads to the explicit numerical scheme
uj+1,m = − σ uj,m+1 + (σ + 1)uj,m ,
(5.38)
in which the parameter
c Δt
(5.39)
Δx
depends on the wave speed and the ratio of time to space step sizes. Since we are employing ﬁrst-order approximations to both derivatives, we should choose the step sizes to be
comparable: Δt ≈ Δx. When working on a bounded interval, say 0 ≤ x ≤ , we will need
to specify a value for the numerical solution at the right end, e.g., setting uj,n = 0, which
corresponds to imposing the boundary condition u(t, ) = 0.
In Figure 5.5, we plot the numerical solutions, at times t = .1, .2, .3, arising from the
following initial condition:
σ=

2

2

u(0, x) = f (x) = .4 e−300(x−.5) + .1 e−300(x−.65) .

(5.40)

We work on the interval 0 ≤ x ≤ 1, and use step sizes Δt = Δx = .005. Let us try four
diﬀerent values of the wave speed. The cases c = .5 and c = −1.5 clearly exhibit some form

196

5 Finite Diﬀerences

c = .5

c = −.5

c = −1

c = −1.5
Figure 5.5.

Numerical solutions to the transport equation.



of numerical instability. The numerical solution when c = −.5 is a bit more reasonable,
although one can already observe some degradation due to the relatively low accuracy of
the scheme. This can be alleviated by employing a smaller step size. The case c = −1
looks exceptionally good, and you are asked to provide an explanation in Exercise 5.3.6.

The CFL Condition
There are two ways to understand the observed numerical instability. First, we recall
that the exact solution (5.36) is constant along the characteristic lines x = c t + ξ, and
hence the value of u(t, x) depends only on the initial value f (ξ) at the point ξ = x − c t.
On the other hand, at time t = tj , the numerical solution uj,m ≈ u(tj , xm ) computed
using (5.38) depends on the values of uj−1,m and uj−1,m+1 . The latter two values have

5.3 Numerical Algorithms for First Order Partial Diﬀerential Equations

x

197

x

t

t

Stable

Unstable
Figure 5.6.

The CFL condition.

been computed from the previous approximations uj−2,m , uj−2,m+1 , uj−2,m+2. And so
on. Going all the way back to the initial time t0 = 0, we ﬁnd that uj,m depends on the
initial values u0,m = f (xm ), . . . , u0,m+j = f (xm + j Δx) at the nodes lying in the interval
xm ≤ x ≤ xm + j Δx. On the other hand, the actual solution u(tj , xm ) depends only on
the value of f (ξ), where
ξ = xm − c tj = xm − c j Δt.
Thus, if ξ lies outside the interval [ xm , xm + j Δx ], then varying the initial condition
near the point x = ξ will change the actual solution value u(tj , xm ) without altering its
numerical approximation uj,m at all! So the numerical scheme cannot possibly provide an
accurate approximation to the solution value. As a result, we must require
xm ≤ ξ = xm − c j Δt ≤ xm + j Δx,

and hence

0 ≤ − c Δt ≤ Δx,

which we rewrite as
Δx
c Δt
≥ −1,
or, equivalently,
−
≤ c ≤ 0.
(5.41)
Δx
Δt
This is the simplest manifestation of what is known as the Courant–Friedrichs–Lewy condition, or CFL condition for short, which was established in the groundbreaking 1928
paper [33] by three of the pioneers in the development of numerical methods for partial
diﬀerential equations: the German (soon to be American) applied mathematicians Richard
Courant, Kurt Friedrichs, and Hans Lewy. Note that the CFL condition requires that the
wave speed be negative, and the time step size not too large. Thus, for allowable wave
speeds, the ﬁnite diﬀerence scheme (5.38) is conditionally stable.
The CFL condition can be recast in a more geometrically transparent manner as
follows. For the ﬁnite diﬀerence scheme (5.38), the numerical domain of dependence of a
point (tj , xm ) is the triangle
.
(5.42)
T(tj ,xm ) = (t, x) 0 ≤ t ≤ tj , xm ≤ x ≤ xm + tj − t .
0≥σ=

The reason for this nomenclature is that, as we have just seen, the numerical approximation
to the solution at the node (tj , xm ) depends on the computed values at the nodes lying

198

5 Finite Diﬀerences

within its numerical domain of dependence; see Figure 5.6. The CFL condition (5.41)
requires that, for all 0 ≤ t ≤ tj , the characteristic passing through the point (tj , xm ) lie
entirely within the numerical domain of dependence (5.42). If the characteristic ventures
outside the domain, then the scheme will be numerically unstable. With this geometric
reformulation, the CFL criterion can be applied to both linear and nonlinear transport
equations that have nonuniform wave speeds.
The CFL criterion (5.41) is reconﬁrmed by a von Neumann stability analysis. As
before, we test the numerical scheme on an exponential function. Substituting
uj,m = e i k xm ,

uj+1,m = λ e i k xm ,

(5.43)

into (5.38) leads to

λ e i k xm = − σ e i k xm+1 + (σ + 1)e i k xm = − σ e i k Δx + σ + 1 e i k xm .
The resulting (complex) magniﬁcation factor


λ = 1 + σ 1 − e i k Δx = 1 + σ − σ cos(k Δx) − i σ sin(k Δx)
satisﬁes the stability criterion | λ | ≤ 1 if and only if


2
2
| λ |2 = 1 + σ − σ cos(k Δx) + σ sin(k Δx)


= 1 + 2 σ(σ + 1) 1 − cos(k Δx) = 1 + 4 σ(σ + 1) sin2 12 k Δx ≤ 1
for all k. Thus, stability requires that σ(σ + 1) ≤ 0, and thus −1 ≤ σ ≤ 0, in complete
accord with the CFL condition (5.41).

Upwind and Lax–Wendroﬀ Schemes
To obtain a ﬁnite diﬀerence scheme that can be used for positive wave speeds, we replace the
forward ﬁnite diﬀerence approximation to ∂u/∂x by the corresponding backwards diﬀerence
quotient, namely, (5.1) with h = − Δx, leading to the alternative ﬁrst-order numerical
scheme
uj+1,m = − (σ − 1) uj,m + σ uj,m−1 ,
(5.44)
where σ = c Δt/Δx is as before. A similar analysis, left to the reader, produces the
corresponding CFL stability criterion
0≤σ=

c Δt
≤ 1,
Δx

and so this scheme can be applied for suitable positive wave speeds.
In this manner, we have produced one numerical scheme that works for negative wave
speeds, and an alternative scheme for positive speeds. The question arises — particularly
when one is dealing with equations with variable wave speeds — whether one can devise
a scheme that is (conditionally) stable for both positive and negative wave speeds. One
might be tempted to use the centered diﬀerence approximation (5.6):

uj,m+1 − uj,m−1
∂u
(tj , xm ) ≈
+ O (Δx)2 .
∂x
2 Δx

(5.45)

5.3 Numerical Algorithms for First Order Partial Diﬀerential Equations

199

x

t

The CFL condition for the centered diﬀerence scheme.

Figure 5.7.

Substituting (5.45) and the previous approximation to the time derivative (5.37) into (5.35)
leads to the numerical scheme
uj+1,m = − 12 σ uj,m+1 + uj,m + 12 σ uj,m−1 ,

(5.46)

where, as usual, σ = c Δt/Δx. In this case, the numerical domain of dependence of the
node (tj , xm ) consists of the nodes in the triangle
T(tj ,xm ) =

-

(t, x)

.
0 ≤ t ≤ tj , xm − tj + t ≤ x ≤ xm + tj − t .

(5.47)

The CFL condition requires that, for 0 ≤ t ≤ tj , the characteristic going through (tj , xm )
lie within this triangle, as in Figure 5.7, which imposes the condition
|σ| =

c Δt
≤ 1,
Δx

or, equivalently,

|c| ≤

Δx
.
Δt

(5.48)

Unfortunately, although it satisﬁes the CFL condition over this range of wave speeds, the
centered diﬀerence scheme is, in fact, always unstable! For instance, the instability of the
numerical solution to the preceding initial value problem (5.40) for c = 1 can be observed
in Figure 5.8. This is conﬁrmed by applying a von Neumann analysis: substitute (5.43)
into (5.46), and cancel the common exponential factors. Provided σ = 0, which means that
c = 0, the resulting magniﬁcation factor
λ = 1 − i σ sin(k Δx)
satisﬁes | λ | > 1 for all k with sin(k Δx) = 0. Thus, for c = 0, the centered diﬀerence
scheme (5.46) is unstable for all (nonzero) wave speeds!

200

5 Finite Diﬀerences

t = .15
Figure 5.8.

t = .3

t = .45

Centered diﬀerence numerical solution to the transport equation.



One possible means of overcoming the sign restriction on the wave speed is to use
the forward diﬀerence scheme (5.38) when the wave speed is negative and the backwards
scheme (5.44) when it is positive. The resulting scheme, valid for varying wave speeds
c(t, x), takes the form

− σj,m uj,m+1 + (σj,m + 1)uj,m , cj,m ≤ 0,
uj+1,m =
(5.49)
− (σj,m − 1)uj,m + σj,m uj,m−1 , cj,m > 0,
where
σj,m = cj,m

Δt
,
Δx

cj,m = c(tj , xm ).

(5.50)

This is referred to as an upwind scheme, since the second node always lies “upwind” —
that is, away from the direction of motion — from the reference point (tj , xm ). The
upwind scheme works reasonably well over short time intervals, assuming that the space
step size is suﬃciently small and the time step satisﬁes the CFL condition Δx/Δt ≤ | cj,m |
at each node, cf. (5.41). However, over longer time intervals, as we already observed in
Figure 5.5, the simple upwind scheme tends to produce a noticeable damping of waves or,
alternatively, require an unacceptably small step size. One way of overcoming this defect is
to use the popular Lax–Wendroﬀ scheme, which is based on second-order approximations
to the derivatives. In the case of constant wave speed, the iterative step takes the form
uj+1,m = 12 σ(σ − 1) uj,m+1 − (σ 2 − 1) uj,m + 12 σ(σ + 1) uj,m−1 .

(5.51)

The stability analysis of the Lax–Wendroﬀ scheme is relegated to the exercises. Extensions
to variable wave speeds are more subtle, and we refer the reader to [80] for a detailed
derivation.

Exercises
5.3.1. Solve the initial value problem ut = 3 ux , u(0, x) = 1/(1 + x2 ), on the interval [ − 10, 10 ]
using an upwind scheme with space step size Δx = .1. Decide on an appropriate time step
size, and graph your solution at times t = .5, 1, 1.5. Discuss what you observe.

5.4 Numerical Algorithms for the Wave Equation

201

5.3.2. Solve Exercise 5.3.1 for the nonuniform transport equations




2

(a) ut + 4 (1 + x2 )−1 ux = 0,

(b) ut = 3 − 2 e− x /4 ux ,

(c) ut + 7 x (1 + x2 )−1 ux = 0,

(d) ut + 2 tan−1 12 x ux = 0.





5.3.3. Consider the initial value problem


2
3x
u(0, x) = 1 − 12 x2 e− x /3 .
ut + 2
ux = 0,
x +1
On the interval [ − 5, 5 ], using space step size Δx = .1 and time step size Δt = .025, apply
(a) the forward scheme (5.38) (suitably modiﬁed for variable wave speed), (b) the backward scheme (5.44) (suitably modiﬁed for variable wave speed), and (c) the upwind scheme
(5.49). Graph the resulting numerical solutions at times t = .5, 1, 1.5, and discuss what you
observe in each case. Which of the schemes are stable?
5.3.4. Use the centered diﬀerence scheme (5.46) to solve the initial value problem in Exercise
5.3.1. Do you observe any instabilities in your numerical solution?
5.3.5. Use the Lax–Wendroﬀ scheme (5.51) to solve the initial value problem in Exercise 5.3.1.
Discuss the accuracy of your solution in comparison with the upwind scheme.
♦ 5.3.6. Can you explain why, in Figure 5.5, the numerical solution in the case c = −1 is signiﬁcantly better than for c = −.5, or, indeed, for any other c in the stable range.
5.3.7. Nonlinear transport equations are often solved numerically by writing them in the form
of a conservation law, and then applying the ﬁnite diﬀerence formulas directly to the conserved density and ﬂux. (a) Devise an upwind scheme for numerically solving our favorite
nonlinear transport equation, ut + 12 (u2 )x = 0.
2

(b) Test your scheme on the initial value problem u(0, x) = e− x .

5.3.8. (a) Design a stable numerical solution scheme for the damped transport equation
2
ut + 34 ux + u = 0. (b) Test your scheme on the initial value problem with u(0, x) = e− x .
♦ 5.3.9. Analyze the stability of the numerical scheme (5.44) by applying (a) the CFL condition;
(b) a von Neumann analysis. Are your conclusions the same?
♦ 5.3.10. For what choices of step size Δt, Δx is the Lax–Wendroﬀ scheme (5.51) stable?

5.4 Numerical Algorithms for the Wave Equation
Let us now develop some basic numerical solution techniques for the second-order wave
equation. As above, although we are in possession of the explicit d’Alembert solution
formula (2.82), the lessons learned in designing viable schemes here will carry over to more
complicated situations, including inhomogeneous media and higher-dimensional problems,
for which analytic solution formulas may no longer be readily available.
Consider the wave equation
2
∂2u
2 ∂ u
=
c
,
∂t2
∂x2

0 < x < ,

t ≥ 0,

(5.52)

on a bounded interval of length  with constant wave speed c > 0. For speciﬁcity, we
impose (possibly time-dependent) Dirichlet boundary conditions
u(t, 0) = α(t),

u(t, ) = β(t),

t ≥ 0,

(5.53)

202

5 Finite Diﬀerences

along with the usual initial conditions
u(0, x) = f (x),

∂u
(0, x) = g(x),
∂t

0 ≤ x ≤ .

(5.54)

As usual, we adopt a uniformly spaced mesh

.
n
Discretization is implemented by replacing the second-order derivatives in the wave equation by their standard ﬁnite diﬀerence approximations (5.5):
tj = j Δt,

xm = m Δx,

where

Δx =

u(tj+1 , xm ) − 2 u(tj , xm ) + u(tj−1 , xm )
∂2u
(tj , xm ) ≈
+ O (Δt)2 ,
2
∂t
(Δt)2
u(tj , xm+1 ) − 2 u(tj , xm ) + u(tj , xm−1 )
∂2u
(tj , xm ) ≈
+ O (Δx)2 .
2
∂x
(Δx)2

(5.55)

Since the error terms are both of second order, we anticipate being able to choose the
space and time step sizes to have comparable magnitudes: Δt ≈ Δx. Substituting the
ﬁnite diﬀerence formulas (5.55) into the partial diﬀerential equation (5.52) and rearranging
terms, we are led to the iterative system
j = 1, 2, . . . ,

uj+1,m = σ 2 uj,m+1 + 2 (1 − σ 2 ) uj,m + σ 2 uj,m−1 − uj−1,m ,

m = 1, . . . , n − 1,

(5.56)

for the numerical approximations uj,m ≈ u(tj , xm ) to the solution values at the nodes. The
parameter
c Δt
> 0
(5.57)
σ=
Δx
depends on the wave speed and the ratio of space and time step sizes. The boundary
conditions (5.53) require that
uj,0 = αj = α(tj ),

uj,n = βj = β(tj ),

j = 0, 1, 2, . . . .

(5.58)

This allows us to rewrite the iterative system in vectorial form
u(j+1) = B u(j) − u(j−1) + b(j) ,
where
⎛

(5.59)

⎛
⎞
⎞
⎛ 2 ⎞
σ2
uj,1
σ αj
2
2
⎟
⎜ uj,2 ⎟
2 (1 − σ ) σ
⎜ 0 ⎟
⎟
⎜
⎟
⎜ . ⎟
..
..
⎟
⎜
⎟
σ2
. ⎟
.
.
⎟, u(j) = ⎜ ... ⎟, b(j) = ⎜
⎜ . ⎟.
⎟
⎜
⎟
⎝
..
..
⎠
⎝u
⎠
0 ⎠
.
. σ2
j,n−2
2
σ βj
uj,n−1
σ 2 2 (1 − σ 2 )
(5.60)
The entries of u(j) ∈ R n−1 are, as in (5.18), the numerical approximations to the solution
values at the interior nodes. Note that (5.59) describes a second-order iterative scheme,
since computing the subsequent iterate u(j+1) requires knowing the values of the preceding
two: u(j) and u(j−1) .
The one subtlety is how to get the method started. We know u(0) , since its entries
u0,m = fm = f (xm ) are determined by the initial position. However, we also need u(1)
2 (1 − σ 2 )
⎜
σ2
⎜
⎜
B=⎜
⎜
⎝

5.4 Numerical Algorithms for the Wave Equation

203

in order to launch the iteration and compute u(2) , u(3) , . . . . Its entries u1,m ≈ u(Δt, xm )
approximate the solution at time t1 = Δt, whereas the initial velocity ut (0, x) = g(x)
prescribes the derivatives ut (0, xm ) = gm = g(xm ) at the initial time t0 = 0. To resolve
this diﬃculty, a ﬁrst thought might be to use the ﬁnite diﬀerence approximation
gm =

u1,m − fm
∂u
u(Δt, xm ) − u(0, xm )
(0, xm ) ≈
≈
∂t
Δt
Δt

(5.61)

to compute the required values u1,m = fm + gm Δt. However, the approximation (5.61) is
accurate only to order Δt, whereas the rest of the scheme has errors proportional to (Δt)2 .
The eﬀect would be to introduce an unacceptably large error at the initial step, and the
resulting solution would fail to conform to the desired order of accuracy.
To construct an initial approximation to u(1) with error on the order of (Δt)2 , we need
to analyze the error in the approximation (5.61) in more depth. Note that, by Taylor’s
Theorem,
1 ∂2u
∂u
u(Δt, xm ) − u(0, xm )
(0, xm )Δt + O (Δt)2
=
(0, xm ) +
Δt
∂t
2 ∂t2
c2 ∂ 2 u
∂u
(0, xm ) +
(0, xm ) Δt + O (Δt)2 ,
=
∂t
2 ∂x2
since u(t, x) solves the wave equation. Therefore,
u1,m = u(Δt, xm ) ≈ u(0, xm ) +

∂u
c2 ∂ 2 u
(0, xm )Δt +
(0, xm )(Δt)2
∂t
2 ∂x2

c2 
f (xm )(Δt)2
2
c2 (fm+1 − 2 fm + fm−1 )(Δt)2
≈ fm + gm Δt +
,
2 (Δx)2
= f (xm ) + g(xm ) Δt +

where the last line, which employs the ﬁnite diﬀerence approximation (5.5) to the second derivative, can be used if the explicit formula for f  (x) is either not known or too
complicated to evaluate directly. Therefore, we initiate the scheme by setting
u1,m = 12 σ 2 fm+1 + (1 − σ 2 )fm + 12 σ 2 fm−1 + gm Δt,

(5.62)

or, in vectorial form,
u(0) = f ,

u(1) = 12 B u(0) + g Δt + 12 b(0) ,
T

(5.63)

T

where f = f1 , f2 , . . . , fn−1 , g = g1 , g2 , . . . , gn−1 , are the sampled values of the
initial data. This serves to maintain the desired second-order accuracy of the scheme.
Example 5.6. Consider the particular initial value problem
2

utt = uxx ,

u(0, x) = e− 400 (x−.3) ,

ut (0, x) = 0,

u(t, 0) = u(t, 1) = 0,

0 ≤ x ≤ 1,
t ≥ 0,

subject to homogeneous Dirichlet boundary conditions on the interval [ 0, 1 ]. The initial
data is a fairly concentrated hump centered at x = .3. As time progresses, we expect the
initial hump to split into two half-sized humps, which then collide with the ends of the
interval, reversing direction and orientation.

204

5 Finite Diﬀerences

t=0

t = .1

t = .3

t = .4
Figure 5.9.

Numerically stable waves.

t=0

t = .04

t = .12

t = .16
Figure 5.10.

Numerically unstable waves.

t = .2



t = .5

t = .08



t = .2

For our numerical approximation, let us use a space discretization consisting of 90
1
equally spaced points, and so Δx = 90
= .0111 . . . . If we choose a time step of Δt = .01,
whereby σ = .9, then we obtain a reasonably accurate solution over a fairly long time
range, as plotted in Figure 5.9. On the other hand, if we double the time step, setting
Δt = .02, so σ = 1.8, then, as shown in Figure 5.10, we induce an instability that eventually

5.4 Numerical Algorithms for the Wave Equation

205

x

x

t

t

Unstable

Stable
Figure 5.11.

The CFL condition for the wave equation.

overwhelms the numerical solution. Thus, the preceding numerical scheme appears to be
only conditionally stable.
Stability analysis proceeds along the same lines as in the ﬁrst-order case. The CFL
condition requires that the characteristics emanating from a node (tj , xm ) remain, for times
0 ≤ t ≤ tj , in its numerical domain of dependence, which, for our particular numerical
scheme, is the same triangle
.
T(tj ,xm ) = (t, x) 0 ≤ t ≤ tj , xm − tj + t ≤ x ≤ xm + tj − t ,
now plotted in Figure 5.11. Since the characteristics are the lines of slope ± c, the CFL
condition is the same as in (5.48):
c Δt
Δx
≤ 1,
or, equivalently,
0<c≤
.
(5.64)
Δx
Δt
The resulting stability criterion explains the observed diﬀerence between the numerically
stable and unstable cases.
However, as we noted above, the CFL condition is, in general, only necessary for stability of the numerical scheme; suﬃciency requires that we perform a von Neumann stability
analysis. To this end, we specialize the calculation to a single complex exponential e i k x .
After one time step, the scheme will have the eﬀect of multiplying it by the magniﬁcation
factor λ = λ(k), after another time step by λ2 , and so on. To determine λ, we substitute
the relevant sampled exponential values
σ=

uj−1,m = e i k xm ,

uj,m = λ e i k xm ,

uj+1,m = λ2 e i k xm ,

(5.65)

into the scheme (5.56). After canceling the common exponential, we ﬁnd that the magniﬁcation factor satisﬁes the following quadratic equation:
λ2 = 2 − 4 σ 2 sin2 12 k Δx

λ − 1,

206

5 Finite Diﬀerences

whence
λ=α±



α2 − 1 ,

where

α = 1 − 2 σ 2 sin2 12 k Δx .

(5.66)

Thus, there are two diﬀerent magniﬁcation factors associated with each complex exponential — which is, in fact, a consequence of the scheme being of second order. Stability
requires that both be ≤ 1 in modulus. Now, if the CFL condition (5.64) holds, then
| α | ≤ 1, which implies that both magniﬁcation factors (5.66) are complex numbers of
modulus | λ | = 1, and thus the numerical scheme satisﬁes the stability criterion (5.26).
On the other hand, if σ > 1, then α < −1 over a range of values of k, which implies that
the two magniﬁcation factors (5.66) are both real and one of them is < −1, thus violating
the stability criterion. Consequently, the CFL condition (5.64) does indeed distinguish
between the stable and unstable ﬁnite diﬀerence schemes for the wave equation.

Exercises
5.4.1. Suppose you are asked to numerically approximate the solution to the initial-boundary

value problem
1 − 2| x − 1 |, 12 ≤ x ≤ 32 ,
utt = 64 uxx , u(t, 0) = u(t, 3) = 0, u(0, x) =
ut (0, x) = 0,
0,
otherwise,
on the interval 0 ≤ x ≤ 3, using (5.56) with space step size Δx = .1. (a) What range of
time steps Δt are allowed? (b) Test your answer by implementing the numerical solution
for one value of Δt in the allowable range and one value outside. Discuss what you observe
in your numerical solutions. (c) In the stable range, compare your numerical solution with
that obtained using the smaller step size Δx = .01 and a suitable time step Δt.
5.4.2. Solve Exercise 5.4.1 for the boundary value problem
utt = 64 uxx ,

u(t, 0) = 0 = u(t, 3),

u(0, x) = 0,

ut (0, x) =



1 − 2| x − 1 |,
0,

5.4.3. Solve the following initial-boundary value problem




utt = 9 uxx , u(t, 0) = u(t, 1) = 0, u(0, x) = 12 +  x − 14  −  2 x − 34  ,

1
3
2 ≤x≤ 2,

otherwise.

ut (0, x) = 0,

on the interval 0 ≤ x ≤ 1, using the numerical scheme (5.56) with space step sizes Δx =
.1, .01 and .001 and suitably chosen time steps. Discuss which features of the solution can
be observed in your numerical approximations.
5.4.4. (a) Use a numerical integrator with space step size Δx = .05 to solve the periodically
forced boundary value problem
utt = uxx ,
u(0, x) = ut (0, x) = 0,
u(t, 0) = sin t,
u(t, 1) = 0.
Is your solution periodic? (b) Repeat the computation using the alternative boundary
condition u(t, 0) = sin π t. Discuss any observed diﬀerences between the two problems.
5.4.5. (a) Design an explicit numerical scheme for solving the initial-boundary value problem
utt = c2 uxx + F (t, x), u(t, 0) = u(t, 1) = 0, u(0, x) = f (x), ut (0, x) = g(x), 0 ≤ x ≤ 1,
for the wave equation with an external forcing term F (t, x). Clearly state any stability
conditions that need to be imposed on the time and space step sizes.


(b) Test your scheme on the particular case c = 14 , F (t, x) = 3 sign x − 12 sin π t, f (x) ≡
g(x) ≡ 0, using space step sizes Δx = .05 and .01, and suitably chosen time steps.
5.4.6. Let β > 0. (a) Design a ﬁnite diﬀerence scheme for approximating the solution to the
initial-boundary value problem
u(t, 0) = u(t, 1) = 0,
u(0, x) = f (x),
ut (0, x) = g(x),
utt + β ut = c2 uxx ,

5.5 Finite Diﬀerence Algorithms for the Laplace and Poisson Equations

207

for the damped wave equation on the interval 0 ≤ x ≤ 1. (b) Discuss the stability of your
scheme. What choice of step sizes will ensure stability? (c) Test your scheme with c = 1,
2

β = 1, using the initial data f (x) = e− (x−.7) , g(x) = 0.

5.5 Finite Diﬀerence Algorithms for
the Laplace and Poisson Equations
Finally, let us discuss the implementation of ﬁnite diﬀference numerical schemes for elliptic
boundary value problems. We concentrate on the simplest cases: the two-dimensional
Laplace and Poisson equations. The basic issues are already apparent in this particular
context, and extensions to more general equations, higher dimensions, and higher-order
schemes are all reasonably straightforward. In Chapter 10, we will present a competitor
— the renowned ﬁnite element method — which, while relying on more sophisticated
mathematical machinery, enjoys several advantages, including more immediate adaptability
to variable mesh sizes and more sophisticated geometries.
For speciﬁcity, we concentrate on the Dirichlet boundary value problem
− Δu = − uxx − uyy = f (x, y),
u(x, y) = g(x, y),

for

(x, y) ∈ Ω,
(x, y) ∈ ∂Ω,

(5.67)

on a bounded planar domain Ω ⊂ R 2 . The ﬁrst step is to discretize the domain Ω by
constructing a rectangular mesh. Thus, the ﬁnite diﬀerence method is particularly suited
to domains whose boundary lines up with the coordinate axes; otherwise, the mesh nodes
do not, generally, lie exactly on ∂Ω, making the approximation of the boundary data more
challenging — although not insurmountable.
For simplicity, let us study the case in which
Ω = { a < x < b, c < y < d }
is a rectangle. We introduce a regular rectanglar mesh, with x and y spacings given,
respectively, by
b−a
c−d
Δx =
,
Δy =
,
m
n
for positive integers m, n. Thus, the interior of the rectangle contains (m−1)(n−1) interior
nodes
0 < i < m, 0 < j < n.
for
(xi , yj ) = (a + i Δx, c + j Δy)
In addition, the 2 m + 2 n boundary nodes (x0 , yj ) = (a, yj ), (xm , yj ) = (b, yj ), (xi , y0 ) =
(xi , c), (xi , yn ) = (xi , d), lie on the boundary of the rectangle.
At each interior node, we employ the centered diﬀerence formula (5.5) to approximate
the relevant second-order derivatives:
u(xi+1 , yj ) − 2 u(xi , yj ) + u(xi−1 , yj )
∂2u
(xi , yj ) =
+ O (Δx)2 ,
2
∂x
(Δx)2
u(xi , yj+1) − 2 u(xi , yj ) + u(xi , yj−1 )
∂2u
(xi , yj ) =
+ O (Δy)2 .
2
∂y
(Δy)2

(5.68)

208

5 Finite Diﬀerences

Substituting these ﬁnite diﬀerence formulae into the Poisson equation produces the linear
system
−

ui+1,j − 2 ui,j + ui−1,j
ui,j+1 − 2 ui,j + ui,j−1
−
= fi,j ,
2
(Δx)
(Δy)2

i = 1, . . . , m − 1,
j = 1, . . . , n − 1,

(5.69)

in which ui,j denotes our numerical approximation to the solution values u(xi , yj ) at the
nodes, while fi,j = f (xi , yj ). If we set
ρ=

Δx
,
Δy

(5.70)

then (5.69) can be rewritten in the form
2 (1 + ρ2 )ui,j − (ui−1,j + ui+1,j ) − ρ2 (ui,j−1 + ui,j+1) = (Δx)2 fi,j ,
i = 1, . . . , m − 1, j = 1, . . . , n − 1.

(5.71)

Since both ﬁnite diﬀerence approximations (5.68) are of second order, one should choose
Δx and Δy to be of comparable size, thus keeping ρ around 1.
The linear system (5.71) forms the ﬁnite diﬀerence approximation to the Poisson
equation at the interior nodes. It is supplemented by the discretized Dirichlet boundary
conditions
ui,0 = gi,0 ,
ui,n = gi,n ,
i = 0, . . . , m,
(5.72)
um,j = gm,j ,
j = 0, . . . , n.
u0,j = g0,j ,
These boundary values can be substituted directly into the system, making (5.71) a system
of (m−1)(n−1) linear equations involving the (m−1)(n−1) unknowns ui,j for 1 ≤ i ≤ m−1,
1 ≤ j ≤ n − 1. We impose some convenient ordering for these entries, e.g., from left to
right and then bottom to top, forming the column vector of unknowns
w = (w1 , w2 , . . . , w(m−1)(n−1) )T
= (u1,1 , u2,1 , . . . , um−1,1 , u1,2 , u2,2 , . . . , um−1,2 , u1,3 , . . . , um−1,n−1 )T .

(5.73)

The combined linear system (5.71–72) can then be rewritten in matrix form
A w = f ,

(5.74)

where the right-hand side is obtained by combining the column vector f = ( . . . fi,j . . . )T
with the boundary data provided by (5.72) according to where they appear in the system.
The implementation will become clearer once we work through a small-scale example.
Example 5.7. To better understand how the process works, let us look at the case
in which Ω = { 0 < x < 1, 0 < y < 1 } is the unit square. In order to write everything in
full detail, we start with a very coarse mesh with Δx = Δy = 14 ; see Figure 5.12. Thus
m = n = 4, resulting in a total of nine interior nodes. In this case, ρ = 1, and hence the

5.5 Finite Diﬀerence Algorithms for the Laplace and Poisson Equations

Figure 5.12.

209

Square mesh with Δx = Δy = 14 .

ﬁnite diﬀerence system (5.71) consists of the following nine equations:
1
− u1,0 − u0,1 + 4 u1,1 − u2,1 − u1,2 = 16
f1,1 ,
1
f2,1 ,
− u2,0 − u1,1 + 4 u2,1 − u3,1 − u2,2 = 16
1
f3,1 ,
− u3,0 − u2,1 + 4 u3,1 − u4,1 − u3,2 = 16
1
f1,2 ,
− u1,1 − u0,2 + 4 u1,2 − u2,2 − u1,3 = 16
1
f2,2 ,
− u2,1 − u1,2 + 4 u2,2 − u3,2 − u2,3 = 16

(5.75)

1
− u3,1 − u2,2 + 4 u3,2 − u4,2 − u3,3 = 16
f3,2 ,
1
− u1,2 − u0,3 + 4 u1,3 − u2,3 − u1,4 = 16 f1,3 ,
1
f2,3 ,
− u2,2 − u1,3 + 4 u2,3 − u3,3 − u2,4 = 16
1
f3,3 .
− u3,2 − u2,3 + 4 u3,3 − u4,3 − u3,4 = 16

(Note that the values at the four corner nodes, u0,0 , u4,0 , u0,4 , u4,4 , do not appear.) The
boundary data imposes the additional conditions (5.72), namely
u0,1 = g0,1 ,

u0,2 = g0,2 ,

u0,3 = g0,3 ,

u1,0 = g1,0 ,

u2,0 = g2,0 ,

u3,0 = g3,0 ,

u4,1 = g4,1 ,

u4,2 = g4,2 ,

u4,3 = g4,3 ,

u1,4 = g1,4 ,

u2,4 = g2,4 ,

u3,4 = g3,4 .

The system (5.75) can be written in matrix form A w = 
f , where
⎛

4
⎜ −1
⎜
⎜ 0
⎜
⎜ −1
⎜
A=⎜ 0
⎜
⎜ 0
⎜
⎜ 0
⎝
0
0

−1
0
4 −1
−1
4
0
0
−1
0
0 −1
0
0
0
0
0
0

−1
0
0
4
−1
0
−1
0
0

0
−1
0
−1
4
−1
0
−1
0

0
0
−1
0
−1
4
0
0
−1

0
0
0
0
0
0
−1
0
0 −1
0
0
4 −1
−1
4
0 −1

⎞
0
0⎟
⎟
0⎟
⎟
0⎟
⎟
0 ⎟,
⎟
−1 ⎟
⎟
0⎟
⎠
−1
4

(5.76)

210

and

5 Finite Diﬀerences

⎛ 1

⎞
⎞ ⎛
u1,1
w1
⎜ w2 ⎟ ⎜ u2,1 ⎟
⎟
⎟ ⎜
⎜
⎜ w3 ⎟ ⎜ u3,1 ⎟
⎟
⎟ ⎜
⎜
⎜ w4 ⎟ ⎜ u1,2 ⎟
⎟
⎟ ⎜
⎜
w = ⎜ w5 ⎟ = ⎜ u2,2 ⎟,
⎟
⎟ ⎜
⎜
⎜ w6 ⎟ ⎜ u3,2 ⎟
⎟
⎟ ⎜
⎜
⎜ w7 ⎟ ⎜ u1,3 ⎟
⎠
⎠ ⎝
⎝
w8
u2,3
w9
u3,3
⎛

⎞

16 f1,1 + g1,0 + g0,1
1
⎜
⎟
⎜ 1 16 f2,1 + g2,0
⎟
⎜ f +g +g ⎟
3,0
4,1 ⎟
⎜ 16 3,1
⎜
⎟
1
⎜
⎟
16 f1,2 + g0,2
⎜
⎟
1

f =⎜
f
⎟.
2,2
16
⎜
⎟
1
⎜
⎟
f
+
g
4,2
⎜ 1 16 3,2
⎟
⎜ f +g +g ⎟
0,3
1,4 ⎟
⎜ 16 1,3
1
⎝
⎠
16 f2,3 + g2,4
1
16 f3,3 + g4,3 + g3,4

Note that the known boundary values, namely ui,j = gi,j when i or j equals 0 or 4, have
been incorporated into the right-hand side 
f of the ﬁnite diﬀerence linear system (5.74).
The resulting linear system is easily solved by Gaussian Elimination, [89]. Finer meshes
lead to correspondingly larger linear systems, all endowed with a common overall structure,
as discussed below.
For example, the function
u(x, y) = y sin( πx)
solves the particular boundary value problem
− Δu = π 2 y sin( πx), u(x, 0) = u(0, y) = u(1, y) = 0, u(x, 1) = sin( πx), 0 < x, y < 1.
Setting up and solving the linear system (5.75) produces the ﬁnite diﬀerence solution values
u1,1 = .1831,

u2,1 = .2589,

u3,1 = .1831,

u1,2 = .3643,

u2,2 = .5152,

u3,2 = .3643,

u1,3 = .5409,

u2,3 = .7649,

u3,3 = .5409,

leading to the numerical approximation plotted in the ﬁrst graph† of Figure 5.13. The
maximal error between the numerical and exact solution values is .01520, which occurs at
the center of the square. In the second and third graphs, the mesh spacing is successively
reduced by half, so there are, respectively, m = n = 8 and 16 nodes in each coordinate
direction. The corresponding maximal numerical errors at the nodes are .004123 and
.001035. Observe that halving the step size reduces the error by a factor of 14 , which is
consistent with the numerical scheme being of second order.
Remark : The preceding test is a particular instance of the method of manufactured
solutions, in which one starts with a preselected function that almost certainly is not
a solution to the exact problem at hand. Nevertheless, substituting this function into
the diﬀerential equation and the relevant initial and/or boundary conditions leads to an
inhomogeneous problem of the same character as the original. After running the numerical
scheme on the modiﬁed problem, one can test for accuracy by comparing the numerical
output with the preselected function.

†
We are using ﬂat triangles to interpolate the nodal data. Smoother interpolation schemes,
e.g., splines, [ 102 ], will produce a more realistic reproduction of the analytic solution graph.

5.5 Finite Diﬀerence Algorithms for the Laplace and Poisson Equations

Δx = Δy = .25
Figure 5.13.

Δx = Δy = .125

211

Δx = Δy = .0625

Finite diﬀerence solutions to a Poisson boundary value problem.

Solution Strategies
The linear algebraic system resulting from a ﬁnite diﬀerence discretization can be rather
large, and it behooves us to devise eﬃcient solution strategies. The general ﬁnite diﬀerence
coeﬃcient matrix A has a very structured form, which can already be inferred from the
very simple case (5.76). When the underlying domain is a rectangle, it assumes a block
tridiagonal form
⎛

Bρ

⎜ −ρ2 I
⎜
⎜
⎜
A=⎜
⎜
⎜
⎜
⎝

⎞

−ρ2 I

⎟
⎟
⎟
−ρ2 I
⎟
Bρ
−ρ2 I
⎟,
⎟
..
..
..
⎟
.
.
.
⎟
Bρ
− ρ2 I ⎠
−ρ2 I
Bρ

−ρ2 I

−ρ2 I

(5.77)

Bρ

where I is the (m − 1) × (m − 1) identity matrix, while
⎛ 2 (1 + ρ2 )
⎜
⎜
⎜
⎜
Bρ = ⎜
⎜
⎜
⎜
⎝

−1

−1
2 (1 + ρ2 )
−1

⎞
−1
2 (1 + ρ2 )
−1

−1
2 (1 + ρ2 )
..

−1
..

.
−1

..
.
.
−1
2 (1 + ρ2 )
−1
2 (1 + ρ2 )

⎟
⎟
⎟
⎟
⎟ (5.78)
⎟
⎟
⎟
⎠

is itself an (m − 1) × (m − 1) tridiagonal matrix. (Here and below, all entries not explicitly
indicated are zero.) There are n − 1 blocks in both the row and column directions.
When the ﬁnite diﬀerence linear system is of moderate size, it can be eﬃciently solved
by Gaussian Elimination, which eﬀectively factorizes A = L U into a product of lower
and upper triangular matrices. (This follows since A is symmetric and nonsingular, as
guaranteed by Theorem 5.8 below.) In the present case, the factors are block bidiagonal

212

5 Finite Diﬀerences

matrices:

⎞

⎛ I
⎜ L1
⎜
⎜
L=⎜
⎜
⎜
⎝
⎛U
⎜
⎜
⎜
U =⎜
⎜
⎜
⎝

I
L2

1

I
..
.

..

.
Ln−3

2

−ρ I
U2

⎟
⎟
⎟
⎟,
⎟
⎟
⎠

− ρ2 I
U3

I
Ln−2
− ρ2 I
..
.

I

⎞

..

.
Un−2

− ρ2 I
Un−1

(5.79)

⎟
⎟
⎟
⎟,
⎟
⎟
⎠

where the individual blocks are again of size (m − 1) × (m − 1). Indeed, multiplying out the
matrix product L U and equating the result to (5.77) leads to the iterative matrix system
U1 = Bρ ,

Lj = − ρ2 Uj−1 ,

Uj+1 = Bρ + ρ2 Lj ,

j = 1, . . . , n − 2,

(5.80)

which produces the individual blocks.
With the L U factors in place, we can apply Forward and Back Substitution to solve
the block tridiagonal linear system A w = 
f by solving the block lower and upper triangular
systems
(5.81)
Lz = 
f,
U w = z.
In view of the forms (5.79) of L and U , if we write
⎞
w(1)
⎜ w(2) ⎟
⎟,
..
w=⎜
⎠
⎝
.
⎛

w(n−1)

⎛

⎞
z(1)
⎜ z(2) ⎟
⎟
z=⎜
⎝ ... ⎠,
z(n−1)

⎛ (1) ⎞
f
(2) ⎟
⎜ 
f

⎟
f =⎜
⎝ ... ⎠,

f (n−1)

so that each w(j) , z(j) , f (j) , is a vector with m − 1 entries, then we must successively solve
f (1) ,
z(1) = 

z(j+1) = 
f (j+1) − Lj z(j) ,

j = 1, 2, . . . , n − 2,

Un−1 w(n−1) = z(n−1) ,

Uk w(k) = z(k) − ρ2 w(k+1) ,

k = n − 2, n − 3, . . . , 1,

(5.82)

in the prescribed order. In view of the identiﬁcation of Lk with − ρ2 times the inverse of
Uk , the last set of equations in (5.82) is perhaps better written as

w(k) = Lk w(k+1) − ρ−2 z(k) ,

k = n − 2, n − 3, . . . , 1.

(5.83)

As the number of nodes becomes large, the preceding elimination/factorization approach to solving the linear system becomes increasingly ineﬃcient, and one often switches
to an iterative solution method such as Gauss–Seidel, Jacobi, or, even better, Successive
Over–Relaxation (SOR); indeed, SOR was originally designed to speed up the solution
of the large-scale linear systems arising from the numerical solution of elliptic partial
diﬀerential equations. Detailed discussions of iterative matrix methods can be found in

5.5 Finite Diﬀerence Algorithms for the Laplace and Poisson Equations

213

[89; Chapter 10] and [118]. For the SOR method, a good choice for the relaxation parameter is
4
/
ω=
.
(5.84)
2
2
2 + 4 − cos (π/m) − cos (π/n)
Iterative solution methods are even more attractive in dealing with irregular domains,
whose ﬁnite diﬀerence coeﬃcient matrix, while still sparse, is less structured than in the
rectangular case, and hence less amenable to fast Gaussian Elimination algorithms.
Finally, let us address the question of unique solvability of the ﬁnite diﬀerence linear
system obtained by discretization of the Poisson equation on a bounded domain subject to
Dirichlet boundary conditions. As in the Uniqueness Theorem 4.10 for the original boundary value, this will follow from an easily established Maximum Principle for the discrete
system that directly mimics the Laplace equation maximum principle of Theorem 4.9.
Theorem 5.8. Let Ω be a bounded domain. Then the ﬁnite diﬀerence linear system
(5.74) has a unique solution.
Proof : The result will follow if we can prove that the only solution to the corresponding
homogeneous linear system A w = 0 is the trivial solution w = 0. The homogeneous
system corresponds to discretizing the Laplace equation subject to zero Dirichlet boundary
conditions.
Now, in view of (5.71), each equation in the homogeneous linear system can be written
in the form
ui−1,j + ui+1,j + ρ2 ui,j−1 + ρ2 ui,j+1
(5.85)
ui,j =
.
2 (1 + ρ2 )
If ρ = 1, then (5.85) says that the value of ui,j at the node (xi , yj ) is equal to the average
of the values at the four neighboring nodes. For general ρ, it says that ui,j is a weighted
average of the four neighboring values. In either case, the value of ui,j must lie strictly
between the maximum and minimum values of ui−1,j , ui+1,j , ui,j−1 and ui,j+1 — unless
all these values are the same, in which case ui,j also has the same value. This observation
suﬃces to establish a Maximum Principle for the ﬁnite diﬀerence system for the Laplace
equation — namely, that its solution cannot achieve a local maximum or minimum at an
interior node.
Now suppose that the homogeneous ﬁnite diﬀerence system A w = 0 for the domain
has a nontrivial solution w = 0. Let ui,j = wk be the maximal entry of this purported
solution. The Maximum Principle requires that all four of its neighboring values must have
the same maximal value. But then the same argument applies to the neighbors of those
entries, to their neighbors, and so on. Eventually one of the neighbors is at a boundary
node, but, since we are dealing with the homogeneous Dirichlet boundary value problem,
its value is zero. This immediately implies that all the entries of w must be zero, which is
a contradiction.
Q.E.D.
Rigorously establishing convergence of the ﬁnite diﬀerence solution to the analytic
solution to the boundary value problem as the step size goes to zero will not be discussed
here, and we refer the reader to [6, 80] for precise results and proofs.

214

5 Finite Diﬀerences

Exercises
♠ 5.5.1. Solve the Dirichlet problem Δu = 0, u(x, 0) = sin3 x, u(x, π) = 0, u(0, y) = 0,
u(π, y) = 0, numerically using a ﬁnite diﬀerence scheme. Compare your approximation with
the solution you obtained in Exercise 4.3.10(a).
♠ 5.5.2. Solve the Dirichlet problem Δu = 0, u(x, 0) = x, u(x, 1) = 1 − x, u(0, y) = y, u(1, y) =
1 − y, numerically via ﬁnite diﬀerences. Compare your approximation with the solution you
obtained in Exercise 4.3.12(d).
♠ 5.5.3. Consider the Dirichlet boundary value problem Δu = 0 u(x, 0) = sin x, u(x, π) = 0,
u(0, y) = 0, u(π, y) = 0, on the square { 0 < x, y < π }. (a) Find the exact solution. (b) Set
up and solve the ﬁnite diﬀerence equations based on a square mesh with m = n = 2 squares
on each side of
 the full square. How close is this value to the exact solution at the center of
the square: u 12 π, 12 π ? (c) Repeat part (b) for m = n = 4 squares per side. Is the value
of your approximation at the center of the unit square closer
to the
true solution? (d) Use


a computer to ﬁnd a ﬁnite diﬀerence approximation to u 21 π, 12 π using m = n = 8 and
16 squares per side. Is your approximation converging to the exact solution as the mesh
becomes ﬁner and ﬁner? Is the convergence rate consistent with the order of the ﬁnite difference approximation?
♠ 5.5.4. (a) Use ﬁnite diﬀerences to approximate a solution to the Helmholtz boundary value
problem Δu = u, u(x, 0) = u(x, 1) = u(0, y) = 0, u(1, y) = 1, on the unit square
0 < x, y < 1. (b) Use separation of variables to construct a series solution. Do your analytic and numerical solutions match? Explain any discrepancies.
♠ 5.5.5. A drum is in the shape of an L, as in the accompanying ﬁgure, whose
short sides all have length 1. (a) Use a ﬁnite diﬀerence scheme with mesh
spacing Δx = Δy = .1 to ﬁnd and graph the equilibrium conﬁguration
when the drum is subject to a unit upwards force while all its sides are
ﬁxed to the (x, y)–plane. What is the maximal deﬂection, and at which
point(s) does it occur? (b) Check the accuracy of your answer in part (a)
by reducing the step size by half: Δx = Δy = .05.
♣ 5.5.6. A metal plate has the shape of a 3 cm square with a 1 cm square hole cut out of the
middle. The plate is heated by making the inner edge have temperature 100◦ while keeping the outer edge at 0◦ . (a) Find the (approximate) equilibrium temperature using ﬁnite
diﬀerences with a mesh width of Δx = Δy = .5 cm. Plot your approximate solution using a three-dimensional graphics program. (b) Let C denote the square contour lying midway between the inner and outer square boundaries of the plate. Using your ﬁnite diﬀerence approximation, determine at what point(s) on C the temperature is (i ) minimized;
(ii ) maximimized; (iii ) equal to the average of the two boundary temperatures.
(c) Repeat part (a) using a smaller mesh width of Δx = Δy = .2. How much does this
aﬀect your answers in part (b)?
♣ 5.5.7. Answer Exercise 5.5.6 when the plate is additionally subjected to a constant heat source
f (x, y) = 600 x + 800 y − 2400.
♠ 5.5.8. (a) Explain how to adapt the ﬁnite diﬀerence method to a mixed boundary value
problem on a rectangle with inhomogeneous Neumann conditions. Hint: Use a one-sided
diﬀerence formula of the appropriate order to approximate the normal derivative at the
boundary. (b) Apply your method to the problem
∂u
(0, y) = y(1 − y),
u(1, y) = 0,
Δu = 0,
u(x, 0) = 0,
u(x, 1) = 0,
∂x
using mesh sizes Δx = Δy = .1, .01, and .001. Compare your answers. (c) Solve the
boundary value problem via separation of variables, and compare the value of the solution
and the numerical approximations at the center of the square.

Chapter 6

Generalized Functions and Green’s Functions

Boundary value problems, involving both ordinary and partial diﬀerential equations, can
be proﬁtably viewed as the inﬁnite-dimensional function space versions of ﬁnite-dimensional systems of linear algebraic equations. As a result, linear algebra not only provides
us with important insights into their underlying mathematical structure, but also motivates
both analytical and numerical solution techniques. In the present chapter, we develop the
method of Green’s functions, pioneered by the early-nineteenth-century self-taught English
mathematician (and miller!) George Green, whose famous Theorem you already encountered in multivariable calculus. We begin with the simpler case of ordinary diﬀerential
equations, and then move on to solving the two-dimensional Poisson equation, where the
Green’s function provides a powerful alternative to the method of separation of variables.
For inhomogeneous linear systems, the basic Superposition Principle says that the
response to a combination of external forces is the self-same combination of responses to the
individual forces. In a ﬁnite-dimensional system, any forcing function can be decomposed
into a linear combination of unit impulse forces, each applied to a single component of the
system, and so the full solution can be obtained by combining the solutions to the individual
impulse problems. This simple idea will be adapted to boundary value problems governed
by diﬀerential equations, where the response of the system to a concentrated impulse
force is known as the Green’s function. With the Green’s function in hand, the solution
to the inhomogeneous system with a general forcing function can be reconstructed by
superimposing the eﬀects of suitably scaled impulses. Understanding this construction will
become increasingly important as we progress to partial diﬀerential equations, where direct
analytic solution techniques are far harder to come by.
The obstruction blocking a direct implementation of this idea is that there is no
ordinary function that represents an idealized concentrated impulse! Indeed, while this
approach was pioneered by Green and Cauchy in the early 1800s, and then developed
into an eﬀective computational tool by Heaviside in the 1880s, it took another 60 years
before mathematicians were able to develop a completely rigorous theory of generalized
functions, also known as distributions. In the language of generalized functions, a unit
impulse is represented by a delta function.† While we do not have the analytic tools to
completely develop the mathematical theory of generalized functions in its full, rigorous
glory, we will spend the ﬁrst section learning the basic concepts and developing the practical
computational skills, including Fourier methods, required for applications. The second
†
Warning: We follow common practice and refer to the “delta distribution” as a function,
even though, as we will see, it is most deﬁnitely not a function in the usual sense.

P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_6, © Springer International Publishing Switzerland 2014

215

216

6 Generalized Functions and Green’s Functions

section will discuss the method of Green’s functions in the context of one-dimensional
boundary value problems governed by ordinary diﬀerential equations. In the ﬁnal section,
we develop the Green’s function method for solving basic boundary value problems for the
two-dimensional Poisson equation, which epitomizes the class of planar elliptic boundary
value problems.

6.1 Generalized Functions
Our goal is to solve inhomogeneous linear boundary value problems by ﬁrst determining
the eﬀect of a concentrated impulse force. The response to a general forcing function is
then found by linear superposition. But before diving in, let us ﬁrst review the relevant
constructions in the case of linear systems of algebraic equations.
T
Consider a system of n linear equations in n unknowns† u = ( u1 , u2 , . . . , un ) , written
in matrix form
Au = f.
(6.1)
Here A is a ﬁxed n × n matrix, assumed to be nonsingular, which ensures the existence
T
of a unique solution u for any choice of right-hand side f = ( f1 , f2 , . . . , fn ) ∈ R n . We
regard the linear system (6.1) as representing the equilibrium equations of some physical
system, e.g., a system of masses interconnected by springs. In this context, the right hand
side f represents an external forcing, so that its ith entry, fi , represents the amount of force
exerted on the ith mass, while the ith entry of the solution vector, ui , represents the ith
mass’ induced displacement.
Let
⎛ ⎞
⎛ ⎞
⎛ ⎞
1
0
0
⎜0⎟
⎜1⎟
⎜0⎟
⎜ ⎟
⎜ ⎟
⎜ ⎟
⎜0⎟
⎜0⎟
⎜0⎟
⎟
⎟
⎟
e2 = ⎜
...
en = ⎜
(6.2)
e1 = ⎜
⎜ ... ⎟,
⎜ ... ⎟,
⎜ ... ⎟,
⎜ ⎟
⎜ ⎟
⎜ ⎟
⎝0⎠
⎝0⎠
⎝0⎠
0

0

1

denote the standard basis vectors of R , so that ej has a single 1 in its j th entry and all
other entries 0. We interpret each ej as a concentrated unit impulse force that is applied
solely to the j th mass in our physical system. Let uj = (uj,1 , . . . , uj,n )T be the induced
response of the system, that is, the solution to
n

A u j = ej .

(6.3)

Let us suppose that we have calculated the response vectors u1 , . . . , un to each such impulse
force. We can express any other force vector as a linear combination,
⎛ ⎞
f1
⎜ f2 ⎟
⎜ ⎟
f = ⎜ .. ⎟ = f1 e1 + f2 e2 + · · · + fn en ,
(6.4)
⎝ . ⎠
fn
†
All vectors are column vectors, but we sometimes write the transpose, which is a row vector,
to save space.

6.1 Generalized Functions

217

of the impulse forces. The Superposition Principle of Theorem 1.7 then implies that the
solution to the inhomogeneous system (6.1) is the selfsame linear combination of the individual impulse responses:
u = f1 u1 + f2 u2 + · · · + fn un .

(6.5)

Thus, knowing how the linear system responds to each impulse force allows us to immediately calculate its response to a general external force.
Remark : The alert reader will recognize that u1 , . . . , un are the columns of the inverse
matrix, A−1 , and so formula (6.5) is, in fact, reconstructing the solution to the linear system
(6.1) by inverting its coeﬃcient matrix: u = A−1 f . Thus, this observation is merely a
restatement of a standard linear algebraic system solution technique.
The Delta Function
The aim of this chapter is to adapt the preceding algebraic solution technique to boundary
value problems. Suppose we want to solve a linear boundary value problem governed by
an ordinary diﬀerential equation on an interval a < x < b, the boundary conditions being
imposed at the endpoints. The key issue is how to characterize an impulse force that is
concentrated at a single point.
In general, a unit impulse at position a < ξ < b will be described by something called
the delta function, and denoted by δξ (x). Since the impulse is supposed to be concentrated
solely at x = ξ, our ﬁrst requirement is
δξ (x) = 0

for

x = ξ.

(6.6)

Moreover, since the delta function represents a unit impulse, we want the total amount
of force to be equal to one. Since we are dealing with a continuum, the total force is
represented by an integral over the entire interval, and so we also require that the delta
function satisfy
 b
(6.7)
δξ (x) dx = 1,
provided
a < ξ < b.
a

Alas, there is no bona ﬁde function that enjoys both of the required properties! Indeed,
according to the basic facts of Riemann (or even Lebesgue) integration, two functions that
are the same everywhere except at a single point have exactly the same integral, [96, 98].
Thus, since δξ is zero except at one point, its integral should be 0, not 1. The mathematical
conclusion is that the two requirements, (6.6–7) are inconsistent!
This unfortunate fact stopped mathematicians dead in their tracks. It took the imagination of a British engineer, Oliver Heaviside, who was not deterred by the lack of rigorous
justiﬁcation, to start utilizing delta functions in practical applications — with remarkable
eﬀect. Despite his success, Heaviside was ridiculed by the mathematicians of his day, and
eventually succumbed to mental illness. But, some thirty years later, the great British
theoretical physicist Paul Dirac resurrected the delta function for quantum-mechanical applications, and this ﬁnally made the mathematicians sit up and take notice. (Indeed, the
term “Dirac delta function” is quite common, even though Heaviside should rightly have
priority.) In 1944, the French mathematician Laurent Schwartz ﬁnally established a rigorous theory of distributions that incorporated such useful but nonstandard objects, [103].
Thus, to be more accurate, we should really refer to the delta distribution; however, we

218

6 Generalized Functions and Green’s Functions

will retain the more common, intuitive designation “delta function” throughout. It is beyond the scope of this introductory text to develop a fully rigorous theory of distributions.
Rather, in the spirit of Heaviside, we shall concentrate on learning, through practice with
computations and applications, how to make eﬀective use of these exotic mathematical
creatures.
There are two possible ways to introduce the delta distribution. Both are important
and worth understanding.
Method #1. Limits: The ﬁrst approach is to regard the delta function δξ (x) as a
limit of a sequence of ordinary smooth functions† gn (x). These will represent progressively
more and more concentrated unit forces, which, in the limit, converge to the desired unit
impulse concentrated at a single point, x = ξ. Thus, we require
lim gn (x) = 0,

x = ξ,

(6.8)

for all n.

(6.9)

n→∞

while the total amount of force remains ﬁxed at
 b
gn (x) dx = 1
a

On a formal level, the limit “function”
δξ (x) = lim gn (x)
n→∞

will satisfy the key properties (6.6–7).
An explicit example of such a sequence is provided by the rational functions
gn (x) =

n
.
π(1 + n2 x2 )

These functions satisfy


lim gn (x) =

n→∞

while‡

0,

x = 0,

∞,

x = 0,

 ∞

(6.10)

(6.11)

∞

1
gn (x) dx =
= 1.
tan−1 n x
π
−∞
x = −∞

(6.12)

Therefore, formally, we identify the limiting function
lim gn (x) = δ(x) = δ0 (x)

n→∞

(6.13)

with the unit-impulse delta function concentrated at x = 0. As sketched in Figure 6.1, as n
gets larger and larger, each successive function gn (x) forms a more and more concentrated
spike, while maintaining a unit total area under its graph. Thus, the limiting delta function
can be thought of as an inﬁnitely tall spike of zero width, entirely concentrated at the origin.
†

To keep the notation compact, we suppress the dependence of the functions gn on the point
ξ where the limiting delta function is concentrated.
‡
For the moment, it will be slightly simpler to consider the entire real line − ∞ < x < ∞.
Exercise 6.1.8 discusses how to adapt the construction to a ﬁnite interval.

6.1 Generalized Functions

219

Figure 6.1.

Delta function as limit.

Remark : There are many other possible choices for the limiting functions gn (x). See
Exercise 6.1.7 for another important example.
Remark : This construction of the delta function highlights the perils of interchanging
limits and integrals without rigorous justiﬁcation. In any standard theory of integration
(Riemann, Lebesgue, etc.), the limit of the functions gn would be indistinguishable from
the zero function, so the limit of their integrals (6.12) would not equal the integral of their
limit:
 ∞
 ∞
1 = lim
gn (x) dx =
lim gn (x) dx = 0.
n→∞

−∞ n → ∞

−∞

The delta function is, in a sense, a means of sidestepping this analytic inconvenience. The
full ramiﬁcations and theoretical constructions underlying such limits must, however, be
deferred to a rigorous course in real analysis, [96, 98].
Once we have deﬁned the basic delta function δ(x) = δ0 (x) concentrated at the origin, we can obtain the delta function concentrated at any other position ξ by a simple
translation:
δξ (x) = δ(x − ξ).
(6.14)
Thus, δξ (x) can be realized as the limit, as n → ∞, of the translated functions
g n (x) = gn (x − ξ) =


π



n
1 + n2 (x − ξ)2

.

(6.15)

Method #2. Duality: The second approach is a bit more abstract, but much closer
in spirit to the proper rigorous formulation of the theory of distributions like the delta
function. The critical property is that if u(x) is any continuous function, then
 b
δξ (x) u(x) dx = u(ξ),
for
a < ξ < b.
(6.16)
a

220

6 Generalized Functions and Green’s Functions

Indeed, since δξ (x) = 0 for x = ξ, the integrand depends only on the value of u at the
point x = ξ, and so
 b
 b
 b
δξ (x) u(x) dx =
δξ (x) u(ξ) dx = u(ξ)
δξ (x) dx = u(ξ).
a

a

a
†

Equation (6.16) serves to deﬁne a linear functional Lξ : C [ a, b ] → R that maps a continuous function u ∈ C0 [ a, b ] to its value at the point x = ξ :
0

Lξ [ u ] = u(ξ).

(6.17)

The basic linearity requirements (1.11) are immediately established:
Lξ [ u + v ] = u(ξ) + v(ξ) = Lξ [ u ] + Lξ [ v ],

Lξ [ c u ] = c u(ξ) = c Lξ [ u ],

for any functions u(x), v(x). In the dual approach to generalized functions, the delta
function is, in fact, deﬁned as this particular linear functional (6.17). The function u(x)
is sometimes referred to as a test function, since it serves to “test” the form of the linear
functional Lξ .
Remark : If the impulse point ξ lies outside the integration domain, then
 b
δξ (x) u(x) dx = 0
whenever
ξ<a
or
ξ > b,

(6.18)

a

because the integrand is identically zero on the entire interval. For technical reasons, we
will not attempt to deﬁne the integral (6.18) if the impulse point ξ = a or ξ = b lies on the
boundary of the interval of integration.
The interpretation of the linear functional Lξ as representing a kind of function δξ (x)
is based on the following line of thought. According to Corollary B.34, every scalar-valued
linear function L : R n → R on the ﬁnite-dimensional vector space R n is obtained by taking
the dot product with a ﬁxed element a ∈ R n , so
L[ u ] = a · u.
In this sense, linear functions on R n are the “same” as vectors. Similarly, on the inﬁnitedimensional function space C0 [ a, b ], the L2 inner product
 b
g(x) u(x) dx,
(6.19)
Lg [ u ] =  g , u  =
a

taken with a ﬁxed continuous function g ∈ C [ a, b ], deﬁnes a real-valued linear functional
Lg : C0 [ a, b ] → R. However, unlike the ﬁnite-dimensional situation, not every real-valued
linear functional is of this form! In particular, there is no bona ﬁde function δξ (x) such
that the identity
 b
δξ (x) u(x) dx = u(ξ)
(6.20)
Lξ [ u ] =  δξ , u  =
0

a

holds for every continuous function u(x). The bottom line is that every (continuous)
function deﬁnes a linear functional, but not every linear functional arises in this manner.
†
The term “functional” is used to refer to a linear function whose domain is a function space,
thus avoiding confusion with the functions it acts on.

6.1 Generalized Functions

221

But the dual interpretation of generalized functions acts as if this were true. Generalized functions are, in actuality, real-valued linear functionals on function space, but
intuitively interpreted as a kind of function via the L2 inner product. Although this identiﬁcation is not to be taken too literally, one can, with some care, manipulate generalized
functions as if they were actual functions, but always keeping in mind that a rigorous
justiﬁcation of such computations must ultimately rely on their innate characterization as
linear functionals.
The two approaches — limits and duality — are completely compatible. Indeed, one
can recover the dual formula (6.20) as the limit
 b
 b
u(ξ) = lim  gn , u  = lim
gn (x) u(x) dx =
δξ (x) u(x) dx =  δξ , u 
(6.21)
n→∞

n→∞

a

a

of the inner products of the function u with the approximating concentrated impulse functions gn (x) satisfying (6.8–9). In this manner, the limiting linear functional represents the
delta function:

u(ξ) = Lξ [ u ] = lim Ln [ u ],
where
Ln [ u ] =
gn (x) u(x) dx.
n→∞

0

The choice of interpretation of the generalized delta function is, at least on an operational
level, a matter of taste. For the beginner, the limit version is perhaps easier to digest
initially. However, the dual, linear functional interpretation has stronger connections with
the rigorous theory and, even in applications, oﬀers some signiﬁcant advantages.
Although the delta function might strike you as somewhat bizarre, its utility throughout modern applied mathematics and mathematical physics more than justiﬁes including
it in your analytical toolbox. While probably not yet comfortable with either deﬁnition,
you are advised to press on and familiarize yourself with its basic properties. With a little
care, you usually won’t go far wrong by treating it as if it were a genuine function. After
you gain more practical experience, you can, if desired, return to contemplate just exactly
what kind of creature the delta function really is.
Calculus of Generalized Functions
In order to make use of the delta function, we need to understand how it behaves under
the basic operations of linear algebra and calculus. First, we can take linear combinations
of delta functions. For example,
h(x) = 2 δ(x) − 3 δ(x − 1) = 2 δ0 (x) − 3 δ1 (x)
represents a combination of an impulse of magnitude 2 concentrated at x = 0 and one
of magnitude −3 concentrated at x = 1. In the dual interpretation, h deﬁnes the linear
functional
Lh [ u ] =  h , u  =  2 δ0 − 3 δ1 , u  = 2  δ0 , u  − 3  δ1 , u  = 2 u(0) − 3 u(1),
or, more explicitly, provided a < 0 and b > 1,
 b
 b


Lh [ u ] =
h(x) u(x) dx =
2 δ(x) − 3 δ(x − 1) u(x) dx
a

a

 b

 b
δ(x) u(x) dx − 3

=2
a

δ(x − 1) u(x) dx = 2 u(0) − 3 u(1).
a

222

6 Generalized Functions and Green’s Functions

Figure 6.2.

Step function as limit.

Next, since δξ (x) = 0 for any x = ξ, multiplying the delta function by an ordinary
function is the same as multiplying by a constant:
g(x) δξ (x) = g(ξ) δξ (x),

(6.22)

provided g(x) is continuous at x = ξ. For example, x δ(x) ≡ 0 is the same as the constant
zero function.
Warning: Since they are inherently linear functionals, it is not permissible to multiply delta functions together, or to apply more complicated nonlinear operations to them.
Expressions like δ(x)2 , 1/δ(x), eδ(x) , etc., are not well deﬁned in the theory of generalized functions — although this makes their application to nonlinear diﬀerential equations
problematic.
The integral of the delta function is the unit step function:

 x
0,
x < ξ,
provided
δξ (t) dt = σξ (x) = σ(x − ξ) =
1,
x > ξ,
a

a < ξ.

(6.23)

Unlike the delta function, the step function σξ (x) is an ordinary function. It is continuous
— indeed constant — except at x = ξ. The value of the step function at the discontinuity
x = ξ is left unspeciﬁed, although a wise choice — compatible with Fourier theory — is to
set σξ (y) = 12 , the average of its left- and right-hand limits.
We note that the integration formula (6.23) is compatible with our characterization of
the delta function as the limit of highly concentrated forces. Integrating the approximating
functions (6.10), we obtain
 x
1
1
fn (x) =
gn (t) dt = tan−1 n x + .
π
2
−∞
Since

lim tan−1 y = 12 π,

y→∞

while

lim

y → −∞

tan−1 y = − 12 π,

these functions converge (nonuniformly) to the step function:
⎧
x < 0,
⎨ 0,
1
,
x = 0,
lim f (x) = σ(x) =
n→∞ n
⎩ 2
1,
x > 0.

(6.24)

6.1 Generalized Functions

Figure 6.3.

223

First and second-order ramp functions.

A graphical illustration of this limiting process appears in Figure 6.2.
The integral of the discontinuous step function (6.23) is the continuous ramp function

 x
0,
x < ξ,
σξ (t) dt = ρξ (x) = ρ(x − ξ) =
provided
a < ξ, (6.25)
x − ξ,
x > ξ,
a
which is graphed in Figure 6.3. Note that ρξ (x) has a corner at x = ξ, and so is not
diﬀerentiable there; indeed, its derivative ρ (x − ξ) = σ(x − ξ) has a jump discontinuity.
We can continue to integrate; the (n + 1)st integral of the delta function is the nth order
ramp function
⎧
x < ξ,
⎨ 0,
ρn,ξ (x) = ρn (x − ξ) =
(6.26)
(x − ξ)n
⎩
,
x > ξ.
n!
Note that ρn,ξ ∈ Cn−1 has only n − 1 continuous derivatives.
What about diﬀerentiation? Motivated by the Fundamental Theorem of Calculus,
we shall use formula (6.23) to identify the derivative of the step function with the delta
function
dσ
= δ.
(6.27)
dx
This fact is highly signiﬁcant. In elementary calculus, one is not allowed to diﬀerentiate
a discontinuous function. Here, we discover that the derivative can be deﬁned, not as an
ordinary function, but rather as a generalized delta function!
In general, the derivative of a piecewise C1 function with jump discontinuities is a generalized function that includes a delta function concentrated at each discontinuity, whose
magnitude equals the jump magnitude. More explicitly, suppose that f (x) is diﬀerentiable, in the usual calculus sense, everywhere except at a point ξ, where it has a jump
discontinuity of magnitude β. Using the step function (3.47), we can re-express
f (x) = g(x) + β σ(x − ξ),

(6.28)

where g(x) is continuous everywhere, with a removable discontinuity at x = ξ, and diﬀerentiable except possibly at the jump. Diﬀerentiating (6.28), we ﬁnd that
f  (x) = g  (x) + β δ(x − ξ)

(6.29)

has a delta spike of magnitude β at the discontinuity. Thus, the derivatives of f and g
coincide everywhere except at the discontinuity.

224

6 Generalized Functions and Green’s Functions

1

1

−1

1

−1

2

−1

1

−1
f  (x)

f (x)
Figure 6.4.

2

The derivative of the discontinuous function in Example 6.1.

Example 6.1. Consider the function

− x,
f (x) =
1 2
5x ,

x < 1,
x > 1,

(6.30)

which we graph in Figure 6.4. We note that f has a single jump discontinuity at x = 1 of
magnitude
f (1+ ) − f (1− ) = 15 − (−1) = 65 .
This means that
f (x) = g(x) + 65 σ(x − 1),


where

g(x) =

− x,

x < 1,

1 2
6
5 x − 5,

x > 1,

is continuous everywhere, since its right- and left-hand limits at the original discontinuity
are equal: g(1+ ) = g(1− ) = −1. Therefore,

−1,
x < 1,
f  (x) = g  (x) + 65 δ(x − 1),
where
g  (x) =
2
x > 1,
5 x,
while g  (1) and f  (1) are not deﬁned. In Figure 6.4, the delta spike in the derivative of f is
symbolized by a vertical line, although this pictorial device fails to indicate its magnitude
of 65 .
Note that in this particular example, g  (x) can be found by directly diﬀerentiating
the formula for f (x). Indeed, in general, once we determine the magnitude and location
of the jump discontinuities of f (x), we can compute its derivative without introducing the
auxiliary function g(x).
Example 6.2. As a second, more streamlined, example, consider the function
⎧
x < 0,
⎨ − x,
2
x
−
1,
0 < x < 1,
f (x) =
⎩
−x
x > 1,
2e ,
which is plotted in Figure 6.5. This function has jump discontinuities of magnitude −1 at
x = 0, and of magnitude 2/e at x = 1. Therefore, in light of the preceding remark,
⎧
x < 0,
⎨ −1,
2

2 x,
0 < x < 1,
f (x) = − δ(x) + δ(x − 1) +
e
⎩
−x
x > 1,
−2e ,

6.1 Generalized Functions

225

4
1
2
−1

1

2

−1

−1

1
−2
f  (x)

f (x)

The derivative of the discontinuous function in Example 6.2.

Figure 6.5.

where the ﬁnal terms are obtained by directly diﬀerentiating f (x).
Example 6.3. The derivative of the absolute value function

x,
x > 0,
a(x) = | x | =
− x,
x < 0,
is the sign function
a (x) = sign x =



+ 1,

x > 0,

− 1,

x < 0.

(6.31)

Note that there is no delta function in a (x) because a(x) is continuous everywhere. Since
sign x has a jump of magnitude 2 at the origin and is otherwise constant, its derivative is
twice the delta function:
d
a (x) =
sign x = 2 δ(x).
dx
Example 6.4. We are even allowed to diﬀerentiate the delta function. Its ﬁrst
derivative δ  (x) can be interpreted in two ways. First, as the limit of the derivatives of the
approximating functions (6.10):
dδ
dgn
− 2 n3 x
= lim
= lim
.
n → ∞ π(1 + n2 x2 )2
dx n → ∞ dx

(6.32)

The graphs of these rational functions take the form of more and more concentrated spiked
“doublets”, as illustrated in Figure 6.6. To determine the eﬀect of the derivative on a test
function u(x), we compute the limiting integral
 ∞
 ∞


δ ,u =
δ (x) u(x) dx = lim
g  (x) u(x) dx
n → ∞ −∞ n
−∞
 ∞
 ∞
(6.33)
gn (x) u (x) dx = −
δ(x) u (x) dx = − u (0).
= − lim
n→∞

−∞

−∞

The middle step is the result of an integration by parts, noting that the boundary terms
at ± ∞ vanish, provided that u(x) is continuously diﬀerentiable and bounded as | x | → ∞.
Pay attention to the minus sign in the ﬁnal answer.

226

6 Generalized Functions and Green’s Functions

Figure 6.6.

Derivative of delta function as limit of doublets.

In the dual interpretation, the generalized function δ  (x) corresponds to the linear
functional
L [ u ] = − u (0) =  δ  , u  =

 b

δ  (x) u(x) dx,

where

a < 0 < b,

(6.34)

a

which maps a continuously diﬀerentiable function u(x) to minus its derivative at the origin.
We note that (6.34) is compatible with a formal integration by parts:
 b

δ  (x) u(x) dx = δ(x) u(x)

a

 b

b

−
x=a

δ(x) u (x) dx = − u (0).

a

The boundary terms at x = a and x = b automatically vanish, since δ(x) = 0 for x = 0.
Remark : While we can test the delta function with any continuous function, we are
permitted to test its derivative only on continuously diﬀerentiable functions. To avoid
keeping track of such technicalities, one often restricts to only inﬁnitely diﬀerentiable test
functions.
Warning: The functions gn (x) = gn (x) + gn (x), cf. (6.10, 32), satisfy lim gn (x) = 0
∞

for all x = 0, while
−∞

n→∞

gn (x) dx = 1. However, lim gn = lim gn + lim gn = δ + δ  .
n→∞

n→∞

n→∞

Thus, our original conditions (6.8–9) are not in fact suﬃcient to characterize whether a
sequence of functions has the delta function as a limit. To be absolutely sure, one must,
in fact, verify the more comprehensive limiting formula (6.21).

6.1 Generalized Functions

227

Exercises
6.1.1. Evaluate the following integrals: (a)
(c)

3
0

δ1 (x) ex dx, (d)

e
1

π
−π

2

δ(x) cos x dx, (b)

δ(x − 2) log x dx, (e)

δ(x) (x − 2) dx,

1

1 
1
δ x − 3 x2 dx,
0

(f )

1
−1

δ(x + 2) dx
.
1 + x2

6.1.2. Simplify the following generalized functions; then write out how they act on a suitable
test function u(x): (a) ex δ(x), (b) x δ(x − 1), (c) 3 δ1 (x) − 3 x δ−1 (x),

δ (x) − δ (x)
δ(x − 1)
(d)
, (e) (cos x) δ(x) + δ(x − π) + δ(x + π) , (f ) 1 2 2
.
x+1
x +1
6.1.3. Deﬁne the generalized function ϕ(x) = δ(x + 1) − δ(x − 1):
(a) as a limit of ordinary functions; (b) using duality.
6.1.4. Find and sketch a graph of the derivative (in the context of generalized functions) of the
following functions:
⎧
2
⎪
⎪
⎨ x ,

0 < x < 3,
(a) f (x) = ⎪ x,
−1 < x < 0,
⎪
⎩
0,
otherwise,
⎧
sin πx, x > 1,
⎪
⎨
(c) h(x) = ⎪ 1 − x2 , −1 < x < 1,
⎩ x
x < −1,
e ,



(b) g(x) =

sin | x |,
0,

| x | < 12 π,
otherwise,

⎧
sin x,
⎪
⎪
⎨
(d) k(x) = ⎪ x2 − π 2 ,
⎪
⎩ −x

e

x < − π,
− π < x < 0,
x > 0.

,

⎧
⎪
⎨ x + 1,

6.1.5. Find the ﬁrst and second derivatives of the functions (a) f (x) = ⎪ 1 − x,
⎩
0,


(b) k(x) =

| x |,
0,

−2 < x < 2,
otherwise,



(c) s(x) =

−1 < x < 0,
0 < x < 1,
otherwise,

−1 < x < 1,
otherwise.

1 + cos πx,
0,

6.1.6. Find the ﬁrst and second derivatives of f (x) = (a) e− | x | , (b) 2 | x | − | x − 1 | ,
(c) | x2 + x | , (d) x sign(x2 − 4), (e) sin | x |, (f ) | sin x |, (g) sign(sin x).
2 2
n
♦ 6.1.7. Explain why the Gaussian functions gn (x) = √ e− n x have the delta function δ(x) as
π
their limit as n → ∞.
♦ 6.1.8. In this exercise, we realize the delta function δξ (x) as a limit of functions on a ﬁnite
interval [ a, b ]. Let a < ξ < b.
g (x − ξ)
(a) Prove that the functions gn (x) = n
, where gn (x) is given by (6.10) and
Mn
b
gn (x − ξ) dx, satisfy (6.8–9), and hence lim gn (x) = δξ (x).
Mn =
n→∞

a

b

(b) One can, alternatively, relax the second condition (6.9) to lim

n→∞

Show that, under this relaxed deﬁnition, lim gn (x − ξ) = δξ (x).
♥ 6.1.9. For each positive integer n, let gn (x)

n→∞

1
2 n,
=

0,

| x | < 1/n,
otherwise.

gn (x). (b) Show that lim gn (x) = δ(x). (c) Evaluate fn (x) =
n→∞

a

gn (x − ξ) dx = 1.

(a) Sketch a graph of
x

g (y) dy and sketch
−∞ n

a graph. Does the sequence fn (x) converge to the step function σ(x) as n → ∞? (d) Find
the derivative hn (x) = gn (x). (e) Does the sequence hn (x) converge to δ  (x) as n → ∞?


♥ 6.1.10. Answer Exercise 6.1.9 for the hat functions gn (x) =

n − n2 | x |,
0,

| x | < 1/n,
otherwise.

228

6 Generalized Functions and Green’s Functions

6.1.11. Justify the formula x δ(x) = 0 using (a) limits, (b) duality.
♦ 6.1.12. (a) Justify the formula δ(2 x) = 12 δ(x) by (i ) limits, (ii ) duality.
formula for δ(a x) when a > 0. (c) What about when a < 0?

(b) Find a similar

6.1.13. (a) Prove that σ(λ x) = σ(x) for any λ > 0. (b) What about if λ < 0? (c) Use parts
1
δ(x) for any λ = 0.
(a,b) to deduce that δ(λ x) =
|λ|
6.1.14. Let g(x) be a continuously diﬀerentiable function with g  (x) = 0 for all x ∈ R. Does the
composition δ(g(x)) make sense as a distribution? If so, can you identify it?
6.1.15. Let ξ < a. Sketch the graphs of (a) s(x) =
 



 x



a

δξ (z) dz, (b) r(x) =

6.1.16. Justify the formula lim n δ x − n1 − δ x + n1



n→∞

 x
a

σξ (z) dz.

= − 2 δ  (x).

6.1.17. Deﬁne the generalized function δ  (x):
(a) as a limit of ordinary functions; (b) using duality.
(k)

6.1.18. Let δξ (x) denote the kth derivative of the delta function δξ (x). Justify the formula
(k)

 δξ

, u  = (−1)k u(k) (ξ) whenever u ∈ Ck is k–times continuously diﬀerentiable.

6.1.19. According to (6.22), x δ(x) = 0. On the other hand, by Leibniz’ rule,
(x δ(x)) = δ(x) + x δ  (x) is apparently not zero. Can you explain this paradox?
6.1.20. If f ∈ C1 , should (f δ) = f δ  or f  δ + f δ  ?
♦ 6.1.21. (a) Use duality to justify the formula f (x) δ  (x) = f (0) δ  (x) − f  (0) δ(x) when f ∈ C1 .
(b) Find a similar formula for f (x) δ (n) (x) as the product of a suﬃciently smooth function
and the nth derivative of the delta function.
6.1.22. Use Exercise 6.1.21 to simplify the following generalized functions; then write out how
they act on a suitable test function u(x):


(a) ϕ(x) = (x − 2) δ  (x), (b) ψ(x) = (1 + sin x) δ(x) + δ  (x) ,




(c) χ(x) = x2 δ(x − 1) − δ  (x − 2) ,

(d) ω(x) = ex δ  (x + 1).

♦ 6.1.23. Prove that if f (x) is a continuous function, and
then f (x) ≡ 0 everywhere.

 b
a

f (x) dx = 0 for every interval [ a, b ],

♦ 6.1.24. Write out a rigorous proof that there is no continuous function δξ (x) such that the inner product identity (6.20) holds for every continuous function u(x).
♦ 6.1.25. Prove from ﬁrst principles that the sequence (6.24) converges nonuniformly to the step
function.
6.1.26. True or false:  δ  = 1.

The Fourier Series of the Delta Function
Let us next investigate the capability of Fourier series to represent generalized functions.
We begin with the delta function δ(x), based at the origin. Using the characterizing
properties (6.16), its real Fourier coeﬃcients are


1 π
1
1 π
1
1
δ(x) cos k x dx = cos k 0 = ,
δ(x) sin k x dx = sin k 0 = 0.
bk =
ak =
π −π
π
π
π −π
π
(6.35)

6.1 Generalized Functions

229

Therefore, at least on a formal level, its Fourier series is
1 
1
+
cos x + cos 2 x + cos 3 x + · · · .
(6.36)
2π
π
Since δ(x) = δ(− x) is an even function (why?), it should come as no surprise that it has
a cosine series. Alternatively, we can rewrite the series in complex form
δ(x) ∼

∞
1 
1
δ(x) ∼
e ikx =
( · · · + e−2 i x + e− i x + 1 + e i x + e2 i x + · · · ),
2π
2π

(6.37)

k = −∞

where the complex Fourier coeﬃcients are computed† as
 π
1
1
.
δ(x) e− i k x dx =
ck =
2 π −π
2π
Remark : Although we stated that the Fourier series (6.36) represents the delta function, this is not entirely correct. Remember that a Fourier series converges to the 2 π–
periodic extension of the original function. Therefore, (6.37) actually represents the periodic extension of the delta function, sometimes called the Dirac comb,

δ(x)
= · · · +δ(x+4 π)+δ(x+2 π)+δ(x)+δ(x−2 π)+δ(x−4 π)+δ(x−6 π)+ · · · , (6.38)
consisting of a periodic array of unit impulses concentrated at all integer multiples of 2 π.
Let us investigate in what sense (if any) the Fourier series (6.36) or, equivalently,
(6.37), represents the delta function. The ﬁrst observation is that, because its summands
do not tend to zero, the series certainly doesn’t converge in the usual, calculus, sense.
Nevertheless, in a “weak” sense, the series can be regarded as converging to the (periodic
extension of the) delta function.
To understand the convergence mechanism, we recall that we already established a
formula (3.129) for the partial sums:

1
n
n
1  ikx
1
1 sin n + 2 x
1 
sn (x) =
e
=
cos k x =
.
+
(6.39)
2π
2π
π
2π
sin 12 x
k = −n
k=1
Graphs of some of the partial sums on the interval [ − π, π ] are displayed in Figure 6.7.
Note that, as n increases, the spike at x = 0 becomes progressively taller and thinner,
converging to an inﬁnitely tall delta spike. (We had to truncate the last two graphs; the
spike extends beyond the top.) Indeed, by l’Hôpital’s Rule,



1
1
1
n + 12
1 sin n + 2 x
1 n + 2 cos n + 2 x
−→ ∞ as n → ∞.
= lim
=
lim
1
1
x→0 2 π
x→0 2 π
π
sin 12 x
2 cos 2 x
(An elementary proof of this formula is to note that, at x = 0, every term in the original
sum (6.36) is equal to 1.) Furthermore, the integrals remain ﬁxed,
 π sin n + 1 x
 π 
 π
n
1
1
2
sn (x) dx =
e i k x dx = 1,
(6.40)
dx
=
1
2
π
2
π
sin
x
−π
−π
−π
2
k = −n

†

Or we could use (3.66).

230

6 Generalized Functions and Green’s Functions

s1 (x)

s5 (x)

s10 (x)

s25 (x)

s50 (x)

s100 (x)

Figure 6.7.

Partial Fourier sums approximating the delta function.

as required for convergence to the delta function. However, away from the spike, the partial
sums do not go to zero! Rather, they oscillate ever more rapidly, while maintaining a ﬁxed
overall amplitude of
1
1
csc 12 x =
.
(6.41)
2π
2 π sin 1 x
2

As n increases, the amplitude function (6.41) can be seen, as in Figure 6.7, as the envelope
of the increasingly rapid oscillations. So, roughly speaking, the convergence sn (x) → δ(x)
means that the “inﬁnitely fast” oscillations are somehow canceling each other out, and the
net eﬀect is zero away from the spike at x = 0. So the convergence of the Fourier sums to
δ(x) is much more subtle than in the original limiting deﬁnition (6.10).
The technical term is weak convergence, which plays a very important role in advanced
mathematical analysis, signal processing, composite materials, and elsewhere.
Deﬁnition 6.5. A sequence of functions fn (x) is said to converge weakly to f (x)
on an interval [ a, b ] if their L2 inner products with every continuous test function u(x) ∈
C0 [ a, b ] converge:
 b

 b
fn (x) u(x) dx −→
a

f (x) u(x) dx

as

n −→ ∞.

(6.42)

a

Weak convergence is often indicated by a half-pointed arrow: fn  g.
Remark : On unbounded intervals, one usually restricts the test functions to have
compact support, meaning that u(x) = 0 for all suﬃciently large | x |  0. One can also
restrict to smooth test functions only, e.g., require that u ∈ C∞ [ a, b ].

6.1 Generalized Functions

231

Example 6.6. Let us show that the trigonometric functions fn (x) = cos n x converge
weakly to the zero function:
cos n x − 0

as

n −→ ∞

on the interval [ − π, π ].

(Actually, this holds on any interval; see Exercise 6.1.38.) According to the deﬁnition, we
need to prove that

π

lim

n→∞

u(x) cos n x dx = 0
−π

for any continuous function u ∈ C0 [ − π, π ]. But this is just a restatement of the Riemann–
Lebesgue Lemma 3.40, which says that the high-frequency Fourier coeﬃcients of a continuous (indeed, even square-integrable) function u(x) go to zero. The same remark establishes
the weak convergence sin n x  0.
Observe that the functions cos n x fail to converge pointwise to 0 at any value of x.
Indeed, if x is an integer multiple of 2 π, then cos n x = 1 for all n. If x is any other
rational multiple of π, the values of cos n x periodically cycle through a ﬁnite number of
diﬀerent values, and never go to 0, while if x is an irrational multiple of π, they oscillate
aperiodically between −1 and +1. The functions also fail to converge in norm to 0, since
their (unscaled) L2 norms remain ﬁxed at

π
√
 cos n x  =
cos2 n x dx = π
for all
n > 0.
−π

The cancellation of oscillations in the high-frequency limit is a characteristic feature of
weak convergence.
Let us now explain why, although the Fourier series (6.36) does not converge to the
delta function either pointwise or in norm (indeed,  δ  is not even deﬁned!), it does
converge weakly on [ − π, π ]. More speciﬁcally, we need to prove that the partial sums
sn  δ, meaning that
 π
 π
lim
sn (x) u(x) dx =
δ(x) u(x) dx = u(0)
(6.43)
n→∞

−π

−π

for every suﬃciently nice function u, or, equivalently,

 π
sin n + 12 x
1
lim
dx = u(0).
u(x)
n → ∞ 2 π −π
sin 12 x

(6.44)

But this is a restatement of a special case of the identities (3.130) used in the proof of
the Pointwise Convergence Theorem 3.8 for the Fourier series of a (piecewise) C1 function.
Indeed, summing the two identities
in (3.130) and then setting x = 0 reproduces (6.44),

since, by continuity, u(0) = 12 u(0+ ) + u(0− ) . In other words, the pointwise convergence
of the Fourier series of a C1 function is equivalent to the weak convergence† of the Fourier
series of the delta function!
†

Deﬁnition 6.5 only requires continuity of the test functions, whereas in (6.44) they need to
be C1 , so the notion of weak convergence here is slightly slightly more reﬁned. One often restricts
further to allow only C∞ test functions.

232

6 Generalized Functions and Green’s Functions

Example 6.7. If we diﬀerentiate the Fourier series
x ∼ 2

∞

(−1)k−1
k=1

k



sin 2 x
sin 3 x
sin 4 x
sin k x = 2 sin x −
+
−
+ ··· ,
2
3
4

we obtain an apparent contradiction:
1 ∼ 2

∞


(−1)k+1 cos k x = 2 cos x − 2 cos 2 x + 2 cos 3 x − 2 cos 4 x + · · · .

(6.45)

k=1

But the Fourier series for 1 consists of just a single constant term! (Why?)
The resolution of this paradox is not diﬃcult. The Fourier series (3.37) does not
converge to x, but rather to its 2 π–periodic extension f(x), which has jump discontinuities
of magnitude 2 π at odd multiples of π; see Figure 3.1. Thus, Theorem 3.22 is not directly
applicable. Nevertheless, we can assign a consistent interpretation to the diﬀerentiated
series. The derivative f (x) of the periodic extension is not equal to the constant function
1, but rather has an additional delta function concentrated at each jump discontinuity:
f (x) = 1 − 2 π

∞


 − π),
δ x − (2 j + 1) π = 1 − 2 π δ(x

j = −∞

where δ denotes the 2 π–periodic extension of the delta function, cf. (6.38). The diﬀerentiated Fourier series (6.45) does, in fact, represent f (x). Indeed, the Fourier coeﬃcients of
 − π) are
δ(x

1 2π
1
(−1)k
δ(x − π) cos k x dx = cos k π =
,
ak =
π 0
π
π

1 2π
1
bk =
δ(x − π) sin k x dx = sin k π = 0.
π 0
π
Observe that we changed the interval of integration to [ 0, 2 π ] to avoid placing the delta
function singularities at the endpoints. Thus,
δ(x − π) ∼

1
1
+
− cos x + cos 2 x − cos 3 x + · · · ,
2π
π

(6.46)

which serves to resolve the contradiction.
Example 6.8. Let us diﬀerentiate the Fourier series


2
sin 3 x
sin 5 x
sin 7 x
1
+
sin x +
+
+
+ ···
σ(x) ∼
2
π
3
5
7
for the unit step function we found in Example 3.9 and see whether we end up with the
Fourier series (6.36) for the delta function. We compute
dσ
2
∼
cos x + cos 3 x + cos 5 x + cos 7 x + · · · ,
dx
π

(6.47)

which does not agree with (6.36) — half the terms are missing! The explanation is similar
to the preceding example: the 2 π–periodic extension σ
(x) of the step function has two

6.1 Generalized Functions

233

jump discontinuities, of magnitudes +1 at even multiples of π and −1 at odd multiples;
see Figure 3.6. Therefore, its derivative
dσ
 
 − π)
= δ(x) − δ(x
dx
is the diﬀerence of the 2 π–periodic extension of the delta function at 0, with Fourier series
(6.36), minus the 2 π–periodic extension of the delta function at π, with Fourier series
(6.46), which produces (6.47).
It is a remarkable, profound fact that Fourier analysis is entirely compatible with the
calculus of generalized functions, [68]. For instance, term-wise diﬀerentiation of the Fourier
series for a piecewise C1 function leads to the Fourier series for the diﬀerentiated function
that incorporates delta functions of the appropriate magnitude at each jump discontinuity.
This fact further reassures us that the rather mysterious construction of delta functions
and their generalizations is indeed the right way to extend calculus to functions that do
not possess derivatives in the ordinary sense.

Exercises
6.1.27. Determine the real and complex Fourier series for δ(x − ξ), where − π < ξ < π. What
periodic generalized function(s) do they represent?
6.1.28. Determine the Fourier sine series and the Fourier cosine series for δ(x − ξ), where
0 < ξ < π. Which periodic generalized functions do they represent?
♥ 6.1.29. Let n > 0 be a positive integer. (a) For integers 0 ≤ j < n, ﬁnd the complex Fourier
series of the 2 π–periodically extended delta functions 
δj (x) = δ(x − 2 j π/n). (b) Prove that
their Fourier coeﬃcients satisfy the periodicity condition ck = cl whenever k ≡ l mod n.
(c) Conversely, given complex Fourier coeﬃcients that satisfy the periodicity condition
ck = cl whenever k ≡ l mod n, prove that the corresponding Fourier series represents a linear combination of the preceding periodically extended delta functions δ0 (x), . . . , δn−1 (x).
Hint: Use Example B.22. (d) Prove that a complex Fourier series represents a 2 π–periodic
function that is constant on the subintervals 2 π j/n < x < 2 π(j + 1)/n, for j ∈ Z, if and
only if its Fourier coeﬃcients satisfy the conditions
k ≡ l ≡ 0 mod n,
ck = 0,
0 = k ≡ 0 mod n.
k c k = l cl ,
♣ 6.1.30. (a) Find the complex Fourier series for the derivative of the delta function δ  (x) by direct evaluation of the coeﬃcient formulas. (b) Verify that your series can be obtained by
term-by-term diﬀerentiation of the series for δ(x). (c) Write a formula for the nth partial
sum of your series. (d) Use a computer graphics package to investigate the convergence of
the series.
6.1.31. What is the Fourier series for the generalized function g(x) = x δ(x)? Can you obtain
this result through multiplication of the individual Fourier series (3.37), (6.37)?
6.1.32. Apply the method of Exercise 3.2.59 to ﬁnd the complex Fourier series for the function
f (x) = δ(x) e i x . Which Fourier series do you get? Can you explain what is going on?
6.1.33. In Exercise 6.1.12 we established the identity δ(x) = 2 δ(2 x). Does this hold on the
level of Fourier series? Can you explain why or why not?
6.1.34. How should one interpret the formula (6.38) for the periodic extension of the delta
function (a) as a limit? (b) as a linear functional?

234

6 Generalized Functions and Green’s Functions

6.1.35. Write down the complex Fourier series for ex . Diﬀerentiate term by term. Do you get
the same series? Explain your answer.
6.1.36. True or false: If you integrate the Fourier series for the delta function δ(x) term by
term, you obtain the Fourier series for the step function σ(x).
6.1.37. Find the Fourier series for the function δ(x) on the interval −1 ≤ x ≤ 1. Which (generalized) function does the Fourier series represent?
♦ 6.1.38.. Prove that cos n x  0 (weakly) as n → ∞ on any bounded interval [ a, b ].
♦ 6.1.39. Prove that if un → u in norm, then un  u weakly.
6.1.40. True or false: (a) If un → u uniformly on [ a, b ], then un  u weakly.
(b) If un (x) → u(x) pointwise, then un  u weakly.
6.1.41. Prove that the sequence fn (x) = cos2 n x converges weakly on [ − π, π ]. What is the
limiting function?
6.1.42. Answer Exercise 6.1.41 when fn (x) = cos3 n x.
6.1.43. Discuss the weak convergence of the Fourier series for the derivative δ  (x) of the delta
function.

6.2 Green’s Functions for
One–Dimensional Boundary Value Problems
We will now put the delta function to work by developing a general method for solving
inhomogeneous linear boundary value problems. The key idea, motivated by the linear
algebra technique outlined at the beginning of the previous section, is to ﬁrst solve the system when subject to a unit delta function impulse, which produces the Green’s function.
We then apply linear superposition to write down the solution for a general forcing inhomogeneity. The Green’s function approach has wide applicability, but will be developed
here in the context of a few basic examples.
Example 6.9. The boundary value problem
− c u = f (x),

u(0) = 0 = u(1),

(6.48)

models the longitudinal deformation u(x) of a homogeneous elastic bar of unit length and
constant stiﬀness c that is ﬁxed at both ends while subject to an external force f (x). The
associated Green’s function refers to the family of solutions
u(x) = Gξ (x) = G(x; ξ)
induced by unit-impulse forces concentrated at a single point 0 < ξ < 1:
− c u = δ(x − ξ),

u(0) = 0 = u(1).

(6.49)

The solution to the diﬀerential equation can be straightforwardly obtained by direct integration. First, by (6.23),
σ(x − ξ)
+ a,
u (x) = −
c

6.2 Green’s Functions for One–Dimensional Boundary Value Problems

235

ξ (1 − ξ)/c

ξ

Figure 6.8.

1

Green’s function for a bar with ﬁxed ends.

where a is a constant of integration. A second integration leads to
ρ(x − ξ)
+ a x + b,
(6.50)
c
where ρ is the ramp function (6.25). The integration constants a, b are ﬁxed by the boundary conditions; since 0 < ξ < 1, we have
u(x) = −

1−ξ
1−ξ
+ a + b = 0,
and so
,
a=
c
c
We deduce that the Green’s function for the problem is

(1 − ξ) x/c,
x ≤ ξ,
(1 − ξ) x − ρ(x − ξ)
=
G(x; ξ) =
c
ξ (1 − x)/c,
x ≥ ξ.
u(0) = b = 0,

u(1) = −

b = 0.

(6.51)

As sketched in Figure 6.8, for each ﬁxed ξ, the function Gξ (x) = G(x; ξ) depends continuously on x; its graph consists of two connected straight line segments, with a corner at the
point of application of the unit impulse force.
Once we have determined the Green’s function, we are able to solve the general inhomogeneous boundary value problem (6.48) by linear superposition. We ﬁrst express the
forcing function f (x) as a linear combination of impulses concentrated at various points
along the bar. Since there is a continuum of possible positions 0 < ξ < 1 at which impulse
forces may be applied, we will use an integral to sum them, thereby writing the external
force as

1

δ(x − ξ) f (ξ) dξ.

f (x) =

(6.52)

0

We can interpret (6.52) as the (continuous) superposition of an inﬁnite collection of impulses, namely f (ξ) δ(x − ξ), of magnitude f (ξ) and concentrated at position ξ.
The Superposition Principle states that linear combinations of inhomogeneities produce the selfsame linear combinations of solutions. Again, we adapt this principle to the
continuum by replacing the sums by integrals. Thus, the solution to the boundary value
problem will be the linear superposition
 1
u(x) =
G(x; ξ) f (ξ) dξ
(6.53)
0

of the Green’s function solutions to the individual unit-impulse problems.

236

6 Generalized Functions and Green’s Functions

For the particular boundary value problem (6.48), we use the formula (6.51) for the
Green’s function. Breaking the resulting integral (6.53) into two parts, over the subintervals
0 ≤ ξ ≤ x and x ≤ ξ ≤ 1, we arrive at the explicit solution formula


1 1
1 x
(1 − x) ξ f (ξ) dξ +
x (1 − ξ) f (ξ) dξ.
(6.54)
u(x) =
c 0
c x
For example, under a constant unit force f , (6.54) yields the solution


f 1
f
f
f
f x
(1 − x) ξ dξ +
x (1 − ξ) dξ =
(1−x) x2 +
x (1−x)2 =
(x−x2 ).
u(x) =
c 0
c x
2c
2c
2c
Let us, ﬁnally, convince ourselves that the superposition formula (6.54) indeed gives the
correct answer. First,
 x
 1
du
c
(1 − ξ) f (ξ) dξ
= (1 − x) x f (x) +
− ξ f (ξ) dξ − x (1 − x) f (x) +
dx
0
x
 1
 1
ξ f (ξ) dξ +
f (ξ) dξ.
=−
x

0

Diﬀerentiating again with respect to x, we see that the ﬁrst term is constant, and so
d2 u
= f (x), as claimed.
−c
dx2
Remark : In computing the derivatives of u, we made use of the calculus formula
 β(x)
 β(x)
d
∂F
dβ
dα
F (x, ξ) dξ = F (x, β(x))
− F (x, α(x))
+
(x, ξ) dξ
(6.55)
dx α(x)
dx
dx
α(x) ∂x
for the derivative of an integral with variable limits — which is a straightforward consequence of the Fundamental Theorem of Calculus and the chain rule, [8, 108]. As always,
one must exercise due care when interchanging diﬀerentiation and integration.
We note the following basic properties, which serve to uniquely characterize the Green’s
function. First, since the delta forcing vanishes except at the point x = ξ, the Green’s
function satisﬁes the homogeneous diﬀerential equation†
∂2G
(x; ξ) = 0
for all
x = ξ.
∂x2
Second, by construction, it must satisfy the boundary conditions
−c

(6.56)

G(0; ξ) = 0 = G(1; ξ).
Third, for each ﬁxed ξ, G(x; ξ) is a continuous function of x, but its derivative ∂G/∂x
has a jump discontinuity of magnitude − 1/c at the impulse point x = ξ. As a result, the
second derivative ∂ 2 G/∂x2 has a delta function discontinuity there, and hence solves the
original impulse boundary value problem (6.49).
Finally, we cannot help but notice that the Green’s function (6.51) is a symmetric
function of its two arguments: G(x; ξ) = G(ξ; x). Symmetry has the interesting physical consequence that the displacement of the bar at position x due to an impulse force
†
Since G(x; ξ) is a function of two variables, we switch to partial derivative notation to indicate
its derivatives.

6.2 Green’s Functions for One–Dimensional Boundary Value Problems

237

concentrated at position ξ is exactly the same as the displacement of the bar at ξ due
to an impulse of the same magnitude being applied at x. This turns out to be a rather
general, although perhaps unanticipated, phenomenon. Symmetry of the Green’s function
is a consequence of the underlying symmetry, or, more accurately, “self-adjointness”, of
the boundary value problem, a topic that will be developed in detail in Section 9.2.
Example 6.10. Let ω 2 > 0 be a ﬁxed positive constant. Let us solve the inhomogeneous boundary value problem
− u + ω 2 u = f (x),

u(0) = u(1) = 0,

(6.57)

by constructing its Green’s function. To this end, we ﬁrst analyze the eﬀect of a delta
function inhomogeneity
− u + ω 2 u = δ(x − ξ),

u(0) = u(1) = 0.

(6.58)

Rather than try to integrate this diﬀerential equation directly, let us appeal to the deﬁning
properties of the Green’s function. The general solution to the homogeneous equation is a
linear combination of the two basic exponentials eω x and e− ω x , or better, the hyperbolic
functions
eω x − e− ω x
eω x + e− ω x
(6.59)
,
sinh ω x =
.
cosh ω x =
2
2
The solutions satisfying the ﬁrst boundary condition are multiples of sinh ω x, while those
satisfying the second boundary condition are multiples of sinh ω (1 − x). Therefore, the
solution to (6.58) has the form

a sinh ω x,
x ≤ ξ,
G(x; ξ) =
b sinh ω (1 − x),
x ≥ ξ.
Continuity of G(x; ξ) at x = ξ requires
a sinh ω ξ = b sinh ω (1 − ξ).

(6.60)

At x = ξ, the derivative ∂G/∂x must have a jump discontinuity of magnitude −1 in order
that the second derivative term in (6.58) match the delta function. (The ω 2 u term clearly
cannot produce the required singularity.) Since

a ω cosh ω x,
x < ξ,
∂G
(x; ξ) =
∂x
− b ω cosh ω (1 − x),
x > ξ,
the jump condition requires
a ω cosh ω ξ − 1 = − b ω cosh ω (1 − ξ).

(6.61)

Multiplying (6.60) by ω cosh ω (1 − ξ) and (6.61) by sinh ω (1 − ξ), and then adding the
results together, we obtain


sinh ω (1 − ξ) = a ω sinh ω ξ cosh ω (1 − ξ) + cosh ω ξ sinh ω (1 − ξ) = a ω sinh ω, (6.62)
where we made use of the addition formula for the hyperbolic sine:
sinh(α + β) = sinh α cosh β + cosh α sinh β,

(6.63)

238

6 Generalized Functions and Green’s Functions

sinh ω ξ sinh ω (1 − ξ)
ω sinh ω

ξ
Figure 6.9.

1

Green’s function for the boundary value problem (6.57).

which you are asked to prove in Exercise 6.2.13. Therefore, solving (6.61–62) for
sinh ω (1 − ξ)
sinh ω ξ
,
b=
,
ω sinh ω
ω sinh ω
produces the explicit formula
⎧
sinh ω x sinh ω (1 − ξ)
⎪
⎨
,
x ≤ ξ,
ω sinh ω
(6.64)
G(x; ξ) =
⎪
⎩ sinh ω (1 − x) sinh ω ξ ,
x ≥ ξ.
ω sinh ω
A representative graph appears in Figure 6.9. As before, a corner, indicating a discontinuity
in the ﬁrst derivative, appears at the point x = ξ where the impulse force is applied.
Moreover, as in the previous example, G(x; ξ) = G(ξ; x) is a symmetric function.
The general solution to the inhomogeneous boundary value problem (6.57) is then
given by the superposition formula (6.53); explicitly,
 1
u(x) =
G(x; ξ)f (ξ) dξ
0
(6.65)
 x
 1
sinh ω (1 − x) sinh ω ξ
sinh ω x sinh ω (1 − ξ)
=
f (ξ) dξ +
f (ξ) dξ.
ω sinh ω
ω sinh ω
0
x
a=

For example, under a constant unit force f (x) ≡ 1, the solution is
 1
 x
sinh ω (1 − x) sinh ω ξ
sinh ω x sinh ω (1 − ξ)
dξ +
dξ
u(x) =
ω sinh ω
ω sinh ω
0
x


sinh ω x cosh ω (1 − x) − 1
sinh ω (1 − x) cosh ω x − 1
+
=
ω 2 sinh ω
ω 2 sinh ω
1
sinh ω x + sinh ω (1 − x)
= 2 −
.
ω
ω 2 sinh ω
For comparative purposes, the reader may wish to rederive this particular solution by a
direct calculation, without appealing to the Green’s function.
Example 6.11. Finally, consider the Neumann boundary value problem
− c u = f (x),

u (0) = 0 = u (1),

(6.66)

6.2 Green’s Functions for One–Dimensional Boundary Value Problems

239

modeling the equilibrium deformation of a homogeneous bar with two free ends when
subject to an external force f (x). The Green’s function should satisfy the particular case
− c u = δ(x − ξ),

u (0) = 0 = u (1),

when the forcing function is a concentrated impulse. As in Example 6.9, the general
solution to the latter diﬀerential equation is
u(x) = −

ρ(x − ξ)
+ a x + b,
c

where a, b are integration constants, and ρ is the ramp function (6.25). However, the
Neumann boundary conditions require that
u (0) = a = 0,

u (1) = −

1
+ a = 0,
c

which cannot both be satisﬁed. We conclude that there is no Green’s function in this case† .
The diﬃculty is that the Neumann boundary value problem (6.66) does not have
a unique solution, and hence cannot admit a Green’s function solution formula (6.53).
Indeed, integrating twice, we ﬁnd that the general solution to the diﬀerential equation is
 
1 x y
u(x) = a x + b −
f (z) dz dy,
c 0 0
where a, b are integration constants. Since
1
u (x) = a −
c


 x
f (z) dz,
0

the boundary conditions require that
1
u (1) = a −
c





u (0) = a = 0,

These equations are compatible if and only if
 1
f (z) dz = 0.

 1
f (z) dz = 0.
0

(6.67)

0

Thus, the Neumann boundary value problem admits a solution if and only if there is no
net force on the bar. Indeed, physically, if (6.67) does not hold, then, because its ends are
not attached to any support, the bar cannot stay in equilibrium, but will move oﬀ in the
direction of the net force. On the other hand, if (6.67) holds, then the solution
 
1 x y
u(x) = b −
f (z) dz dy
c 0 0
is not unique, since b is not constrained by the boundary conditions, and so can assume
any constant value. Physically, this means that any equilibrium conﬁguration of the bar
can be freely translated to assume another valid equilibrium.
†

However, one can suitably extend the notion of Green’s function in such situations; see,
for instance, a preprint by J. Franklin, Green’s functions for Neumann boundary conditions;
arXiv 1201.6059, 2012.

240

6 Generalized Functions and Green’s Functions

Remark : The constraint (6.67) is a manifestation of the Fredholm Alternative, to be
developed in detail in Section 9.1.
Let us summarize the fundamental properties that serve to completely characterize
the Green’s function of boundary value problems governed by second-order linear ordinary
diﬀerential equations
d2 u
du
p(x)
+ r(x) u(x) = f (x),
(6.68)
+ q(x)
dx2
dx
combined with a pair of homogeneous boundary conditions at the ends of the interval
[ a, b ]. We assume that the coeﬃcient functions are continuous, p, q, r, f ∈ C0 [ a, b ], and
that p(x) = 0 for all a ≤ x ≤ b.
Basic Properties of the Green’s Function G(x; ξ)
(i ) Solves the homogeneous diﬀerential equation at all points x = ξ.
(ii ) Satisﬁes the homogeneous boundary conditions.
(iii ) Is a continuous function of its arguments.
(iv ) For each ﬁxed ξ, its derivative ∂G/∂x is piecewise C1 , with a single jump discontinuity
of magnitude 1/p(ξ) at the impulse point x = ξ.
With the Green’s function in hand, we deduce that the solution to the general boundary value problem (6.68) subject to the appropriate homogeneous boundary conditions is
expressed by the Green’s Function Superposition Formula
 b
G(x; ξ) f (ξ) dξ.
(6.69)
u(x) =
a

The symmetry of the Green’s function is more subtle, for it relies on the self-adjointness of
the boundary value problem, an issue to be addressed in detail in Chapter 9. In the present
situation, self-adjointness requires that q(x) = p (x), in which case G(ξ; x) = G(x; ξ) will
be symmetric in its arguments.
Finally, as we saw in Example 6.11, not every such boundary value problem admits
a solution, and one expects to ﬁnd a Green’s function only in cases in which the solution
exists and is unique.
Theorem 6.12. The following are equivalent:
• The only solution to the homogeneous boundary value problem is the zero function.
• The inhomogeneous boundary value problem has a unique solution for every choice of
forcing function.
• The boundary value problem admits a Green’s function.

Exercises
6.2.1. Let c > 0. Find the Green’s function for the boundary value problem − c u = f (x),
u(0) = 0, u (1) = 0, which models the displacement of a uniform bar of unit length with
one ﬁxed and one free end under an external force. Then use superposition to write down
a formula for the solution. Verify that your integral formula is correct by direct diﬀerentiation and substitution into the diﬀerential equation and boundary conditions.

6.3 Green’s Functions for the Planar Poisson Equation

241

6.2.2. A uniform bar of length = 4 has constant stiﬀness c = 2. Find the Green’s function for
the case that (a) both ends are ﬁxed; (b) one end is ﬁxed and the other is free. (c) Why is
there no Green’s function when both ends are free?
6.2.3. A point 2 cm along a 10 cm bar experiences a displacement of 1 mm under a concentrated force of 2 newtons applied at the midpoint of the bar. How far does the midpoint
deﬂect when a concentrated force of 1 newton is applied at the point 2 cm along the bar?
♥ 6.2.4. The boundary value problem −

d
dx



c(x)

du
dx



= f (x), u(0) = u(1) = 0, models the

1
for 0 ≤ x ≤ 1.
1 + x2
(a) Find the displacement when the bar is subjected to a constant external force, f ≡ 1.
(b) Find the Green’s function for the boundary value problem. (c) Use the resulting superposition formula to check your solution to part (a). (d) Which point 0 < ξ < 1 on the
bar is the “weakest”, i.e., the bar experiences the largest displacement under a unit impulse
concentrated at that point?
displacement u(x) of a nonuniform elastic bar with stiﬀness c(x) =

6.2.5. Answer Exercise 6.2.4 when c(x) = 1 + x.
♥ 6.2.6. Consider the boundary value problem − u = f (x), u(0) = 0, u(1) = 2 u (1).
(a) Find the Green’s function. (b) Which of the fundamental properties does your Green’s
function satisfy? (c) Write down an explicit integral formula for the solution to the boundary value problem, and prove its validity by a direct computation. (d) Explain why the
related boundary value problem − u = f, u(0) = 0, u(1) = u (1), does not have a Green’s
function.


| x − ξ | < n1 ,
0,
otherwise.
(a) Find the solution un (x) to the boundary value problem − u = fn (x), u(0) = u(1) = 0,
assuming 0 < ξ − n1 < ξ + n1 < 1. (b) Prove that lim un (x) = G(x; ξ) converges to the

♥ 6.2.7. For n a positive integer, set fn (x) =

1
2 n,

n→∞

Green’s function (6.51). Why should this be the case? (c) Reconﬁrm the result in part (b)
by graphing u5 (x), u15 (x), u25 (x), along with G(x; ξ) when ξ = .3.
6.2.8. Solve the boundary value problem − 4 u + 9 u = 0, u(0) = 0, u(2) = 1. Is your solution
unique?
6.2.9. True or false: The Neumann boundary value problem − u + u = 1, u (0) = u (1) = 0,
has a unique solution.
6.2.10. Use the Green’s function
⎧ (6.64) to solve the boundary value problem (6.57) when the
⎨
1, 0 ≤ x < 12 ,
forcing function is f (x) = ⎩
−1, 12 < x ≤ 1.
6.2.11. Let ω > 0. (a) Find the Green’s function for the mixed boundary value problem
− u + ω 2 u = f (x), u(0) = 0, u (1) = 0. ⎧
⎨
1, 0 ≤ x < 12 ,
(b) Use your Green’s function to ﬁnd the solution when f (x) = ⎩
−1, 12 < x ≤ 1.
6.2.12. Suppose ω > 0. Does the Neumann boundary value problem − u + ω 2 u = f (x),
u (0) = u (1) = 0 admit a Green’s function? If not, explain why not. If so, ﬁnd it, and then
write down an integral formula for the solution of the boundary value problem.
♦ 6.2.13. (a) Prove the addition formula (6.63) for the hyperbolic sine function.
(b) Find the corresponding addition formula for the hyperbolic cosine.
♦ 6.2.14. Prove the diﬀerentiation formula (6.55).

242

6 Generalized Functions and Green’s Functions

6.3 Green’s Functions for the Planar Poisson Equation
Now we develop the Green’s function approach to solving boundary value problems involving the two-dimensional Poisson equation (4.84). As before, the Green’s function is
characterized as the solution to the homogeneous boundary value problem in which the
inhomogeneity is a concentrated unit impulse — a delta function. The solution to the
general forced boundary value problem is then obtained via linear superposition, that is,
as a convolution integral with the Green’s function.
However, before proceeding, we need to quickly review some basic facts concerning
vector calculus in the plane. The student may wish to consult a standard multivariable
calculus text, e.g., [8, 108], for additional details.
Calculus in the Plane
Let x = (x, y) denote the usual Cartesian coordinates on R 2 . The term scalar ﬁeld is
synonymous with a real-valued function u(x, y), deﬁned on a domain Ω ⊂ R 2 . A vectorvalued function


v1 (x, y)
v(x) = v(x, y) =
(6.70)
v2 (x, y)
is known as a (planar) vector ﬁeld . A vector ﬁeld assigns a vector v(x, y) ∈ R 2 to each point
(x, y) ∈ Ω in its domain of deﬁnition, and hence deﬁnes a function v: Ω → R 2 . Physical
examples include velocity vector ﬁelds of ﬂuid ﬂows, heat ﬂux ﬁelds in thermodynamics,
and gravitational and electrostatic force ﬁelds.
The gradient operator ∇ maps a scalar ﬁeld u(x, y) to the vector ﬁeld


∂u/∂x
∇u =
.
(6.71)
∂u/∂y
The scalar ﬁeld u is often referred to as a potential function for its gradient vector ﬁeld
v = ∇u. On a connected domain Ω, the potential, when it exists, is uniquely determined
up to addition of a constant.
T
The divergence of the planar vector ﬁeld v = ( v1 , v2 ) is the scalar ﬁeld
∇ · v = div v =

∂v
∂v1
+ 2.
∂x
∂y

(6.72)

∂v
∂v2
− 1.
∂x
∂y

(6.73)

Its curl is deﬁned as
∇ × v = curl v =

Notice that the curl of a planar vector ﬁeld is a scalar ﬁeld. (In contrast, in three dimensions, the curl of a vector ﬁeld is another vector ﬁeld.) Given a smooth potential u ∈ C2 ,
the curl of its gradient vector ﬁeld automatically vanishes:
∇ × ∇u =

∂ ∂u
∂ ∂u
−
≡ 0,
∂x ∂y
∂y ∂x

by the equality of mixed partials. Thus, a necessary condition for a vector ﬁeld v to admit
a potential is that it be irrotational, meaning ∇ × v = 0; this condition is suﬃcient if

6.3 Green’s Functions for the Planar Poisson Equation

Figure 6.10.

243

Orientation of the boundary of a planar domain.

the underlying domain Ω is simply connected , i.e., has no holes. On the other hand, the
divergence of a gradient vector ﬁeld coincides with the Laplacian of the potential function:
∂2u ∂2u
+ 2.
(6.74)
∂x2
∂y
A vector ﬁeld is incompressible if it has zero divergence: ∇ · v = 0; for the velocity vector
ﬁeld of a steady-state ﬂuid ﬂow, incompressibility means that the ﬂuid does not change
volume. (Water is, for all practical purposes, an incompressible ﬂuid.) Therefore, an
irrotational vector ﬁeld with potential u is also incompressible if and only if the potential
solves the Laplace equation Δu = 0.
∇ · ∇u = Δu =

Remark : Because of formula (6.74), the Laplacian operator is also sometimes written
as Δ = ∇2 . The factorization of the Laplacian into the product of the divergence and the
gradient operators is, in fact, of great importance, and underlies its “self-adjointness”, a
fundamental property whose ramiﬁcations will be explored in depth in Chapter 9.
Let Ω ⊂ R 2 be a bounded domain whose boundary ∂Ω consists of one or more piecewise
smooth closed curves. We orient the boundary so that the domain is always on one’s left
as one goes around the boundary curve(s). Figure 6.10 sketches a domain with two holes;
its three boundary curves are oriented according to the directions of the arrows. Note that
the outer boundary curve is traversed in a counterclockwise direction, while the two inner
boundary curves are oriented clockwise.
Green’s Theorem, ﬁrst formulated by George Green to use in his seminal study of
partial diﬀerential equations and potential theory, relates certain double integrals over a
domain to line integrals around its boundary. It should be viewed as the extension of the
Fundamental Theorem of Calculus to double integrals.
Theorem 6.13. Let v(x) be a smooth† vector ﬁeld deﬁned on a bounded domain
Ω ⊂ R 2 . Then the line integral of v around the boundary ∂Ω equals the double integral of
its curl over the domain:

&
∇ × v dx dy =
v · dx,
(6.75)
Ω

†

∂Ω

To be precise, we require v to be continuously diﬀerentiable within the domain, and continuous up to the boundary, so v ∈ C0 (Ω) ∩ C1 (Ω), where Ω = Ω ∪ ∂Ω denotes the closure of the
domain Ω.

244

6 Generalized Functions and Green’s Functions

or, in full detail,




Ω

∂v
∂v2
− 1
∂x
∂y



&
dx dy =

v1 dx + v2 dy .

(6.76)

∂Ω

Example 6.14. Let us apply Green’s Theorem 6.13 to the particular vector ﬁeld
T

v = ( y, 0 ) . Since ∇ × v ≡ −1, we obtain

&
y dx =
(−1) dx dy = − area Ω.
∂Ω

(6.77)

Ω

This means that we can determine the area of a planar domain by computing the negative
of the indicated line integral around its boundary.
For later purposes, we rewrite the basic Green identity (6.75) in an equivalent “diverT
gence form”. Given a planar vector ﬁeld v = ( v1 , v2 ) , let


− v2
v⊥ =
(6.78)
v1
denote the “perpendicular” vector ﬁeld. We note that its curl
∇ × v⊥ =

∂v
∂v1
+ 2 =∇·v
∂x
∂y

(6.79)

coincides with the divergence of the original vector ﬁeld.
When we replace v in Green’s identity (6.75) by v⊥ , the result is

&
&

⊥
⊥
∇ · v dx dy =
∇ × v dx dy =
v · dx =
v · n ds,
Ω

∂Ω

Ω

∂Ω

where n denotes the unit outwards normal to the boundary of our domain, while ds denotes
the arc-length element along the boundary curve. This yields the divergence form of Green’s
Theorem:

&
∇ · v dx dy =
v · n ds.
(6.80)
∂Ω

Ω

Physically, if v represents the velocity vector ﬁeld of a steady-state ﬂuid ﬂow, then the
line integral in (6.80) represents the net ﬂuid ﬂux out of the region Ω. As a result, the
divergence ∇ · v represents the local change in area of the ﬂuid at each point, which serves
to justify our earlier statement on incompressibility.
Consider next the product vector ﬁeld u v obtained by multiplying a vector ﬁeld v by
a scalar ﬁeld u. An elementary computation proves that its divergence is
∇ · (u v) = u ∇ · v + ∇u · v.

(6.81)

Replacing v by u v in the divergence formula (6.80), we deduce what is usually referred to
as Green’s formula

&
u (v · n) ds,
(6.82)
u ∇ · v + ∇u · v dx dy =
∂Ω

Ω

which is valid for arbitrary bounded domains Ω, and arbitrary C1 scalar and vector ﬁelds
deﬁned thereon. Rearranging the terms produces
&


∇u · v dx dy =
u (v · n) ds −
u ∇ · v dx dy.
(6.83)
Ω

∂Ω

Ω

6.3 Green’s Functions for the Planar Poisson Equation

245

We will view this identity as an integration by parts formula for double integrals. Indeed,
comparing with the one-dimensional integration by parts formula
 b

b



u (x) v(x) dx = u(x) v(x)
x=a

a

 b
−

u(x) v  (x) dx,

(6.84)

a

we observe that the single integrals have become double integrals; the derivatives are vector
derivatives (gradient and divergence), while the boundary contributions at the endpoints
of the interval are replaced by a line integral around the entire boundary of the twodimensional domain.
A useful special case of (6.82) is that in which v = ∇v is the gradient of a scalar ﬁeld
v. Then, in view of (6.74), Green’s formula (6.82) becomes

&
∂v
ds,
(6.85)
u Δv + ∇u · ∇v dx dy =
u
∂n
Ω
∂Ω
where ∂v/∂n = ∇v · n is the normal derivative of the scalar ﬁeld v on the boundary of the
domain. In particular, setting v = u, we deduce

&
∂u
ds.
(6.86)
u Δu +  ∇u 2 dx dy =
u
∂n
Ω
∂Ω
As an application, we establish a basic uniqueness theorem for solutions to the boundary
value problems for the Poisson equation:
Theorem 6.15. Suppose u
 and u both satisfy the same inhomogeneous Dirichlet or
mixed boundary value problem for the Poisson equation on a connected, bounded domain
Ω. Then u
 = u. On the other hand, if u
 and u satisfy the same Neumann boundary value
problem, then u
 = u + c for some constant c.
Proof : Since, by assumption, − Δ
u = f = − Δu, the diﬀerence v = u
 − u satisﬁes
the Laplace equation Δv = 0 in Ω, and satisﬁes the homogeneous boundary conditions.
Therefore, applying (6.86) to v, we ﬁnd

&
∂v
 ∇v 2 dx dy =
v
ds = 0,
∂n
Ω
∂Ω
since, at every point on the boundary, either v = 0 or ∂v/∂n = 0. Since the integrand is
continuous and everywhere nonnegative, we immediately conclude that  ∇v 2 = 0, and
hence ∇v = 0 throughout Ω. On a connected domain, the only functions annihilated by
the gradient operator are the constants:
Lemma 6.16. If v(x, y) is a C1 function deﬁned on a connected domain Ω ⊂ R 2 ,
then ∇v ≡ 0 if and only if v(x, y) ≡ c is a constant.
Proof : Let a, b be any two points in Ω. Then, by connectivity, we can ﬁnd a curve C
connecting them. The Fundamental Theorem for line integrals, [8, 108], states that

∇v · dx = v(b) − v(a).
C

Thus, if ∇v ≡ 0, then v(b) = v(a) for all a, b ∈ Ω, which implies that v must be constant.
Q.E.D.

246

6 Generalized Functions and Green’s Functions

Returning to our proof, we conclude that u
 = u + v = u + c, which proves the result
in the Neumann case. In the Dirichlet or mixed problems, there is at least one point on
the boundary where v = 0, and hence the only possible constant is v = c = 0, proving that
u
 = u.
Q.E.D.
Thus, the Dirichlet and mixed boundary value problems admit at most one solution,
while the Neumann boundary value problem has either no solutions or inﬁnitely many
solutions. Proof of existence of solutions is more challenging, and will be left to a more
advanced text, e.g., [35, 44, 61, 70].
If we subtract from formula (6.85) the formula

&
∂u
ds,
(6.87)
v
v Δu + ∇u · ∇v dx dy =
∂n
Ω
∂Ω
obtained by interchanging u and v, we obtain the identity

& 

∂u
∂v
−v
ds,
u
u Δv − v Δu dx dy =
∂n
∂n
Ω
∂Ω

(6.88)

which will play a major role in our analysis of the Poisson equation. Setting v = 1 in (6.87)
yields

&
∂u
Δu dx dy =
ds.
(6.89)
∂n
Ω
∂Ω
Suppose u solves the Neumann boundary value problem
− Δu = f,

in

∂u
=h
∂n

Ω

on ∂Ω.

Then (6.89) requires that


&
f dx dy +

Ω

h ds = 0,

(6.90)

∂Ω

which thus forms a necessary condition for the existence of a solution u to the inhomogeneous Neumann boundary value problem. Physically, if u represents the equilibrium
temperature of a plate, then the integrals in (6.89) measure the net gain or loss in heat energy due to, respectively, the external heat source and the heat ﬂux through the boundary.
Equation (6.90) is telling us that, for the plate to remain in thermal equilibrium, there can
be no net change in its total heat energy.
The Two–Dimensional Delta Function
Now let us return to the business at hand — solving the Poisson equation on a bounded
domain Ω ⊂ R 2 . We will subject the solution to either homogeneous Dirichlet boundary
conditions or homogeneous mixed boundary conditions. (As we just noted, the Neumann
boundary value problem does not admit a unique solution, and hence does not possess a
Green’s function.) The Green’s function for the boundary value problem arises when the
forcing function is a unit impulse concentrated at a single point in the domain.
Thus, our ﬁrst task is to establish the proper form for a unit impulse in our twodimensional context. The delta function concentrated at a point ξ = (ξ, η) ∈ R 2 is denoted
by
δ(ξ,η) (x, y) = δξ (x) = δ(x − ξ) = δ(x − ξ, y − η),
(6.91)

6.3 Green’s Functions for the Planar Poisson Equation

Figure 6.11.

247

Gaussian functions converging to the delta function.

and is designed so that

δξ (x) = 0,

x = ξ,

δ(ξ,η) (x, y) dx dy = 1,

ξ ∈ Ω.

(6.92)

Ω

In particular, δ(x, y) = δ0 (x, y) represents the delta function at the origin. As in the
one-dimensional version, there is no ordinary function that satisﬁes both criteria; rather,
δ(x, y) is to be viewed as the limit of a sequence of more and more highly concentrated
functions gn (x, y), with

while
gn (x, y) dx dy = 1.
lim gn (x, y) = 0, for (x, y) = (0, 0),
n→∞

R2

A good example of a suitable sequence is provided by the radial Gaussian functions
gn (x, y) =

n − n (x2 +y2 )
e
.
π

(6.93)

As plotted in Figure 6.11, as n → ∞, the Gaussian proﬁles become more and more concentrated near the origin, while maintaining a unit volume underneath their graphs. The
fact that their integral over R 2 equals 1 is a consequence of (2.99).
Alternatively, one can assign the delta function a dual interpretation as the linear
functional
L(ξ,η) [ u ] = Lξ [ u ] = u(ξ) = u(ξ, η),
(6.94)
which assigns to each continuous function u ∈ C0 (Ω) its value at the point ξ = (ξ, η) ∈ Ω.
Then, using the L2 inner product

u(x, y) v(x, y) dx dy
(6.95)
u,v =
Ω

between scalar ﬁelds u, v ∈ C0 (Ω), we formally identify the linear functional L(ξ,η) with
the delta “function” by the integral formula


u(ξ, η),
(ξ, η) ∈ Ω,
 δ(ξ,η) , u  =
(6.96)
δ(ξ,η) (x, y) u(x, y) dx dy =
0,
(ξ, η) ∈ R 2 \ Ω,
Ω

248

6 Generalized Functions and Green’s Functions

for any u ∈ C0 (Ω). As in the one-dimensional version, we will avoid deﬁning the integral
when the delta function is concentrated at a boundary point of the domain.
Since double integrals can be evaluated as repeated one-dimensional integrals, we can
conveniently view
δ(ξ,η) (x, y) = δξ (x) δη (y) = δ(x − ξ) δ(y − η)
(6.97)
as the product† of a pair of one-dimensional delta functions. Indeed, if the impulse point
.
(ξ, η) ∈ R = a < x < b, c < y < d ⊂ Ω
is contained in a rectangle that lies within the domain, then


δ(ξ,η) (x, y) u(x, y) dx dy =
δ(ξ,η) (x, y) u(x, y) dx dy
Ω
R





b

d

b

δ(x − ξ) δ(y − η) u(x, y) dy

=
a

c

δ(x − ξ) u(x, η) dx = u(ξ, η).

dx =
a

The Green’s Function
As in the one-dimensional context, the Green’s function is deﬁned as the solution to the
inhomogeneous diﬀerential equation when subject to a concentrated unit delta impulse at
a prescribed point ξ = (ξ, η) ∈ Ω inside the domain. In the current situation, the Poisson
equation takes the form
− Δu = δξ ,

−

or, explicitly,

∂2u ∂2u
− 2 = δ(x − ξ) δ(y − η).
∂x2
∂y

(6.98)

The function u(x, y) is also subject to some homogeneous boundary conditions, e.g., the
Dirichlet conditions u = 0 on ∂Ω. The resulting solution is called the Green’s function for
the boundary value problem, and written
Gξ (x) = G(x; ξ) = G(x, y; ξ, η).

(6.99)

Once we know the Green’s function, the solution to the general Poisson boundary
value problem
− Δu = f
in
Ω,
u=0
on
∂Ω
(6.100)
is reconstructed as follows. We regard the forcing function

δ(x − ξ) δ(y − η)f (ξ, η) dξ dη
f (x, y) =
Ω

as a superposition of delta impulses, whose strength equals the value of f at the impulse
point. Linearity implies that the solution to the boundary value problem is the corresponding superposition of Green’s function responses to each of the constituent impulses. The
net result is the fundamental superposition formula

u(x, y) =
G(x, y; ξ, η) f (ξ, η) dξ dη
(6.101)
Ω

†
This is an exception to our earlier injunction not to multiply delta functions. Multiplication
is allowed when they depend on diﬀerent variables.

6.3 Green’s Functions for the Planar Poisson Equation

249

for the solution to the boundary value problem. Indeed,

− ΔG(x, y; ξ, η) f (ξ, η) dξ dη
− Δu(x, y) =
 Ω
=
δ(x − ξ, y − η) f (ξ, η) dξ dη = f (x, y),
Ω

while the fact that G(x, y; ξ, η) = 0 for all (x, y) ∈ ∂Ω implies that u(x, y) = 0 on the
boundary.
The Green’s function inevitably turns out to be symmetric under interchange of its
arguments:
G(ξ, η; x, y) = G(x, y; ξ, η).
(6.102)
As in the one-dimensional case, symmetry is a consequence of the self-adjointness of the
boundary value problem, and will be explained in full in Chapter 9. Symmetry has the
following intriguing physical interpretation: Let x, ξ ∈ Ω be any two points in the domain.
We apply a concentrated unit force to the membrane at the ﬁrst point and measure its
deﬂection at the second; the result is exactly the same as if we applied the impulse at
the second point and measured the deﬂection at the ﬁrst. (Deﬂections at other points
in the domain will typically have no obvious relation with one another.) Similarly, in
electrostatics, the solution u(x, y) is interpreted as the electrostatic potential for a system
of charges in equilibrium. A delta function corresponds to a point charge, e.g., an electron.
The symmetry property says that the electrostatic potential at x due to a point charge
placed at position ξ is exactly the same as the potential at ξ due to a point charge at x.
The reader may wish to meditate on the physical plausibility of these striking facts.
Unfortunately, most Green’s functions cannot be written down in closed form. One
important exception occurs when the domain is the entire plane: Ω = R 2 . The solution
to the Poisson equation (6.98) is the free-space Green’s function G0 (x, y; ξ, η) = G0 (x; ξ),
which measures the eﬀect of a unit impulse, concentrated at ξ, throughout two-dimensional
space, e.g., the gravitational potential due to a point mass or the electrostatic potential
due to a point charge. To motivate the construction, let us appeal to physical intuition.
First, since the concentrated impulse is zero when x = ξ, the function must solve the
homogeneous Laplace equation
− ΔG0 = 0

for all

x = ξ.

(6.103)

Second, since the Poisson equation is modeling a homogeneous, uniform medium, in the
absence of boundary conditions the eﬀect of a unit impulse should depend only on the
distance from its source. Therefore, we expect G0 to be a function of the radial variable
alone:

G0 (x, y; ξ, η) = v(r),
where
r =  x − ξ  = (x − ξ)2 + (y − η)2 .
According to (4.113), the only radially symmetric solutions to the Laplace equation are
v(r) = a + b log r,

(6.104)

where a and b are constants. The constant term a has zero derivative, and so cannot
contribute to the delta function singularity. Therefore, we expect the required solution to
be a multiple of the logarithmic term. To determine the multiple, consider a closed disk of
radius ε > 0 centered at ξ,
. .
Dε = 0 ≤ r ≤ ε =  x − ξ  ≤ ε ,

250

6 Generalized Functions and Green’s Functions

with circular boundary
Cε = ∂Dε = { r =  x − ξ  = ε } = { ( ξ + ε cos θ, η + ε sin θ ) | − π ≤ θ ≤ π } .
Then, by (6.89),


δ(x, y) dx dy = − b
1=

∂(log r)
ds
Δ(log r) dx dy = − b
∂n
Dε
Dε
Cε
 π
∂(log r)
1
= −b
ds = − b
ds = − b
dθ = −2 π b,
∂r
−π
Cε
Cε r

(6.105)

and hence b = −1/(2 π). We conclude that the free-space Green’s function should have the
logarithmic form


1
1
1
log r = −
log  x − ξ  = −
log (x − ξ)2 + (y − η)2 .
2π
2π
4π
(6.106)
A fully rigorous, albeit more diﬃcult, justiﬁcation of (6.106) comes from the following
important result, known as Green’s representation formula.
G0 (x, y; ξ, η) = −

Theorem 6.17. Let Ω ⊂ R 2 be a bounded domain, with piecewise C1 boundary ∂Ω.
Suppose u ∈ C2 (Ω) ∩ C1 (Ω). Then, for any (x, y) ∈ Ω,

u(x, y) = −
G0 (x, y; ξ, η) Δu(ξ, η) dξ dη
Ω


∂G0
∂u
(ξ, η) −
(x, y; ξ, η) u(ξ, η) ds,
+
G0 (x, y; ξ, η)
∂n
∂n
∂Ω
(6.107)
where the Laplacian and the normal derivatives on the boundary are all taken with respect
to the integration variables ξ = (ξ, η).
In particular, if both u and ∂u/∂n vanish on ∂Ω, then (6.107) reduces to

u(x, y) = −
G0 (x, y; ξ, η) Δu(ξ, η) dξ dη.
Ω

Invoking the deﬁnition of the delta function on the left-hand side and formally applying
the Green identity (6.88) to the right-hand side produces




δ(x − ξ) δ(y − η) u(ξ, η) dξ dη =
(6.108)
− ΔG0 (x, y; ξ, η) u(ξ, η) dξ dη.
Ω

Ω

It is in this dual sense that we justify the desired formula
− ΔG0 (x; ξ) =

1 
Δ log  x − ξ  = δ(x − ξ).
2π

(6.109)

Proof of Theorem 6.17 : We ﬁrst note that, even though G0 (x, ξ) has a logarithmic
singularity at x = ξ, the double integral in (6.107) is ﬁnite. Indeed, after introducing polar
coordinates ξ = x + r cos θ, η = y + r sin θ, and recalling dξ dη = r dr dθ, we see that it
equals

1
(r log r) Δu dr dθ.
2π

6.3 Green’s Functions for the Planar Poisson Equation

251

x
Cε
∂Ω
Figure 6.12.

Domain Ωε = Ω \ Dε (x).

The product r log r is everywhere continuous — even at r = 0 — and so, provided Δu is
well behaved, e.g., continuous, the integral is ﬁnite. There is, of course, no problem with
the line integral in (6.107), since the contour does not go through the singularity.
Let us now avoid dealing directly with the singularity by working on a subdomain
Ωε = Ω \ Dε (x) = { ξ ∈ Ω |  x − ξ  > ε }
obtained by cutting out a small disk
Dε (x) = { ξ |  x − ξ  ≤ ε }
of radius ε > 0 centered at x. We choose ε suﬃciently small in order that Dε (x) ⊂ Ω, and
hence
.
∂Ωε = ∂Ω ∪ Cε ,
where
Cε =  x − ξ  = ε
is the circular boundary of the disk. The subdomain Ωε is represented by the shaded
region in Figure 6.12. Since the double integral is well deﬁned, we can approximate it by
integrating over Ωε :


G0 (x, y; ξ, η) Δu(ξ, η) dξ dη = lim
G0 (x, y; ξ, η) Δu(ξ, η) dξ dη.
(6.110)
Ω

ε→0

Ωε

Since G0 has no singularities in Ωε , we are able to apply the Green formula (6.85) and then
(6.103) to evaluate

G0 (x, y; ξ, η) Δu(ξ, η) dξ dη
Ωε

& 
∂u
∂G0
G0 (x, y; ξ, η)
=
(ξ, η) −
(x, y; ξ, η) u(ξ, η) ds
(6.111)
∂n
∂n
∂Ω

& 
∂u
∂G0
G0 (x, y; ξ, η)
−
(ξ, η) −
(x, y; ξ, η) u(ξ, η) ds,
∂n
∂n
Cε
where the line integral around Cε is taken in the usual counterclockwise direction — the
opposite orientation to that induced by its status as part of the boundary of Ωε . Now, on

252

6 Generalized Functions and Green’s Functions

the circle Cε ,
G0 (x, y; ξ, η) = −

log r
2π

log ε
,
2π

=−
r=ε

(6.112)

while, in view of Exercise 6.3.1,
∂G0
1 ∂(log r)
(x, y; ξ, η) = −
∂n
2π
∂r
Therefore,

=−
r=ε

&

∂G0
1
(x, y; ξ, η) u(ξ, η) ds = −
2 πε
Cε ∂n

1
.
2 πε

(6.113)

&
u(ξ, η) ds,
Cε

which we recognize as minus the average of u on the circle of radius ε. As ε → 0, the circles
shrink down to their common center, and so, by continuity, the averages tend to the value
u(x, y) at the center; thus,
&
∂G0
lim
(x, y; ξ, η) u(ξ, η) ds = − u(x, y).
(6.114)
ε → 0 C ∂n
ε
On the other hand, using (6.112), and then (6.89) on the disk Dε , we have
&
&
log ε
∂u
∂u
(ξ, η) ds = −
(ξ, η) ds
G0 (x, y; ξ, η)
∂n
2
π
∂n
Cε
C
 ε
log ε
=−
Δu(ξ, η) dξ dη = − (ε2 log ε) Δuε ,
2π
Dε
where

1
Δuε =
2 πε2


Δu(ξ, η) dξ dη
Dε

is the average of Δu over the disk Dε . As above, as ε → 0, the averages over the disks
converge to the value at their common center, Δuε → Δu(x, y), and hence
&
∂u
(ξ, η) ds = lim (− ε2 log ε) Δuε = 0.
lim
G0 (x, y; ξ, η)
(6.115)
ε→0 C
ε→0
∂n
ε
In view of (6.110, 114, 115), the ε → 0 limit of (6.111) is exactly the Green representation
formula (6.107).
Q.E.D.
As noted above, the free space Green’s function (6.106) represents the gravitational
potential in empty two-dimensional space due to a unit point mass, or, equivalently, the
two-dimensional electrostatic potential due to a unit point charge sitting at position ξ. The
corresponding gravitational or electrostatic force ﬁeld is obtained by taking its gradient:
F = ∇G0 = −

x−ξ
.
2 π  x − ξ 2

Its magnitude
F =

1
2π  x − ξ 

is inversely proportional to the distance from the mass or charge, which is the twodimensional form of Newton’s and Coulomb’s three-dimensional inverse square laws.

6.3 Green’s Functions for the Planar Poisson Equation

253

The gravitational potential due to a two-dimensional mass, e.g., a ﬂat plate, in the
shape of a domain Ω ⊂ R 2 is obtained by superimposing delta function sources with
strengths equal to the density of the material at each point. The result is the potential
function



1
ρ(ξ, η) log (x − ξ)2 + (y − η)2 dξ dη,
(6.116)
u(x, y) = −
4π
Ω
in which ρ(ξ, η) denotes the density at position (ξ, η) ∈ Ω.
Example 6.18. The gravitational potential due to a circular disk D = { x2 + y 2 ≤ 1 }
of unit radius and unit density ρ ≡ 1 is



1
u(x, y) = −
log (x − ξ)2 + (y − η)2 dξ dη.
(6.117)
4π
D
A direct evaluation of this double integral is not so easy. However, we can write down the
potential in closed form by recalling that it solves the Poisson equation

1,
 x  < 1,
− Δu =
(6.118)
0,
 x  > 1.
Moreover, u is clearly radially symmetric, and hence a function of r alone. Thus, in the
polar coordinate expression (4.105) for the Laplacian, the θ derivative terms vanish, and
so (6.118) reduces to

−1,
r < 1,
d2 u 1 du
=
+
dr2
r dr
0,
r > 1,
which is eﬀectively a ﬁrst-order linear ordinary diﬀerential equation for du/dr. Solving
separately on the two subintervals produces

a + b log r − 14 r2 ,
r < 1,
u(r) =
c + d log r,
r > 1,
where a, b, c, d are constants. Continuity of u(r) and u (r) at r = 1 implies c = a − 14 ,
d = b − 12 . Moreover, the potential for a non-concentrated mass cannot have a singularity
at the origin, and so b = 0. Direct evaluation of (6.117) at x = y = 0, using polar
coordinates, proves that a = 14 . We conclude that the gravitational potential (6.117) due
to a uniform disk of unit radius, and hence total mass (area) π, is, explicitly,

2
2
2
1
1
x2 + y 2 ≤ 1,
4 (1 − r ) = 4 (1 − x − y ),
(6.119)
u(x, y) =
x2 + y 2 ≥ 1.
− 12 log r = − 14 log(x2 + y 2 ),
Observe that, outside the disk, the potential is exactly the same as the logarithmic potential
due to a point mass of magnitude π located at the origin. Consequently, the gravitational
force ﬁeld outside a uniform disk is the same as if all its mass were concentrated at the
origin.
With the free-space logarithmic potential in hand, let us return to the question of ﬁnding the Green’s function for a boundary value problem on a bounded domain Ω ⊂ R 2 . Since
the logarithmic potential (6.106) is a particular solution to the Poisson equation (6.98), the
general solution, according to Theorem 1.6, is given by u = G0 + z, where z is an arbitrary
solution to the homogeneous equation Δz = 0, i.e., an arbitrary harmonic function. Thus,
constructing the Green’s function has been reduced to the problem of ﬁnding the harmonic
function z such that G = G0 + z satisﬁes the desired homogeneous boundary conditions.
Let us explicitly formulate this result for the (inhomogeneous) Dirichlet problem.

254

6 Generalized Functions and Green’s Functions

Theorem 6.19. The Green’s function for the homogeneous Dirichlet boundary value
problem for the Poisson equation on a bounded domain Ω ⊂ R 2 has the form
G(x, y; ξ, η) = G0 (x, y; ξ, η) + z(x, y; ξ, η),

(6.120)

where the ﬁrst term is the logarithmic potential (6.106), while, for each (ξ, η) ∈ Ω, the
second term is the harmonic function that solves the boundary value problem
Δz = 0
on
Ω,


1
log (x − ξ)2 + (y − η)2
z(x, y; ξ, η) =
4π

for

(x, y) ∈ ∂Ω.

(6.121)

If u(x, y) is a solution to the inhomogeneous Dirichlet problem
− Δu = f,
then

x ∈ Ω,


u(x, y) =
Ω

G(x, y; ξ, η) f (ξ, η) dξ dη −

x ∈ ∂Ω,

u = h,

∂G
(x, y; ξ, η) h(ξ, η) ds,
∂Ω ∂n

(6.122)

(6.123)

where the normal derivative of G is taken with respect to (ξ, η) ∈ ∂Ω.
Proof : To show that (6.120) is the Green’s function, we note that
− ΔG = − ΔG0 − Δz = δ(ξ,η)

in

Ω,

G(x, y; ξ, η) = G0 (x, y; ξ, η) + z(x, y; ξ, η) = 0

on

(6.124)

while
∂Ω.

(6.125)

Next, to establish the solution formula (6.123), since both z and u are C2 , we can use
(6.88) (with v = z, keeping in mind that Δz = 0) to establish

z(x, y; ξ, η) Δu(ξ, η) dξ dη
0=−
Ω


∂z
∂u
(ξ, η) −
(x, y; ξ, η) u(ξ, η) ds.
+
z(x, y; ξ, η)
∂n
∂n
∂Ω
Adding this to Green’s representation formula (6.107), and using (6.125), we deduce that

∂G(x, y; ξ, η)
u(x, y) = −
u(ξ, η) ds,
G(x, y; ξ, η) Δu(ξ, η) dξ dη −
∂n
Ω
∂Ω
which, given (6.122), produces (6.123).

Q.E.D.

The one subtle issue left unresolved is the existence of the solution. Read properly,
Theorem 6.19 states that if a classical solution exists, then it is necessarily given by the
Green’s function formula (6.123). Proving existence of the solution — and also the existence
of the Green’s function, or equivalently, the solution z to (6.121) — requires further indepth analysis, lying beyond the scope of this text. In particular, to guarantee existence,
the underlying domain must have a reasonably nice boundary, e.g., a piecewise smooth
curve without sharp cusps. Interestingly, lack of regularity at sharp cusps in the boundary
underlies the electromagnetic phenomenon known as St. Elmo’s ﬁre, cf. [121]. Extensions
to irregular domains, e.g., those with fractal boundaries, is an active area of contemporary
research. Moreover, unlike one-dimensional boundary value problems, mere continuity of

6.3 Green’s Functions for the Planar Poisson Equation

255

the forcing function f is not quite suﬃcient to ensure the existence of a classical solution to
the Poisson boundary value problem; diﬀerentiability does suﬃce, although this assumption
can be weakened. We refer to [61, 70], for a development of the Perron method based on
approximating the solution by a sequence of subsolutions, which, by deﬁnition, solve the
diﬀerential inequality − Δu ≤ f . An alternative proof, using the direct method of the
calculus of variations, can be found in [35]. The latter proof relies on the characterization
of the solution by a minimization principle, which we discuss in some detail in Chapter 9.

Exercises
♦ 6.3.1. Let CR be a circle of radius R centered at the origin and n its unit outward normal. Let
f (r, θ) be a function expressed in polar coordinates. Prove that ∂f /∂n = ∂f /∂r on CR .
6.3.2. Let f (x) > 0 be a continuous, positive function on the interval a ≤ x ≤ b. Let Ω be the
domain lying between the graph of f (x) on the interval [ a, b ] and the x–axis. Explain why
(6.77) reduces to the usual calculus formula for the area under the graph of f .
6.3.3. Explain what happens to the conclusion of Lemma 6.16 if Ω is not a connected domain.
6.3.4. Can you ﬁnd constants cn such that the functions gn (x, y) = cn [ 1 + n2 (x2 + y 2 ) ]−1
converge to the two-dimensional delta function: gn (x, y) → δ(x, y) as n → ∞?
♦ 6.3.5. Explain why the two-dimensional delta function satisﬁes the scaling law
1
δ(β x, β y) = 2 δ(x, y),
for
β = 0.
β
♦ 6.3.6. Write out a polar coordinate formula, in terms of δ(r − r0 ) and δ(θ − θ0 ), for the twodimensional delta function δ(x − x0 , y − y0 ) = δ(x − x0 ) δ(y − y0 ).
6.3.7. True or false: δ(x) = δ( x ).
♦ 6.3.8. Suppose that ξ = f (x, y), η = g(x, y) deﬁnes a one-to-one C1 map from a domain
D ⊂ R 2 to the domain Ω = { (ξ, η) = (f (x, y), g(x, y)) | (x, y) ∈ D } ⊂ R 2 , and has nonzero
Jacobian determinant: J(x, y) = fx gy − fy gx = 0 for all (x, y) ∈ D. Suppose further that
(0, 0) = (f (x0 , y0 ), g(x0 , y0 )) ∈ Ω for (x0 , y0 ) ∈ D. Prove the following formula governing
the eﬀect of the map on the two-dimensional delta function:
δ(x − x0 , y − y0 )
.
(6.126)
δ(f (x, y), g(x, y)) =
| J(x0 , y0 ) |


6.3.9. Suppose f (x, y) =

1,
0,

3 x − 2 y > 1,
3 x − 2 y < 1.

Compute its partial derivatives

∂f
∂f
and
in
∂x
∂y

the sense of generalized functions.
6.3.10. Find a series solution to the rectangular boundary value problem (4.91–92) when the
boundary data f (x) = δ(x − ξ) is a delta function at a point 0 < ξ < a. Is your solution
inﬁnitely diﬀerentiable inside the rectangle?
6.3.11. Answer Exercise 6.3.10 when f (x) = δ  (x − ξ) is the derivative of the delta function.
6.3.12. A 1 meter square plate is subject to the Neumann boundary conditions ∂u/∂n = 1 on
its entire boundary. What is the equilibrium temperature? Explain.
♦ 6.3.13. A conservation law for an equilibrium system in two dimensions is, by deﬁnition, a divergence expression
∂X
∂Y
+
=0
(6.127)
∂x
∂y

256

6 Generalized Functions and Green’s Functions
that vanishes for all solutions.
(a) Given a conservation law prescribed by v = (X, Y ) deﬁned on a simply connected
domain D, show that the line integral

C

v · n ds =

C

X dy − Y dx is path-independent,

meaning that its value depends only on the endpoints of the curve C.
(b) Show that the Laplace equation can be written as a conservation law, and write down
the corresponding path-independent line integral.
Note: Path-independent integrals are of importance in the study of cracks, dislocations, and
other material singularities, [ 49 ].
♦ 6.3.14. In two-dimensional dynamics, a conservation law is an equation of the form
∂T
∂X
∂Y
+
+
= 0,
(6.128)
∂t
∂x
∂y
in which T is the conserved density, while v = (X, Y ) represents the associated ﬂux .
T dx dy
(a) Prove that, on a bounded domain Ω ⊂ R 2 , the rate of change of the integral
Ω
of the conserved density depends only on the ﬂux through the boundary ∂Ω.
(b) Write the partial diﬀerential equation ut + u ux + u uy = 0 as a conservation law. What
is the integrated version?

The Method of Images
The preceding analysis exposes the underlying form of the Green’s function, but we are
still left with the determination of the harmonic component z(x, y) required to match the
logarithmic potential boundary values, cf. (6.121). We will discuss two principal analytic
techniques employed to produce explicit formulas. The ﬁrst is an adaptation of the method
of separation of variables, which leads to inﬁnite series expressions. We will not dwell on
this approach here, although a couple of the exercises ask the reader to work through some
of the details; see also the discussion leading up to (9.110). The second is the Method
of Images, which will be developed in this section. Another approach is based on the
theory of conformal mapping; it can be found in books on complex analysis, including
[53, 98]. While the ﬁrst two methods are limited to a fairly small class of domains, they
extend to higher-dimensional problems, as well as to certain other types of elliptic boundary
value problems, whereas conformal mapping is, unfortunately, restricted to two-dimensional
problems involving the Laplace and Poisson equations.
We already know that the singular part of the Green’s function for the two-dimensional
Poisson equation is provided by a logarithmic potential. The problem, then, is to construct
the harmonic part, called z(x, y) in (6.120), so that the sum has the correct homogeneous
boundary values, or, equivalently, so that z(x, y) has the same boundary values as the
logarithmic potential. In certain cases, z(x, y) can be thought of as the potential induced
by one or more hypothetical electric charges (or, equivalently, gravitational point masses)
that are located outside the domain Ω, arranged in such a manner that their combined
electrostatic potential happens to coincide with the logarithmic potential on the boundary
of the domain. The goal, then, is to place image charges of suitable strengths in the
appropriate positions.
Here, we will only consider the case of a single image charge, located at a position
η ∈ Ω. We scale the logarithmic potential (6.106) by the charge strength, and, for added

6.3 Green’s Functions for the Planar Poisson Equation

257

x
η
ξ

0

Figure 6.13.

Method of Images for the unit disk.

ﬂexibility, include an additional constant — the charge’s potential baseline:
z(x, y) = a log  x − η  + b,

η ∈ R 2 \ Ω.

The function z(x, y) is harmonic inside Ω, since the logarithmic potential is harmonic
everywhere except at the external singularity η. For the Dirichlet boundary value problem,
then, for each point ξ ∈ Ω, we must ﬁnd a corresponding image point η ∈ R 2 \ Ω and
constants a, b ∈ R such that†
log  x − ξ  = a log  x − η  + b

for all

x ∈ ∂Ω,

or, equivalently,
 x − ξ  = λ  x − η a

for all

x ∈ ∂Ω,

(6.129)

where λ = eb . For each ﬁxed ξ, η, λ, a, the equation in (6.129) will, typically, implicitly
prescribe a plane curve, but it is not clear that one can always arrange that these curves
all coincide with the boundary of our domain.
To make further progress, we appeal to a geometric construction based on similar
triangles. Let us select η = c ξ to be a point lying on the ray through ξ. Its location
is chosen so that the triangle with vertices 0, x, η is similar to the triangle with vertices
0, ξ, x, noting that they have the same angle at the common vertex 0 — see Figure 6.13.
Similarity requires that the triangles’ corresponding sides have a common ratio, and so
ξ
x
x− ξ
=
=
= λ.
x
η
x−η

(6.130)

The last equality implies that (6.129) holds with a = 1. Consequently, if we choose
η =

1
,
ξ

so that

η=

ξ
,
 ξ 2

(6.131)

then
 x 2 =  ξ   η  = 1.
†
To simplify the formulas, we have omitted the 1/(2 π) factor, which can easily be reinstated
at the end of the analysis.

258

6 Generalized Functions and Green’s Functions

Figure 6.14.

Green’s function for the unit disk.

Thus x lies on the unit circle, and, as a result, λ =  ξ  = 1/ η . The map taking a
point ξ inside the disk to its image point η deﬁned by (6.131) is known as inversion with
respect to the unit circle.
We have now demonstrated that the potentials

1
1
1
  ξ 2 x − ξ 
log  x − ξ  =
log  ξ   x − η  =
log
,
2π
2π
2π
ξ

 x  = 1,
(6.132)

have the same boundary values on the unit circle. Consequently, their diﬀerence
G(x; ξ) = −

1
1
  ξ 2 x − ξ 
1
  ξ 2 x − ξ 
log  x − ξ  +
log
=
log
2π
2π
ξ
2π
ξ x−ξ

(6.133)

has the required properties for the Green’s function for the Dirichlet problem on the unit
disk. Writing this in terms of polar coordinates
x = (r cos θ, r sin θ),

ξ = (ρ cos φ, ρ sin φ),

and applying the Law of Cosines to the triangles in Figure 6.13 produces the explicit
formula


1
1 + r2 ρ2 − 2 rρ cos(θ − φ)
G(r, θ; ρ, φ) =
log
.
(6.134)
4π
r2 + ρ2 − 2 rρ cos(θ − φ)
In Figure 6.14 we sketch the Green’s function for the Dirichlet boundary value problem
corresponding to a unit impulse being applied at a point halfway between the center and
the edge of the disk. We also require its radial derivative
1 − r2
∂G
1
(r, θ; 1, φ) = −
,
∂ρ
2 π 1 + r2 − 2 r cos(θ − φ)

(6.135)

which coincides with its normal derivative on the unit circle. Thus, specializing (6.123),
we arrive at a solution to the general Dirichlet boundary value problem for the Poisson
equation in the unit disk.

6.3 Green’s Functions for the Planar Poisson Equation

Figure 6.15.

259

The Poisson kernel.

Theorem 6.20. The solution to the inhomogeneous Dirichlet boundary value problem
− Δu = f,

for r =  x  < 1,

u = h,

for r = 1,

is, when expressed in polar coordinates,


 π  1
1
1 + r2 ρ2 − 2 rρ cos(θ − φ)
ρ dρ dφ
u(r, θ) =
f (ρ, φ) log
r2 + ρ2 − 2 rρ cos(θ − φ)
4 π −π 0
(6.136)
 π
1
1 − r2
+
dφ.
h(φ)
2 π −π
1 + r2 − 2 r cos(θ − φ)
When f ≡ 0, formula (6.136) recovers the Poisson integral formula (4.126) for the
solution to the Dirichlet boundary value problem for the Laplace equation. In particular,
the boundary data h(θ) = δ(θ − φ), corresponding to a concentrated unit heat source
applied to a single point on the boundary, produces the Poisson kernel
u(r, θ) =

1 − r2
.
2 π 1 + r2 − 2 r cos(θ − φ)

(6.137)

The reader may enjoy verifying that this function indeed solves the Laplace equation and
has the correct boundary values in the limit as r → 1.

Exercises
6.3.15. A circular disk of radius 1 is subject to a heat source of unit magnitude on the subdisk
r ≤ 12 . Its boundary is kept at 0◦ .
(a) Write down an integral formula for the equilibrium temperature.
(b) Use radial symmetry to ﬁnd an explicit formula for the equilibrium temperature.

260

6 Generalized Functions and Green’s Functions

6.3.16. A circular disk of radius 1 meter is subject to a unit concentrated heat source at its
center and has completely insulated boundary. What is the equilibrium temperature?
♥ 6.3.17. (a) For n > 0, ﬁnd the solution to the boundary value problem
− Δu =

n − n (x2 +y2 )
e
,
π

x2 + y 2 < 1,

u(x, y) = 0,

x2 + y 2 = 1.

(b) Discuss what happens in the limit as n → ∞.
♥ 6.3.18. (a) Use the Method of Images to construct the Green’s function for a half-plane { y > 0 }
that is subject to homogeneous Dirichlet boundary conditions. Hint: The image point is
obtained by reﬂection. (b) Use your Green’s function to solve the boundary value problem
− Δu =

1
,
1+y

y > 0,

u(x, 0) = 0.

6.3.19. Construct the Green’s function for the half-disk Ω = { x2 + y 2 < 1, y > 0 } when
subject to homogeneous Dirichlet boundary conditions. Hint: Use three image points.
6.3.20. Prove directly that the Poisson kernel (6.137) solves the Laplace equation for all r < 1.
♥ 6.3.21. Provide the details for the following alternative method for solving the homogeneous
Dirichlet boundary value problem for the Poisson equation on the unit square:
− uxx − uyy = f (x, y),

u(x, 0) = 0,

u(x, 1) = 0,

u(0, y) = 0,

u(1, y) = 0,

0 < x, y < 1.

(a) Write both u(x, y) and f (x, y) as Fourier sine series in y whose coeﬃcients depend on x.
(b) Substitute these series into the diﬀerential equation, and equate Fourier coeﬃcients to
obtain an inﬁnite system of ordinary boundary value problems for the x-dependent Fourier
coeﬃcients of u. (c) Use the Green’s functions for each boundary value problem to write
out the solution and hence a series for the solution to the original boundary value problem.
(d) Implement this method for the following forcing functions:
(i ) f (x, y) = sin πy, (ii ) f (x, y) = sin πx sin 2 πy, (iii ) f (x, y) = 1.
♦ 6.3.22. Use the method of Exercise 6.3.21 to ﬁnd a series representation for the Green’s function
of a unit square subject to Dirichlet boundary conditions.
6.3.23. Write out the details of how to derive (6.134) from (6.133).
6.3.24. True or false: If the gravitational potential at a point a is greater than its value at the
point b, then the magnitude of the gravitational force at a is greater than its value at b.
♠ 6.3.25. (a) Write down integral formulas for the gravitational potential and force due to a square
plate S = { −1 ≤ x, y ≤ 1 } of unit density ρ = 1.√ (b)√Use
numerical integration to calculate

the gravitational force at the points (2, 0) and
2 , 2 . Before starting, try to predict
which point experiences the stronger force, and then check your prediction.
♠ 6.3.26. An equilateral triangular plate with unit area exerts a gravitational force on an
observer sitting a unit distance away from its center. Is the force greater if the observer is
located opposite a vertex of the triangle or opposite a side? Is the force greater than or less
than that exerted by a circular plate of the same area? Use numerical integration to evaluate
the double integrals.
6.3.27. Consider the wave equation utt = c2 uxx on the line − ∞ < x < ∞. Use the d’Alembert
formula (2.82) to solve the initial value problem u(0, x) = δ(x − a), ut (0, x) = 0. Can you
realize your solution as the limit of classical solutions?
♦ 6.3.28. Consider the wave equation utt = c2 uxx on the line − ∞ < x < ∞. Use the d’Alembert
formula (2.82) to solve the initial value problem u(0, x) = 0, ut (0, x) = δ(x − a), modeling
the eﬀect of striking the string with a highly concentrated blow at the point x = a. Graph
the solution at several times. Discuss the behavior of any discontinuities in the solution. In
particular, show that u(t, x) = 0 on the domain of inﬂuence of the point (0, a).

6.3 Green’s Functions for the Planar Poisson Equation

261

6.3.29. (a) Write down the solution u(t, x) to the wave equation utt = 4uxx on the real line

∂u
1 − | x |, | x | ≤ 1,
with initial data u(0, x) =
(0, x) = 0. (b) Explain why u(t, x) is
0,
otherwise, ∂t
not a classical solution to the wave equation. (c) Determine the derivatives ∂ 2 u/∂t2 and
∂ 2 u/∂x2 in the sense of distributions (generalized functions) and use this to justify the fact
that u(t, x) solves the wave equation in a distributional sense.
♥ 6.3.30. A piano string of length = 3 and wave speed c = 2 with both ends ﬁxed is hit by a
hammer 13 of the way along. The initial-boundary value problem that governs the resulting
vibrations of the string is
∂2u
∂2u
∂u
(0, x) = δ(x − 1).
=
4
,
u(t, 0) = 0 = u(t, 3),
u(0, x) = 0,
∂t2
∂x2
∂t
(a) What are the fundamental frequencies of vibration?
(b) Write down the solution to the initial-boundary value problem in Fourier series form.
(c) Write down the Fourier series for the velocity ∂u/∂t of your solution.
(d) Write down the d’Alembert formula for the solution, and sketch a picture of the string
at four or ﬁve representative times.
(e) True or false: The solution is periodic in time. If true, what is the period? If false,
explain what happens as t increases.
6.3.31. (a) Write down a Fourier series for the solution to the initial-boundary value problem
∂2u
∂u
∂2u
=
,
u(t, −1) = 0 = u(t, 1),
u(0, x) = δ(x),
(0, x) = 0.
2
∂t
∂x2
∂t
(b) Write down an analytic formula for the solution, i.e., sum your series.
(c) In what sense does the series solution in part (a) converge to the true solution? Do the
partial sums provide a good approximation to the actual solution?
6.3.32. Answer Exercise 6.3.31 for
∂2u
∂2u
=
,
u(t, −1) = 0 = u(t, 1),
∂t2
∂x2

u(0, x) = 0,

∂u
(0, x) = δ(x).
∂t

Chapter 7

Fourier Transforms

Fourier series and their ilk are designed to solve boundary value problems on bounded
intervals. The extension of the Fourier calculus to the entire real line leads naturally to the
Fourier transform, a powerful mathematical tool for the analysis of aperiodic functions.
The Fourier transform is of fundamental importance in a remarkably broad range of applications, including both ordinary and partial diﬀerential equations, probability, quantum
mechanics, signal and image processing, and control theory, to name but a few.
In this chapter, we motivate the construction by investigating how (rescaled) Fourier
series behave as the length of the interval goes to inﬁnity. The resulting Fourier transform
maps a function deﬁned on physical space to a function deﬁned on the space of frequencies,
whose values quantify the “amount” of each periodic frequency contained in the original
function. The inverse Fourier transform then reconstructs the original function from its
transformed frequency components. The integrals deﬁning the Fourier transform and its
inverse are, remarkably, almost identical, and this symmetry is often exploited, for example
in assembling tables of Fourier transforms.
One of the most important properties of the Fourier transform is that it converts
calculus — diﬀerentiation and integration — into algebra — multiplication and division.
This underlies its application to linear ordinary diﬀerential equations and, in the following
chapters, partial diﬀerential equations. In engineering applications, the Fourier transform
is sometimes overshadowed by the Laplace transform, which is a particular subcase. The
Fourier transform is used to analyze boundary value problems on the entire line. The
Laplace transform is better suited to solving initial value problems, [23], but will not be
developed in this text.
The Fourier transform is, like Fourier series, completely compatible with the calculus
of generalized functions, [68]. The ﬁnal section contains a brief introduction to the analytic
foundations of the subject, including the basics of Hilbert space. However, a full, rigorous
development requires more powerful analytical tools, including the Lebesgue integral and
complex analysis, and the interested reader is therefore referred to more advanced texts,
including [37, 68, 98, 117].

7.1 The Fourier Transform
We begin by motivating the Fourier transform as a limiting case of Fourier series. Although
the rigorous details are subtle, the underlying idea can be straightforwardly explained. Let
f (x) be a function deﬁned for all −∞ < x < ∞. The goal is to construct a Fourier expanP.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_7, © Springer International Publishing Switzerland 2014

263

264

7 Fourier Transforms

sion for f (x) in terms of basic trigonometric functions. One evident approach is to construct
its Fourier series on progressively longer and longer intervals, and then take the limit as
their lengths go to inﬁnity. This limiting process converts the Fourier sums into integrals,
and the resulting representation of a function is renamed the Fourier transform. Since we
are dealing with an inﬁnite interval, there are no longer any periodicity requirements on
the function f (x). Moreover, the frequencies represented in the Fourier transform are no
longer constrained by the length of the interval, and so we are eﬀectively decomposing a
quite general aperiodic function into a continuous superposition of trigonometric functions
of all possible frequencies.
Let us present the details in a more concrete form. The computations will be significantly simpler if we work with the complex version of the Fourier series from the outset.
Our starting point is the rescaled Fourier series (3.86) on a symmetric interval [ −  ,  ] of
length 2 , which we rewrite in the adapted form

∞

π f (kν ) i kν x
e
.
(7.1)
f (x) ∼
2

ν = −∞
The sum is over the discrete collection of frequencies
πν
,
ν = 0, ±1, ±2, . . . ,
(7.2)

corresponding to those trigonometric functions that have period 2 . For reasons that will
soon become apparent, the Fourier coeﬃcients of f are now denoted as


π f (kν )
1
,
(7.3)
cν =
f (x) e− i kν x dx =
2 −
2

kν =

so that
1
f (kν ) = √
2π



f (x) e− i kν x dx.

(7.4)

−

This reformulation of the basic Fourier series formula allows us to easily pass to the limit
as the interval’s length  → ∞.
On an interval of length 2 , the frequencies (7.2) required to represent a function in
Fourier series form are equally distributed, with interfrequency spacing
π
.
(7.5)

As  → ∞, the spacing Δk → 0, and so the relevant frequencies become more and more
densely packed in the line −∞ < k < ∞. In the limit, we thus anticipate that all possible
frequencies will be represented. Indeed, letting kν = k be arbitrary in (7.4), and sending
 → ∞, results in the inﬁnite integral
 ∞
1

√
f (k) =
f (x) e− i k x dx,
(7.6)
2 π −∞
Δk = kν+1 − kν =

known as the Fourier transform of the function f (x). If f (x) is a suﬃciently nice function,
e.g., piecewise continuous and decaying to 0 reasonably quickly as | x | → ∞, its Fourier
transform f(k) is deﬁned for all possible frequencies k ∈ R. The preceding formula will
sometimes conveniently be abbreviated as
f(k) = F [ f (x) ],

(7.7)

7.1 The Fourier Transform

265

where F is the Fourier transform operator , which maps each (suﬃciently nice) function of
the spatial variable x to a function of the frequency variable k.
To reconstruct the function from its Fourier transform, we apply a similar limiting
procedure to the Fourier series (7.1), which we ﬁrst rewrite in a more suggestive form,
∞

1
f (x) ∼ √
f (kν ) e i kν x Δk,
2 π ν = −∞

(7.8)

using (7.5). For each ﬁxed value of x, the right-hand side has the form of a Riemann sum
approximating the integral
 ∞
1
√
f (k) e i k x dk.
2 π −∞
As  → ∞, the functions (7.4) converge to the Fourier transform: f (k) → f(k); moreover,
the interfrequency spacing Δk = π/ → 0, and so one expects the Riemann sums to
converge to the limiting integral
 ∞
1
f(k) e i k x dk.
f (x) ∼ √
(7.9)
2 π −∞
The resulting formula serves to deﬁne the inverse Fourier transform, which is used to recover the original signal from its Fourier transform. In this manner, the Fourier series has
become a Fourier integral that reconstructs the function f (x) as a (continuous) superposi√
tion of complex exponentials e i k x of all possible frequencies, with f(k)/ 2 π quantifying
the amount contributed by the complex exponential of frequency k. In abbreviated form,
formula (7.9) can be written
(7.10)
f (x) = F −1 [ f(k) ],
thus deﬁning the inverse of the Fourier transform operator (7.7).
It is worth pointing out that both the Fourier transform (7.7) and its inverse (7.10)
deﬁne linear operators on function space. This means that the Fourier transform of the
sum of two functions is the sum of their individual transforms, while multiplying a function
by a constant multiplies its Fourier transform by the same factor:
F [ f (x) + g(x) ] = F [ f (x) ] + F [ g(x) ] = f(k) + 
g(k),

F [ c f (x) ] = c F [ f (x) ] = c f(k).

(7.11)

A similar statement holds for the inverse Fourier transform F −1 .
Recapitulating, by letting the length of the interval go to ∞, the discrete Fourier series
has become a continuous Fourier integral, while the Fourier coeﬃcients, which were deﬁned
only at a discrete collection of possible frequencies, have become a complete function f(k)
deﬁned on all of frequency space. The reconstruction of f (x) from its Fourier transform
f(k) via (7.9) can be rigorously justiﬁed under suitable hypotheses. For example, if f (x)
is piecewise C1 on all of R and decays reasonably rapidly, f (x) → 0 as | x | → ∞, so
that its Fourier integral (7.6) converges absolutely, then it can be proved, [37, 117], that
the inverse Fourier integral (7.9) will converge to f (x) at all points of continuity, and to
the midpoint 12 (f (x− ) + f (x+ )) at jump discontinuities — just like a Fourier series. In
particular, its Fourier transform f(k) → 0 must also decay as | k | → ∞, implying that (as
with Fourier series) the very high frequency modes make negligible contributions to the

266

7 Fourier Transforms

f(k)

f (x)

Figure 7.1.

Fourier transform of a rectangular pulse.

reconstruction of such a signal. A more precise result will be formulated in Theorem 7.15
below.
Example 7.1. The Fourier transform of the rectangular pulse †

1,
− a < x < a,
f (x) = σ(x + a) − σ(x − a) =
0,
| x | > a,
of width 2 a, is easily computed:

 a
ika
−i ka
1
−
e
2 sin a k
e
−
i
k
x
√
f(k) = √
.
=
e
dx =
π
k
2 π −a
2π i k

(7.12)

(7.13)

On the other hand, the reconstruction of the pulse via the inverse transform (7.9) tells us
that
⎧ 1,
−a < x < a,
 ∞ ikx
⎨
1
e
sin a k
1
,
x = ± a,
(7.14)
dk = f (x) =
⎩ 2
π −∞
k
0,
| x | > a.
Note the convergence to the middle of the jump discontinuities at x = ± a. The real part
of this complex integral produces a striking trigonometric integral identity:
⎧ 1, −a < x < a,
 ∞
⎨
1
cos x k sin a k
1
, x = ± a,
dk =
(7.15)
⎩ 2
π −∞
k
0, | x | > a.
Just as many Fourier series yield nontrivial summation formulas, the reconstruction of a
function from its Fourier transform often leads to nontrivial integration formulas. One
†

σ(x) is the unit step function (3.46).

7.1 The Fourier Transform

267

cannot compute the integral (7.14) by the Fundamental Theorem of Calculus, since there
is no elementary function whose derivative equals the integrand.† In Figure 7.1 we display
the box function with a = 1, its Fourier transform, along with a reconstruction obtained
by numerically integrating (7.15). Since we are dealing with an inﬁnite integral, we must
break oﬀ the numerical integrator by restricting it to a ﬁnite interval. The ﬁrst graph
in the second row is obtained by integrating from −5 ≤ k ≤ 5, while the second is from
−10 ≤ k ≤ 10. The nonuniform convergence of the integral leads to the appearance
of a Gibbs phenomenon at the two discontinuities, similar to what we observed in the
nonuniform convergence of a Fourier series.
On the other hand, the identity resulting from the imaginary part,
 ∞
1
sin k x sin a k
dk = 0,
π −∞
k
is, on the surface, not surprising, because the integrand is odd. However, it is far from
obvious that either integral converges; indeed, the amplitude of the oscillatory integrand
decays like 1/| k |, but the latter function does not have a convergent integral, and so the
usual comparison test for inﬁnite integrals, [8, 97], fails to apply. Their convergence is
marginal at best, and the trigonometric oscillations somehow manage to ameliorate the
slow rate of decay of 1/k.
Example 7.2. Consider an exponentially decaying right-handed pulse‡
 −ax
e
,
x > 0,
fr (x) =
0,
x < 0,

(7.16)

where a > 0. We compute its Fourier transform directly from the deﬁnition:
 ∞
∞
1
1 e−(a+ i k)x
1
−ax − i kx

e
e
dx = − √
= √
.
f r (k) = √
a
+
i
k
2π 0
2π
2 π (a + i k)
x=0
As in the preceding example, the inverse Fourier transform produces a nontrivial integral
identity:
⎧
−ax
⎪
,
x > 0,
 ∞
⎨ e
ikx
e
1
1
(7.17)
dk =
x = 0,
2,
⎪
2 π −∞ a + i k
⎩
0,
x < 0.
Similarly, a pulse that decays to the left,
 ax
e ,
fl (x) =
0,

x < 0,
x > 0,

(7.18)

where a > 0 is still positive, has Fourier transform
1
.
fl (k) = √
2 π (a − i k)

(7.19)

†

One can use Euler’s formula (3.59) to reduce (7.14) to a complex version of the exponential
integral (eα k /k) dk, but it can be proved, [ 25 ], that neither integral can be written in terms of
elementary functions.
‡
Note that we cannot Fourier transform the entire exponential function e− a x , because it does
not go to zero at both ± ∞, which is required for the integral (7.6) to converge.

268

7 Fourier Transforms

Right pulse fr (x)

Left pulse fl (x)

Even pulse fe (x)

Odd pulse fo (x)

Figure 7.2.

Exponential pulses.

This also follows from the general fact that the Fourier transform of f (− x) is f(− k); see
Exercise 7.1.10. The even exponentially decaying pulse
fe (x) = e− a | x |
is merely the sum of left and right pulses: fe = fr + fl . Thus, by linearity,

a
2
1
1



f e (k) = f r (k) + f l (k) = √
+ √
=
.
2
π k + a2
2 π (a + i k)
2 π (a − i k)

(7.20)

(7.21)

The resulting Fourier transform is real and even because fe (x) is a real-valued even function; see Exercise 7.1.12. The inverse Fourier transform (7.9) produces another nontrivial
integral identity:
 ∞

a e ikx
1
a ∞ cos k x
−a| x |
e
=
dk =
dk.
(7.22)
π −∞ k 2 + a2
π −∞ k 2 + a2
(The imaginary part of the integral vanishes, because its integrand is odd.) On the other
hand, the odd exponentially decaying pulse,
 −ax
,
x > 0,
e
−a| x |
fo (x) = (sign x) e
=
(7.23)
ax
x < 0,
−e ,
is the diﬀerence of the right and left pulses, fo = fr − fl , and has purely imaginary and
odd Fourier transform

k
2
1
1
fo (k) = fr (k) − fl (k) = √
. (7.24)
− √
= −i
2 + a2
π
k
2 π (a + i k)
2 π (a − i k)

7.1 The Fourier Transform

269

The inverse transform is
(sign x) e− a | x | = −

i
π

 ∞

k e ikx
1
dk =
2
2
π
−∞ k + a

 ∞

k sin k x
dk.
2
2
−∞ k + a

(7.25)

a > 0.

(7.26)

As a ﬁnal example, consider the rational function
f (x) =

1
x2 + a2

,

Its Fourier transform requires integrating
1
f(k) = √
2π

where
 ∞

e− i k x
dx.
2
2
−∞ x + a

(7.27)

The indeﬁnite integral (anti-derivative) does not appear in basic integration tables, and, in
fact, cannot be done in terms of elementary functions. However, we have just managed to
evaluate this particular integral! Look at (7.22). If we change x to k and k to − x, then we

exactly recover the integral (7.27) up to a factor of a 2/π. We conclude that the Fourier
transform of (7.26) is

π e− a | k |
f(k) =
.
(7.28)
2
a
This last example is indicative of an important general fact. The reader has no doubt
already noted the remarkable similarity between the Fourier transform (7.6) and its inverse
(7.9). Indeed, the only diﬀerence is that the former has a minus sign in the exponential.
This implies the following Symmetry Principle relating the direct and inverse Fourier transforms.
Theorem 7.3. If the Fourier transform of the function f (x) is f(k), then the Fourier
transform of f(x) is f (− k).
The Symmetry Principle allows us to reduce the tabulation of Fourier transforms by
half. For instance, referring back to Example 7.1, we deduce that the Fourier transform of

the function
2 sin a x
f (x) =
π
x
⎧
is
− a < k < a,
⎨ 1,
1
,
k = ± a,
f(k) = σ(− k + a) − σ(− k − a) = σ(k + a) − σ(k − a) =
(7.29)
⎩ 2
0,
| k | > a.

Note that, by linearity, we can divide both f (x) and f(k) by 2/π to deduce the Fourier
sin a x
transform of
.
x
√
Warning: Some authors omit the 2 π factor in the deﬁnition (7.6) of the Fourier
transform f(k). This alternative convention does have a slight advantage of eliminating
√
many 2 π factors in the transformed expressions. However, this necessitates an
√ extra
such factor in the reconstruction formula (7.9), which is achieved by replacing 2 π by
2 π. A signiﬁcant disadvantage is that the resulting formulas for the Fourier transform and
its inverse are less similar, and so the Symmetry Principle of Theorem 7.3 requires some
modiﬁcation. (On the other hand, convolution — to be discussed below — is a little easier
without the extra factor.) Yet another, more recent, convention can be found in Exercise
7.1.18. When consulting any particular reference, the reader always needs to check which
version of the Fourier transform is being used.

270

7 Fourier Transforms

All of the functions in Example 7.2 required a > 0 for the Fourier integrals to converge.
The functions that emerge in the limit as a goes to 0 are of special interest. Let us start
with the odd exponential pulse (7.23). When a → 0, the function fo (x) converges to the
sign function

+1,
x > 0,
f (x) = sign x = σ(x) − σ(− x) =
(7.30)
−1,
x < 0.
Taking the limit of the Fourier transform (7.24) leads to

2 1
f(k) = − i
.
π k

(7.31)

The nonintegrable singularity of f(k) at k = 0 is indicative of the fact that the sign function
does not decay as | x | → ∞. In this case, neither the Fourier transform integral nor its
inverse are well deﬁned as standard (Riemann, or even Lebesgue) integrals. Nevertheless, it
is possible to rigorously justify these results within the framework of generalized functions.
More interesting are the even pulse functions fe (x), which, in the limit a → 0, become
the constant function
f (x) ≡ 1.
(7.32)
The limit of the Fourier transform (7.21) is


0,
a
2
=
lim
2
2
a→0
π k +a
∞,

k = 0,
k = 0.

(7.33)

This limiting behavior should remind the reader of our construction (6.10) of the delta
function as the limit of the functions
δ(x) = lim

n

n → ∞ π (1 + n2 x2 )

= lim

a

a → 0 π (a2 + x2 )

.

Comparing with (7.33), we conclude that the Fourier transform of the constant function
(7.32) is a multiple of the delta function in the frequency variable:
√
f(k) = 2 π δ(k).
(7.34)
The direct transform integral
δ(k) =

1
2π

 ∞

e− i k x dx

(7.35)

−∞

is, strictly speaking, not deﬁned, because the inﬁnite integrals of the oscillatory sine and
cosine functions don’t converge! However, this identity can be validly interpreted within
the framework of weak convergence and generalized functions. On the other hand, the
inverse transform formula (7.9) yields
 ∞
δ(k) e i k x dk = e i k 0 = 1,
−∞

which is in accord with the basic deﬁnition (6.16) of the delta function. As in the preceding
case, the delta function singularity at k = 0 manifests the lack of decay of the constant
function.

7.1 The Fourier Transform

271

Conversely, the delta function δ(x) has constant Fourier transform
1

δ(k)
= √
2π

 ∞

e− i k 0
1
δ(x) e− i k x dx = √
≡√
,
2π
2π
−∞

(7.36)

a result that also follows from the Symmetry Principle of Theorem 7.3. To determine the
Fourier transform of a delta spike δξ (x) = δ(x − ξ) concentrated at position x = ξ, we
compute
 ∞
1
e− i k ξ
δξ (k) = √
.
(7.37)
δ(x − ξ) e− i k x dx = √
2 π −∞
2π
The result is a pure exponential in frequency space. Applying the inverse Fourier transform
(7.9) leads, at least on a formal level, to the remarkable identity
δξ (x) = δ(x − ξ) =

1
2π

 ∞

e i k(x−ξ) dk =

−∞

1
 e ikx , e ikξ  ,
2π

(7.38)

where  · , ·  denotes the L2 Hermitian inner product of complex-valued functions of
k ∈ R. Since the delta function vanishes for x = ξ, this identity is telling us that complex
exponentials of diﬀering frequencies are mutually orthogonal. However, as with (7.35),
this makes sense only within the language of generalized functions. On the other hand,
multiplying both sides of (7.38) by f (ξ) and then integrating with respect to ξ produces
1
f (x) =
2π

 ∞  ∞
−∞

f (ξ) e i k(x−ξ) dx dk.

(7.39)

−∞

This is a perfectly valid formula, being a restatement (or, rather, combination) of the
basic formulas (7.6) and (7.9) connecting the direct and inverse Fourier transforms of the
function f (x).
Conversely, the Symmetry Principle tells √
us that the Fourier transform of a pure
exponential e i κ x will be a shifted delta spike 2 π δ(k − κ), concentrated at frequency
k = κ. Both results are particular cases of the following Shift Theorem, whose proof is left
as an exercise for the reader.
Theorem 7.4. If f (x) has Fourier transform f(k), then the Fourier transform of the
shifted function f (x − ξ) is e− i k ξ f(k). Similarly, the transform of the product function
e i κ x f (x), for real κ, is the shifted transform f(k − κ).
In a similar vein, the Dilation Theorem gives the eﬀect of a scaling transformation on
the Fourier transform. Again, the proof is left to the reader.

Theorem 7.5. If f (x) has Fourier transform
 f (k), then the Fourier transform of the
1  k
rescaled function f (c x) for 0 = c ∈ R is
f
.
|c|
c

272

7 Fourier Transforms

Concise Table of Fourier Transforms

f(k)

f (x)
√

1
δ(x)
σ(x)
sign x
σ(x + a) − σ(x − a)
e− a x σ(x)
ea x (1 − σ(x))
e− a | x |
e

− a x2

tan−1 x
f (c x + d)



2 π δ(k)
1
√
2π

i
π
δ(k) − √
2
2π k

2 1
−i
π k

2 sin a k
π
k
1
√
2 π (a + i k)
1
2 π (a − i k)

2
a
2
π k + a2

√

2

e− k /(4 a)
√
2a

π e− | k |
−i
2
k


e i k d/c  k
f
|c|
c

f (x)

f(− k)

f(x)

f (− k)

f  (x)

i k f(k)

x f (x)
f ∗ g(x)

√

i f (k)
2 π f(k) g(k)

Note: The parameters a, c, d are real, with a > 0 and c = 0.

7.1 The Fourier Transform

273

Example 7.6. Let us determine the Fourier transform of the Gaussian function
2
g(x) = e− x . To evaluate its Fourier integral, we ﬁrst complete the square in the exponent:
 ∞
 ∞
2
2
2
1
1
g(k) = √
e− x − i k x dx = √
e− (x+ i k/2) −k /4 dx
2 π −∞
2 π −∞
2
− k2 /4  ∞
2
e− k /4
e
.
e− y dy = √
= √
2π
2
−∞
The next-to-last equality employed the change of variables† y = x + 12 i k, while the ﬁnal
step used formula (2.100).
2
More generally, to ﬁnd the Fourier transform of ga (x) = e− a x , where a > 0, we invoke
√
√
2
the Dilation Theorem 7.5 with c = a to deduce that ga (k) = e− k /(4 a) / 2 a.
Since the Fourier transform uniquely associates a function f(k) on frequency space
with each (reasonable) function f (x) on physical space, one can characterize functions by
their transforms. Many practical applications rely on tables (or, even better, computer
algebra systems such as Mathematica and Maple) that recognize a wide variety of
transforms of basic functions of importance in applications. The accompanying table lists
some of the most important examples of functions and their Fourier transforms, based
on our convention (7.6). Keep in mind that, by applying the Symmetry Principle of
Theorem 7.3, each entry can be used to deduce two diﬀerent Fourier transforms. A more
extensive collection of Fourier transforms can be found in [82].

Exercises
7.1.1. Find the Fourier transform of the following functions:
⎧

⎨ e− 2 x ,
x, | x | < 1,
− (x+4)2
− | x+1 |
(d) ⎩ 3 x
,
(b) e
, (c)
(a) e
0, otherwise,
e ,
⎧
⎨ e− | x | ,

(e) ⎩

e−1 ,

| x | ≥ 1,
| x | ≤ 1,



(f )

e− x sin x,
0,

x > 0,
x ≤ 0,



(g)

x ≥ 0,
x ≤ 0,

1 − | x |,
0,

| x | ≤ 1,
otherwise.
2

7.1.2. Find the Inverse Fourier transform of the following functions: (a) e− k , (b) e− | k | ,



1, α < k < β,
1 − | k |, | k | < 1,
e− k sin k, k ≥ 0,
(d)
(e)
(c)
0,
otherwise,
0,
otherwise.
0,
k ≤ 0,
7.1.3. Find the inverse Fourier transform of the function 1/(k + c) when (a) c = a is real;
(b) c = i b is purely imaginary; (c) c = a + i b is an arbitrary complex number.
7.1.4. Find the inverse Fourier transform of 1/(k2 − a2 ), where a > 0 is real.
Hint: Use Exercise 7.1.3.
♦ 7.1.5. (a) Find the Fourier transform of e i ω x . (b) Use this to ﬁnd the Fourier transforms of
the basic trigonometric functions cos ω x and sin ω x.
7.1.6. Write down two real integral identites that result from the inverse Fourier transform
of (7.28).
†
Since this represents a complex change of variables, a fully rigorous justiﬁcation of this step
requires the use of complex integration.

274

7 Fourier Transforms

7.1.7. Write down two real integral identities that follow from (7.17).


n − n2 | x |,
7.1.8. (a) Find the Fourier transform of the hat function fn (x) =
0,
(b) What is the limit, as n → ∞, of fn (k)?
(c) In what sense is the limit the Fourier transform of the limit of fn (x)?

| x | ≤ 1/n,
otherwise.

7.1.9. (a) Justify the linearity of the Fourier transform, as in (7.11).
(b) State and justify the linearity of the inverse Fourier transform.
♦ 7.1.10. If the Fourier transform of f (x) is f (k), prove that (a) the Fourier transform of f (− x)
is f (− k); (b) the Fourier transform of the complex conjugate function f (x) is f (− k).
7.1.11. True or false: If the complex-valued function f (x) = g(x) + i h(x) has Fourier transform


f (k) = g(k)+ i h(k),
then g(x) has Fourier transform g(k) and h(x) has Fourier transform h(k).
♦ 7.1.12. (a) Prove that the Fourier transform of an even function is even. (b) Prove that the
Fourier transform of a real even function is real and even. (c) What can you say about the
Fourier transform of an odd function? (d) Of a real odd function? (e) What about a
general real function?
♦ 7.1.13. Prove the Shift Theorem 7.4.
♦ 7.1.14. Prove the Dilation Theorem 7.5.
7.1.15. Given that the Fourier transform of f (x) is f (k), ﬁnd, from ﬁrst principles, the Fourier
transform of g(x) = f (a x + b), where a and b are ﬁxed real constants.
7.1.16. Let a be a real constant. Given the Fourier transform f (k) of f (x), ﬁnd the Fourier
transforms of (a) f (x) e i a x , (b) f (x) cos a x, (c) f (x) sin a x.
♦ 7.1.17. A common alternative convention for the Fourier transform is to deﬁne
∞
f1 (k) =
f (x) e− i k x dx.
−∞

(a) What is the formula for the corresponding inverse Fourier transform?
(b) How is f1 (k) related to our Fourier transform f (k)?
♦ 7.1.18. Another convention for the Fourier transform is to deﬁne f2 (k) =

∞
−∞

f (x) e− 2 π i k x dx.

Answer the questions in Exercise 7.1.17 for this version of the Fourier transform.
♥ 7.1.19. The cosine and sine transforms of a real function f (x) are deﬁned as

c (k) =

∞

−∞

f (x) cos k x dx,

(i ) Prove that f (k) = c (k) − i s (k).


s (k) =

∞

−∞

f (x) sin k x dx.

(7.40)

(ii ) Find the cosine and sine transforms of the functions in Exercise 7.1.1. (iii ) Show that c (k) is an even function, while s (k) is an odd function. (iv ) Show that if f is an even function, then s (k) ≡ 0, while if f is an odd function,
then c (k) ≡ 0.

♦ 7.1.20. The two-dimensional Fourier transform of a function f (x, y) deﬁned for (x, y) ∈ R 2 is
∞
∞
1
f (k, l) =
f (x, y) e− i (k x+l y) dx dy.
(7.41)
2 π −∞ −∞
(a) Compute the Fourier transform of the following functions:
2
2
(i ) e− | x |−| y | ;
(ii ) e− x −y ;
(iii ) the delta function δ(x − ξ) δ(y − η),


1, | x |, | y | ≤ 1,
1, | x | + | y | ≤ 1,
(iv )
(v )
(vi ) cos(x − y).
0, otherwise,
0, otherwise,

(b) Show that if f (x, y) = g(x) h(y), then f (k, l) = g(k) h(l).
(c) What is the formula for the inverse two-dimensional Fourier transform, i.e., how can you
reconstruct f (x, y) from f (k, l)?

7.2 Derivatives and Integrals

275

7.2 Derivatives and Integrals
One of the most signiﬁcant features of the Fourier transform is that it converts calculus
into algebra! More speciﬁcally, the two basic operations in calculus — diﬀerentiation and
integration of functions — are realized as algebraic operations on their Fourier transforms.
(The downside is that algebraic operations become more complicated in the frequency
domain.)
Diﬀerentiation
Let us begin with derivatives. If we diﬀerentiate† the basic inverse Fourier transform
formula
 ∞
1
f(k) e i k x dk
f (x) ∼ √
2 π −∞
with respect to x, we obtain
1
f (x) ∼ √
2π


 ∞

i k f(k) e i k x dk.

(7.42)

−∞

The resulting integral is itself in the form of an inverse Fourier transform, namely of i k f(k),
which immediately implies the following key result.
Proposition 7.7. The Fourier transform of the derivative f  (x) of a function is
obtained by multiplication of its Fourier transform by i k:
F [ f  (x) ] = i k f(k).

(7.43)

Similarly, the Fourier transform of the product function x f (x) is obtained by diﬀerentiating
the Fourier transform of f (x):
df
F [ x f (x) ] = i
.
(7.44)
dk
The second statement follows easily from the ﬁrst via the Symmetry Principle of
Theorem 7.3. While the result is stated for ordinary functions, as noted earlier, the Fourier
transform — just like Fourier series — is entirely compatible with the calculus of generalized
functions.
Example 7.8. The derivative of the even exponential pulse fe (x) = e− a | x | is a
multiple of the odd exponential pulse fo (x) = (sign x) e− a | x | :
fe (x) = − a (sign x) e− a | x | = − afo (x).
Proposition 7.7 says that their Fourier transforms are related by

2
ka

= − a fo (k),
i k f e (k) = i
π k 2 + a2
†
We are assuming that the integrand is suﬃciently nice in order to bring the derivative under
the integral sign; see [ 37, 117 ] for a fully rigorous justiﬁcation.

276

7 Fourier Transforms

as previously noted in (7.21, 24). On the other hand, the odd exponential pulse has a jump
discontinuity of magnitude 2 at x = 0, and so its derivative contains a delta function:
fo (x) = − a e− a | x | + 2 δ(x) = − a fe (x) + 2 δ(x).
This is reﬂected in the relation between their Fourier transforms. If we multiply (7.24) by
i k, we obtain



2
k
a2
2
2
2
 − a f (k).
−
=
= 2 δ(k)
i k fo (k) =
e
π k 2 + a2
π
π k 2 + a2
Higher-order derivatives are handled by iterating the ﬁrst-order formula (7.43).
Corollary 7.9. The Fourier transform of f (n) (x) is ( i k)n f(k).
This result has an important consequence: the smoothness of the function f (x) is
manifested in the rate of decay of its Fourier transform f(k). We already noted that the
Fourier transform of a (nice) function must decay to zero at large frequencies: f(k) → 0
as | k | → ∞. (This result can be viewed as the Fourier transform version of the Riemann–
Lebesgue Lemma 3.46.) If the nth derivative f (n) (x) is also a reasonable function, then its
(n) (k) = ( i k)n f(k) must go to zero as | k | → ∞. This requires that
Fourier transform f
f(k) go to zero more rapidly than | k |− n . Thus, the smoother f (x), the more rapid the
decay of its Fourier transform. As a general rule of thumb, local features of f (x), such as
smoothness, are manifested by global features of f(k), such as the rate of decay for large
| k |. The Symmetry Principle implies that the reverse is also true: global features of f (x)
correspond to local features of f(k). For instance, the degree of smoothness of f(k) governs
the rate of decay of f (x) as x → ± ∞. This local-global duality is one of the major themes
of Fourier theory.
Integration
Integration is the inverse operation to diﬀerentiation, and so should correspond to division
by i k in frequency space. As with Fourier series, this is not completely correct; there is
an extra constant involved, which contributes an additional delta function.

Proposition 7.10.
 If f (x) has Fourier transform f (k), then the Fourier transform
x

of its integral g(x) =

f (y) dy is
−∞

g(k) = −

i 
f (k) + π f(0) δ(k).
k

(7.45)

Proof : Note that
 ∞

 x
f (ξ) dξ =

g(x) =
−∞

−∞

σ(x − ξ) f (ξ) dξ,

where σ(x) is the step function (6.23). The latter expression is just the convolution integral
(7.52) between the two functions:
g(x) = σ ∗ f (x).

7.2 Derivatives and Integrals

277

Thus, according to the convolution formula (7.53), the Fourier transform of the integral is
given (up to multiple) by the product of the individual Fourier transforms:
√

g(k) = 2 π σ
 (k) f(k).
Consulting our table of Fourier transforms, we ﬁnd


√
i
i
π
i
f(k) = − f(k)+ π f(k) δ(k) = − f(k)+ π f(0) δ(k),
g(k) = 2 π
δ(k) − √
2
k
k
2π k
which establishes the desired formula (7.45).

Q.E.D.

Example 7.11. The Fourier transform of the inverse tangent function
 x
 x
dy
dy
π
f (x) = tan−1 x =
=
−
2
2
2
0 1+y
−∞ 1 + y
can be computed by combining Proposition 7.10 with (7.28, 34):




i π −|k|
π e− | k |
π 3/2
π 3/2

f (k) = −
e
.
+ √ δ(k) − √ δ(k) = − i
k 2
2
k
2
2
The singularity at k = 0 reﬂects the lack of decay of the inverse tangent as | x | → ∞.

Exercises
7.2.1. Determine the Fourier transform of the following functions:
2
2
2
(a) e− x /2 , (b) x e− x /2 , (c) x2 e− x /2 , (d) x, (e) x e− 2 | x | ,

(f ) x tan−1 x.

2  x − z2
e
dz;
(a) the error function erf x = √
π 0
 ∞
2
2
e− z dz.
(b) the complementary error function erfc x = √
x
π

7.2.2. Find the Fourier transform of

7.2.3. Find the inverse Fourier transform of the following functions:
2
k
k2
1
(a) k, (b) k e− k , (c)
,
(d)
, (e) 2
.
2
2
(1 + k )
k− i
k −k
7.2.4. Is the usual formula σ  (x) = δ(x) relating the step and delta functions compatible with
their Fourier transforms? Justify your answer.
7.2.5. Find the Fourier transform of the derivative δ  (x) of the delta function in three ways:
(a) First, directly from the deﬁnition of δ  (x); (b) second, using the formula for the Fourier
transform of the derivative of a function; (c) third, as a limit of the Fourier transforms of
the derivatives of the functions in Exercise 7.1.8. (d) Are your answers all the same? If
not, can you explain any discrepancies?
2

7.2.6. Show that one can obtain the Fourier transform of the Gaussian function f (x) = e− x /2
by the following trick. First, prove that f  (k) = − k f (k). Use this to deduce that f (k) =
2
c e− k /2 for some constant c. Finally, use the Symmetry Principle to determine c.
f (k)
7.2.7. If f (x) has Fourier transform f (k), which function has Fourier transform
?
k

278

7 Fourier Transforms

♦ 7.2.8. If f (x) has Fourier transform f (k), what is the Fourier transform of
7.2.9. Use Exercise 7.2.8 to ﬁnd the Fourier transform of
2
(a) 1/x, (b) x−1 e− | x | , (c) x−1 e− x ,

f (x)
x ?

(d) (x3 + 4 x)−1 .

7.2.10. Directly justify formula (7.43) by integrating the relevant Fourier transform integral by
parts. What do you need to assume about the behavior of f (x) for large | x |?
7.2.11. Given
the Fourier transform f (k) of f (x), ﬁnd the Fourier transform of its integral

g(x) =

x

a

f (y) dy starting at the point a ∈ R.

♦ 7.2.12. (a) Explain why the Fourier transform of a 2 π–periodic function f (x) is a linear combi∞
√

nation of delta functions, f (k) = 2 π
cn δ(k − n), where cn are the (complex) Fourier
n = −∞

series coeﬃcients (3.65) of f (x) on [ − π, π ].
(b) Find the Fourier transform of the following periodic functions:
(i ) sin 2 x, (ii ) cos3 x, (iii ) the 2 π–periodic extension of f (x) = x,
(iv ) the sawtooth function h(x) = x mod 1, i.e., the fractional part of x.
7.2.13. Determine the Fourier transforms of (a) cos x − 1, (b)
Hint: Use Exercises 7.2.8 and 7.2.12.

cos x − 1
,
x

(c)

cos x − 1
.
x2

♦ 7.2.14. Write down the formulas for diﬀerentiation and integration for the alternative Fourier
transforms of Exercises 7.1.17 and 7.1.18.
7.2.15. (a) What is the two-dimensional Fourier transform, (7.41), of the gradient ∇f (x, y) of a
function of two variables?
2
2
(b) Use your formula to ﬁnd the Fourier transform of the gradient of f (x, y) = e− x −y .

7.3 Green’s Functions and Convolution
The fact that the Fourier transform converts diﬀerentiation in the physical domain into
multiplication in the frequency domain is one of its most compelling features. A particularly
important consequence is that it eﬀectively transforms diﬀerential equations into algebraic
equations, and thereby facilitates their solution by elementary algebra. One begins by applying the Fourier transform to both sides of the diﬀerential equation under consideration.
Solving the resulting algebraic equation will produce a formula for the Fourier transform of
the desired solution, which can then be immediately reconstructed via the inverse Fourier
transform. In the following chapter, we will use these techniques to solve partial diﬀerential
equations.

Solution of Boundary Value Problems
The Fourier transform is particularly well adapted to boundary value problems on the
entire real line. In place of the boundary conditions used on ﬁnite intervals, we look for
solutions that decay to zero suﬃciently rapidly as | x | → ∞ — in order that their Fourier
transform be well deﬁned (in the context of ordinary functions). In quantum mechanics,
[66, 72], these solutions are known as the bound states, and they correspond to subatomic

7.3 Green’s Functions and Convolution

279

particles that are trapped or localized in a region of space. For example, the electrons in
an atom are bound states localized by the electrostatic attraction of the nucleus.
As a speciﬁc example, consider the boundary value problem
d2 u
(7.48)
+ ω 2 u = h(x),
− ∞ < x < ∞,
dx2
where ω > 0 is a positive constant. The boundary conditions require that the solution
decay: u(x) → 0, as | x | → ∞. We will solve this problem by applying the Fourier
transform to both sides of the diﬀerential equation. Taking Corollary 7.9 into account, the
result is the linear algebraic equation
−

(k) + ω 2 u
(k) = 
h(k)
k2 u
relating the Fourier transforms of u and h. Unlike the diﬀerential equation, the transformed
equation can be immediately solved for
u
(k) =


h(k)
k2 + ω2

.

(7.49)

Therefore, we can reconstruct the solution by applying the inverse Fourier transform formula (7.9):
 ∞ 
h(k) e i k x
1
dk.
(7.50)
u(x) = √
2 π −∞ k 2 + ω 2
For example, if the forcing function is an even exponential pulse,

2
1
−|x|

h(x) = e
with
h(k) =
,
π k2 + 1
then (7.50) writes the solution as a Fourier integral:
 ∞
 ∞
1
e i kx
cos k x
1
dk
=
dk ,
u(x) =
2
2
2
2
π −∞ (k + ω )(k + 1)
π −∞ (k + ω 2 )(k 2 + 1)
where we note that the imaginary part of the complex integral vanishes because the integrand is an odd function. (Indeed, if the forcing function is real, the solution must also be
real.) The Fourier integral can be explicitly evaluated using partial fractions to rewrite




1
1
1
1
2
2
u
(k) =
=
−
,
ω 2 = 1.
π (k 2 + ω 2 )(k 2 + 1)
π ω2 − 1 k2 + 1 k2 + ω2
Thus, according to our table of Fourier transforms, the solution to this boundary value
problem is
1
e− | x | − e− ω | x |
ω
when
ω 2 = 1.
u(x) =
(7.51)
ω2 − 1
The reader may wish to verify that this function is indeed a solution, meaning that it is
twice continuously diﬀerentiable (which is not so immediately apparent from the formula),
decays to 0 as | x | → ∞, and satisﬁes the diﬀerential equation everywhere. The “resonant”
case ω 2 = 1 is left to Exercise 7.3.6.
Remark : The method of partial fractions that you may have learned in ﬁrst-year
calculus is often an eﬀective tool for evaluating (inverse) Fourier transforms of such rational
functions.

280

7 Fourier Transforms

A particularly important case is that in which the forcing function
h(x) = δξ (x) = δ(x − ξ)
represents a unit impulse concentrated at x = ξ. The resulting solution is the Green’s
function G(x; ξ) for the boundary value problem. According to (7.49), its Fourier transform
with respect to x is
e− i k ξ
 ξ) = √1
,
G(k;
2 π k2 + ω2
which is the product of an exponential factor e− i k ξ , representing the Fourier transform of
δξ (x), times a multiple of the Fourier transform of the even exponential pulse e− ω | x | . We
apply the Shift Theorem 7.4, and conclude that the Green’s function for this boundary
value problem is an exponential pulse centered at ξ, namely
G(x; ξ) =

1 − ω | x−ξ |
= g(x − ξ),
e
2ω

where

g(x) = G(x; 0) =

1 −ω | x |
. (7.52)
e
2ω

Observe that, as with other self-adjoint boundary value problems, the Green’s function
is symmetric under interchange of x and ξ, so G(x; ξ) = G(ξ; x). As a function of x, it
satisﬁes the homogeneous diﬀerential equation − u + ω 2 u = 0, except at the point x = ξ,
where its derivative has a jump discontinuity of unit magnitude. It also decays as | x | → ∞,
as required by the boundary conditions. The fact that G(x; ξ) = g(x − ξ) depends only
on the diﬀerence x − ξ is a consequence of the translation invariance of the boundary
value problem. The superposition principle based on the Green’s function tells us that the
solution to the inhomogeneous boundary value problem (7.48) under a general forcing can
be represented in the integral form
 ∞
 ∞
 ∞
1
u(x) =
G(x; ξ) h(ξ) dξ =
g(x − ξ) h(ξ) dξ =
e− ω| x−ξ | h(ξ) dξ. (7.53)
2
ω
−∞
−∞
−∞
The reader may enjoy recovering the particular exponential solution (7.51) from this integral formula.

Exercises
7.3.1. Use partial fractions to compute the inverse Fourier transform of the following rational
functions. Hint: First solve Exercise 7.1.3.
1
eik
1
sin 2 k
(a) 2
, (b) 2
, (c) 4
, (d) 2
.
k − 5k − 6
k −1
k −1
k + 2k − 3
1
:
k2 + 2 k + 5
(a) using partial fractions; (b) by completing the square. Are your answers the same?

7.3.2. Find the inverse Fourier transform of the function

7.3.3. Use partial fractions to compute the Fourier transform of the following functions:
1
1
cos x
(a) 2
, (b) 3
, (c) 2
.
x −x−2
x +x
x −9
d2 u
+ 4 u = δ(x) by using the Fourier
7.3.4. Find a solution to the diﬀerential equation −
dx2
transform.

7.3 Green’s Functions and Convolution

281

7.3.5. Use the Fourier transform to solve the boundary value problem
− u + u = δ  (x − 1) for − ∞ < x < ∞, with u(x) → 0 as x → ±∞.
♦ 7.3.6. (a) Use the Fourier transform to solve (7.48) with h(x) = e− | x | when ω = 1. Hint: You
may wish to solve Exercise 7.3.12 ﬁrst.
(b) Verify that your solution can be obtained as a limit of (7.51) as ω → 1.
7.3.7. Use the Fourier transform to ﬁnd a bounded solution to the diﬀerential equation
u + u = e−2 | x | .
7.3.8. Use the Fourier transform to ﬁnd an integral formula for a bounded solution to the
d2 u
= x u.
Airy diﬀerential equation −
dx2
♦ 7.3.9. Prove that (7.51) is a twice continuously diﬀerentiable function of x and satisﬁes the
diﬀerential equation (7.48).

Convolution
In our solution to the boundary value problem (7.48), we ended up deriving a formula for
its Fourier transform (7.49) as the product of two known Fourier transforms. The ﬁnal
Green’s function formula (7.53), obtained by applying the inverse Fourier transform, is
indicative of a general property, in that it is given by a convolution product.
Deﬁnition 7.12. The convolution of scalar functions f (x) and g(x) is the scalar
function h = f ∗ g deﬁned by the formula
 ∞
h(x) = f ∗ g(x) =
f (x − ξ) g(ξ) dξ.
(7.54)
−∞

We list the basic properties of the convolution product, leaving their veriﬁcation as
exercises for the reader. All of these assume that the implied convolution integrals converge.
(a) Symmetry:



(b) Bilinearity:
(c) Associativity:
(d) Zero function:
(e) Delta function:

f ∗ g = g ∗ f,
f ∗ (a g + b h) = a (f ∗ g) + b (f ∗ h),
(a f + b g) ∗ h = a (f ∗ h) + b (g ∗ h),
f ∗ (g ∗ h) = (f ∗ g) ∗ h,
f ∗ 0 = 0,
f ∗ δ = f.

a, b ∈ C,

One tricky feature is that the constant function 1 is not a unit for the convolution
product; indeed,

f ∗1=1∗f =

∞

f (ξ) dξ
−∞

is a constant function, namely the total integral of f , and not the original function f (x). In
fact, according to the ﬁnal property, the delta function plays the role of the “convolution
unit”:
 ∞
f (x − ξ) δ(ξ) dξ = f (x).
f ∗ δ(x) =
−∞

282

7 Fourier Transforms

In particular, our solution (7.52) has the form of a convolution product between an
even exponential pulse g(x) = (2 ω)−1 e− ω| x | and the forcing function:
u(x) = g ∗ h(x).
On the other hand, its Fourier transform (7.49) is, up to a factor, the ordinary multiplicative
product
√
g(k) 
h(k)
u
(k) = 2 π 
of the Fourier transforms of g and h. In fact, this is a general property of the Fourier transform: convolution in the physical domain corresponds to multiplication in the frequency
domain, and conversely.
Theorem 7.13. The Fourier transform of the convolution h(x) = f ∗ g(x) of two
functions is a multiple of the product of their Fourier transforms:
√

h(k) = 2 π f(k) 
g(k).
(7.55)
Conversely, the Fourier transform of their product h(x) = f (x) g(x) is, up to a multiple,
the convolution of their Fourier transforms:
 ∞
1 
1

h(k) = √
f(k − κ) 
g(κ) dκ.
(7.56)
f ∗ g(k) = √
2π
2 π −∞
Proof : Combining the deﬁnition of the Fourier transform with the convolution formula (7.54), we obtain
 ∞
 ∞  ∞
1
1

h(k) = √
h(x) e− i k x dx = √
f (x − ξ) g(ξ) e− i k x dx dξ.
2 π −∞
2 π −∞ −∞
Applying the change of variables η = x − ξ in the inner integral produces
 ∞ ∞
1

h(k) = √
f (η) g(ξ) e− i k(ξ+η) dξ dη
2 π −∞ −∞



 ∞
 ∞
√
√
1
1
−ikη
−i kξ
√
√
= 2π
f (η) e
dη
g(ξ) e
dξ = 2 π f(k) 
g(k),
2 π −∞
2 π −∞
proving (7.55). The second formula can be proved in a similar fashion, or by simply noting
that it follows directly from the Symmetry Principle of Theorem 7.3.
Q.E.D.
Example 7.14. We already know, (7.29), that the Fourier transform of
sin x
f (x) =
x
is the box function
⎧ 

π

 ⎨
π
,
−1 < k < 1,
f(k) =
σ(k + 1) − σ(k − 1) =
2
⎩
2
0,
| k | > 1.
We also know that the Fourier transform of

π
1
g(x) =
sign k.
is
g(k) = − i
x
2
Therefore, the Fourier transform of their product
h(x) = f (x) g(x) =

sin x
x2

7.3 Green’s Functions and Convolution

283

can be obtained by convolution:
1 
1

h(k) = √
f ∗ g(k) = √
2π
2π

 ∞

f(κ) 
g (k − κ) dκ
⎧ 
π
⎪
⎪
i
⎪
⎪
2
⎪
⎪
⎪
  1
⎪

⎨
π
π
= −i
sign(k − κ) dκ =
k,
−i
⎪
8 −1
2
⎪
⎪
⎪

⎪
⎪
⎪
π
⎪
⎩ −i
2
−∞

k < −1,
−1 < k < 1,
k > 1.

Exercises
7.3.10. (a) Find the Fourier transform of the convolution h(x) = fe ∗ g(x) of an even exponential
2

pulse fe (x) = e− | x | and a Gaussian g(x) = e− x .

(b) What is h(x)?
2

7.3.11. What is the convolution of a Gaussian kernel e− x with itself? Hint: Use the Fourier
transform.
7.3.12. Find the function whose Fourier transform is f (k) = (k2 + 1)−2 .
⎧
⎨ 1,

♥ 7.3.13. (a) Write down the Fourier transform of the box function f (x) = ⎩

| x | < 12 ,

0, | x | > 12 .
(b) Graph the hat function h(x) = f ∗ f (x) and ﬁnd its Fourier transform.
(c) Determine the cubic B spline s(x) = h ∗ h(x) and its Fourier transform.


7.3.14. Let f (x) =

sin x,
0,

0 < x < π,
otherwise,



g(x) =

cos x,
0,

0 < x < π,
otherwise.

(a) Find the Fourier transforms of f (x) and g(x); (b) compute the convolution

h(x) = f ∗ g(x); (c) ﬁnd its Fourier transform h(k).

7.3.15. Use convolution to ﬁnd an integral formula for the function whose Fourier transform is
2
sin k
sin2 k
sign k
e− k
, (b)
,
(c)
.
(a) 2
, (d)
k +1
k(k2 + 1)
k2
1+ ik
If possible, evaluate the resulting convolution integral.
7.3.16. Let f (x) be a smooth function. (a) Find its convolution δ  ∗f with the derivative of the
delta ﬁunction. (b) More generally, ﬁnd δ (n) ∗ f .
7.3.17. According to Proposition 7.7, the Fourier transform of the derivative f  (x) is
obtained by multiplying f (k) by i k. Can you reconcile this result with the Convolution
Theorem 7.13?
♦ 7.3.18. The Hilbert transform of a function f (x) is deﬁned as the integral
1 ∞ f (ξ) dξ
h(x) =
.
−
π −∞ ξ − x

(7.57)


Find a formula for its Fourier transform h(k)
in terms of f (k). Remark
: The baron the

∞
x−δ
f (ξ) dξ
integral indicates the principal value integral , [ 2 ], which is lim+
+
,
x+δ
−∞
ξ−x
δ→0
and is employed to avoid the integral diverging at the singular point x = ξ.

284

7 Fourier Transforms

7.3.19. Use the Fourier transform to solve the integral equation
Then verify your solution when f (x) = e− 2 | x | .

∞
−∞

e− | x−ξ | u(ξ) dξ = f (x).

7.3.20. Suppose that f (x) and g(x) are identically 0 for all
x < 0. Prove that their convolution
⎧
⎨

product h = f ∗ g reduces to a ﬁnite integral: h(x) = ⎩

x

0

f (x − ξ) g(ξ) dξ,

x > 0,

0,

x ≤ 0.

7.3.21. Given that the support of f (x) is contained in the interval [ a, b ] and the support of
g(x) is contained in [ c, d ], what can you say about the support of their convolution
h(x) = f ∗ g(x)?
♦ 7.3.22. Prove the convolution properties (a–e).
♦ 7.3.23. In this exercise, we explain how convolution can be used to smooth out rough data. Let
ε
. (a) If f (x) is any (reasonable) function, show that fε (x) = gε ∗ f (x)
gε (x) =
π(ε2 + x2 )
∞
for ε = 0 is a C function. (b) Show that lim fε (x) = f (x).
ε→0

7.3.24. Explain why the Shift Theorem 7.4 is a special case of the Convolution Theorem 7.13.
♦ 7.3.25. Suppose f (x) and g(x) are 2 π–periodic and have respective complex Fourier coeﬃcients
ck and dk . Prove that the complex Fourier coeﬃcients ek of the product function f (x) g(x)
∞

are given by the convolution summation ek =

j = −∞

cj dk−j . Hint: Substitute the formulas

for the complex Fourier coeﬃcients into the summation, making sure to use two diﬀerent
integration variables, and then use (6.37).

7.4 The Fourier Transform on Hilbert Space
While we do not possess all the analytic tools to embark on a fully rigorous treatment of the
mathematical theory underlying the Fourier transform, it is worth outlining a few of the
more important features. We have already noted that the Fourier transform, when deﬁned,
is a linear operator, taking functions f (x) on physical space to functions f(k) on frequency
space. A critical question is the following: to precisely which function space should the
theory be applied? Not every function admits a Fourier transform in the classical sense†
— the Fourier integral (7.6) is required to converge, and this places restrictions on the
function and its asymptotics at large distances.
It turns out the proper setting for the rigorous theory is the Hilbert space of complexvalued square-integrable functions — the same inﬁnite-dimensional vector space that lies
at the heart of modern quantum mechanics. In Section 3.5, we already introduced the
Hilbert space L2 [ a, b ] on a ﬁnite interval; here we adapt Deﬁnition 3.34 to the entire real
line. Thus, the Hilbert space L2 = L2 (R) is the inﬁnite-dimensional vector space consisting
of all complex-valued functions f (x) that are deﬁned for all x ∈ R and have ﬁnite L2 norm:
f  =
2

†

 ∞
−∞

| f (x) |2 dx < ∞.

We leave aside the more advanced issues involving generalized functions.

(7.58)

7.4 The Fourier Transform on Hilbert Space

285

For example, any piecewise continuous function that satisﬁes the decay criterion
| f (x) | ≤

M
,
| x |1/2+δ

for all suﬃciently large

| x |  0,

(7.59)

for some M > 0 and δ > 0, belongs to L2 . However, as in Section 3.5, Hilbert space contains
many more functions, and the precise deﬁnitions and identiﬁcation of its elements is quite
subtle. On the other hand, most nondecaying functions do not belong to L2 , including the
constant function f (x) ≡ 1 as well as all oscillatory complex exponentials, e i kx for k ∈ R.
The Hermitian inner product on the complex Hilbert space L2 is prescribed in the
usual manner,

f ,g =

∞

f (x) g(x) dx,

(7.60)

−∞

so that  f 2 =  f , f . The Cauchy–Schwarz inequality
|f ,g| ≤ f  g

(7.61)

ensures that the inner product integral is ﬁnite whenever f, g ∈ L . Observe that the
Fourier transform (7.6) can be regarded as a multiple of the inner product of the function
f (x) with the complex exponential functions:
 ∞
1
1

f (k) = √
f (x) e− i k x dx = √
 f (x) , e i k x .
(7.62)
2 π −∞
2π
However, when interpreting this formula, one must bear in mind that the exponentials are
not themselves elements of L2 .
Let us state the fundamental result governing the eﬀect of the Fourier transform
on functions in Hilbert space. It can be regarded as a direct analogue of the Pointwise
Convergence Theorem 3.8 for Fourier series.
Theorem 7.15. If f (x) ∈ L2 is square-integrable, then its Fourier transform f(k) ∈
2
L is a well-deﬁned, square-integrable function of the frequency variable k. If f (x) is
continuously diﬀerentiable at a point x, then the inverse Fourier transform integral (7.9)
equals its value f (x). More generally, if the left- and right-hand limits f (x− ), f (x+ ),
f  (x− ),f  (x+ ) exist, then
 the inverse Fourier transform integral converges to the average
1
−
+
value 2 f (x ) + f (x ) .
2

Thus, the Fourier transform f = F [ f ] deﬁnes a linear transformation from L2 functions of x to L2 functions of k. In fact, the Fourier transform preserves inner products.
This important result is known as Parseval’s formula, whose Fourier series counterpart
appeared in (3.122).
Theorem 7.16. If f(k) = F [ f (x) ] and g(k) = F [ g(x) ], then  f , g  =  f , 
g , i.e.,
 ∞
 ∞
f(k) g(k) dk.
f (x) g(x) dx =
(7.63)
−∞

−∞

Proof : Let us sketch a formal proof that serves to motivate why this result is valid.
We use the deﬁnition (7.6) of the Fourier transform to evaluate


 ∞
 ∞
 ∞
 ∞
1
1
−i kx
+i ky

√
√
f (k) g(k) dk =
f (x) e
dx
g(y) e
dy dk
2 π −∞
2 π −∞
−∞
−∞


 ∞
 ∞ ∞
1
− i k(x−y)
=
f (x) g(y)
e
dk dx dy.
2 π −∞
−∞ −∞

286

7 Fourier Transforms

Now according to (7.38), the inner k integral can be replaced by the delta function δ(x−y),
and hence
 ∞
 ∞ ∞
 ∞

f (k) g(k) dk =
f (x) g(y) δ(x − y) dx dy =
f (x) g(x) dx.
−∞

−∞

−∞

−∞

This completes our “proof”; see [37, 68, 117] for a rigorous version.

Q.E.D.

In particular, orthogonal functions, satisfying  f , g  = 0, will have orthogonal Fourier
transforms,  f, g  = 0. Choosing f = g in Parseval’s formula (7.63) produces Plancherel’s
formula
 ∞
 ∞
2
2
2

f  = f  ,
or, explicitly,
| f (x) | dx =
| f(k) |2 dk.
(7.64)
−∞

−∞

Thus, the Fourier transform F : L2 → L2 deﬁnes a norm-preserving, or unitary, linear
transformation on Hilbert space, mapping L2 functions of the physical variable x to L2
functions of the frequency variable k.
Quantum Mechanics and the Uncertainty Principle
In its popularized form, the Heisenberg Uncertainty Principle is a by now familiar philosophical concept. First formulated in the 1920s by the German physicist Werner Heisenberg, one of the founders of modern quantum mechanics, it states that, in a physical
system, certain quantities cannot be simultaneously measured with complete accuracy.
For instance, the more precisely one measures the position of a particle, the less accuracy
there will be in the measurement of its momentum; conversely, the greater the accuracy
in the momentum, the less certainty in its position. A similar uncertainty couples energy
and time. Experimental veriﬁcation of the uncertainty principle can be found even in fairly
simple situations. Consider a light beam passing through a small hole. The position of the
photons is constrained by the hole; the eﬀect of their momenta is observed in the pattern
of light diﬀused on a screen placed beyond the hole. The smaller the hole, the more constrained the photon’s position as it passes through, hence, according to the Uncertainty
Principle, the less certainty there is in the observed momentum, and, consequently, the
wider and more diﬀuse the resulting image on the screen.
This is not the place to discuss the philosophical and experimental consequences of
Heisenberg’s Principle. What we will show is that the Uncertainty Principle is, in fact, a
mathematical property of the Fourier transform! In quantum theory, each of the paired
quantities, e.g., position and momentum, are interrelated by the Fourier transform. Indeed,
Proposition 7.7 says that the Fourier transform of the diﬀerentiation operator representing
momentum is a multiplication operator representing position and vice versa. This Fouriertransform-based duality between position and momentum, that is, between multiplication
and diﬀerentiation, lies at the heart of the Uncertainty Principle.
In quantum mechanics, the wave functions of a quantum system are characterized as
the elements of unit norm,  ϕ  = 1, belonging to the underlying state space, which, in
a one-dimensional model of a single particle, is the Hilbert space L2 = L2 (R) consisting
of square-integrable complex-valued functions of x. As we already noted in Section 3.5,
the squared modulus of the wave function, | ϕ(x) |2 , represents the probability density of
the particle being found at position x. Consequently, the mean or expected value of any

7.4 The Fourier Transform on Hilbert Space

287

function f (x) of the position variable is given by its integral against the system’s probability
density and denoted by
 ∞
 f (x)  =
f (x) | ϕ(x) |2 dx.
(7.65)
−∞

In particular,
x =

 ∞
−∞

x | ϕ(x) |2 dx

(7.66)

is the expected measured position of the particle, while Δx, deﬁned by
2

(Δx)2 =  x −  x 

 =  x2  −  x 2 ,

(7.67)

which is the probability density’s variance, is the statistical deviation of the particle’s
measured position from the mean. We note that the next-to-last term equals
 ∞
2
x2 | ϕ(x) |2 dx =  x ϕ(x) 2 .
(7.68)
x  =
−∞

On the other hand, the momentum variable p is related to the Fourier transform
frequency via the de Broglie relation p =  k, where
=

h
≈ 1.055 × 10−34 joule seconds
2π

(7.69)

is Planck’s constant, whose value governs the quantization of physical quantities. Therefore, the mean, or expected value, of any function of momentum g(p) is given by its integral
against the squared modulus of the Fourier transformed wave function:
 ∞
g( k) | ϕ(k)
 |2 dk.
(7.70)
 g(p)  =
−∞

In particular, the mean of the momentum measurements of the particle is
 ∞
 ∞
k | ϕ(k)
 |2 dk = − i 
ϕ (x) ϕ(x) dx = − i   ϕ , ϕ ,
p = 
−∞

(7.71)

−∞

where we used Parseval’s formula (7.63) to convert to an integral over position, and (7.43)
to infer that k ϕ(k)

is the Fourier transform of − i ϕ (x). Similarly,
(Δp)2 =  p −  p 

2

 =  p2  −  p 2

(7.72)

is the squared variance of the momentum, where, by Plancherel’s formula (7.64) and (7.43),
p  = 
2

2

 ∞
−∞

k | ϕ(k)
 | dk = 
2

2

2

= 2

 ∞
−∞
 ∞
−∞

| i k ϕ(k)
 |2 dk
(7.73)
| ϕ (x) |2 dx = 2  ϕ (x) 2 .

With this interpretation, the Uncertainty Principle for position and momentum measurements can be stated.

288

7 Fourier Transforms

Theorem 7.17. If ϕ(x) is a wave function, so  ϕ  = 1, then the observed variances
in position and momentum satisfy the inequality
Δx Δp ≥ 12 .

(7.74)

Now, the smaller the variance of a quantity such as position or momentum, the more
accurate will be its measurement. Thus, the Heisenberg inequality (7.74) eﬀectively quantiﬁes the statement that the more accurately we are able to measure the momentum p, the
less accurate will be any measurement of its position x, and vice versa. For more details,
along with physical and experimental consequences, you should consult an introductory
text on mathematical quantum mechanics, e.g., [66, 72].
Proof : For any value of the real parameter t,
0 ≤  t x ϕ(x) + ϕ (x) 2


= t2  x ϕ(x) 2 + t  ϕ (x) , x ϕ(x)  +  x ϕ(x) , ϕ (x)  +  ϕ (x) 2 .

(7.75)

The middle term in the ﬁnal expression can be evaluated as follows:
 ∞




 ϕ (x) , x ϕ(x)  +  x ϕ(x) , ϕ (x)  =
x ϕ (x) ϕ(x) + x ϕ(x) ϕ (x) dx
−∞
 ∞
 ∞
d
2
| ϕ(x) | dx = −
x
| ϕ(x) |2 dx = −1,
=
dx
−∞
−∞
via an integration by parts, noting that the boundary terms vanish, provided ϕ(x) satisﬁes
the L2 decay criterion (7.59). Thus, in view of (7.68) and (7.73), the inequality in (7.75)
reads
 p2 
 x2  t2 − t + 2 ≥ 0
for all
t ∈ R.

The minimum value of the left-hand side occurs at t = 1/(2  x2 ), where its value is
 p2 
1
≥ 0,
−
2

4  x2 

which implies

 x2   p2  ≥ 14 2 .

To obtain the uncertainty relation (7.74), one performs the selfsame calculation, but with
x −  x  replacing x and p −  p  replacing p. The result is
2
1
1
2 2
(p −  p )2
(Δp)2
2
2 2
=
(Δx)
t
−
t
+
≥ 0.
(7.76)
(x −  x ) t − t +
2
2
Substituting t = 1/(2 (Δx)2) produces the Heisenberg inequality (7.74).

Q.E.D.

Exercises


7.4.1. (a) Write out the Plancherel formula for the square wave pulse f (x) =
∞ sin2 x
(b) What is
dx?
0
x2

1,
0,

| x | < 1,
| x | > 1.

7.4 The Fourier Transform on Hilbert Space

289

7.4.2. Apply the Plancherel formula to the even decaying pulse (7.20) to evaluate
 ∞
dx
. How would you compute this integral using elementary calculus?
−∞ (a2 + x2 )2


− n2 sign x, | x | < n1 , where
0,
otherwise,
n is a positive integer. (b) Write out the Plancherel formula for fn (x). (c) Determine the
limit, as n → ∞, of the Fourier transform of fn (x). (d) Explain why the limit should be
the Fourier transform of the derivative of the delta function δ  (x).

♥ 7.4.3. (a) Find the Fourier transform of the function fn (x) =

7.4.4. Prove that Parseval’s formula is a consequence of Plancherel’s formula. Hint: Use the
identity in Exercise 3.5.34(b).
♦ 7.4.5. Prove that the Hilbert space L2 (R) is a complex vector space.
♦ 7.4.6. We did not quite tell the truth when we said that L2 functions must decay at large
distances: Prove that the following function is in L2 but does not go to zero as | x | → ∞:


f (x) =

1,
0,

n − n−2 < x < n + n−2
otherwise.

for

n = ±1, ±2, ±3, . . . ,

7.4.7. Modify the function in Exercise 7.4.6 to produce a function f ∈ L2 that nevertheless
satisﬁes lim f (n) = ∞ for n ∈ Z.
n → ±∞

♦ 7.4.8. Suppose f ∈ L2 is continuously diﬀerentiable, f ∈ C1 , and has bounded derivative:
| f  (x) | ≤ M for all x ∈ R. Prove that f (x) → 0 as x → ± ∞.
7.4.9. (a) Find the constant a > 0 such that ϕ(x) = a e− | x | is a wave function.
(b) Verify the Heisenberg inequality (7.74) for this particular wave function.
2

7.4.10. Answer Exercise 7.4.9 when (i ) ϕ(x) = a e− x ;

(ii ) ϕ(x) =

♦ 7.4.11. Write out a detailed derivation of the ﬁnal inequality (7.76).

a
.
1 + x2

Chapter 8

Linear and Nonlinear Evolution Equations

The term evolution equation refers to a dynamical partial diﬀerential equation that involves
both time t and space x = (x1 , . . . , xn ) as independent variables and takes the form
∂u
= K[ u ],
∂t

(8.1)

whose left-hand side is just the ﬁrst-order time derivative of the dependent variable u,
while the right-hand side, which can be linear or nonlinear, involves only u and its space
derivatives and, possibly, t and x. Examples already encountered include the linear and
nonlinear transport equations in Chapter 2 and the heat equation. (But not the wave
equation or Laplace equation.) In this chapter, we will analyze several important evolution
equations, both linear and nonlinear, involving a single spatial variable.
Our ﬁrst stop is to revisit the heat equation. We introduce the fundamental solution,
which, for dynamical partial diﬀerential equations, assumes the role of the Green’s function,
in that its initial condition is a concentrated delta impulse. The fundamental solution
leads to an integral superposition formula for the solutions produced by more general
initial conditions or by external forcing. For the heat equation on the entire real line, the
Fourier transform enables us to construct an explicit formula that identiﬁes its fundamental
solution as a Gaussian ﬁlter. We next discuss the Black–Scholes equation, the paradigmatic
model for investment portfolios, ﬁrst proposed in the early 1970s and now lying at the
heart of the modern ﬁnancial industry. We will ﬁnd that the Black–Scholes equation can
be transformed into the linear heat equation, whose fundamental solution is applied to
establish the celebrated Black–Scholes formula for option pricing.
The following section provides a brief introduction to symmetry-based solution techniques for linear and nonlinear partial diﬀerential equations. Knowing a symmetry of a
partial diﬀerential equation allows one to readily construct additional solutions from any
known solution. Solutions that remain invariant under a one-parameter family of symmetries can be found by solving a reduced ordinary diﬀerential equation. The most important are the traveling wave solutions, which are invariant under translation symmetries,
and similarity solutions, which are invariant under scaling symmetries. The next section
presents the Maximum Principle that rigorously justiﬁes the entropic decay of temperature
in a heated body and underlies much of the advanced mathematical analysis of parabolic
partial diﬀerential equations.
The next evolution equation to appear is a paradigmatic model of nonlinear diﬀusion
known as Burgers’ equation. It can be regarded as a very simpliﬁed model of ﬂuid dynamics,
combining both nonlinear and viscous eﬀects. We discover a remarkable nonlinear change
of variables that maps Burgers’ equation to the linear heat equation, and thereby facilitates
P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_8, © Springer International Publishing Switzerland 2014

291

292

8 Linear and Nonlinear Evolution Equations

of variables that maps Burgers’ equation to the linear heat equation, and thereby facilitates
its analysis, allowing us to construct explicit solutions, and investigate how they converge
to shock wave solutions of the nonlinear transport equation in the inviscid limit.
Next, we turn our attention to the simplest third-order linear evolution equation, which
arises as a model for wave mechanics. Unlike ﬁrst- and second-order wave equations, its
solutions are not simple traveling waves, but instead exhibit dispersion, in which oscillatory
waves of diﬀerent frequencies move at diﬀerent speeds. As a result, initially localized
disturbances will spread out or disperse, even while they conserve the underlying energy.
Dispersion implies that the individual wave velocities diﬀer from the group velocity, which
measures the speed of propagation of energy in the system. An everyday manifestation of
this phenomenon can be observed in the ripples caused by throwing a rock into a pond:
the individual waves move faster than the overall disturbance. Finally, we present the
remarkable Talbot eﬀect, only recently discovered, in which solutions having discontinuous
initial data and subject to periodic boundary conditions exhibit radically diﬀerent proﬁles
at rational and irrational times.
Our ﬁnal example is the celebrated Korteweg–de Vries equation, which originally arose
in the work of the nineteenth-century French applied mathematician Joseph Boussinesq as
a model for surface waves on shallow water. It combines the eﬀects of linear dispersion and
nonlinear transport. Unlike the linearly dispersive model, the Korteweg–de Vries equation
admits explicit, localized traveling wave solutions, now known as “solitons”. Remarkably, despite the potentially complicated nonlinear nature of their interaction, two solitons
emerge from a collision with their individual proﬁles preserved, the only residual eﬀect
being a relative phase shift. The Korteweg–de Vries equation is the prototype of a completely integrable partial diﬀerential equation, whose many remarkable properties were
ﬁrst discovered in the mid 1960s. A surprising number of such completely integrable nonlinear systems appear in a variety of applications, including dynamical models in ﬂuids,
plasmas, optics, and solid mechanics. Their analysis remains an extremely active area of
contemporary research, [2, 36].

8.1 The Fundamental Solution to the Heat Equation
One disadvantage of the Fourier series solution to the heat equation is that it is not nearly
as explicit as one might desire for practical applications, numerical computations, or even
further theoretical investigations and developments. An alternative approach is based on
the idea of the fundamental solution, which plays the role of the Green’s function in solving
initial value problems. The fundamental solution measures the eﬀect of a concentrated,
instantaneous impulse, either in the initial conditions or as an external force on the system.
We restrict our attention to homogeneous boundary conditions — keeping in mind
that these can always be included by use of linear superposition. The basic idea is to
analyze the case in which the initial data u(0, x) = δξ (x) = δ(x − ξ) is a delta function,
which we can interpret as a highly concentrated unit heat source, e.g., a soldering iron or
laser beam, that is instantaneously applied at a position ξ along a metal bar. The heat
will diﬀuse away from its initial concentration, and the resulting fundamental solution is
denoted by
u(t, x) = F (t, x; ξ),
with
F (0, x; ξ) = δ(x − ξ).
(8.2)
For each ﬁxed ξ, the fundamental solution, considered as a function of t > 0 and x, must

8.1 The Fundamental Solution to the Heat Equation

293

satisfy the underlying partial diﬀerential equation, and so, for the heat equation,
∂F
∂2F
,
=γ
∂t
∂x2

(8.3)

along with the speciﬁed homogeneous boundary conditions.
As with the Green’s function, once we have determined the fundamental solution, we
can then use linear superposition to reconstruct the general solution to the initial-boundary
value problem. Namely, we ﬁrst write the initial data
 b
u(0, x) = f (x) =
δ(x − ξ) f (ξ) dξ
(8.4)
a

as a superposition of delta functions, as in (6.16). Linearity implies that the solution
can be expressed as the corresponding superposition of the responses to those individual
concentrated delta proﬁles:
 b
u(t, x) =
F (t, x; ξ) f (ξ) dξ.
(8.5)
a

Assuming that we can diﬀerentiate under the integral sign, the fact that F (t, x; ξ) satisﬁes the diﬀerential equation and the homogeneous boundary conditions for each ﬁxed ξ
immediately implies that the integral (8.5) is also a solution with the correct initial and
(homogeneous) boundary conditions.
Unfortunately, most boundary value problems do not have fundamental solutions that
can be written down in closed form. An important exception is the case of an inﬁnitely
long homogeneous bar, which requires solving the heat equation on the entire real line:
∂u
∂2u
=
,
∂t
∂x2

− ∞ < x < ∞,

for

t > 0.

(8.6)

For simplicity, we have chosen units in which the thermal diﬀusivity is γ = 1. The solution
u(t, x) is deﬁned for all x ∈ R, and has initial conditions
u(0, x) = f (x)

− ∞ < x < ∞.

for

(8.7)

In order to specify the solution uniquely, we shall require that the temperature be squareintegrable, i.e., in L2 , at all times, so that
 ∞
| u(t, x) |2 dx < ∞
for all
t ≥ 0.
(8.8)
−∞

Roughly speaking, square-integrability requires that the temperature be vanishingly small
at large distances, and hence plays the role of boundary conditions in this context.
To solve the initial value problem (8.6–7), we apply the Fourier transform, in the x
variable, to both sides of the diﬀerential equation. In view of the eﬀect of the Fourier
transform on derivatives, cf. (7.43), the result is
∂u

,
= − k2 u
∂t
where

1
u
(t, k) = √
2π

 ∞
−∞

u(t, x) e− i k x dx

(8.9)

(8.10)

294

Figure 8.1.

8 Linear and Nonlinear Evolution Equations

t = .05

t = .1

t=1

t = 10

The fundamental solution to the one-dimensional heat equation.



is the Fourier transformed solution. For each ﬁxed k, (8.9) can be viewed as a ﬁrst-order
linear ordinary diﬀerential equation for u
(t, k), with initial conditions
 ∞
1

u
(0, k) = f (k) = √
f (x) e− i k x dx
(8.11)
2 π −∞
given by Fourier transforming the initial data (8.7). The solution to the initial value
problem (8.9, 11) is immediate:
u
(t, k) = e− k t f(k).
2

(8.12)

We can thus recover the solution to the initial value problem (8.6–7) by applying the inverse
Fourier transform to (8.12), leading to the explicit integral formula
 ∞
 ∞
2
1
1
u(t, x) = √
(8.13)
e ikx u
(t, k) dk = √
e i k x− k t f(k) dk.
2 π −∞
2 π −∞
In particular, to construct the fundamental solution, we take the initial temperature
proﬁle to be a delta function δξ (x) = δ(x − ξ) concentrated at x = ξ. According to (7.37),
its Fourier transform is
e− i k ξ
δξ (k) = √
.
2π
Plugging this into (8.13), and then referring to our table of Fourier transforms, we are led
to the following explicit formula for the fundamental solution:
 ∞
2
2
1
1
e i k(x−ξ)− k t dk = √ e− (x−ξ) /(4 t)
for
t > 0.
(8.14)
F (t, x; ξ) =
2 π −∞
2 πt
As you can verify, for each ﬁxed ξ, the function F (t, x; ξ) is indeed a solution to the heat
equation for all t > 0. In addition,

0,
x = ξ,
lim F (t, x; ξ) =
+
t→0
∞,
x = ξ.

8.1 The Fundamental Solution to the Heat Equation

Furthermore, its integral

295

 ∞
F (t, x; ξ) dx = 1

(8.15)

−∞

is constant — in accordance with the law of conservation of thermal energy; see Exercise
8.1.20. Therefore, as t → 0+ , the fundamental solution satisﬁes the original limiting
deﬁnition (6.8–9) of the delta function, and so F (0, x; ξ) = δξ (x) has the desired initial
temperature proﬁle.
In Figure 8.1, we graph F (t, x; 0) at the indicated times. It starts life as a delta
spike concentrated at the origin, and then immediately smooths out into a tall and narrow
bell-shaped curve, centered at x = 0. As time increases, the solution shrinks and widens,
eventually decaying everywhere to zero. Its amplitude is proportional to t−1/2 , while its
overall width is proportional to t1/2 . The thermal energy (8.15), which is the area under
the graph, remains ﬁxed while gradually spreading out over the entire real line.
Remark : In probability, these exponentially bell-shaped curves are known as normal or
Gaussian distributions, [39]. The width of the bell curve measures its standard deviation.
For this reason, the fundamental solution to the heat equation is sometimes referred to as
a Gaussian ﬁlter .
Remark : The fact that the fundamental solution depends only on the diﬀerence x − ξ,
and hence has the same proﬁle at all ξ ∈ R, is a consequence of the translation invariance
of the heat equation, reﬂecting the fact that it models the thermodynamics of a uniform
medium. See Section 8.2 for additional symmetry properties of the heat equation and its
solutions.
Remark : One of the striking properties of the heat equation is that thermal energy
propagates with inﬁnite speed. Indeed, because, at any t > 0, the fundamental solution
is nonzero for all x, the eﬀect of an initial concentration of heat will immediately be felt
along the entire length of an inﬁnite bar. (The graphs in Figure 8.1 are a little misleading
because they fail to show the extremely small, but still positive, exponentially decreasing
tails.) This eﬀect, while more or less negligible at large distances, is nevertheless in clear
violation of physical intuition — not to mention relativity, which postulates that signals
cannot propagate faster than the speed of light. Despite this non-physical artifact, the heat
equation remains an accurate model for heat propagation and similar diﬀusive phenomena,
and so continues to be successfully used in applications.
With the fundamental solution in hand, we can adapt the linear superposition formula (8.5) to reconstruct the general solution
 ∞
2
1
u(t, x) = √
e− (x−ξ) /(4 t) f (ξ) dξ
(8.16)
2 π t −∞
to our initial value problem (8.6). This solution formula is merely a restatement of (8.13)
combined with the Fourier transform formula (8.11). Comparing with (7.54), we see that
the solutions are obtained by convolution of the initial data with a one-parameter family
of progressively wider and shorter Gaussian ﬁlters:
2

u(t, x) = F0 (t, x) ∗ f (x),

where

e− x /(4 t)
√
F0 (t, x) = F (t, x; 0) =
.
2 πt

Since u(t, x) solves the heat equation, we conclude that Gaussian ﬁlter convolution has the
same smoothing eﬀect on the initial signal f (x). Indeed, the convolution integral (8.16)

296

8 Linear and Nonlinear Evolution Equations

t=0

t = .1

t=1

t=5

t = 30

t = 300

Figure 8.2.

Error function solution to the heat equation.



serves to replace each initial value f (x) by a weighted average of nearby values, the weight
being determined by the Gaussian distribution. This has the eﬀect of smoothing out highfrequency variations in the signal, and, consequently, the Gaussian convolution formula
(8.16) provides an eﬀective method for denoising rough signals and data.
Example 8.1. An inﬁnite bar is initially heated to unit temperature along a ﬁnite
interval. The initial temperature proﬁle is thus a box function

1,
a < x < b,
u(0, x) = f (x) = σ(x − a) − σ(x − b) =
0,
otherwise.
The ensuing temperature is provided by the solution to the heat equation obtained by the
integral formula (8.16):
 



 b
1
1
x−a
x−b
− (x−ξ)2 /(4 t)
√
√
u(t, x) = √
erf
− erf
,
(8.17)
e
dξ =
2
2 πt a
2 t
2 t
where erf denotes the error function, as deﬁned in (2.87). Graphs of the solution (8.17) for
a = −5, b = 5, at the indicated times, are displayed in Figure 8.2. Observe the instantaneous smoothing of the sharp interface and instantaneous propagation of the disturbance,
followed by a gradual decay to thermal equilibrium, with u(t, x) → 0 as t → ∞.

The Forced Heat Equation and Duhamel’s Principle
The fundamental solution approach can be also applied to solve the inhomogeneous heat
equation
ut = uxx + h(t, x),
(8.18)
modeling a bar subject to an external heat source h(t, x), which might depend on both
position and time. We begin by solving the particular case
ut = uxx + δ(t − τ ) δ(x − ξ),

(8.19)

8.1 The Fundamental Solution to the Heat Equation

297

whose inhomogeneity represents a heat source of unit magnitude that is concentrated at a
position x = ξ and applied at a single time t = τ > 0. Physically, this models the eﬀect of
instantaneously applying a soldering iron to a single spot on the bar. Let us also impose
homogeneous initial conditions
u(0, x) = 0
(8.20)
as well as homogeneous boundary conditions of one of our standard types. The resulting
solution
u(t, x) = G(t, x; τ, ξ)
(8.21)
will be referred to as the general fundamental solution to the heat equation. Since a heat
source that is applied at time τ will aﬀect the solution only at later times t ≥ τ , we expect
that
G(t, x; τ, ξ) = 0
for all
t < τ.
(8.22)
Indeed, since u(t, x) solves the unforced heat equation at all times t < τ subject to homogeneous boundary conditions and has zero initial temperature, this follows immediately
from the uniqueness of the solution to the initial-boundary value problem.
Once we know the general fundamental solution (8.21), we are able to solve the problem
for a general external heat source (8.18). We ﬁrst write the forcing as a superposition
 ∞ b
h(t, x) =

δ(t − τ ) δ(x − ξ) h(τ, ξ) dξ dτ

(8.23)

a

0

of concentrated instantaneous heat sources. Linearity allows us to conclude that the solution is given by the self-same superposition formula
 t b
G(t, x; τ, ξ) h(τ, ξ) dξ dτ.
(8.24)
u(t, x) =
0

a

The fact that we only need to integrate over times 0 ≤ τ ≤ t is a consequence of (8.22).
Remark : If we have a nonzero initial condition, u(0, x) = f (x), then, by linear superposition, the solution
 t b
 b
F (t, x; ξ) f (ξ) dξ +
G(t, x; τ, ξ) h(τ, ξ) dξ dτ
(8.25)
u(t, x) =
a

0

a

is a combination of (a) the solution with no external heat source, but nonzero initial
conditions, plus (b) the solution with homogeneous initial conditions but nonzero heat
source.
Let us explicitly solve the forced heat equation on an inﬁnite interval − ∞ < x < ∞.
We begin by computing the general fundamental solution. As before, we take the Fourier
transform of both sides of the partial diﬀerential equation (8.18) with respect to x. In view
of (7.37, 43), we ﬁnd
1
∂u

= √
(8.26)
+ k2 u
e− i k ξ δ(t − τ ),
∂t
2π
which is an inhomogeneous ﬁrst-order ordinary diﬀerential equation for the Fourier transform u
(t, k) of u(t, x), while (8.20) implies the initial condition
u
(0, k) = 0.

(8.27)

298

8 Linear and Nonlinear Evolution Equations

We solve the initial value problem (8.26–27) by the usual method, [18, 23]. Multiplying
2
the diﬀerential equation by the integrating factor ek t yields
2
2
∂
1
) = √
ek t− i k ξ δ(t − τ ).
( ek t u
∂t
2π

Integrating both sides from 0 to t and using the initial condition, we obtain
2
1
e− k (t−τ )− i k ξ σ(t − τ ),
u
(t, k) = √
2π

where σ(s) is the usual step function (6.23). Finally, we apply the inverse Fourier transform
formula (7.9), and then (8.14), to deduce that

σ(t − τ ) ∞ − k2 (t−τ )+ i k (x−ξ)
e
dk
u(t, x) = G(t, x; τ, ξ) =
2π
−∞


(8.28)
(x − ξ)2
σ(t − τ )
exp −
= 
= σ(t − τ )F (t − τ, x; ξ) .
4(t − τ )
2 π(t − τ )
Thus, the general fundamental solution is obtained by translating the fundamental solution
F (t, x; ξ) for the initial value problem to a starting time of t = τ instead of t = 0. Finally,
the superposition principle (8.24) produces the solution,


 t ∞
(x − ξ)2
h(τ, ξ)

exp −
dξ dτ,
(8.29)
u(t, x) =
4 (t − τ )
π(t − τ )
0 −∞ 2
to the heat equation with source term and zero initial condition on an inﬁnite bar. A
nonzero initial condition u(0, x) = f (x) leads, as in the superposition formula (8.25), to an
additional term of the form (8.16) in the solution formula.
Remark : The fact that an initial condition has the same aftereﬀect on the temperature as an instantaneous applied heat source of the same magnitude, thus implying the
identiﬁcation (8.28) of the two types of fundamental solution, is known as Duhamel’s Principle, named after the nineteenth-century French mathematician Jean–Marie Duhamel.
Duhamel’s Principle remains valid over a broad range of linear evolution equations.
Example 8.2. An inﬁnitely long bar with unit thermal diﬀusivity starts out uniformly at zero degrees. Beginning at time t = 0, a concentrated heat source of unit
magnitude is continually applied at the origin. The resulting temperature is the solution
u(t, x) to the initial value problem
ut = uxx + δ(x),

u(0, x) = 0,

t > 0,

− ∞ < x < ∞.

According to (8.29), the solution is given by


 t ∞
(x − ξ)2
δ(ξ)

exp −
u(t, x) =
dξ dτ
4 (t − τ )
π(t − τ )
0 −∞ 2


x2
1

dτ =
exp −
4 (t − τ )
π(t − τ )
0 2

 t
=




 x erf
x2
t
exp −
+
π
4t



x
√
2 t
2


− |x|

Three snapshots can be seen in Figure 8.3. Observe that the solution is even in x and
monotonically decreasing as | x | → ∞. Moreover, it has a corner at the origin with limiting

.

8.1 The Fundamental Solution to the Heat Equation

t=1

t=2

Figure 8.3.

Eﬀect of a concentrated heat source.

299

t=3


tangent lines of slopes ± 12 , which implies that its second x derivative produces the deltafunction forcing term. At each time t, the solution can be viewed as the linear superposition
of a continuous family of fundamental solutions, corresponding to the cumulative eﬀect of
individual heat sources applied at each previous time 0 ≤ τ ≤ t. Moreover, it is not
diﬃcult to see that, at each ﬁxed x, the temperature is monotonically increasing in t, with
u(t, x) → ∞ as t → ∞, and hence the continuous heat source eventually produces an
unbounded temperature in the entire inﬁnite bar.

The Black–Scholes Equation and Mathematical Finance
The most important and inﬂuential partial diﬀerential equation in ﬁnancial modeling and
investment is the celebrated Black–Scholes equation
∂u
∂u σ 2 2 ∂ 2 u
+ rx
+
x
− r u = 0,
∂t
2
∂x2
∂x

(8.30)

ﬁrst proposed in 1973 by the American economists Fischer Black and Myron Scholes, [19],
and Robert Merton, [71]. The dependent variable u(t, x) represents the monetary value
of a single ﬁnancial option, meaning a contract to either buy or sell an asset at a speciﬁed
exercise price p at a certain future time t . The value u(t, x) of the option will depend
on the current time t ≤ t and the current price x ≥ 0 of the underlying asset. As with
many ﬁnancial models, one assumes the absence of arbitrage, meaning that there is no
way to make a riskless proﬁt. The constant σ > 0 represents the asset’s volatility, while
r denotes the (assumed ﬁxed) interest rate for bank deposits, where investors could place
their money with a guaranteed rate of return instead of buying the option. (Investors
borrowing money to buy the asset would use a negative value of r.) The derivation of
the Black–Scholes equation from basic ﬁnancial modeling relies on the theory of stochastic
diﬀerential equations, [83], which would take us too far aﬁeld to explain here; instead, we
refer the interested reader to [123]. The Black–Scholes equation and its generalizations
form the basis of much of the modern ﬁnancial world, and, increasingly, the insurance
industry.
Observe ﬁrst that the Black–Scholes equation is a backwards diﬀusion process, since,
upon solving for
∂u
σ2 2 ∂ 2 u
∂u
=−
x
+ r u,
(8.31)
−rx
2
∂t
2
∂x
∂x
the coeﬃcient of the diﬀusion term uxx is negative. This implies that the initial value
problem is well-posed only when time runs backwards. In other words, given a prescribed

300

8 Linear and Nonlinear Evolution Equations

value of the option at some speciﬁed time in the future, we can use the Black–Scholes
equation to determine its current value. However, ill-posedness implies that we cannot
predict future values from the current worth of the portfolio.
The “ﬁnal value problem” for the Black–Scholes equation is to determine the option’s
value u(t, x) at the current time t and asset value x ≥ 0, given the ﬁnal condition
u(t , x) = f (x)

(8.32)

at the exercise time t > t. For a so-called European call option, whereby the asset is to
be bought at the exercise price p > 0 at the speciﬁed time, the ﬁnal condition is
u(t , x) = max{ x − p, 0 },

(8.33)

representing the investor’s proﬁt when x > p, or, when x ≤ p, the option not being exercised
so as to avoid a loss. Analogously, for a put option, where the asset is to be sold, the ﬁnal
condition is
u(t , x) = max{ p − x, 0 }.
(8.34)
The solution u(t, x) will be deﬁned for all t < t and all x > 0, subject to the boundary
conditions
u(t, 0) = 0,
u(t, x) ∼ x
as
x → ∞,
where the asymptotic boundary condition means that the ratio u(t, x)/x tends to a constant
as x → ∞.
Fortunately, the Black–Scholes equation can be solved explicitly by transforming it
into the heat equation. The ﬁrst step is to convert it to a forward diﬀusion process, by
setting
τ = 12 σ 2 (t − t),
v(τ, x) = u(t − 2 τ /σ 2 , x),
so that τ eﬀectively runs forward from 0 as the actual time t runs backwards from t . This
substitution has the eﬀect of converting the ﬁnal condition (8.32) into an initial condition
v(0, x) = f (x). Moreover, a straightforward chain rule computation shows that v satisﬁes
∂v
∂2v
∂v
2r
= x2 2 + κ x
− κ v,
where
κ= 2 .
∂τ
∂x
∂x
σ
The next step is to remove the explicit dependence on the independent variable x. The
hint is that the right-hand side has the form of an Euler ordinary diﬀerential equation,
[23, 89]. According to Exercise 4.3.23, these terms can be placed into constant-coeﬃcient
form by the change of independent variables x = ey . Indeed, writing
w(τ, y) = v(τ, ey ) = v(τ, x)

when

x = ey ,

we apply the chain rule to compute the derivatives
∂v
∂w
=
,
∂τ
∂τ

∂v
∂w
∂v
= ey
=x
,
∂y
∂x
∂x

∂2v
∂v
∂2v
∂2w
∂v
= x2 2 + x
.
= e2 y
+ ey
2
2
∂y
∂x
∂x
∂x
∂x

As a result, we ﬁnd that w solves the partial diﬀerential equation
∂w
∂w
∂2w
+ (κ − 1)
=
− κ w.
2
∂τ
∂y
∂y

(8.35)

This is getting closer to the heat equation, and, in fact, can be changed into it by setting
w(τ, y) = eα τ +β y z(τ, y)

8.1 The Fundamental Solution to the Heat Equation

301

for suitable constants α, β. Indeed, diﬀerentiating and substituting into (8.35) yields


∂2z
∂z
∂z
∂z
2
+ αz =
+ β z + (κ − 1)
+ β z − κ z.
+ 2β
∂τ
∂y 2
∂y
∂y
The terms involving ∂z/∂y and z are eliminated by setting
α = − 14 (κ + 1)2 ,

β = − 12 (κ − 1).

(8.36)

We conclude that the function
2

z(τ, y) = e(κ+1) τ /4+(κ−1)y/2 w(τ, y)

(8.37)

∂z
∂2z
=
.
∂τ
∂y 2

(8.38)

satisﬁes the heat equation

Unwinding the preceding argument, we have managed to prove the following:
Proposition 8.3. If z(τ, y) is the solution to the initial value problem
∂2z
∂z
=
,
∂τ
∂y 2

z(0, y) = h(y) = e(κ−1)y/2 f (ey ),

(8.39)

for τ > 0, − ∞ < y < ∞, then
2

2

u(t, x) = x− (κ−1)/2 e− (κ+1) σ (t −t)/8 z 12 σ 2 (t − t), log x

(8.40)

solves the ﬁnal value problem (8.30, 32) for the Black–Scholes equation for t < t and
0 < x < ∞.
Now, according to (8.16), the solution to the initial value problem (8.39) can be written
as a convolution integral of the initial data with the heat equation’s fundamental solution:
 ∞
 ∞
2
2
1
1
z(τ, y) = √
e− (y−η) /(4 τ ) h(η) dη = √
e− (y−η) /(4 τ )+(κ−1)η/2 f (eη ) dη.
2 π τ −∞
2 π τ −∞
(8.41)
Combining this formula with (8.40) produces an explicit solution formula for the general
ﬁnal value problem for the Black–Scholes equation. In particular, for the European call
option (8.33), the initial condition is
z(0, y) = h(y) = e(κ−1)y/2 max{ ey − p, 0 },
and so
z(τ, y) =

1
√
2 πτ

 ∞

2

e− (y−η) /(4 τ )+(κ−1)η/2 (eη − p) dη.

log p

The integral can evaluated by completing the square inside the exponential, producing



1 (κ+1)2 τ /4+(κ+1)y/2
log p − (κ + 1)τ − y
√
z(τ, y) =
e
erfc
2
2 τ


(8.42)
log p − (κ − 1)τ − y
(κ−1)2 τ /4+(κ−1)y/2
√
−pe
,
erfc
2 τ

302

8 Linear and Nonlinear Evolution Equations

20

20

20

10

10

10

10

10

20

10

20

t=8

t=4

t=0
20

20

20

10

10

10

10

20

10

t=9

20

t = 9.5

Figure 8.4.

20

Solution to the Black–Scholes equation.

10

20

t = 10


where
2
erfc x = √
π

 ∞

2

e− z dz = 1 − erf x

(8.43)

x

is the complementary error function, cf. (2.87). Substituting (8.42) into (8.40) results in
the celebrated Black–Scholes formula for a European call option:

1
u(t, x) =
2

$


r + 12 σ 2 (t − t) + log(x/p)

x erfc −
2 σ 2 (t − t)
% (8.44)

r − 12 σ 2 (t − t) + log(x/p)
− r(t −t)

.
−pe
erfc −
2 σ 2 (t − t)


A graph of the solution for the speciﬁc values t = 10, r = .1, σ = .2, p = 10 appears in
Figure 8.4. Observe that the option’s value slowly decreases as the time gets closer and
closer to the exercise time t , thereby lessening any chances of further proﬁt stemming
from the option’s underlying price volatility.

8.1 The Fundamental Solution to the Heat Equation

303

Exercises
8.1.1. Find the solution to the heat equation ut = uxx on the real line having the following
initial condition at time t = 0. Then sketch graphs of the resulting temperature distribution
at times t = 0, 1, and 5.

2
1 − | x |, | x | < 1,
(a) e− x , (b) the step function σ(x), (c) e− | x | , (d)
0,
otherwise.
8.1.2. On an inﬁnite bar with unit thermal diﬀusivity, a concentrated unit heat source is instantaneously applied at the origin at time t = 0. A heat sensor measures the resulting
temperature in the bar at position x = 1. Determine the maximum temperature measured
by the sensor. At what time is the maximum achieved?
8.1.3. (a) Find the solution to the heat equation (8.6) whose initial data corresponds to a pair
of unit heat sources placed at positions x = ± 1. (b) Graph the solution at times t =
.1, .25, .5, 1. (c) At what time(s) does the origin experience its maximum overall temperature? What is the maximum temperature at the origin?
8.1.4. (a) Use the Fourier transform to solve the initial value problem
∂u
∂2u
,
u(0, x) = δ  (x − ξ),
−∞ < x < ∞, t > 0,
=
∂t
∂x2
whose initial data is the derivative of the delta function at a ﬁxed position ξ.
(b) Show that your solution can be written as the derivative ∂F/∂x of the fundamental
solution F (t, x; ξ). Explain why this observation should be valid.
8.1.5. Suppose that the initial data u(0, x) = f (x) is real. Explain why the Fourier transform
solution formula (8.13) deﬁnes a real function u(t, x) for all t > 0.
8.1.6. (a) What is the maximum value of the fundamental solution√at time t?
(b) Can you justify the claim that its width is proportional to t ?
8.1.7. Prove directly that (8.5) is indeed a solution to the heat equation, and, moreover, has
the correct initial and boundary conditions.
8.1.8. Show, by a direct computation, that the ﬁnal formula in (8.14) is a solution to the heat
equation for all t > 0.
♦ 8.1.9. Justify formula (8.15).
8.1.10. According to Exercises 4.1.11–12, both the t and x partial derivatives of the fundamental solution solve the heat equation. (a) Write down the initial value problem satisﬁed by
these two solutions. (b) Set ξ = 0 and then sketch graphs of each solution at several
selected times. (c) Reconstruct each solution as a Fourier integral.
∂F
(t, x; 0) denote the x derivative of the fundamental solution (8.14).
∂x
(a) Prove that u(t, x) is a solution to the heat equation ut = uxx on the domain
{ − ∞ < x < ∞, t > 0 }. (b) For ﬁxed x, prove that lim+ u(t, x) = 0. (c) Explain why,

8.1.11. Let u(t, x) =

t→0

despite the results in parts (a) and (b), u(t, x) is not a classical solution to the initial value
problem ut = uxx , u(0, x) = 0. What is the classical solution? (d) What initial value
problem does u(t, x) satisfy?
8.1.12. Justify all the statements in Example 8.2.
♥ 8.1.13. (a) Solve the heat equation on an inﬁnite bar when the initial temperature is equal to 1
for | x | < 1 and 0 elsewhere, while a unit heat source is applied to the same part of the bar
| x | < 1 for a unit time period 0 < t < 1. (b) At what time and what location is the bar
the hottest? (c) What is the ﬁnal equilibrium temperature of the bar?

304

8 Linear and Nonlinear Evolution Equations

8.1.14. An insulated bar 1 meter long, with constant diﬀusivity γ = 1, is taken from a freezer
that is kept at −10◦ C, and then has its ends kept at room temperature of 20◦ C. A
soldering iron with temperature 350◦ C is continually held at the midpoint of the bar.
(a) Set up an initial value problem modeling the temperature distribution in the bar.
(b) Find the corresponding equilibrium temperature distribution.
♥ 8.1.15. Consider the heat equation with unit thermal diﬀusivity on the interval 0 < x < 1
subject to homogeneous Dirichlet boundary conditions.
 (t, x; ξ) that solves
(a) Find a Fourier series representation for the fundamental solution F
the initial-boundary value problem
ut = uxx , t > 0, 0 < x < 1,
u(0, x) = δ(x − ξ), u(t, 0) = 0 = u(t, 1).
Your solution should depend on t, x and the point ξ where the initial delta impulse is
applied.
(b) For the value ξ = .3, use a computer program to sum the ﬁrst few terms in the series
and graph the result at times t = .0001, .001, .01, and .1. Make sure you have included
enough terms to obtain a reasonably accurate graph.
(c) Compare your graphs with those of the fundamental solution F (t, x; .3) on an inﬁnite
interval at the same times. What is the maximum deviation between the two solutions
on the entire interval 0 ≤ x ≤ 1?
 (t, x; ξ) to construct a series solution to the general
(d) Use your fundamental solution F
initial value problem u(0, x) = f (x). Is your series the same as the usual Fourier series
solution? If not, explain any discrepancy.
8.1.16. True or false: Periodic forcing of the heat equation at a particular frequency can
produce resonance. Justify your answer.
8.1.17. Find the fundamental solution for the cable equation vt = γ vxx − α v on the real line.
Hint: See Exercise 4.1.16.
8.1.18. The partial diﬀerential equation ut + c ux = γ uxx models transport of a diﬀusing
pollutant in a ﬂuid ﬂow. Assuming that the speed c is constant, write down a solution to
the initial value problem u(0, x) = f (x) for − ∞ < x < ∞. Hint: Look at Exercise 4.1.17.
♦ 8.1.19. Use the Fourier transform to solve the initial value problem i ut = uxx , u(0, x) = f (x),
for the one-dimensional Schrödinger equation on the real line − ∞ < x < ∞.
♦ 8.1.20. Let u(t, x) be a solution to the heat equation having ﬁnite thermal energy,
E(t) =

∞

−∞

u(t, x) dx < ∞, and satisfying ux (t, x) → 0 as x → ±∞, for all t ≥ 0. Prove the

law of conservation of thermal energy: E(t) = constant.
8.1.21. Explain in your own words how a function u(t, x) can satisfy u(t, x) → 0 uniformly as
t → ∞ while maintaining the constancy of

∞

−∞

u(t, x) dx = 1 for all t. Discuss what this

signiﬁes regarding the interchange of limits and integrals.
2

8.1.22. (a) Prove that if f (k) ∈ L2 is square-integrable, then so is e− a k f (k) for any a > 0.
(b) Prove that when the initial data f (x) ∈ L2 is square integrable, so is the Fourier
integral solution (8.13) for all t ≥ 0.
8.1.23. Find the solution to the Black–Scholes equation for a put option (8.34).
8.1.24. (a) If we increase the interest rate r, does the value of a call option (i ) increase;
(ii ) decrease; (iii ) stay the same; (iv ) could do any of the above? Justify your answer.
(b) Answer the same question when rate stays ﬁxed, but the volatility σ is increased.
♦ 8.1.25. Justify formula (8.42).

8.2 Symmetry and Similarity

305

8.2 Symmetry and Similarity
The geometric approach to partial diﬀerential equations enables one to exploit their symmetry properties to construct explicit solutions of both mathematical and physical interest.
Unlike separation of variables, which is restricted to special types of linear partial diﬀerential equations,† symmetry methods can also be successfully applied to a broad range of
nonlinear partial diﬀerential equations. While we do not have the mathematical tools to
develop the full range of symmetry techniques, we will learn how to exploit some of the
most basic symmetry properties: translations, leading to traveling wave solutions; scalings,
leading to similarity solutions; and, in subsequent chapters, rotational symmetries.
In general, by a symmetry of an equation, we mean a transformation that takes solutions to solutions. Thus, knowing a symmetry transformation, if we are in possession of
one solution, then we can construct a second solution by applying the symmetry. And,
possibly, a third solution by applying the symmetry yet again. And so on. If we know lots
of symmetries, then we can produce lots of solutions by this simple device.
Remark : General symmetry techniques are founded on the theory of Lie groups,
named after the inﬂuential nineteenth-century Norwegian mathematician Sophus Lie (pronounced “Lee”). Lie’s theory is a profound synthesis of group theory and diﬀerential
geometry, and provides an algorithm for completely determining all the (continuous) symmetries of a given diﬀerential equation. Although the theory lies beyond the scope of
this introductory text, direct inspection and/or physical intuition will often produce the
most important symmetries of the system, which can then be directly exploited. Modern
applications of Lie’s symmetry methods to partial diﬀerential equations arising in physics
and engineering can be traced back to an inﬂuential book on hydrodynamics by the author’s thesis advisor, Garrett Birkhoﬀ, [17]. A complete and comprehensive treatment
of Lie symmetry methods can be found in the author’s ﬁrst book [87], and, at a more
introductory level, in the recent books [27, 58], the ﬁrst having a particular emphasis on
applications in ﬂuid mechanics.
The heat equation serves as an excellent testing ground for the general methodology,
since it admits a rich variety of symmetry transformations that take solutions to solutions.
The simplest are the translations. Moving the space and time coordinates by a ﬁxed
amount,
t −→ t + a,
x −→ x + b,
(8.45)
where a, b are constants, changes the function u(t, x) into the translated function‡
U (t, x) = u(t − a, x − b).

(8.46)

A simple application of the chain rule proves that the partial derivatives of U with respect
to t and x agree with the corresponding partial derivatives of u, so
∂U
∂u
=
,
∂t
∂t

∂U
∂u
=
,
∂x
∂x

∂2U
∂2u
=
,
∂x2
∂x2

†

This is not entirely fair: separation of variables can also be applied to certain nonlinear
partial diﬀerential equations such as Hamilton–Jacobi equations, [ 73 ].
‡
 = x + b, then the translated function
The minus signs arise because when we set t = t + a, x
 ) = u(t, x) = u( 
 − b). Dropping the hats produces the stated formula.
is U ( t, x
t − a, x

306

8 Linear and Nonlinear Evolution Equations

and so on. In particular, the function U (t, x) is a solution to the heat equation Ut = γ Uxx
whenever u(t, x) also solves ut = γ uxx . Physically, translation symmetry formalizes the
property that the heat equation models a homogeneous medium, and hence the solution
does not depend on the choice of reference point or origin of our coordinate system.
As a consequence, each solution to the heat equation will produce an inﬁnite family
of translated solutions. For example, starting with the separable solution
u(t, x) = e− γ t sin x,
we immediately produce the additional translated solutions
U (t, x) = e− γ (t−a) sin(x − b),
valid for any choice of constants a, b.
Warning: Typically, the symmetries of a diﬀerential equation do not respect initial
or boundary conditions. For instance, if u(t, x) is deﬁned for t ≥ 0 and in the domain
0 ≤ x ≤ , then its translated version (8.46) is deﬁned for t ≥ a and in the translated
domain b ≤ x ≤  + b, and so will solve a translated initial-boundary value problem.
A second important class of symmetries consists of the scaling invariances. We already
know that if u(t, x) is a solution, then so is the scalar multiple c u(t, x) for any constant c;
this is a simple consequence of linearity of the heat equation. We can also add an arbitrary
constant to the temperature, noting that
U (t, x) = c u(t, x) + k

(8.47)

is a solution for any choice of constants c, k. Physically, the transformation (8.47) amounts
to a change in the scale used to measure temperature. For instance, if u is measured in
degrees Celsius, and we set c = 95 and k = 32, then U = 95 u+32 will be measured in degrees
Fahrenheit. Thus, reassuringly, the physical processes described by the heat equation do
not depend on our choice of thermometer.
More interestingly, suppose we rescale the space and time variables:
t −→ α t,

x −→ β x,

(8.48)

where α, β = 0 are nonzero constants. The eﬀect of such a scaling transformation is to
convert u(t, x) into a rescaled function†
U (t, x) = u(α−1 t, β −1 x).

(8.49)

The derivatives of U are related to those of u according to the formulas
∂U
1 ∂u
=
,
∂t
α ∂t

∂U
1 ∂u
=
,
∂x
β ∂x

1 ∂2u
∂2U
= 2
.
2
∂x
β ∂x2

Therefore, if u satisﬁes the heat equation ut = γ uxx , then U satisﬁes the rescaled heat
equation
1
γ
β2 γ
Ut = ut = uxx =
Uxx ,
α
α
α
†

 = β x, produces the rescaled function U ( 
 ) = u(t, x) =
As before, setting t = α t, x
t, x
 ), and we then drop the hats.
u(α−1 t, β −1 x

8.2 Symmetry and Similarity

307

which we rewrite as
β2 γ
.
(8.50)
α
Thus, the net eﬀect of scaling space and time is merely to rescale the diﬀusion coeﬃcient.
Physically, the scaling symmetry (8.48) corresponds to a change in the physical units used
to measure time and distance. For instance, to change from minutes to seconds, set α = 60,
and from yards to meters, set β = .9144. The net eﬀect (8.50) on the diﬀusion coeﬃcient
γ is a reﬂection of its physical units, namely distance2 /time.
In particular, if we choose
Ut = Γ Uxx ,

α = γ,

where

Γ=

β = 1,

then the rescaled diﬀusion coeﬃcient becomes Γ = 1. This observation has the following
important consequence. If U (t, x) solves the heat equation for a unit diﬀusivity, Γ = 1,
then
u(t, x) = U (γ t, x)
(8.51)
solves the heat equation for the diﬀusivity γ > 0. Thus, the only eﬀect of the diﬀusion
coeﬃcient is to speed up or slow down time. A body with diﬀusivity γ = 2 will cool down
twice as fast as a body (of the same shape subject to similar boundary conditions and initial
conditions) with diﬀusivity γ = 1. Note that this particular rescaling has not altered the
space coordinates, and so U (t, x) is deﬁned on the same spatial domain as u(t, x).
On the other hand, if we set α = β 2 , then the rescaled diﬀusion coeﬃcient is exactly
the same as the original: Γ = γ. Thus, the transformation
t −→ β 2 t,

x −→ β x,

(8.52)

does not alter the equation, and hence deﬁnes a scaling symmetry — also known as a similarity transformation — for the heat equation. Combining (8.52) with the linear rescaling
u → c u, we make the elementary, but important, observation that if u(t, x) is any solution
to the heat equation, then so is the function
U (t, x) = c u(β −2 t, β −1 x),

(8.53)

for the same diﬀusion coeﬃcient γ. For example, rescaling the solution
u(t, x) = e− γ t cos x

leads to the solution

2

U (t, x) = c e− γ t/β cos

x
.
β

Warning: As in the case of translations, rescaling space by a factor β = 1 will alter
the domain of deﬁnition of the solution. If u(t, x) is deﬁned for a ≤ x ≤ b, then U (t, x), as
given in (8.53), is deﬁned for β a ≤ x ≤ β b (or, when β < 0, for β b ≤ x ≤ β a).
For example, suppose that we have solved the heat equation for the temperature u(t, x)
on a bar of length 1, subject to certain initial and boundary conditions. We are then given
a bar composed of the same material of length 2. Since the diﬀusivity coeﬃcient has not
changed, we can directly construct the new solution U (t, x) by rescaling. Setting β = 2
will serve to double the length. If we also rescale time by a factor α = β 2 = 4, then the
rescaled function U (t, x) = u 14 t, 12 x will be a solution of the heat equation on the longer
bar with the same diﬀusivity constant. The net eﬀect is that the rescaled solution will be
evolving four times as slowly as the original, and hence it eﬀectively takes a bar that is
twice the length four times as long to cool down.

308

8 Linear and Nonlinear Evolution Equations

Similarity Solutions
A similarity solution of a partial diﬀerential equation is one that remains unchanged (invariant) under a one-parameter family† of scaling symmetries. For a partial diﬀerential
equation in two variables — say t and x — the similarity solutions can be found by solving
an ordinary diﬀerential equation.
Suppose our partial diﬀerential equation admits the scaling symmetries
t −→ β a t,

x −→ β b x,

u −→ β c u,

β = 0,

(8.54)

where a, b, c are ﬁxed constants with a, b not both zero. As above, this means that if u(t, x)
is a solution to the diﬀerential equation, so is the rescaled function
U (t, x) = β c u(β − a t, β − b x)

(8.55)

for all values of β = 0. Checking that this indeed deﬁnes a symmetry is a simple matter of
applying the chain rule, which implies that the derivatives scale according to
ut −→ β c−a ut , ux −→ β c−b ux , utt −→ β c−2 a utt ,

uxt −→ β c−a−b uxt ,

(8.56)

and so on. Products of derivatives scale multiplicatively, e.g., x4 u uxt → β 2 c−a+3 b x4 u uxt .
In order that a (polynomial) diﬀerential equation admit such a scaling symmetry, each of
its terms must scale by the same overall power of β.
By deﬁnition, u(t, x) is called a similarity solution if it remains unchanged (invariant)
under the scaling symmetries (8.54), so that
u(t, x) = β c u(β − a t, β − b x)

(8.57)

for all β > 0. Let us, for speciﬁcity, assume that a = 0, leaving the case a = 0, b = 0,
for the reader to complete in Exercise 8.2.13. Since the left-hand side of (8.57) does not
depend on β, we can ﬁx its value to be‡ β = t1/a , and conclude that the similarity solution
must have the form
u(t, x) = tc/a v(ξ),

where

ξ = x t− b/a

and

v(ξ) = u(1, ξ),

(8.58)

are referred to as the similarity variables, since they remain invariant when subjected to
the scaling transformations (8.54). We then use the chain rule to ﬁnd the formulas for the
partial derivatives of u in terms of the ordinary derivatives of v with respect to ξ. Substituting these expressions into the scale-invariant partial diﬀerential equation for u(t, x), and
then canceling a common factor of t, will eﬀectively reduce it to an ordinary diﬀerential
equation for the function v(ξ). Each solution to the resulting ordinary diﬀerential equation then gives rise to a scale-invariant solution to the original partial diﬀerential equation
through the similarity ansatz (8.58).
Example 8.4. As a ﬁrst example, let us return to the nonlinear transport equation
ut + u ux = 0,
†

Or, more accurately, a one-parameter group, [ 87 ].

‡

This assumes t > 0; for t < 0, just replace t by − t.

(8.59)

8.2 Symmetry and Similarity

309

which we studied in Section 2.3. Under (8.54, 56), the equation rescales to
β c−a ut + β 2 c−b u ux = 0,
which is unchanged, provided c − a = 2 c − b, and hence c = b − a. Setting a = 1, c = b − 1,
we conclude that if u(t, x) is any solution, then so is the rescaled function
U (t, x) = β b−1 u(β −1 t, β − b x)
for any b and any β = 0.
To ﬁnd the associated similarity solutions, we use (8.58) to introduce the ansatz
u(t, x) = tb−1 v(ξ),

where

ξ = x t− b .

(8.60)

Diﬀerentiating, we obtain


ut = − b x t−2 v  (ξ) + (b − 1) tb−2 v(ξ) = tb−2 − b ξ v  (ξ) + (b − 1) v(ξ) ,

ux = t−1 v  (ξ).

Substituting these expressions into the transport equation (8.59) yields


0 = ut + u ux = tb−2 (v − b ξ) v  + (b − 1) v ,
and so
(v − b ξ)

dv
+ (b − 1) v = 0.
dξ

(8.61)

Any solution to this nonlinear ﬁrst-order ordinary diﬀerential equation will, when substituted into (8.60), produce a similarity solution to the nonlinear transport equation.
If b = 1, then either v = b ξ, producing the particular similarity solution u(t, x) = x/t
that we earlier used to construct the rarefaction wave (2.54), or v is constant, and so is u.
Otherwise, we can, in fact, linearize (8.61) by treating ξ as a function of v, whence
dξ
− b ξ = − v.
dv
The general solution to such a linear ﬁrst-order ordinary diﬀerential equation is found by
the standard method, [18, 23], resulting in
(b − 1) v

ξ = v + k v b/(b−1) ,
where k is the constant of integration. Recalling (8.60), we ﬁnd that the similarity solutions
u(t, x) are deﬁned by an implicit equation
x = k ub/(b−1) + t u.
For example, if b = 2, the (multi-valued) solution is a sideways-moving parabola:
√
− t ± t2 + 4 k x
2
x = k u + t u,
.
so that
u=
2k
Example 8.5. Consider the linear heat equation
ut = uxx .

(8.62)

Under the rescaling (8.54), the equation becomes β c−a ut = β c−2 b uxx , and thus (8.54)
represents a symmetry if and only if a = 2 b. Therefore, if u(t, x) is any solution, so is the
rescaled function
U (t, x) = β c u(β −2 t, β −1 x).

310

8 Linear and Nonlinear Evolution Equations

Of course, the initial scaling factor stems from the linearity of the equation.
The scale-invariant solutions are constructed through the similarity ansatz
√
u(t, x) = tc/2 v(ξ),
where
ξ = x/ t .
Diﬀerentiation yields


ut = − 12 x tc/2−3/2 v  (ξ) + 12 c tc/2−1 v(ξ) = tc/2−1 − 12 ξ v  (ξ) + 12 c v(ξ) ,
uxx = tc/2−1 v  (ξ).
Substituting these expressions into the heat equation and canceling a common power of t,
we ﬁnd that v must satisfy the linear ordinary diﬀerential equation
v  + 12 ξ v  − 12 c v = 0.

(8.63)

If c = 0, then (8.63) is eﬀectively a linear ﬁrst-order ordinary diﬀerential equation for v  (ξ),
which can be readily solved by the usual method, thereby producing the solution

v(ξ) = c1 + c2 erf 12 ξ ,
where c1 , c2 are arbitrary constants and erf is the error function (2.87). The corresponding
similarity solution to the heat equation is


x
√ .
u(t, x) = c1 + c2 erf
2 t
The error function solutions that we encountered in (8.17) can be built up as a linear
combination of translations of this similarity solution.
If c = 0, most solutions to the ordinary diﬀerential equation (8.63) are not elementary
functions.† One is in need of more sophisticated techniques, e.g., the method of power
series to be developed in Section 11.3, to understand its solutions, and hence the resulting
similarity solutions to the heat equation.

Exercises
8.2.1. If it takes a 2 cm long insulated bar 23 minutes to cool down to room temperature, how
long does it take a 4 cm bar?
8.2.2. If it takes a 5 centimeter long insulated iron bar 10 minutes to cool down so as not to
burn your hand, how long does it take a 20 centimeter bar made out of the same material
to cool down to the same temperature?
♦ 8.2.3. (a) Given γ > 0, use a scaling transformation to write down the formula for the fundamental solution for the general heat equation ut = γ uxx for x ∈ R. (b) Write down the
corresponding integral formula for the solution to the initial value problem.

†
According to [ 87; Example 3.3], the general solution can be written in terms of parabolic
cylinder functions, [ 86 ].

8.2 Symmetry and Similarity

311

8.2.4. Use scaling to construct the series solution for a heated circular ring of radius r and
thermal diﬀusivity γ. Does scaling also give the correct formulas for the Fourier coeﬃcients
in terms of the initial temperature distribution?
8.2.5. A solution u(t, x) to the heat equation is measured in degrees Fahrenheit. What is the
corresponding temperature in degrees Kelvin? Which symmetry transformation takes the
ﬁrst solution to the second solution, and how does it aﬀect the diﬀusion coeﬃcient?
8.2.6. Is time reversal, t → − t, a symmetry of the heat equation? Write down a physical explanation, and then a mathematical justiﬁcation.
8.2.7. According to Exercise 4.1.17, the partial diﬀerential equation ut + c ux = γ uxx models
diﬀusion in a convective ﬂow. Show how to use scaling to place the diﬀerential equation in
the form ut + ux = P −1 uxx , where P is called the Péclet number , and controls the rate of
mixing. Is there a scaling that will reduce the problem to the case P = 1?
8.2.8. Suppose you know a solution u (t, x) to the heat equation that satisﬁes u (1, x) = f (x).
Explain how to solve the initial value problem with u(0, x) = f (x).
8.2.9. Solve the following initial value problems for the heat equation ut = uxx for x ∈ R:
2

2

(a) u(0, x) = e− x /4 . Hint: Use Exercise 8.2.8.
(b) u(0, x) = e− 4 x .
2
(c) u(0, x) = x2 e− x /4 . Hint: Use Exercise 4.1.12.

8.2.10. Deﬁne the functions Hn (x) for n = 0, 1, 2, . . . , by the formula
2
dn − x 2
e
= (−1)n Hn (x) e− x .
n
dx

(8.64)

(a) Prove that Hn (x) is a polynomial of degree n, known as the nth Hermite polynomial .
(b) Calculate the ﬁrst four Hermite polynomials.
(c) Assuming γ = 1, ﬁnd the solution to the heat equation for − ∞ < x < ∞ and t > 0,
2
given the initial data u(0, x) = Hn (x) e− x . Hint: Combine Exercises 4.1.11, 8.2.8.
8.2.11. Find the scaling symmetries and corresponding similarity solutions of the following
partial diﬀerential equations:
(a) ut = x2 ux , (b) ut + u2 ux = 0, (c) utt = uxx .
8.2.12. Show that the wave equation utt = c2 uxx has the following invariance properties: if
u(t, x) is a solution, so is (a) any time translate: u(t − a, x), where a is ﬁxed; (b) any space
translate: u(t, x − b), where b is ﬁxed; (c) the dilated function u(β t, β x) for β = 0; (d) any
derivative: say ∂u/∂x or ∂ 2 u/∂t2 , provided u is suﬃciently smooth.
♦ 8.2.13. Suppose a = 0, b = 0 in the scaling transformation (8.57).
(a) Discuss how to reduce the partial diﬀerential equation to an ordinary diﬀerential
equation for the corresponding similarity solutions.
(b) Illustrate your method with the partial diﬀerential equation t ut = u uxx .
8.2.14. True or false: (a) A homogeneous polynomial solution to a partial diﬀerential equation
is always a similarity solution. (b) An inhomogeneous polynomial solution to a partial differential equation can never be a similarity solution.
8.2.15. (a) Find all scaling symmetries of the two-dimensional Laplace equation uxx + uyy = 0.
(b) Write down the ordinary diﬀerential equation for the similarity solutions. (c) Can you
ﬁnd an explicit formula for the similarity solutions? Hint: Look at Exercise 8.2.14(a).
♥ 8.2.16. Besides the translations and scalings, Lie symmetry methods, [ 87 ], produce two other
classes of symmetry transformations for the heat equation ut = uxx . Given that u(t, x) is a
solution to the heat equation:
2
(a) Prove that U (t, x) = ec t−c x u(t, x − 2 c t) is also a solution to the heat equation for any
c ∈ R. What solution do you obtain if u(t, x) = a is a constant solution? Remark : This
transformation can be interpreted as the eﬀect of a Galilean boost to a coordinate frame
that is moving with speed c.

312

8 Linear and Nonlinear Evolution Equations
2

e− c x /(4(1+c t))  t
x 
√
u
is a solution to the heat equa,
1 + ct 1 + ct
1 + ct
tion for any c ∈ R. What solution do you obtain if u(t, x) = a is a constant?

(b) Prove that U (t, x) =

8.3 The Maximum Principle
We have already noted the temporal decay of temperature, as governed by the heat equation, to thermal equilibrium. While the temperature at any individual point in a physical
medium can ﬂuctuate — depending on what is happening elsewhere, thermodynamics tells
us that the overall heat content of an isolated body must continually decrease. The Maximum Principle is the mathematical formulation of this physical law, and states that the
temperature of a body cannot, in the absence of external heat sources, ever become larger
than its initial or boundary values. This can be viewed as a dynamical counterpart to the
Maximum Principle for the Laplace equation, as formulated in Theorem 4.9, stating that
the maximum temperature of a body in equilibrium is achieved only on its boundary.
The proof of the Maximum Principle will be facilitated if we analyze the more general
situation in which heat energy is being continually extracted throughout the body.
Theorem 8.6. Let γ > 0. Suppose u(t, x) is a solution to the forced heat equation
∂u
∂2u
+ F (t, x)
=γ
∂t
∂x2

(8.65)

on the rectangular domain
R = { a < x < b, 0 < t < c }.
Assume that the forcing term is nowhere positive: F (t, x) ≤ 0 for all (t, x) ∈ R. Then the
maximum of u(t, x) on the closed rectangle R is attained at t = 0 or x = a or x = b.
In other words, if no new heat is being introduced, the maximum overall temperature
occurs either at the initial time or on the body’s boundary. In particular, in the fully
insulated case F (t, x) ≡ 0, (8.65) reduces to the heat equation, and Theorem 8.6 applies
as stated.
Proof : First let us ﬁrst prove the result under the stronger assumption F (t, x) < 0,
which implies that
∂u
∂2u
<γ
(8.66)
∂t
∂x2
everywhere in the rectangle R. Suppose ﬁrst that u(t, x) has a (local) maximum at a point
(t , x ) in the interior of R. Then, by multivariable calculus, [8, 108], its gradient must
vanish there, ∇u(t , x ) = 0, and hence
ut (t , x ) = ux (t , x ) = 0.

(8.67)

Our assumption implies that the scalar function h(x) = u(t , x) has a maximum at x = x .
Thus, by the second derivative test for functions of a single variable,
h (x ) = uxx (t , x ) ≤ 0.

(8.68)

8.3 The Maximum Principle

313

But the requirements (8.67–68) are clearly incompatible with the initial inequality (8.66).
We conclude that the solution u(t, x) cannot have a local maximum at any point in the
interior of R.
We still need to exclude the possibility of a maximum occurring at a non-corner point
(t , x ) = (c, x ), a < x < b, on the right-hand edge of the rectangle. If such were
to occur, then the function g(t) = u(t, x ) would be nondecreasing at t = c, and hence
g  (t) = ut (c, x ) ≥ 0 there. The preceding argument also implies that uxx (c, x ) ≤ 0, and
again these two requirements are incompatible with (8.66). We conclude that any (local)
maximum must occur on one of the other three sides of the rectangle, in accordance with
the statement of the theorem.
To generalize the argument to the case F (t, x) ≤ 0 — which includes the heat equation
— requires a little trick. Starting with the solution u(t, x) to (8.65), we set
v(t, x) = u(t, x) + ε x2 ,

where

ε > 0.

Then,
∂v
∂u
∂2u
∂2v
∂2v
=
=γ
+ F (t, x) = γ
− 2 γ ε + F (t, x) = γ
+ F (t, x),
2
2
∂t
∂t
∂x
∂x
∂x2
where, by our original assumption on F (t, x),
F (t, x) = F (t, x) − 2 γ ε < 0
everywhere in R. Thus, by the previous argument, a local maximum of v(t, x) can occur
only when t = 0 or x = a or x = b. Now we let ε → 0 and conclude the same for u. More
rigorously, let M denote the maximum value of u(t, x) on the indicated three sides of the
rectangle. Then
v(t, x) ≤ M + ε max{ a2 , b2 }
there, and hence, by the preceding argument,
u(t, x) ≤ v(t, x) ≤ M + ε max{ a2 , b2 }

for all

(t, x) ∈ R.

Now, letting ε → 0+ proves that u(t, x) ≤ M everywhere in R.

Q.E.D.

For the unforced heat equation, we can bound the solution from both above and below
by its boundary and initial temperatures:
Corollary 8.7. Suppose u(t, x) solves the heat equation ut = γ uxx , with γ > 0, for
a < x < b, 0 < t < c. Set
B = { (0, x) | a ≤ x ≤ b } ∪ { (t, a) | 0 ≤ t ≤ c } ∪ { (t, b) | 0 ≤ t ≤ c } ,
and let
M = max { u(t, x) | (t, x) ∈ B } ,

m = min { u(t, x) | (t, x) ∈ B } ,

(8.69)

be, respectively, the maximum and minimum values for the initial and boundary temperatures. Then m ≤ u(t, x) ≤ M for all a ≤ x ≤ b, 0 ≤ t ≤ c.
Proof : The upper bound u(t, x) ≤ M follows from the Maximum Principle of Theorem 8.6. To establish the lower bound, we note that u
(t, x) = − u(t, x) also solves the heat
equation, satisfying u
(t, x) ≤ − m on B, and hence, by the Maximum Principle, everywhere
in the rectangle. But this implies u(t, x) = − u
(t, x) ≥ m.
Q.E.D.

314

8 Linear and Nonlinear Evolution Equations

Remark : Theorem 8.6 is sometimes referred to as the Weak Maximum Principle for
the heat equation. The Strong Maximum Principle states that, provided the solution
 =
u(t, x) is not constant, its value at any non-initial, non-boundary point (t, x) ∈ R
{ a < x < b, 0 < t ≤ c } is strictly less than its maximum initial and boundary values; in
 where M is given in (8.69). Similarly, the
other words, u(t, x) < M for (t, x) ∈ R,
Strong Maximum Principle implies that, for nonconstant solutions to the heat equation,
 Proofs of
the inequalities in Corollary 8.7 are strict: m < u(t, x) < M for all (t, x) ∈ R.
the Strong Maximum Principle are more delicate, and can be found in [38, 61].
One immediate application of the Maximum Principle is to prove uniqueness of solutions to the heat equation.
Theorem 8.8. There is at most one solution to the Dirichlet initial-boundary value
problem for the forced heat equation.
Proof : Suppose u and u
 are any two solutions with the same initial and boundary
values. Then their diﬀerence v = u − u
 solves the homogeneous initial-boundary value
problem for the unforced heat equation, with minimum and maximum boundary values
m = 0 ≤ v(t, x) ≤ 0 = M for t = 0, a ≤ x ≤ b, and also x = a or b, 0 ≤ t ≤ c. But then
Corollary 8.7 implies that 0 ≤ v(t, x) ≤ 0 everywhere, which implies that u ≡ u
, thereby
establishing uniqueness.
Q.E.D.
Remark : Existence of the solution follows from the convergence of our Fourier series
— assuming that the initial and boundary data and the forcing function are suﬃciently
nice.

Exercises
8.3.1. True or false: Assuming no external heat source, if the initial and boundary temperatures of a one-dimensional body are always positive, the temperature within the body is
necessarily positive.
8.3.2. Suppose u(t, x) and v(t, x) are two solutions to the heat equation such that u ≤ v when
t = 0 and when x = a or x = b. Prove that u(t, x) ≤ v(t, x) for all a ≤ x ≤ b and all t ≥ 0.
Provide a physical interpretation of this result.
8.3.3. For t > 0, let u(t, x) be a solution to the unforced heat equation on an interval a < x < b,
subject to homogeneous Dirichlet boundary conditions. Prove that
M (t) = max{ u(t, x) | a ≤ x ≤ b } is a nonincreasing function of t.
8.3.4. (a) State and prove a Maximum Principle for the convection-diﬀusion equation
ut = uxx + ux . (b) Does the equation ut = uxx − ux also admit a Maximum Principle?
∂2u
∂u
∂u
=x 2 +
on the interval 1 < x < 2, with initial
∂t
∂x
∂x
and boundary conditions u(0, x) = f (x), u(t, 1) = α(t), u(t, 2) = β(t).
(a) State and prove a version of the Maximum Principle for this problem.
(b) Establish uniqueness of the solution to this initial-boundary value problem.

8.3.5. Consider the parabolic equation

8.3.6. (a) Show that u(t, x) = − x2 − 2 x t is a solution to the diﬀusion equation ut = x uxx .
(b) Explain why this diﬀerential equation does not admit a Maximum Principle.

8.4 Nonlinear Diﬀusion

315

8.3.7. Suppose that u(t, x) is a nonconstant solution to the heat equation on the interval
0 < x < when subject to either homogeneous (a) Dirichlet, (b) Neumann, or (c) mixed
boundary conditions. Prove that the function E(t) =

0

u(t, x)2 dx is everywhere

decreasing: E(t1 ) > E(t2 ) whenever t1 < t2 .
8.3.8. True or false: The wave equation utt = c2 uxx satisﬁes a Maximum Principle. If true,
clearly state the principle; if false, explain why not.

8.4 Nonlinear Diﬀusion
First-order partial diﬀerential equations serve to model conservative wave motion, beginning with the basic one-dimensional scalar transport equations that we studied in Chapter 2, and progressing on to higher-dimensional systems, the equations of gas dynamics,
the full-blown Euler equations of ﬂuid mechanics, and yet more complicated systems of
partial diﬀerential equations modeling plasmas, magneto-hydrodynamics, etc. However,
such systems fail to account for frictional and viscous eﬀects, which are typically modeled
by parabolic diﬀusion equations such as the heat equation and its generalizations, both linear and nonlinear. In this section, we investigate the consequences of combining nonlinear
wave motion with linear diﬀusion by analyzing the simplest such model. As we will see, the
dissipative term has the eﬀect of smoothing out abrupt shock discontinuities, and the result is a well-determined, smooth dynamical process with classical solutions. Moreover, in
the inviscid limit, the smooth solutions converge (nonuniformly) to a discontinuous shock
wave, leading to the method of viscosity solutions that has been successfully employed to
analyze such nonlinear dynamical processes.
Burgers’ Equation
The simplest nonlinear diﬀusion equation is known as† Burgers’ equation
ut + u ux = γ uxx ,

(8.70)

which is obtained by appending a simple linear diﬀusion term to the nonlinear transport
equation (2.31). As with the heat equation, the diﬀusion coeﬃcient γ ≥ 0 must be nonnegative in order that the initial value problem be well-posed in forwards time. In ﬂuid and
gas dynamics, one interprets the right-hand side as modeling the eﬀect of viscosity, and
so Burgers’ equation represents a very simpliﬁed version of the equations of viscous ﬂuid
ﬂows, including the celebrated and widely applied Navier–Stokes equations (1.4), [122].
When the viscosity coeﬃcient vanishes, γ = 0, Burgers’ equation reduces to the nonlinear
transport equation (2.31), which, as a consequence, is often referred to as the inviscid
Burgers’ equation.
†

The equation is named after the Dutch physicist Johannes Martinus Burgers, [ 26 ], and so
the apostrophe goes after the “s”. Burgers’ equation was apparently ﬁrst studied as a physical
model by the British (later American) applied mathematician Harry Bateman, [ 13 ], in the early
twentieth century.

316

8 Linear and Nonlinear Evolution Equations

Since Burgers’ equation is of ﬁrst order in t, we expect that its solutions will be
uniquely prescribed by their initial values
u(0, x) = f (x),

− ∞ < x < ∞.

(8.71)

(For simplicity, we will ignore boundary eﬀects here.) Small, slowly varying solutions —
more speciﬁcally, those for which both | u(t, x) | and | ux (t, x) | are small — tend to act like
solutions to the heat equation, smoothing out and decaying to 0 as time progresses. On the
other hand, when the solution is large or rapidly varying, the nonlinear term tends to play
the dominant role, and we might expect the solution to behave like nonlinear transport
waves, perhaps steepening into some sort of shock. But, as we will learn, the smoothing
eﬀect of the diﬀusion term, no matter how small, ultimately prevents the appearance of a
discontinuous shock wave. Indeed, it can be proved that, under rather mild assumptions
on the initial data, the solution to the initial value problem (8.70–71) remains smooth and
well deﬁned for all subsequent times, [122].
The simplest explicit solutions are the traveling waves, for which
u(t, x) = v(ξ) = v(x − c t),

where

ξ = x − c t,

(8.72)

indicates a ﬁxed proﬁle, moving to the right with constant speed c. By the chain rule,
∂u
= − c v  (ξ),
∂t

∂u
= v  (ξ),
∂x

∂2u
= v  (ξ).
∂x2

Substituting these expressions into Burgers’ equation (8.70), we conclude that v(ξ) must
satisfy the nonlinear second-order ordinary diﬀerential equation
− c v  + v v  = γ v  .
This equation can be solved by ﬁrst integrating both sides with respect to ξ, and so
γ v  = k − c v + 12 v 2 ,
where k is a constant of integration. Following the analysis after Proposition 2.3, as
ξ → ± ∞, the bounded solutions to such an autonomous ﬁrst-order ordinary diﬀerential
equation tend to one of the ﬁxed points provided by the roots of the quadratic polynomial
on the right-hand side. Therefore, for there to be a bounded traveling-wave solution v(ξ),
the quadratic polynomial must have two real roots, which requires k < 12 c2 . Assuming
this holds, we rewrite the equation in the form
2γ

dv
= (v − a)(v − b),
dξ

where

c = 12 (a + b),

k = 12 a b.

(8.73)

To obtain bounded solutions, we must require a < v < b. Integrating (8.73) by the usual
method, cf. (2.19), we ﬁnd



2γ
b−v
2 γ dv
=
log
= ξ − δ,
(v − a)(v − b)
b−a
v−a
where δ is another constant of integration. Solving for
v(ξ) =

a e(b−a)(ξ−δ)/(2 γ) + b
,
e(b−a)(ξ−δ)/(2 γ) + 1

8.4 Nonlinear Diﬀusion

317

γ = .25

γ = .025

Traveling-wave solutions to Burgers’ equation.
γ = .1

Figure 8.5.

and recalling (8.73), we conclude that the bounded traveling-wave solutions to Burgers’
equation all have the explicit form
u(t, x) =

a e(b−a)(x−c t−δ)/(2 γ) + b
,
e(b−a)(x−c t−δ)/(2 γ) + 1

(8.74)

where a < b and δ are arbitrary constants. Observe that our solution is a monotonically
decreasing function of x, with asymptotic values
lim

x → −∞

u(t, x) = b,

lim u(t, x) = a,

x→∞

at large distances. The wave travels to the right, unchanged in form, with speed c = 12 (a+b)
equal to the average of its asymptotic values. In particular, if a = − b, the result is a
stationary-wave solution. In Figure 8.5 we graph sample proﬁles, corresponding to a = .1,
b = 1, for three diﬀerent values of the diﬀusion coeﬃcient. Note that the smaller γ is, the
sharper the transition layer between the two asymptotic values of the solution.
In the inviscid limit as the diﬀusion becomes vanishingly small, γ → 0, the travelingwave solutions (8.74) converge to the step shock-wave solutions (2.51) of the nonlinear
transport equation. Indeed, this can be proved to hold in general: as γ → 0, solutions to
Burgers’ equation (8.70) converge to the corresponding solutions to the nonlinear transport
equation (2.31) that are subject to the Rankine–Hugoniot and entropy conditions (2.53, 55).
Thus, the method of vanishing viscosity allows one to monitor solutions to the nonlinear
transport equation as they evolve into regimes where multiple shocks interact and merge.
This approach also reconﬁrms our physical intuition, in that most physical systems retain
a very small dissipative component that serves to mollify abrupt discontinuities that might
appear in a theoretical model that fails to take friction or viscous eﬀects into account. In
the modern theory of partial diﬀerential equations, the resulting viscosity solution method
has been successfully used to characterize the discontinuous solutions to a broad range of
inviscid nonlinear wave equations as limits of classical solutions to a viscously regularized
system. We refer the interested reader to [64, 107, 122] for further details.
The Hopf–Cole Transformation
By a remarkable stroke of good fortune, the nonlinear Burgers’ equation can be converted into the linear heat equation and thereby explicitly solved. The transformation
that linearizes the nonlinear Burgers’ equation ﬁrst appeared in an obscure exercise in a
nineteenth-century diﬀerential equations textbook, [41; vol. 6, p. 102]. Its rediscovery by

318

8 Linear and Nonlinear Evolution Equations

the applied mathematicians Eberhard Hopf, [56], and Julian Cole, [32], was a milestone
in the modern era of nonlinear partial diﬀerential equations, and it is now named the
Hopf–Cole transformation in their honor.
In general, linearization — that is, converting a given nonlinear diﬀerential equation
into a linear equation — is extremely challenging, and, in most instances, impossible. On
the other hand, the reverse process — “nonlinearizing” a linear equation — is trivial:
any nonlinear change of dependent variables will do the trick! However, the resulting
nonlinear equation, while evidently linearizable by inverting the change of variables, is
rarely of independent interest. But sometimes there is a lucky accident, and the resulting
linearization of a physically relevant nonlinear diﬀerential equation can have a profound
impact on our understanding of more complicated nonlinear systems.
In the present context, our starting point is the linear heat equation
vt = γ vxx .

(8.75)

Among all possible nonlinear changes of dependent variable, one of the simplest that might
spring to mind is an exponential function. Let us, therefore, investigate the eﬀect of an
exponential change of variables
v(t, x) = eα ϕ(t,x) ,

so

ϕ(t, x) =

1
log v(t, x),
α

(8.76)

where α is a nonzero constant. The function ϕ(t, x) is real, provided v(t, x) is a positive
solution to the heat equation. Fortunately, this is not hard to arrange: if the initial
data v(0, x) > 0 is strictly positive, then, as a consequence of the Maximum Principle in
Corollary 8.7, the resulting solution v(t, x) > 0 is positive for all t > 0.
To determine the diﬀerential equation satisﬁed by the function ϕ, we invoke the chain
and product rules to diﬀerentiate (8.76):
vt = α ϕt eα ϕ ,

vx = α ϕx eα ϕ ,

vxx = (α ϕxx + α2 ϕ2x ) eα ϕ .

Substituting the ﬁrst and last formulas into the heat equation (8.75) and canceling a common exponential factor, we conclude that ϕ(t, x) satisﬁes the nonlinear partial diﬀerential
equation
ϕt = γ ϕxx + γ α ϕ2x ,
(8.77)
known as the potential Burgers’ equation, for reasons that will soon become apparent.
The second step in the process is to diﬀerentiate the potential Burgers’ equation with
respect to x; the result is
(8.78)
ϕtx = γ ϕxxx + 2 γ α ϕx ϕxx .
If we now set
∂ϕ
= u,
∂x

(8.79)

so that ϕ acquires the status of a potential function, then the resulting partial diﬀerential
equation
ut = γ uxx + 2 γ α u ux
coincides with Burgers’ equation (8.70) when α = − 1/(2 γ). In this manner, we have
arrived at the famous Hopf–Cole transformation.

8.4 Nonlinear Diﬀusion

319

Figure 8.6.

Theorem 8.9.
vt = γ vxx , then

Trignometric solution to Burgers’ equation.



If v(t, x) > 0 is any positive solution to the linear heat equation


vx
∂ 
− 2 γ log v(t, x) = − 2 γ
v
∂x
solves Burgers’ equation ut + u ux = γ uxx .
u(t, x) =

(8.80)

Do all solutions to Burgers’ equation arise in this way? In order to answer this question,
we run the argument in reverse. First, choose a potential function ϕ(t,
 x) that satisﬁes
(8.79); for example,

x

ϕ(t,
 x) =

u(t, y) dy.
0

If u(t, x) is any solution to Burgers’ equation, then ϕ(t,
 x) satisﬁes (8.78). Integrating both
sides of the latter equation with respect to x, we conclude that
xx + γ α ϕ
x2 + g(t),
ϕ
t = γ ϕ
for some integration “constant” g(t). Thus, unless g(t) ≡ 0, our potential function ϕ

doesn’t satisfy the potential Burgers’ equation (8.77), but that is because we chose the
“wrong” potential. Indeed, if we deﬁne
ϕ(t, x) = ϕ(t,
 x) − G(t),

where

G (t) = g(t),

then
t − g(t) = γ ϕ
xx + γ α ϕ
x2 = γ ϕxx + γ α ϕ2x ,
ϕt = ϕ
and hence the modiﬁed potential ϕ(t, x) is a solution to the potential Burgers’ equation
(8.77). From this it easily follows that
v(t, x) = e− ϕ(t,x)/(2 γ)

(8.81)

is a positive solution to the heat equation, from which the Burgers’ solution u(t, x) can
be recovered through (8.80). We conclude that every solution to Burgers’ equation comes
from a positive solution to the heat equation via the Hopf–Cole transformation.

320

8 Linear and Nonlinear Evolution Equations

Example 8.10. As a simple example, the separable solution
2

v(t, x) = a + b e− γ ω t cos ω x
to the heat equation leads to the following solution to Burgers’ equation:
u(t, x) =

2 γ b ω sin ω x
.
a eγ ω2 t + b cos ω x

(8.82)

A representative example is plotted in Figure 8.6. We should require that a > | b | in
order that v(t, x) > 0 be a positive solution to the heat equation for t ≥ 0; otherwise the
resulting solution to Burgers’ equation will have singularities at the roots of u — as in
the ﬁrst graph in Figure 8.6. This family of solutions is primarily aﬀected by the viscosity
term, and rapidly decays to zero.
To solve the initial value problem (8.70–71) for Burgers’ equation, we note that, under
the Hopf–Cole transformation (8.80),




 x
ϕ(0, x)
1
v(0, x) = exp −
= exp −
f (y) dy ≡ h(x).
(8.83)
2γ
2γ 0
Remark : The lower limit of the integral can be changed from 0 to any other convenient
value. The only eﬀect is to multiply v(t, x) by an overall constant, which does not change
the ﬁnal form of u(t, x) in (8.80).
According to formula (8.16) (adapted to general diﬀusivity, as in Exercise 8.2.3), the
solution to the initial value problem (8.75, 83) for the heat equation can be expressed as a
convolution integral with the fundamental solution
 ∞
2
1
v(t, x) = √
e− (x−ξ) /(4 γ t) h(ξ) dξ.
2 πγ t −∞
√
Therefore, setting 
v (t, x) = 2 πγ t v(t, x), the solution to the Burgers’ initial value problem
(8.70–71), valid for t > 0, is given by
 ∞
⎧
⎪
⎪ v(t, x) =
e− H(t,x;ξ) dξ,
⎪
⎨
−∞
2 γ ∂ v
(8.84)
u(t, x) = −
,
where
 ξ
⎪
v(t, x) ∂x
⎪
1
(x − ξ)2
⎪
⎩ H(t, x; ξ) =
+
f (η) dη.
4γ t
2γ 0
Example 8.11. To demonstrate the smoothing eﬀect of the diﬀusion terms, let us
see what happens to the initial data

a, x < 0,
u(0, x) =
(8.85)
b, x > 0,
in the form of a step function. We assume that a > b, which corresponds to a shock wave
in the inviscid limit γ = 0. (In Exercise 8.4.4, the reader is asked to analyze the case a < b,
which corresponds to a rarefaction wave.) In this case,
⎧
aξ
⎪
⎪
,
ξ < 0,
⎨
2
(x − ξ)
2γ
H(t, x; ξ) =
+
(8.86)
bξ
⎪
4γ t
⎪
⎩
,
ξ > 0.
2γ

8.4 Nonlinear Diﬀusion

321

t = .01

t = .5

t=1

t=2

Figure 8.7.

Shock-wave solution to Burgers’ equation.



After some algebraic manipulations, the solution (8.84) is found to have the explicit form


u(t, x) = a +
1 + exp

b−a

3

 ,
x − at
bt − x
b−a
√
√
(x − c t) erfc
erfc
2γ
2 γt
2 γt


(8.87)

with c = 12 (a + b), where erfc z = 1 − erf z denotes the complementary error function
(8.43). The solution, for a = 1, b = .1, and γ = .03, is plotted at various times in
Figure 8.7. Observe that, as with the heat equation, the jump discontinuity is immediately
smoothed out, and the solution soon assumes the form of a smoothly varying transition
between its two original heights. The larger the diﬀusion coeﬃcient in relation to the
jump magnitude, the more pronounced the smoothing eﬀect. Moreover, as γ → 0, the
solution u(t, x) converges to the shock-wave solution (2.51) to the transport equation, in
which the speed of the shock is c, the average of the step heights — in accordance with
the Rankine–Hugoniot shock rule. Indeed, in view of (2.88),
lim erfc z = 0,

z→∞

lim

z → −∞

erfc z = 2.

(8.88)

Thus, for t > 0, as γ → 0, the ratio of the two complementary error functions in (8.87)
tends to ∞ when x < b t, to 1 when b t < x < a t, and to 0 when x > a t. On the other
hand, since a > b, the exponential term tends to ∞ when x < c t, and to 0 when x > c t.
Put together, these imply that the solution u(t, x) → a when x < c t, while u(t, x) → b,
when x > c t, thus proving convergence to the shock-wave solution.
Example 8.12. Consider the case in which the initial data u(0, x) = δ(x) is a
concentrated delta function impulse at the origin. In the solution formula (8.84), starting
the integral for H(t, x; ξ) at 0 is problematic, but as noted earlier, we are free to select any

322

8 Linear and Nonlinear Evolution Equations

1

1

5

5

10

t=1

10

t=5

1

1

5

10

5

t = 10
Figure 8.8.

t = 50

Triangular-wave solution to Burgers’ equation.

10



other starting point, e.g., − ∞. Thus, we take
⎧
(x − ξ)2
⎪
⎪
,
⎨
(x − ξ)2
1
4γ t
H(t, x; ξ) =
+
δ(η) dη =
2
⎪
4γ t
2 γ −∞
⎪
⎩ 1 + (x − ξ) ,
2γ
4γ t
 ξ

ξ < 0,
ξ > 0.

We then evaluate





 
 ∞
√
x
x
− H(t,x;ξ)
−1/(2 γ)
√
√
e
dξ = π γ t 1 − erf
1 + erf
+e
.
v(t, x) =
2 γt
2 γt
−∞
Therefore, the solution to the initial value problem is
u(t, x) = −

2 γ ∂ v
=2
v(t, x) ∂x

where
coth z =



γ
πt

2


coth

e− x /(4 γ t)


,
1
x
√
− erf
4γ
2 γt

(8.89)

cosh z
e2 z + 1
ez + e− z
=
= z
sinh z
e − e− z
e2 z − 1

is the hyperbolic cotangent function. A graph of this solution when γ = .02 and a = 1
appears in Figure 8.8. As you can see, the initial concentration diﬀuses out, but, in contrast
to the heat equation, does not remain symmetric, since the nonlinear advection term causes
the wave to steepen in front. Eventually, as the eﬀect of the diﬀusion accumulates, the
propagating triangular wave becomes vanishingly small.

8.5 Dispersion and Solitons

323

Exercises
8.4.1. Find the solution to Burgers’ equation that has thefollowing initial data:
1, 0 < x < 1,
u(0, x) = (a) σ(x),
(b) σ(− x),
(c)
0, otherwise.
2

8.4.2. Starting with the heat equation solution v(t, x) = 1 + t− 1/2 e− x /(4 γ t) 6, ﬁnd the corresponding solution to Burgers’ equation and discuss its behavior.
8.4.3. Justify the solution formula (8.87).
2
√
♦ 8.4.4. (a) Prove that lim z ez erfc z = 1/ π . (b) Show that when a < b, the Burgers’

z→∞

solution (8.87) converges to the rarefaction wave (2.54) in the inviscid limit γ → 0+ .
8.4.5. True or false: If u(t, x) solves Burgers’ equation for the step function initial condition
u(0, x) = σ(x), then v(t, x) = ux (t, x) solves the initial value problem with v(0, x) = δ(x).
8.4.6. True or false: If v(t, x) is as given in (8.84), then
 ∞
∂ v
ξ − x − H(t,x;ξ)
dξ,
=
e
−∞ 2 γ t
∂x
and hence the solution to the Burgers’ initial value problem (8.70–71) can be written as
 ∞
x − ξ − H(t,x;ξ)
e
dξ
(x − ξ)2
1 ξ
−∞
t
,
where
H(t, x; ξ) =
f (η) dη.
+
u(t, x) =
 ∞
4γ t
2γ 0
e− H(t,x;ξ) dξ
−∞

8.4.7. Show that if u(t, x) solves Burgers’ equation, then U (t, x) = u(t, x − c t) + c is also a
solution. What is the physical interpretation of this symmetry?
8.4.8. (a) What is the eﬀect of a scaling transformation (t, x, u) −→ (α t, β x, λ u) on Burgers’
equation? (b) Use your result to solve the initial value problem for the rescaled Burgers’
equation Ut + ρ U Ux = σ Uxx , U (0, x) = F (x).
♥ 8.4.9. (a) Find all scaling symmetries of Burgers’ equation. (b) Determine the ordinary
diﬀerential equation satisﬁed by the similarity solutions. (c) True or false: The Hopf–Cole
transformation maps similarity solutions of the heat equation to similarity solutions of
Burgers’ equation.
8.4.10. What happens if you nonlinearize the heat equation (8.75) using the change of
√
variables (a) v = ϕ2 ; (b) v = ϕ ; (c) v = log ϕ ?
8.4.11. What partial diﬀerential equation results from applying the exponential change of
variables (8.76) to:
(a) the wave equation vtt = c2 vxx ?
(b) the Laplace equation vxx + vyy = 0?

8.5 Dispersion and Solitons
In this section, we ﬁnally venture beyond the by now familiar terrain of second-order
partial diﬀerential equations. While considerably less common than those of ﬁrst and
second order, higher-order equations arise in certain applications, particularly third-order

324

8 Linear and Nonlinear Evolution Equations

dispersive models for wave motion, [2, 122], and fourth-order systems modeling elastic
plates and shells, [7]. We will focus our attention on two basic third-order evolution
equations. The ﬁrst is a simple linear equation with a third derivative term. It arises as
a simpliﬁed model for unidirectional wave motion, and thus has more in common with
ﬁrst-order transport equations than with the second-order dissipative heat equation. The
third-order derivative induces a process of dispersion, in which waves of diﬀerent frequencies
propagate at diﬀerent speeds. Thus, unlike the ﬁrst- and second-order wave equations, in
which waves maintain their initial proﬁle as they move, dispersive waves will spread out and
decay even while conserving energy. Waves on the surface of a liquid are familiar examples
of dispersive waves — an initially concentrated disturbance, caused by, say, throwing a
rock in a pond, spreads out over the surface as its diﬀerent vibrational components move
oﬀ at diﬀerent speeds.
Our second example is a remarkable nonlinear third-order evolution equation known
as the Korteweg–de Vries equation, which combines dispersive eﬀects with nonlinear transport. As with Burgers’ equation (but for very diﬀerent mathematical reasons), the dispersive term thwarts the tendency for solutions to break into shock waves, and, in fact,
classical solutions exist for all time. Moreover, a general localized initial disturbance will
break up into a ﬁnite number of solitary waves; the taller the wave, the faster it moves.
Even more remarkable are the interactive properties of these solitary waves. One ordinarily expects nonlinearity to induce very complicated and not easily predictable behavior.
However, when two solitary-wave solutions to the Korteweg–de Vries equation collide, they
eventually emerge from the interaction unchanged, save for a phase shift. This unexpected
and remarkable phenomenon was ﬁrst detected through numerical simulations in the 1960s
and distinguished with the neologism soliton. It was then found that solitons appear in
a surprising number of basic nonlinear physical models. The investigation of their mathematical properties has had deep ramiﬁcations, not just within partial diﬀerential equations
and ﬂuid mechanics, but throughout applied mathematics and theoretical physics; it has
even contributed to the solution of long-outstanding problems in complex function theory.
Further development of the modern theory and amazing properties of integrable soliton
equations can be found in [2, 36].

Linear Dispersion
The simplest nontrivial third-order partial diﬀerential equation is the linear equation
ut + uxxx = 0,

(8.90)

which models the unidirectional† propagation of linear dispersive waves. To avoid complications engendered by boundary conditions, we shall initially look only at solutions on the
entire line, so − ∞ < x < ∞. Since the equation involves only a ﬁrst-order time derivative,
one expects its solutions to be uniquely speciﬁed by a single initial condition
u(0, x) = f (x),

− ∞ < x < ∞.

(8.91)

†
Bidirectional propagation, as we saw in the wave equation, requires a second-order time
derivative. As in the d’Alembert solution to the second-order wave equation, the reduction to a
unidirectional model is based on an (approximate) factorization of the bidirectional operator.

8.5 Dispersion and Solitons

325

t=0

t = .1

t=1

t=5

Figure 8.9.

t = .5

t = 30

Gaussian solution to the dispersive wave equation.

In wave mechanics, u(t, x) represents the height of the ﬂuid at time t and position x, and
the initial condition (8.91) speciﬁes the initial disturbance.
As with the heat equation (and, indeed, any linear constant-coeﬃcient evolution equation), the Fourier transform is an eﬀective tool for solving the initial value problem on the
real line. Assuming that the solution u(t, ·) ∈ L2 (R) remains square integrable at all times
t (a fact that can be justiﬁed a priori — see Exercise 8.5.18(b)), let
 ∞
1
u(t, x) e− i k x dx
u
(t, k) = √
2 π −∞
be its spatial Fourier transform. Owing to its eﬀect on derivatives, the Fourier transform
converts the partial diﬀerential equation (8.90) into a ﬁrst-order linear ordinary diﬀerential
equation:
∂u

∂u

+ ( i k)3 u
− i k3 u
=
 = 0,
(8.92)
∂t
∂t
in which the frequency variable k appears as a parameter. The corresponding initial conditions
 ∞
1
f (x) e− i k x dx
(8.93)
u
(0, k) = f(k) = √
2 π −∞
are provided by the Fourier transform of (8.91). The solution to the initial value problem
(8.92–93) is
3
u
(t, k) = f(k) e i k t .
Inverting the Fourier transform yields the explicit formula for the solution
 ∞
3
1
f(k) e i (k x+k t) dk
u(t, x) = √
2 π −∞
to the initial value problem (8.90–91) for the dispersive wave equation.

(8.94)

326

8 Linear and Nonlinear Evolution Equations

Example 8.13. Suppose that the initial proﬁle
u(0, x) = f (x) = e− x

2

is a Gaussian. According to our table of Fourier transforms (see page 272),
− k2 /4

e
f(k) = √

2

,

and hence the corresponding solution to the dispersive wave equation (8.90) is
 ∞
 ∞
3
2
2
1
1
e i (k x+k t)−k /4 dk = √
e− k /4 cos(k x + k 3 t) dk;
u(t, x) = √
2 π −∞
2 π −∞
the imaginary part vanishes thanks to the oddness of the integrand. (Indeed, the solution
must be real, since the initial data is real.) A plot of the solution at various times appears
in Figure 8.9. Note the propagation of initially rapid oscillations to the rear (negative x)
of the initial disturbance. The dispersion causes the oscillations to gradually spread out
and decrease in amplitude, with the eﬀect that u(t, x) → 0 uniformly as t → ∞, even
∞

though, according to Exercise 8.5.7, both the mass M =
u(t, x) dx and the energy
−∞
 ∞
E=
u(t, x)2 dx of the wave are conserved, i.e., are both constant in time.
−∞

Example 8.14. The fundamental solution to the dispersive wave equation is generated by a concentrated initial disturbance:
u(0, x) = δ(x).
√

The Fourier transform of the delta function is just δ(k)
= 1/ 2 π . Therefore, the corresponding solution (8.94) is
 ∞
 ∞
1
1
i (k x+k3 t)
u(t, x) =
e
dk =
cos(k x + k 3 t) dk,
(8.95)
2 π −∞
π 0
since the solution is real (or, equivalently, the imaginary part of the integrand is odd),
while the real part of the integrand is even.
A priori, it appears that the integral (8.95) does not converge, because the integrand
does not go to zero as | k | → ∞. However, the increasingly rapid oscillations induced by
the cubic term tend to cancel each other out and allow convergence. To prove this, given
l > 0, we perform a (non-obvious) integration by parts:
 l

 l

3

1
d
(8.96)
sin(k x + k 3 t) dk
2
0 x + 3 k t dk


 l
l
sin(k x + k 3 t)
d
1
−
=
sin(k x + k 3 t) dk
x + 3 k2 t
x + 3 k2 t
0 dk
k=0
 l
sin(l x + l3 t)
6 k t sin(k x + k3 t)
=
dk.
+
x + 3 l2 t
(x + 3 k 2 t)2
0

cos(k x + k t) dk =
0

Provided t = 0, as l → ∞, the ﬁrst term on the right goes to zero, while the ﬁnal integral
converges absolutely due to the rapid decay of the integrand.

8.5 Dispersion and Solitons

327

t = .03

t = .1

t = .33333

t=1

t=5

t = 20

Figure 8.10.

Fundamental solution to the dispersive wave equation.



While the integral in the solution formula (8.95) cannot be evaluated in terms of
elementary functions, it is related to the integral deﬁning the Airy function
 ∞
1
Ai(z) =
cos s z + 13 s3 ds,
(8.97)
π 0
an important special function, [86], that was ﬁrst employed by the nineteenth-century
British applied mathematician George Airy in his studies of optical caustics (the focusing
of light waves through a lens, e.g., a magnifying glass) and rainbows, [4]. Indeed, applying
the change of variables
√
x
3
s = k 3t ,
,
z= √
3
3t
to the Airy function integral (8.97), we deduce that the fundamental solution to the dispersive wave equation (8.90) can be written as


1
x
u(t, x) = √
Ai √
.
(8.98)
3
3
3t
3t
See Figure 8.10 for a graph of the solution at several times; in particular, at t = 1/3
the solution is exactly the Airy function. We see that the immediate eﬀect of the initial
delta impluse is to spawn a highly oscillatory wave trailing oﬀ to − ∞. (As with the heat
equation, signals propagate with inﬁnite speed.) As time progresses, the dispersive eﬀects
cause the oscillations to spread out, with their overall amplitude decaying in proportion to
t−1/3 . On the other hand, as t → 0+ , the solution becomes more and more oscillatory for
negative x, and so converges weakly to the initial delta function. We also note that (8.98)
has the form of a similarity solution, since it is invariant under the scaling symmetry
(t, x, u) −→ (λ−3 t, λ−1 x, λ u).
Equation (8.98) gives the response to an initial delta function concentrated at the

328

8 Linear and Nonlinear Evolution Equations

t = .1

t = .2

t = .3

t = .4

t = .5

t = .6

Figure 8.11.

Periodic dispersion at irrational (with respect to π) times.



origin. By translation invariance, we immediately deduce that
1
Ai
F (t, x; ξ) = √
3
3t



x−ξ
√
3
3t



is the fundamental solution corresponding to an initial delta impulse at x = ξ. Therefore,
we can use linear superposition to ﬁnd an explicit formula for the solution to the initial
value problem that bypasses the Fourier transform. Namely, writing the general initial
data as a superposition of delta functions,
 ∞
u(0, x) = f (x) =
−∞

f (ξ) δ(x − ξ) dξ,

we conclude that the resulting solution is the selfsame combination of fundamental solutions:


 ∞
1
x−ξ
u(t, x) = √
dξ.
(8.99)
f (ξ) Ai √
3
3
3 t −∞
3t
Example 8.15. Dispersive Quantization. Let us investigate the periodic initialboundary value problem for our basic linear dispersive equation on the interval − π ≤ x ≤ π:
ut + uxxx = 0, u(t, − π) = u(t, π), ux (t, − π) = ux (t, π), uxx (t, − π) = uxx (t, π),
(8.100)
with initial data u(0, x) = f (x). The Fourier series formula for the resulting solution is
straightforwardly constructed:
u(t, x) =

∞


3

ck e i (k x+k t) ,

(8.101)

k = −∞

where ck are the usual (complex) Fourier coeﬃcients (3.65) of the initial data f (x).

8.5 Dispersion and Solitons

329

1
t = 30
π

1
t = 15
π

1
t = 10
π

2
t = 15
π

t = 16 π

t = 15 π

Figure 8.12.

Periodic dispersion at rational (with respect to π) times.



Let us take the initial data to be the unit step function: u(0, x) = σ(x). In view of its
Fourier series (3.67), the resulting solution formula (8.101) becomes
∞


3

e i [ (2 l+1)x+(2 l+1) t ]
2l + 1
l = −∞


∞
1
2  sin (2 l + 1) x + (2 l + 1)3 t
=
+
.
2
π
2l + 1

1
i
u(t, x) =
−
2
π

(8.102)

l=0

Let us graph this solution. At times uniformly spaced by Δt = .1, the resulting solution
proﬁles are plotted in Figure 8.11. The solution appears to have a continuous but fractallike structure, reminiscent of Weierstrass’ continuous but nowhere diﬀerentiable function,
[55; pp. 401–421]. The temporal evolution continues in this fashion until the initial data
are formed again at t = 2 π, after which the process periodically repeats.
1
However, when the times are spaced by Δt = 30
π ≈ .10472, the resulting solution
proﬁles, as plotted in Figure 8.12, are strikingly diﬀerent! Indeed, as you are asked to
prove in Exercise 8.5.8, at each rational time t = 2 π p/q, where p, q are integers, the
solution (8.102) to the initial-boundary value problem is discontinuous but constant on
subintervals of length 2 π/q. This remarkable behavior, in which the solution proﬁles of
linearly dispersive periodic boundary value problems have markedly diﬀerent behaviors at
rational and irrational times (with respect to π), was ﬁrst observed, in the 1990’s, in optics
and quantum mechanics by the British physicist Michael Berry, [16, 115], and named the
Talbot eﬀect, after an optical experiment conducted by the inventor of the photographic
negative, William Henry Fox Talbot. While writing this book, I rediscovered the eﬀect,
which I like to call dispersive quantization, [88], and found that it arises in a wide range
of linearly dispersive periodic initial-boundary value problems, [30].

330

8 Linear and Nonlinear Evolution Equations

The Dispersion Relation
As noted earlier, a key feature of the third-order wave equation (8.90) is that waves disperse,
in the sense that those of diﬀerent frequencies move at diﬀerent speeds. Our goal now is
to better understand the dispersion process. To this end, consider a solution whose initial
proﬁle
u(0, x) = e i k x
is a complex oscillatory function. Since the initial data does not decay as | x | → ∞, we
cannot use the Fourier integral solution formula (8.94) directly. Instead, anticipating the
induced wave to exhibit temporal oscillations, let us try an exponential solution ansatz
u(t, x) = e i (k x−ω t)

(8.103)

representing a complex oscillatory wave of temporal frequency ω and wave number (spatial
frequency) k. Since
∂u
∂3u
= − i ω e i (k x−ω t) ,
= − i k 3 e i (k x−ω t) ,
∂t
∂x3
(8.103) satisﬁes the partial diﬀerential equation (8.90) if and only if its frequency and wave
number satisfy the dispersion relation
ω = − k3 .

(8.104)

Therefore, the exponential solution (8.103) of wave number k takes the form
3

u(t, x) = e i (k x+k t) .

(8.105)

Our Fourier transform formula (8.94) for the solution can thus be viewed as a (continuous) linear superposition of these elementary exponential solutions. In general, to ﬁnd the
dispersion relation for a linear constant-coeﬃcient partial diﬀerential equation, one substitutes the exponential ansatz (8.103). On cancellation of the common exponential factors,
the result is an equation expressing the frequency ω as a function of the wave number k.
Any exponential solution (8.103) is automatically in the form of a traveling wave, since
we can write
ω
u(t, x) = e i (k x−ω t) = e i k (x−cp t) ,
where
cp =
(8.106)
k
is the wave speed or, as it is more usually called, the phase velocity. If the dispersion
relation is linear in the wave number, ω = c k, as occurs in the linear transport equation
ut + c ux = 0, then all waves move at an identical speed cp = c, and hence localized
disturbances stay localized as they propagate through the medium. In the dispersive case,
ω is no longer a linear function of k, and so waves of diﬀerent spatial frequencies move at
diﬀerent speeds. In the particular case (8.90), those with wave number k move at speed
cp = ω/k = − k 2 , and so the higher the wave number, the faster the wave propagates to the
left. As the individual exponential constituents separate, the overall eﬀect is the dispersive
decay of an initially localized wave, with slowly diminishing amplitude and increasingly
rapid oscillation as x → − ∞.
The general solution to the linear partial diﬀerential equation under consideration is
then built up by linear superposition of the exponential solutions,
 ∞
u(t, x) =
e i (k x−ω t) g(k) dk,
(8.107)
−∞

8.5 Dispersion and Solitons

331

where ω = ω(k) is determined by the relevant dispersion relation. While the evolution of
the individual waves is an immediate consequence of the dispersion relation, the evolution
of the localized wave packet represented by (8.107) is less evident. To determine its speed
of propagation, let us switch to a moving coordinate frame of speed c by setting x = c t + ξ.
The solution formula (8.107) then becomes
 ∞
u(t, c t + ξ) =
e i (c k−ω)t e i k ξ g(k) dk.
(8.108)
−∞

For a ﬁxed value of ξ, the integral is of the general oscillatory form
 ∞
H(t) =
e i ϕ(k) t h(k) dk,

(8.109)

−∞

where, in our case, ϕ(k) = c k − ω(k) and h(k) = e i k ξ g(k). We are interested in understanding the behavior of such an oscillatory integral as t → ∞. Now, if ϕ(k) = k, then
(8.109) is just a Fourier integral, (7.9), and, as we learned in Chapter 7, H(t) → 0 as
t → ∞, for any reasonable function h(k). Intuitively, the increasingly rapid oscillations of
the exponential factor tend to cancel each other out in the high-frequency limit. A similar
result holds wherever ϕ(k) has no stationary points, i.e., ϕ (k) = 0, since one can then
perform a local change of variables 
k = ϕ(k) to convert that part of the oscillatory integral
to Fourier form, and again the increasingly rapid oscillations cause the limit to vanish. In
this fashion, we arrive at the key insight of Stokes and Kelvin that produced the powerful
Method of Stationary Phase. Namely, for large t  0, the primary contribution to the
highly oscillatory integral (8.109) occurs at the stationary points of the phase function,
that is, where ϕ (k) = 0. A rigorous justiﬁcation of the method, along with precise error
bounds, can be found in [85].
In the present context, the Method of Stationary Phase implies that the most signiﬁcant contribution to the integral (8.108) occurs when
0=

d
dω
(ω − c k) =
− c.
dk
dk

(8.110)

Thus, surprisingly, the principal contribution of the components at wave number k is felt
when moving at the group velocity
dω
.
(8.111)
cg =
dk
Interestingly, unless the dispersion relation is linear in the wave number, the group velocity
(8.111), which determines the speed of propagation of the energy, is not the same as the
phase velocity (8.106), which governs the speed of propagation of an individual oscillatory
wave. For example, in the case of the dispersive wave equation (8.90), ω = − k3 , and so
cg = −3 k 2 , which is three times as fast as the phase velocity, cp = ω/k = − k2 . Thus, the
energy propagates faster than the individual waves. This can be observed in Figure 8.9:
while the bulk of the disturbance is spreading out rather rapidly to the left, the individual
wave crests are moving slower.
On the other hand, the dispersion
relation associated with deep water waves is √
(ig√
noring physical constants) ω = k , [122]. Now, the phase velocity is cp = ω/k = 1/ k ,
√
whereas the group velocity is cg = dω/dk = 1/(2 k ) = 12 cp , and so the individual waves
move twice as fast as the speed of propagation of the underlying wave energy. For an experimental veriﬁcation, just throw a stone in a still pond. An individual wave crest emerges

332

8 Linear and Nonlinear Evolution Equations

in back and then steadily grows as it moves through the disturbance, eventually subsiding
and disappearing into the still water ahead of the expanding wave packet triggered by the
stone. The distinction between group velocity and phase velocity is also well understood
by surfers, who know that the largest waves seen out to sea are not the largest when they
break upon the shore.

Exercises
8.5.1. Sketch a picture of the solution for the initial value problem in Example 8.13 at times
t = −.1, −.5, and −1.
♠ 8.5.2. (a) Write down an integral
formula for the solution to the dispersive wave equation (8.90)

1, 0 < x < 1,
(b) Use a computer package to plot your
with initial data u(0, x) =
0, otherwise.
solution at several times and discuss what you observe.
8.5.3. (a) Write down an integral formula for the solution to the initial value problem
ut + ux + uxxx = 0,

u(0, x) = f (x).

(b) Based on the results in Example 8.13, discuss the behavior of the solution to the initial
2
value problem u(0, x) = e− x as t increases.
8.5.4. Find the (i ) dispersion relation, (ii ) phase velocity, and (iii ) group velocity for the
following partial diﬀerential equations. Which are dispersive? (a) ut + ux + uxxx = 0,
(b) ut = uxxxxx , (c) ut + ux − uxxt = 0, (d) utt = c2 uxx , (e) utt = uxx − uxxxx .
8.5.5. Find all linear evolution equations for which the group velocity equals the phase velocity.
Justify your answer.
8.5.6. Show that the phase velocity is greater than the group velocity if and only if the phase
velocity is a decreasing function of k for k > 0 and an increasing function of k for k < 0.
How would you observe this in a physical system?
♦ 8.5.7. (a) Conservation of Mass: Prove that T = u is a density associated with a conservation
law of the dispersive wave equation (8.90). What is the corresponding ﬂux? Under what
conditions is total mass conserved? (b) Conservation of Energy: Establish the same result
for the energy density T = u2 . (c) Is u3 the density of a conservation law?
♦ 8.5.8. Prove that when t = π p/q, where p, q are integers, the solution (8.102) is constant on
each interval π j/q < x < π(j + 1)/q for integers j ∈ Z. Hint: Use Exercise 6.1.29(d).
Remark : The proof that the solution is continuous and fractal at irrational times is considerably more diﬃcult, [ 90 ].
♦ 8.5.9. (a) Find the complex Fourier series representing the fundamental solution F (t, x; ξ) to
the periodic initial-boundary value problem (8.100). (b) Prove that at time t = 2 π p/q,
where p, q are relatively prime integers, F (t, x; ξ) is a linear combination of delta functions
based at the points ξ + 2 π j/q. Hint: Use Exercise 6.1.29(c). (c) Let u(t, x) be any solution
to (8.100). Prove that u(2 π p/q, x) is a linear combination of a ﬁnite number of translates,
f (x − xj ), of the initial data.

8.5 Dispersion and Solitons

333

The Korteweg–de Vries Equation
The simplest wave model that combines dispersion with nonlinearity is the celebrated
Korteweg–de Vries equation
(8.112)
ut + uxxx + u ux = 0.
It was ﬁrst derived, in 1872, by the French applied mathematician Joseph Boussinesq, [21;
eq. (30)], [22; eqs. (283, 291)], as a model for surface waves on shallow water. Two decades
later, it was rediscovered by the Dutch applied mathematician Diederik Korteweg and his
student Gustav de Vries, [65], and, despite Boussinesq’s priority, it is nowadays named
after them. In the early 1960s, the American mathematical physicists Martin Kruskal and
Norman Zabusky, [125], used the Korteweg–de Vries equation as a continuum model for
a one-dimensional chain of masses interconnected by nonlinear springs: the Fermi–Pasta–
Ulam problem, [40]. Numerical experimentation revealed its many remarkable properties,
which were soon rigorously established. Their work sparked the rapid development of one
of the most remarkable and far-reaching discoveries of the modern era: integrable nonlinear
partial diﬀerential equations, [2, 36].
The most important special solutions to the Korteweg–de Vries equation are the traveling waves. We seek solutions
u = v(ξ) = v(x − c t),

ξ = x − c t,

where

that have a ﬁxed proﬁle while moving with speed c. By the chain rule,
∂u
= − c v  (ξ),
∂t

∂3u
= v  (ξ).
∂x3

∂u
= v  (ξ),
∂x

Substituting these expressions into the Korteweg–de Vries equation (8.112), we conclude
that v(ξ) must satisfy the nonlinear third-order ordinary diﬀerential equation
v  + v v  − c v  = 0.

(8.113)

Let us further assume that the traveling wave is localized , meaning that the solution and
its derivatives are vanishingly small at large distances:
lim u(t, x) =

x → ±∞

∂u
∂2u
(t, x) = 0.
(t, x) = lim
x → ± ∞ ∂x
x → ± ∞ ∂x2
lim

(8.114)

This implies that we should impose the boundary conditions
lim v(ξ) =

ξ → ±∞

lim v  (ξ) =

ξ → ±∞

lim v  (ξ) = 0.

ξ → ±∞

(8.115)

The ordinary diﬀerential equation (8.113) can, in fact, be solved in closed form. First,
note that it has the form
d  1 2
v + 2 v − c v = 0,
dξ

and hence

v  + 12 v 2 − c v = a,

where a indicates the constant of integration. The localizing boundary conditions (8.115)
imply that a = 0. Multiplying the resulting equation by v  allows us to integrate a second
time:
d  1  2 1 3 1 2
0 = v  v  + 12 v 2 − c v =
(v ) + 6 v − 2 c v = 0.
dξ 2

334

8 Linear and Nonlinear Evolution Equations

Figure 8.13.

Thus,

Solitary wave/soliton.



 2
2
1
1 3
1
2 (v ) + 6 v − 2 c v = b,

where b is a second constant of integration, which, again by the boundary conditions
(8.115), is also zero. Setting b = 0, and solving for v  , we conclude that v(ξ) satisﬁes the
autonomous ﬁrst-order ordinary diﬀerential equation
/
dv
= v c − 13 v ,
dξ
which is integrated by the standard method:

dv
/
= ξ + δ,
v c − 13 v
where δ is constant. Consulting a table of integrals, e.g., [48], and then solving for v, we
conclude that the solution has the form
√
v(ξ) = 3 c sech2 12 c ξ + δ ,
(8.116)
where
sech y =

2
1
= y
cosh y
e + e−y

is the hyperbolic secant function. The solution has the form graphed in Figure 8.13. It is
a symmetric, monotone, exponentially decreasing function on either side of its maximum
height of 3 c. (Despite its suggestive proﬁle, it is not a Gaussian.) The resulting localized
traveling-wave solutions to the Korteweg–de Vries equation are thus
 √

u(t, x) = 3 c sech2 12 c (x − c t) + δ ,
(8.117)
where c > 0 represents the wave speed — which is necessarily positive, and so all such
solutions move to the right — while δ represents an overall phase shift.
√ The amplitude of
the wave is three times its speed, while its width is proportional to 1/ c . Thus, the taller
(and narrower) the wave, the faster it moves.
Localized traveling waves are commonly known as solitary waves. They were ﬁrst
observed in nature by the British engineer J. Scott Russell, [104], who recounts how one was
triggered by the sudden motion of a barge along an Edinburgh canal. Scott Russell ended
up chasing the propagating wave on horseback for several miles — a physical indication
of its stability. Russell’s observations were dismissed by his contemporary Airy, who,
relying on his linearly dispersive model for surface waves (8.90), claimed that such localized

8.5 Dispersion and Solitons

Figure 8.14.

335

Interaction of two solitons.



disturbances could not exist. Much later, Boussinesq derived the proper nonlinear surface
wave model (8.112), valid for long waves in shallow water, along with its solitary wave
solutions (8.117), thereby fully exonerating Russell’s physical observations and insight.
It took almost a century before all the remarkable properties of these solutions came
to light. The most striking is how two such solitary waves interact. While linear equations
always admit a superposition principle, one cannot naı̈vely combine two solutions to a
nonlinear equation. However, in the case of the Korteweg–de Vries equation, suppose the
initial data represent a taller solitary wave to the left of a shorter one. As time evolves,
the taller wave will move faster, and eventually catch up to the shorter one. They then
experience a complicated nonlinear interaction, as expected. But, remarkably, after a
while, they emerge from the interaction unscathed! The smaller wave is now in back and
the larger one in front, and both unchanged in speed, amplitude, and proﬁle. They then

336

8 Linear and Nonlinear Evolution Equations

proceed independently, with the smaller solitary wave lagging farther and farther behind
the faster, taller wave. The only eﬀect of their encounter is an overall phase shift, so that
the taller wave is a bit behind where it would be if it had not encountered the shorter
wave, while the shorter wave is a little ahead of its unhindered position. Figure 8.14 plots
a typical such interaction.
Owing to this “particle-like” behavior under interaction, these solutions were given
a special name: soliton. An explicit formula for a two-soliton solution to the Korteweg–
de Vries equation can be written in the following form:
u(t, x) = 12
where

⎛
⎜
Δ(t, x) = det ⎝

∂2
log Δ(t, x),
∂x2

1 + ε1 (t, x)
2 b2
ε (t, x)
b1 + b2 1

(8.118)

⎞
2 b1
ε2 (t, x)
b1 + b2
⎟
⎠,
1 + ε2 (t, x)

(8.119)

where 0 < b1 < b2 , and



εj (t, x) = exp bj (x − b2j t) + dj ,

j = 1, 2.

(8.120)

The constants cj = b2j represent the wave speeds, while the dj correspond to phase shifts of
the individual solitons. Proving that (8.118) is indeed a solution to the Korteweg–de Vries
equation is a straightforward, albeit tedious, exercise in diﬀerentiation. In Exercise 8.5.14,
the reader is asked to investigate its asymptotic behavior, as t → ± ∞, and prove that the
solution does, indeed, break up into two solitons, having the same proﬁles, speeds, and
amplitudes in both the distant past and future.
A similar dynamic occurs when there are multiple collisions among solitons. Faster
solitons catch up to slower ones moving to their right. After the various solitons ﬁnish
colliding and interacting, they emerge in order, from smallest to largest, each moving at
its characteristic speed and becoming more and more separated from its peers. An explicit
formula for the n–soliton solution is provided by the same logarithmic derivative (8.118) in
which Δ(t, x) now represents the determinant of an n × n matrix whose ith diagonal entry
2 bi
ε (t, x), using the same
is 1 + εi (t, x), while the oﬀ-diagonal (i, j) entry, i = j, is
bi + bj j
formula (8.120) for the εj ’s, and where 0 < b1 < · · · < bn correspond to the n diﬀerent
soliton wave speeds cj = b2j . Furthermore, it can be shown that, starting with an arbitrary
localized initial disturbance u(0, x) = f (x) that decays suﬃciently rapidly as | x | → ∞, the
resulting solution eventually emits a ﬁnite number of solitons of diﬀerent heights, moving
oﬀ at their respective speeds to the right, and so arranged in order from smallest to largest,
followed by a small, asymptotically self-similar dispersive tail that gradually disappears.
The source of these highly non-obvious facts and formulas lies beyond the scope of
this introductory text. Soon after the initial numerical studies, Gardner, Green, Kruskal,
and Miura, [45], discovered a profound connection between the solutions to the Korteweg–
de Vries equation and the eigenvalues λ of the Sturm–Liouville boundary value problem
−

d2 ψ
+ 6 u(t, x) ψ = λ ψ, − ∞ < x < ∞,
dx2

with

ψ(t, x) −→ 0

as | x | −→ ∞.

(8.121)
Their remarkable result is that whenever u(t, x) is a localized solution to the Korteweg–
de Vries equation (8.112), the eigenvalues of (8.121) are constant, meaning that they do not

8.5 Dispersion and Solitons

337

vary with the time t, while the continuous spectrum has a very simple temporal evolution.
In physical applications of the stationary Schrödinger equation (8.121), in which u(t, x)
represents a quantum-mechanical potential, the eigenvalues correspond to bound states,
while the continuous spectrum governs its scattering behavior. The solution to the socalled inverse scattering problem reconstructs the potential u(t, x) from its spectrum, and
can be viewed as a nonlinear version of the Fourier transform, in that it eﬀectively linearizes
the Korteweg–de Vries equation and thereby reveals its many remarkable properties. In
particular, the eigenvalues are responsible for the preceding determinantal formulae for the
multi-soliton solutions, while, when present, the continuous spectrum governs the dispersive
tail. See [2, 36] for additional details.

Exercises
8.5.10. Justify the statement that the width of a soliton is proportional to the inverse of the
square root of its speed.
8.5.11. Prove that the function (8.116) is a symmetric, monotone, exponentially decreasing
function on either side of its maximum height of 3 c.
8.5.12. Let u(t, x) solve the Korteweg–de Vries equation.
(a) Show that U (t, x) = u(t, x − c t) + c is also a solution.
(b) Give a physical interpretation of this symmetry.
8.5.13. (a) Find all scaling symmetries of the Korteweg–de Vries equation.
(b) Write down an ansatz for the similarity solutions, and then ﬁnd the corresponding
reduced ordinary diﬀerential equation. (Unfortunately, the similarity solutions cannot
be written in terms of elementary functions, [ 2 ].)
 (t, ξ) = u(t, ξ + c t)
♥ 8.5.14. (a) Let u(t, x) be the two-soliton solution deﬁned in (8.118). Let u
represent the solution as viewed⎧in a coordinate
frame
moving
with
speed
c. Prove that

2 1√
⎪
⎪
3
c
sech
c
ξ
+
δ
,
c
=
c
,
⎪
1
1
1
1
2
⎨

2 1√
 (t, ξ) =
lim u
3
c
sech
c
ξ
+
δ
,
c
=
c
,
⎪
t→∞

⎪
⎪
⎩

2

2

2

2

2

0,
otherwise,
for suitable constants δ1 , δ2 . Explain why this justiﬁes the statement that the solution
 (t, ξ) has a
indeed breaks up into two individual solitons as t → ∞. (b) Explain why u
similar limiting behavior as t → − ∞, but with possibly diﬀerent constants 
δ1 , δ2 .
(c) Use your formulas to discuss how the solitons are aﬀected by the collision.
8.5.15. Let α, β = 0. Find the soliton solutions to the rescaled Korteweg–de Vries equation
ut + α uxxx + β u ux = 0. How are their speed, amplitude, and width interrelated?
8.5.16. (a) Find the solitary wave solutions to the modiﬁed Korteweg–de Vries equation
ut + uxxx + u2 ux = 0. (b) Discuss how the amplitude and width of a solitary wave is
related to its speed. Note: The modiﬁed Korteweg–de Vries equation is also integrable, and
its solitary wave solutions are solitons, cf. [ 36 ].
8.5.17. Answer Exercise 8.5.16 for the Benjamin–Bona–Mahony equation ut − uxxt + u ux = 0,
[ 14 ]. Note: The BBM equation is not integrable, and collisions between its solitary waves
produce a small, but measurable, inelastic eﬀect, [ 1 ].
♦ 8.5.18. (a) Show that T1 = u is the density for a conservation law for the Korteweg–de Vries
equation. (b) Show that T2 = u2 is also a conserved density. (c) Find a conserved density
of the form T3 = u2x + μ u3 for a suitable constant μ. Remark : The Korteweg–de Vries

338

8 Linear and Nonlinear Evolution Equations
equation in fact has inﬁnitely many conservation laws, whose densities depend on higher
and higher-order derivatives of the solution, [ 76, 87 ]. It was this discovery that unlocked
the door to all its remarkable integrability properties, [ 2, 36 ].

8.5.19. Find two conservation laws of
(a) the modiﬁed Korteweg–de Vries equation ut + uxxx + u2 ux = 0;
(b) the Benjamin–Bona–Mahony equation ut − uxxt + u ux = 0.

Chapter 9

A General Framework for
Linear Partial Diﬀerential Equations

Before pressing on to the higher-dimensional manifestations of the heat, wave, and
Laplace/ Poisson equations, it is worth pausing to develop a general, abstract, linearalgebraic framework that underlies many of the linear partial diﬀerential equations arising
throughout the subject and its applications. The power of mathematical abstraction is
that concentrating on the essential features and not being distracted by the at times messy
particular details enables one to establish, relatively painlessly, very general results that can
be applied throughout the subject and beyond. Each abstract concept has, as its source,
an elementary ﬁnite-dimensional version valid for linear algebraic systems and matrices,
which is then generalized and extended to include linear boundary value problems and
then initial-boundary value problems governed by diﬀerential equations. All of the abstract
deﬁnitions and results contained here will be immediately applicable to the boundary and
initial value problems of physical interest, and serve to deepen our understanding of the
underlying commonalities among systems and solution techniques. Nevertheless, a more
applications-oriented reader may prefer to skip ahead to the more concrete developments
contained in the following chapters, referring to the background material presented here as
necessary.
Most equilibrium systems are modeled as boundary value problems involving a linear
diﬀerential operator that satisﬁes the two key conditions of being “self-adjoint” and either
“positive deﬁnite” or, slightly more generally, “positive semi-deﬁnite”. So, our ﬁrst task
is to introduce the adjoint of a linear function in general, and, for our speciﬁc purposes, a
linear diﬀerential operator. The adjoint is a far-reaching generalization of the elementary
matrix transpose. Its formulation relies on the speciﬁcation of inner products on both the
domain and target spaces of the operator, and, when one is dealing with linear diﬀerential
operators, the imposition of suitable homogeneous boundary conditions on the spaces of
allowable functions. In applications, the relevant inner products are typically dictated
by the underlying physics. One immediate application of the adjoint is the Fredholm
Alternative, which delineates the constraints required for the existence of solutions to
linear systems, including linear boundary value problems.
A linear operator that equals its own adjoint is called self-adjoint. The simplest example is the linear function deﬁned by a symmetric matrix. The most important subclasses
are the positive deﬁnite and positive semi-deﬁnite operators, which are the natural analogues of positive (semi-)deﬁnite matrices. We will learn how to construct self-adjoint
positive (semi-)deﬁnite operators in a canonical manner. Almost all of the linear diﬀerential operators studied in this text, including the Laplacian, are, when subject to suitable
P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_9, © Springer International Publishing Switzerland 2014

339

340

9 A General Framework for Linear Partial Diﬀerential Equations

boundary conditions, self-adjoint and either positive deﬁnite or positive semi-deﬁnite. The
key distinction is that positive deﬁnite linear systems and boundary value problems admit
unique solutions, whereas in the positive semi-deﬁnite case, the solution either does not
exist, since the Fredholm constraints are not satisﬁed, or, when it exists, is not unique. In
their dynamical manifestations, positive deﬁnite operators induce stable vibrational systems, whereas the positive semi-deﬁnite cases contain unstable modes that can lead to
disastrous physical consequences.
A critically important fact is that the solution to a positive deﬁnite linear system
can be characterized by a minimization principle, provided by a certain quadratic function
or, in the inﬁnite-dimensional function-space version, quadratic functional. In physical
contexts, the function(al) often represents the potential energy of the system, and the
solution minimizes said energy among all possible conﬁgurations satisfying the prescribed
boundary conditions, thereby quantifying the maxim that Nature is inherently conservative
and seeks to minimize energy. In mathematics, minimization principles underlie advanced
functional-analytic methods used to establish existence theorems, as well as the ﬁnite
element numerical schemes to be presented in Chapter 10.
For linear dynamical systems like the heat and wave equations, separation of variables
leads to an eigenvalue problem for the linear diﬀerential operator governing the corresponding equilibrium system. In the simple one-dimensional cases discussed in Chapter 4,
the eigenfunctions are trigonometric, producing the classical Fourier expansions for the
solutions. The eﬀectuality of the Fourier method relies on the eigenfunctions’ orthogonality, and we already hinted that this is no accident. Rather, it is a consequence of their
status as the eigenfunctions of a self-adjoint linear operator. Not only are such eigenfunctions automatically mutually orthogonal with respect to the underlying inner product, the
eigenvalues are necessarily real and, when the operator is positive deﬁnite, also positive.
Orthogonality underlies the Fourier-like expansion of quite general functions as series
in the eigenfunctions, whose convergence, in general, requires that the eigenfunctions form
a complete system. For positive deﬁnite boundary value problems on bounded domains,
we will establish completeness by combining the eigenfunction expansion for the associated Green’s function with a basic minimization principle for the eigenvalues based on the
Rayleigh quotient. On the other hand, problems on unbounded domains do not typically
admit complete systems of eigenfunctions and require the more advanced analytical concepts of continuous spectrum and generalized Fourier transforms that lie beyond the scope
of this text.
The chapter concludes by describing a general framework for dynamics that produces
time-dependent series solutions, in terms of the eigenfunctions of the underlying equilibrium
operator, for diﬀusion equations, vibration equations, and quantum-mechanical systems.
The ﬁnal two chapters will then specialize these general theories and constructions to
analyze initial-boundary value problems for the two- and three-dimensional heat, wave,
and Schrödinger equations in simple geometries. More advanced developments and further
applications can be found in higher-level texts, including [35, 38, 44, 61, 99].

9.1 Adjoints
Our starting point is a linear operator
L : U −→ V

(9.1)

9.1 Adjoints

341

that maps a vector space U to another vector space V . For most of the development,
we deal with real vector spaces, although the ﬁnal discussion of the Schrödinger equation
requires us to venture into the complex realm. For our purposes, L represents a linear
diﬀerential operator, and the elements of the domain space U and the target space V
are suitable scalar- or vector-valued functions. In elastomechanics, the elements of U are
displacements of a deformable body, while the elements of V are the associated strains. In
electromagnetism and gravitation, elements of U represent potentials, and elements of V
are electric or magnetic or gravitational ﬁelds. In thermodynamics, U contains temperature
distributions, while V contains temperature gradients. In ﬂuid mechanics, U is the space
of potential functions, while V is the space of ﬂuid velocities. And so on.
The abstract deﬁnition of the adjoint of a linear operator relies on an inner product
structure on both its domain and target spaces. We distinguish the inner products on U
and V (which may be diﬀerent even when U and V happen to be the same vector space)
by using a single angle bracket
u,u


to denote the inner product between u, u
 ∈ U,

and a double angle bracket
 v , 
v 

to denote the inner product between v, v ∈ V.

In applications, the appropriate inner products are often based on the underlying physics.
Deﬁnition 9.1. Let U, V be inner product spaces, and let L: U → V be a linear
operator. The adjoint of L is the unique linear operator L∗ : V → U that satisﬁes
 L[ u ] , v  =  u , L∗ [ v ] 

for all

u ∈ U,

v ∈ V.

(9.2)

Observe that the adjoint goes in the reverse direction, that is, from V back to U . To
master the deﬁnition, let us ﬁrst look at the ﬁnite-dimensional case.
Example 9.2. According to Theorem B.33, every linear function L: R n → R m is
given by matrix multiplication, so that L[ u ] = A u for u ∈ R n , where A is an m × n
matrix. The adjoint function L∗ : R m → R n is also linear, so it is also represented by
matrix multiplication, L∗ [ v ] = A∗ v for v ∈ R m , by an n × m matrix A∗ .
Suppose ﬁrst that we impose the ordinary Euclidean dot products
= u·u
 = uT u
,
u,u

 ∈ Rn,
u, u

  = v · v
 = vT v
,
 v , v

 ∈ Rm,
v, v

as our inner products on both R n and R m . Evaluation of both sides of the adjoint identity
(9.2) yields
 L[ u ] , v  =  A u , v  = (A u)T v = uT AT v,
(9.3)
 u , L∗[ v ]  =  u , A∗ v  = uT A∗ v.
Since these expressions must agree for all u, v, we conclude (see Exercise 9.1.6) that the
matrix A∗ representing L∗ is equal to the transposed matrix AT . Therefore, the adjoint
of a matrix with respect to the Euclidean dot product is its transpose: A∗ = AT . So one
can regard the adjoint as a vast generalization of the elementary operation of transposing
a matrix.
More generally, suppose we take weighted inner products on the domain and target
spaces:
  = uT M u
,
u,u

 ∈ Rn,
u, u

  = vT C v
,
 v , v

 ∈ Rm,
v, v

(9.4)

342

9 A General Framework for Linear Partial Diﬀerential Equations

where M and C are symmetric, positive deﬁnite matrices of respective sizes n × n and
m × m, cf. Proposition B.13. Then, repeating the previous calculation (9.3), we ﬁnd
 L[ u ] , v  =  A u , v  = (A u)T C v = uT AT C v,
 u , L∗ [ v ]  =  u , A∗ v  = uT M A∗ v.

(9.5)

Comparing these expressions, we conclude that the weighted adjoint matrix is
A∗ = M −1 AT C.

(9.6)

Therefore, the adjoint does indeed depend on which inner products are being used on both
the domain and target spaces.

Diﬀerential Operators
For applications to linear diﬀerential equations, our attention is focused on adjoints of
diﬀerential operators deﬁned on inﬁnite-dimensional function spaces. Let us begin with
the simplest example.
Example 9.3. Consider the derivative v = D[ u ] = du/dx, which deﬁnes a linear
operator D : U → V mapping a vector space U of diﬀerentiable functions u(x) to a vector
space containing their derivatives v(x) = u (x). We assume that the functions in question
are deﬁned on a ﬁxed bounded interval a ≤ x ≤ b.
In order to compute its adjoint, we need to impose inner products on both the domain
space U and the target space V . The simplest context is to adopt the standard L2 inner
product on both:
 b
 b
(9.7)
u,u
 =
u(x) u
(x) dx,
 v , v  =
v(x) 
v (x) dx.
a

a

According to the deﬁning equation (9.2), the adjoint operator D∗ : V → U must satisfy the
inner product identity
 D[ u ] , v  =  u , D∗ [ v ] 

u ∈ U,

for all

v ∈ V.

(9.8)

First, we compute the left-hand side:
44
 D[ u ] , v  =

du
,v
dx

55

 b
=

du
v dx.
a dx

On the other hand, the right-hand side should equal
 b
∗
 u , D [v]  =
u D∗ [ v ] dx.

(9.9)

(9.10)

a

Now, in the latter integral, we see u multiplying the result of applying the linear operator
D∗ to v. To identify this integrand with that in (9.9), we need to somehow remove the
derivative from u. The secret is integration by parts, which allows us to rewrite the ﬁrst
integral in the form
 b
 b


du
dv
u
v dx = u(b) v(b) − u(a) v(a) −
dx.
(9.11)
dx
a dx
a

9.1 Adjoints

343

Ignoring the two boundary terms for a moment, we observe that the remaining integral
has the form of an inner product

4
5
 b 
 b
dv
dv
dv
dx =
dx = u , −
=  u , − D[ v ] .
(9.12)
u
u −
−
dx
dx
dx
a
a
Equating (9.9) and (9.12), we deduce that
44
55 4
5
du
dv
 D[ u ] , v  =
,v
= u, −
=  u , − D[ v ] .
dx
dx
Thus, to satisfy the adjoint equation (9.8), we must have
 u , D∗ [ v ]  =  u , − D[ v ] 

for all

u ∈ U,

v ∈ V,

and so the adjoint of the derivative operator is its negative:
D∗ = − D.

(9.13)

However, the preceding argument is valid only if the boundary terms in the integration
by parts formula (9.11) vanish:
u(b) v(b) − u(a) v(a) = 0,

(9.14)

which necessitates imposing suitable boundary conditions on the functions u and v. For
example, imposing Dirichlet boundary conditions
u(a) = 0,

u(b) = 0,

(9.15)

will ensure that (9.14) holds, and therefore validates (9.13). In this case, the domain space
of D: U → V is the vector space
U = { u(x) | u(a) = u(b) = 0 } ,
while no boundary conditions need be imposed on the functions v(x) in the target space
V . An evident alternative is to require that v(a) = v(b) = 0. In this case, the target space
V = { v(x) | v(a) = v(b) = 0 }
consists of all functions that vanish at the endpoints. Since the derivative D: U → V is
required to map a function u(x) ∈ U to an allowable function v(x) ∈ V , the domain space
now consists of functions satisfying the Neumann boundary conditions:
U = { u(x) | u (a) = u (b) = 0 } .
These are evidently not the only two possibilities. Let us list the most important combinations of boundary conditions that imply the vanishing of the boundary terms (9.14), and
so ensure the validity of the adjoint equation (9.13):
(a) Dirichlet boundary conditions:

u(a) = u(b) = 0.

(b) Mixed boundary conditions:
(c) Neumann boundary conditions:

u(a) = u (b) = 0, or u (a) = u(b) = 0.
u (a) = u (b) = 0.

(d) Periodic boundary conditions:

u(a) = u(b)

and

u (a) = u (b).

In all cases, the boundary conditions impose restrictions on the domain space U and, in
cases (b–d) when we are identifying v(x) = u (x), the target space V also.

344

9 A General Framework for Linear Partial Diﬀerential Equations

Remark : In the preceding discussion, we were purposely vague about the required
diﬀerentiability of the functions. In ﬁnite dimensions, every linear function L: R n → R m
is given by matrix multiplication L[ u ] = A u, and hence is deﬁned on all of the underlying
vector space R n . Linear operators on inﬁnite-dimensional function spaces are typically not
deﬁned on all possible functions. For example, the derivative operator L = D: U → V
requires the function u ∈ U to be diﬀerentiable. However, the target function v = D[ u ] =
u is not necessarily as smooth, and so may belong to a diﬀerent function space; for instance
if u ∈ C1 [ a, b ], then v = u ∈ C0 [ a, b ]. On the other hand, the adjoint D∗ = − D is deﬁned
only on diﬀerentiable functions v, so if v ∈ C1 [ a, b ], then u = − v  ∈ C0 [ a, b ]. Keeping a
detailed account of the various smoothness requirements quickly becomes distracting.
To circumvent this technical annoyance, we will always deal with a ﬁxed class of functions, e.g., continuous functions or, more generally, L2 functions, that are constrained only
by the imposed boundary conditions. When we write L: U → V , we allow the possibility that the linear operator L may be deﬁned only on a “dense” subspace of the domain
space U . For instance, we will write D: U → V with U = V = C0 [ a, b ], even though
D[ u ] = u ∈ V only if u belongs to the dense subspace C1 [ a, b ] ⊂ U = C0 [ a, b ]. Similarly,
D∗ : V → U is also deﬁned only on the dense subspace C1 [ a, b ] ⊂ V = C0 [ a, b ]. The term
dense refers to the fact that any continuous function in the full space U = C0 [ a, b ] can
be arbitrarily closely approximated in norm by a continuously diﬀerentiable function in
the subspace C1 [ a, b ]. Or, to put it another way, given a continuous function u ∈ C0 [ a, b ],
there exists a sequence of continuously diﬀerentiable functions u1 , u2 , u3 , . . . ∈ C1 [ a, b ] such
that  uk − u  → 0 as k → ∞. A similar density result can be proved for U = L2 [ a, b ]; see
[37, 96, 98] for details.
Warning: In more advanced treatments, our notion of adjoint is usually called the
formal adjoint. A true adjoint requires more subtle technical hypotheses on the operator
and its domain, cf. [95].
Example 9.4. Let us recompute the adjoint of the derivative operator D: U → V ,
this time with respect to the weighted L2 inner products
 b

 b
u(x) u
(x) ρ(x) dx,

u,u
 =

 v , v  =

a

v(x) v(x) κ(x) dx,

(9.16)

a

where ρ(x) > 0 and κ(x) > 0 are strictly positive functions that, physically, might represent
the density and stiﬀness of a nonuniform bar. Now we need to compare
 b
 D[ u ] , v  =
a

du
v(x) κ(x) dx,
dx

with

 u , D∗ [ v ]  =

 b

u(x) D∗ [ v ] ρ(x) dx.

a

Integrating the ﬁrst expression by parts, we obtain
 b
a

 b


du
d(κ v)
v κ dx = u(b) v(b) κ(b) − u(a) v(a) κ(a) − u
dx
dx
dx
a

 b 
1 d(κ v)
ρ dx,
u −
=
ρ dx
a

(9.17)

provided that we select our boundary conditions so that
u(b) v(b) κ(b) − u(a) v(a) κ(a) = 0.

(9.18)

9.1 Adjoints

345

As you can check, this follows from any of the listed boundary conditions: Dirichlet,
Neumann, or mixed, as well as periodic, provided κ(a) = κ(b). We conclude that, in such
situations, the weighted adjoint of the derivative operator D is the diﬀerential operator
D∗ [ v(x) ] = −


d 
κ(x) dv
κ (x)
1
κ(x) v(x) = −
−
v(x).
ρ(x) dx
ρ(x) dx
ρ(x)

(9.19)

As with matrices, the adjoint of a diﬀerential operator depends crucially on the speciﬁcation
of inner products.
The following basic results are left as exercises for the reader. The ﬁrst generalizes
the fact that transposing a transposed matrix reverts to the original.
Proposition 9.5. The adjoint of the adjoint is the original operator: L = (L∗ )∗ .
The second generalizes the fact that the transpose of the product of two matrices is
the product of the transposes, but in the reverse order.
Proposition 9.6. If L: U → V and M : V → W are linear operators on inner product
spaces, with L∗ : V → U and M ∗ : W → V their respective adjoints, then the composite
linear operator M ◦ L: U → W has adjoint (M ◦ L)∗ = L∗ ◦ M ∗ : W → U .
Example 9.7. Let us compute the adjoint of the second derivative operator D2 =
D ◦ D with respect to the standard L2 inner products on both the domain and target spaces.
According to Proposition 9.6 and equation (9.13), at least on a formal level,
(D2 )∗ = D∗ ◦ D∗ = (− D) ◦ (− D) = D2 ,

(9.20)

and hence D2 equals its own adjoint. However, the validity of (9.13) required that the
functions in the domain and target spaces of both D’s satisfy appropriate boundary conditions. For example, the domain of the ﬁrst D: U → V could be U = { u(x) | u(a) =
u(b) = 0 }, while its target space V is unconstrained; the second D could then map V to
W = { w(x) | w(a) = w(b) = 0 }, which will thus also require that u (a) = u (b) = 0 in
order that D2 = D ◦ D map U to W . Another option would be to impose Neumann conditions on the ﬁrst D, with U = { u (a) = u (b) = 0 } and thus V = { v(a) = v(b) = 0 }, while
W remains unconstrained. Under either these or other suitably compatible constraints,
both adjoint identiﬁcations D∗ = − D are valid, thus justifying (9.20). Keep in mind that,
according to our earlier remark, the diﬀerentiation operators are, in fact, deﬁned only on
the dense subspaces containing suﬃciently smooth functions.

Higher–Dimensional Operators
The most natural multi-dimensional analogue of the derivative is the gradient operator,
which, on a two-dimensional space, is given by


∂u/∂x
∇u = grad u =
.
∂u/∂y
The gradient ∇ deﬁnes a linear operator that takes a scalar-valued function u(x, y) to
the vector-valued function consisting of its two ﬁrst-order partial derivatives. Thus, the
domain space U consists of scalar-valued functions u(x, y), or scalar ﬁelds, deﬁned for
(x, y) ∈ Ω, where the domain Ω ⊂ R 2 is assumed to be both bounded and connected,

346

9 A General Framework for Linear Partial Diﬀerential Equations

and with a nice boundary ∂Ω. (Similar considerations apply to three- and even higherdimensional problems.) The target space V consists of vector-valued functions, or vector
ﬁelds, v(x, y) = ( v1 (x, y), v2 (x, y) )T deﬁned on Ω. As in the preceding subsection, the gradient operator ∇: U → V is well deﬁned only on the dense subspace C1 (Ω) ⊂ U consisting
of continuously diﬀerentiable scalar ﬁelds.
In accordance with the general Deﬁnition 9.1, the adjoint of the gradient must go in
the reverse direction,
∇∗ : V −→ U,
mapping a vector ﬁeld v(x, y) to a scalar ﬁeld w(x, y) = ∇∗ v. The deﬁning equation (9.2)
for the adjoint, namely
 ∇u , v  =  u , ∇∗v ,
(9.21)
relies on the choice of inner products on the two vector spaces. Let us start with the L2
inner product between scalar ﬁelds:

u(x, y) u
(x, y) dx dy.
(9.22)
u,u
 =
Ω

Similarly, the L2 inner product between vector ﬁelds deﬁned on Ω is obtained by integrating
their usual dot product:




  =
 (x, y) dx dy =
 v , v
v(x, y) · v
v1 (x, y) + v2 (x, y) 
v2 (x, y) dx dy.
v1 (x, y) 
Ω

Ω

(9.23)
The adjoint identity (9.21) is supposed to hold for all appropriate scalar ﬁelds u and vector
ﬁelds v. For the L2 inner products (9.22, 23), the two sides of the identity read

 

∂u
∂u
v1 +
v2 dx dy,
∇u · v dx dy =
 ∇u , v  =
∂x
∂y
Ω
 Ω
 u , ∇∗ v  =
u ∇∗ v dx dy.
Ω

Thus, to compare these two double integrals, we must somehow remove the derivatives
from the scalar ﬁeld u. As in the one-dimensional computation (9.8), the mechanism is an
integration by parts formula for double integrals:
&


∇u · v dx dy =
u (v · n) ds −
u (∇ · v) dx dy,
(9.24)
∂Ω

Ω

Ω

which was already noted in (6.83). The left-hand side is just  ∇u , v . If the boundary
line integral vanishes,
&
u (v · n) ds = 0,

(9.25)

∂Ω

then the right-hand side of formula (9.24) reduces to

−
u (∇ · v) dx dy = −  u , ∇ · v  =  u , − ∇ · v .
Ω

Therefore, subject to the boundary constraint (9.25), we deduce the L2 inner product
identity
 ∇u , v  =  u , − ∇ · v ,
(9.26)

9.1 Adjoints

347

which implies that the L2 adjoint of the gradient operator is minus the divergence operator :
∇∗ v = − ∇ · v.

(9.27)

The vanishing of the boundary integral (9.25) will be ensured by the imposition of
suitable homogeneous boundary conditions on the scalar ﬁeld u and/or the vector ﬁeld v.
Clearly the line integral will vanish if either u = 0 or v · n = 0 at each point on the boundary. These possibilities lead immediately to the three principal types of (homogeneous)
boundary conditions. The ﬁrst are the Dirichlet boundary conditions, which require
u=0

on

∂Ω.

(9.28)

v·n=0

on

∂Ω,

(9.29)

Alternatively, we can set
which requires that v be everywhere tangent to the boundary. Since ∇ must map the
scalar ﬁeld u ∈ U to an admissible vector ﬁeld v = ∇u ∈ V , the boundary condition (9.29)
requires that u satisfy the homogeneous Neumann boundary conditions
∂u
= ∇u · n = 0
∂n

on

∂Ω.

(9.30)

One can evidently also mix the boundary conditions, imposing Dirichlet conditions on part
of the boundary and Neumann conditions on the complementary part:
u=0

on D ⊂ ∂Ω,

v·n=

∂u
=0
∂n

on N = ∂Ω \ D,

(9.31)

with neither D nor N empty.
More generally, when modeling deﬂections of nonuniform membranes, heat ﬂow through
heterogeneous media, and similar physical equilibria, we replace the L2 inner product between scalar and vector ﬁelds (9.23) by suitably weighted versions†

u,u
 =
u(x, y) u
(x, y) ρ(x, y) dx dy,
Ω

(9.32)


  =
 v , v
v1 (x, y) κ1 (x, y) + v2 (x, y) 
v2 (x, y) κ2 (x, y) dx dy,
v1 (x, y) 
Ω

in which ρ(x, y), κ1 (x, y), κ2 (x, y) > 0 are strictly positive functions for (x, y) ∈ Ω. In applications, ρ represents a density, while κ1 , κ2 represent stiﬀnesses or thermal conductivities.
To compute the weighted adjoint of the gradient operator, we apply a similar integration
by parts argument based on (6.83):

 
∂u
∂u
 ∇u , v  =
+ κ2 v2
dx dy
(9.33)
κ1 v1
∂x
∂y
Ω


&

∂(κ1 v1 ) ∂(κ2 v2 )
=
+
dx dy
u − κ2 v2 dx + κ1 v1 dy −
u
∂x
∂y
∂Ω

Ω


1 ∂(κ1 v1 ) ∂(κ2 v2 )
=
+
ρ dx dy,
u −
ρ
∂x
∂y
Ω
†

Exercise 9.2.14 treats an even more general pair of inner products.

348

9 A General Framework for Linear Partial Diﬀerential Equations

provided the boundary integral vanishes. Equating the left-hand side to

∗
u (∇∗ v) ρ dx dy,
u,∇ v =
Ω

we deduce that the adjoint of the gradient operator with respect to the weighted inner
products (9.32) is minus the “weighted divergence operator”:
1
∇∗ v = −
ρ



∂(κ1 v1 ) ∂(κ2 v2 )
+
∂x
∂y


=−

κ1 ∂v1 κ2 ∂v2 1 ∂κ1
1 ∂κ2
−
−
v1 −
v . (9.34)
ρ ∂x
ρ ∂y
ρ ∂x
ρ ∂y 2

The vanishing of the boundary integral,


&

&

 · n ds,
uv

u − κ2 v2 dx + κ1 v1 dy =

0=
∂Ω

∂Ω

where

=
v


κ1 v1
,
κ2 v2

 ·n = 0 on ∂Ω. The former is the usual homogeneous Dirichlet
is ensured if either u = 0 or v
condition, but the latter is a “weighted” version of the homogeneous Neumann boundary
 · n = 0 on the boundary, where ∇u
 = κ u , κ u T reprecondition, requiring that ∇u
1 x
2 y
sents a “weighted normal ﬂux vector”.
Example 9.8. Let us compute the adjoint of the second-order Laplacian operator
Δ = ∂ 2 /∂x2 + ∂ 2 /∂y 2 with respect to the L2 inner products on both its domain and target
spaces. The computation is a simple consequence of the double integral identity (6.88),
which we rewrite as



& 
∂u
∂v
 Δu , v  =
−v
ds +
v Δu dx dy =
u Δv dx dy =  u , Δv .
u
∂n
∂n
Ω
∂Ω
Ω
Thus, provided the boundary integral vanishes, we can conclude that the Laplacian equals
its own adjoint: Δ∗ = Δ. This is assured when u ∂v/∂n = v ∂u/∂n at each point in ∂Ω.
For example, the adjoint computation is valid if either u = v = 0 or ∂u/∂n = ∂v/∂n = 0
at every point of the boundary of the domain. Keep in mind that if we require v = 0 on
some or all of ∂Ω, then this imposes the condition Δu = 0 there in order that Δ map u to
an admissible v; similar considerations apply when ∂v/∂n = 0.

Exercises
9.1.1. Choose
one from the following list of inner products on R 2 . Then ﬁnd the adjoint of

1 2
A=
when your inner product is used on both its domain and target space.
−1 3
(a) The Euclidean dot product; (b) the weighted inner product  v , w  = 2 v1 w1 +3 v2 w2 ;
(c) the inner
product
 v , w  = vT C w deﬁned by the symmetric positive deﬁnite matrix


2 −1
C=
.
−1
4
9.1.2. From the list in Exercise 9.1.1, choose a diﬀerent inner product on the domain and the
target space, and then determine the adjoint of the matrix A.

9.1 Adjoints

349

3
9.1.3. Choose one from the following list of⎛inner products
⎞ on R for both the domain and tar1
1 0
0 1⎟
get space, and ﬁnd the adjoint of A = ⎜
⎝ −1
⎠. (a) The Euclidean dot product on
0 −1 2
R 3 ; (b) the weighted inner product  v , w  = v1 w1 + 2 v2 w2 + 3 v3 w3 ; (c) the inner prod⎛
⎞
2 1 0
⎟
uct  v , w  = vT C w deﬁned by the symmetric positive deﬁnite matrix C = ⎜
⎝ 1 2 1 ⎠.
0 1 2

9.1.4. From the list in Exercise 9.1.3, choose diﬀerent inner products on the domain and target
space, and then compute the adjoint of the matrix A.
on
9.1.5. Choose an inner product on R 2 from the list in Exercise 9.1.1 and an ⎛inner product
⎞
1 3
⎟
R 3 from the list in Exercise 9.1.3, and then compute the adjoint of A = ⎜
⎝ 0 2 ⎠.
−1 1
♦ 9.1.6. (a) Let C be an m × n matrix. Suppose uT C v = 0 for all u ∈ R m and v ∈ R n . Prove
that C = O must be the zero matrix. (b) Let A, B be m × n matrices such that
uT A v = uT B v for all u ∈ R m and v ∈ R n . Prove that A = B. (c) Find an n × n matrix
C = O such that uT C u = 0 for all u ∈ R n .
9.1.7. Let U = C0 [ 0, 1 ]. Find the adjoint I ∗ of the identity operator I : U → U under the
weighted inner products (9.16).
9.1.8. Compute the adjoint of the derivative operator v = D[ u ] = u under the weighted inner
1

 =
 (x) dx,  v , v
  =
products  u , u
ex u(x) u
0
boundary conditions that you are imposing.

1
0

(1 + x) v(x) v(x) dx. Clearly state any

9.1.9. Let L[ u ] = x u (x) + u(x) and 0 < a < x < b. When subject to homogeneous Dirichlet
boundary conditions u(a) = u(b) = 0, determine the adjoint L∗ [ v ] with respect to
(a) the L2 inner products (9.7); (b) the weighted inner products (9.16).




u
9.1.10. Consider the linear operator L[ u ] =
that maps u(x) ∈ C1 to the vector-valued
u
function whose components consist of the function and its ﬁrst derivative. Imposing the
boundary conditions u(0) = u(1), compute the adjoint L∗ with respect to the L2 inner
products on both the domain and target spaces.
9.1.11. True or false: The adjoint of the divergence operator ∇ · v with respect to the L2 inner
products (9.22, 23) is minus the gradient operator: (∇ · )∗ u = − ∇u. If true, what boundary
conditions do you need to assume? If false, what is the adjoint?
9.1.12. Find the adjoint of the two-dimensional curl operator ∇ × v, as deﬁned in (6.73),
with respect to the L2 inner products (9.22, 23). Carefully state any required boundary
conditions.
♦ 9.1.13. Prove that (a) the adjoint of a linear operator is also a linear operator;
(b) the adjoint is unique.
♦ 9.1.14. Let L, M : U → V be linear operators on the same inner product spaces. Prove that
(a) (L + M )∗ = L∗ + M ∗ , (b) (c L)∗ = c L∗ for c ∈ R.
♦ 9.1.15. Prove Proposition 9.5.
♦ 9.1.16. Prove Proposition 9.6.
9.1.17. True or false: If L: U → U is invertible, then (L−1 )∗ = (L∗ )−1 .

350

9 A General Framework for Linear Partial Diﬀerential Equations

The Fredholm Alternative
Given a linear operator L: U → V between inner product spaces U, V , a fundamental
problem is to solve the associated inhomogeneous linear system
L[ u ] = f

(9.35)

for various forcing functions f ∈ V . In ﬁnite dimensions, this reduces to a linear algebraic
system, A u = f , deﬁned by a coeﬃcient matrix A. For the linear ordinary and partial
diﬀerential operators of interest to us, (9.35) represents a linear boundary value problem.
In general, an inhomogeneous linear system will not be solvable unless its right-hand side
satisﬁes certain constraints, ensuring that f belongs to the range of L. These conditions
can be readily characterized using the adjoint operator via the so-called Fredholm Alternative, named after the early-twentieth-century Swedish mathematician Ivar Fredholm.
Fredholm’s primary interest was in solving linear integral equations, but his solvability criterion was then recognized to be a completely general property of linear systems, including
linear algebraic systems, linear diﬀerential equations, linear boundary value problems, and
so on.
Recall that the kernel of a linear operator L is the set of solutions to the homogeneous
linear system L[ u ] = 0.
Deﬁnition 9.9. The cokernel of a linear operator L: U → V between inner product
spaces is deﬁned as the kernel of its adjoint:
.
coker L = ker L∗ = v ∈ V L∗ [ v ] = 0 .
(9.36)
We can now state and prove the Fredholm Alternative.
Theorem 9.10. If the linear system L[ u ] = f has a solution, then the right-hand
side must be orthogonal to the cokernel of L, i.e.,
 v , f  = 0

for all

v ∈ coker L.

(9.37)

Proof : If L[ u ] = f , then, given v ∈ coker L, the adjoint equation (9.2) implies
 v , f  =  v , L[ u ]  =  L∗ [ v ] , u  = 0,
since L∗ [ v ] = 0 by the deﬁnition of the cokernel.

Q.E.D.

Remark : In practice, one needs to check the orthogonality constraints (9.37) only
when v runs through a basis of the cokernel. In particular, if the only solution to the
homogeneous adjoint system L∗ [ v ] = 0 is the trivial solution v = 0, then there are no
constraints, and we expect that the inhomogeneous linear system (9.35) can be solved
for any “reasonable” forcing function f . In ﬁnite dimensions, this is certainly the case,
[89]. For boundary value problems deﬁned by linear diﬀerential operators, one needs to
determine what “reasonable” means, and then prove an appropriate existence theorem.
Although valid for all of the boundary value problems presented here, when subject to
continuous or even piecewise continuous forcing functions f , rigorous proofs of the existence
of solutions for partial diﬀerential equations involve the advanced mathematical machinery
of functional analysis — see, e.g., [38, 44, 61, 99] — and lie beyond the scope of this
introductory text.

9.1 Adjoints

351

Example 9.11. Consider the linear algebraic system
u1 − u3 = f 1 ,

u2 − 2 u3 = f 2 ,

u1 − 2 u2 + 3 u3 = f 3 .

(9.38)

Using Gaussian Elimination (or by inspection), one easily sees that (9.38) admits a solution
if and only if the compatibility condition
− f1 + 2 f2 + f3 = 0

(9.39)

holds. Moreover, when this occurs, a solution exists but is not unique. To connect this to
the Fredholm Alternative, we write the system in matrix form L[ u ] = f , where L[ u ] = A u
represents multiplication by the coeﬃcient matrix
⎛
⎞
1
0 −1
A = ⎝0
1 −2 ⎠ .
1 −2
3
Using the dot product on R 3 , the adjoint linear function L∗ [ v ] = AT v is represented by
the transposed matrix
⎛
⎞
1
0
1
AT = ⎝ 0
1 −2 ⎠ .
−1 −2
3
Therefore, the cokernel is found by solving the homogeneous adjoint linear system AT y = 0,
i.e.,
v2 − 2 v3 = 0,
− v1 − 2 v2 + 3 v3 = 0,
v1 + v3 = 0,
T

whose solutions consist of all scalar multiples of v = ( −1, 2, 1 ) . We recognize the compatibility condition (9.39) as requiring that the right-hand side be orthogonal (under the
dot product) to the cokernel basis vector,
v · f = − f1 + 2 f2 + f3 = 0,
in accordance with the Fredholm Alternative constraint (9.37).
Example 9.12. Let us solve the boundary value problem
u = f (x),

u (0) = 0,

u () = 0,

(9.40)

modeling the displacement, under an external force, of a uniform elastic bar of length 
both of whose ends are free. Solving the diﬀerential equation by direct integration, we ﬁnd
that

 x  y
f (z) dz dy,
u(x) = a x + b +
0

0

where the constants a, b are to be determined by the boundary conditions. Since
 x

f (z) dz,
u (x) = a +
0

the boundary condition u (0) = 0 implies that a = 0. The second boundary condition
requires


f (x) dx = 0.
(9.41)
u () =
0

352

9 A General Framework for Linear Partial Diﬀerential Equations

If this fails, then the boundary value problem has no solution. On the other hand, if
the forcing function f (x) satisﬁes the constraint (9.41), then the resulting solution of the
boundary value problem has the form

 x  y
f (z) dz dy,
(9.42)
u(x) = b +
0

0

where the constant b is arbitrary. Thus, when it exists, the solution to the boundary value
problem is not unique. The constant b solves the corresponding homogeneous problem,
and represents a rigid translation of the entire bar by a distance b.
The solvability constraint (9.41) follows from the Fredholm Alternative. Indeed, according to Example 9.7, under the L2 inner products and the given boundary conditions,
(D2 )∗ = D2 , and hence the adjoint system is the unforced homogeneous boundary value
problem
v  (0) = 0,
v  () = 0,
v  = 0,
with solution v(x) = c for any constant c. Thus, the cokernel consists of all scalar multiples
of the constant function v (x) ≡ 1. The Fredholm Alternative requires that the forcing
function in the original boundary value problem be orthogonal to the cokernel functions,
and so

1,f  =

f (x) dx = 0,
0

which is precisely the condition (9.41) required for existence of a (nonunique) equilibrium
solution.
Example 9.13. Consider the homogeneous Neumann boundary value problem for
the Poisson equation on a bounded domain Ω ⊂ R 2 , namely,
∂u
(9.43)
= 0 on ∂Ω.
∂n
According to Example 9.8, the Laplacian is self-adjoint under the L2 inner product and
the prescribed boundary conditions: Δ∗ = Δ. Thus, the homogeneous adjoint system is
merely
∂v
− Δv = 0 in Ω,
= 0 on ∂Ω.
∂n
Theorem 6.15 tells us that the only solutions to the adjoint problem are the constant
functions, v(x, y) ≡ c. Thus, a basis for the cokernel consists of the function v(x, y) ≡ 1,
and so the Fredholm Alternative requires that the forcing function in (9.43) satisfy

f (x, y) dx dy = 0,
(9.44)
1,f  =
− Δu = f

in

Ω,

Ω

reproducing our earlier constraint (6.90) for the homogeneous Neumann case.

Exercises
9.1.18. Use the Fredholm Alternative to determine whether the following linear systems are
compatible. When compatible, write down the general solution.

9.2 Self–Adjoint and Positive Deﬁnite Linear Functions

353

2 x + 3 y = −1,
3 x + 7 y = 1,
2 x − 4 y = −2,
6 x − 3 y + 9 z = 6,
(a)
(b)
(c) x + 4 y = 2,
− x + 2 y = 3,
2 x − y + 3 z = 2,
− x + y = 3,
2 x1 − 3 x2 − x3 = −1,
2 x1 + 3 x2 − x4 = −1,
3 x1 − x2 = 1,
(d)
(e) 3 x1 + 2 x3 − x4 = 0,
4 x1 + x2 + x3 = 2,
x1 − x2 + x3 = 1.
9.1.19. Use the Fredholm Alternative to ﬁnd the compatibility conditions for the following systems of linear equations.
(a) 2 x + y = a, x + 4 y = b, − 3 x + 2 y = c;
(b) x + 2 y + 3 z = a, − x + 5 y − 2 z = b, 2 x − 3 y + 5 z = c;
(c) x1 + 2 x2 + 3 x3 = b1 , x2 + 2 x3 = b2 , 3 x1 + 5 x2 + 7 x3 = b3 , − 2 x1 + x2 + 4 x3 = b4 ;
(d) x−3 y +2 z +w = a, 4 x−2 y +2 z +3 w = b, 5 x−5 y +4 z +4 w = c, 2 x+4 y −2 z +w = d.
9.1.20. Suppose A is a symmetric matrix. Show that the linear system A x = b has a solution
if and only if b is orthogonal to ker A.
9.1.21. Use the Fredholm Alternative to determine whether or not there exists a solution to the
following boundary value problem: x u + u = 1 − 23 x, u (1) = u (2) = 0. If so, write down
all solutions.
9.1.22. Analyze the periodic boundary value problem
u(0) = u(2 π),
u (0) = u (2 π),
− u = f (x),
along the same lines as in Example 9.12. Characterize the forcing functions for which the
problem has a solution. Explain why the constraints, if any, are in accordance with the
Fredholm Alternative. Write down a forcing function f (x) that satisﬁes all your constraints,
and then ﬁnd all corresponding solutions.
9.1.23. Answer Exercise 9.1.22 for the boundary value problems:
(a) u = f (x), u (0) = u (0) = 0, u (1) = u (1) = 0;
(b) u = f (x), u (0) = u (0) = 0, u(1) = u (1) = 0.
♥ 9.1.24. Let λ be a real parameter. (a) For which values of λ does the boundary value problem
u + λ u = h(x), u(0) = 0, u(1) = 0, have a unique solution? (b) Construct the Green’s
function for all such λ. (c) In the nonunique cases, use the Fredholm Alternative to ﬁnd
conditions on the forcing function h(x) that are required for the existence of a solution.
9.1.25. Let Ω ⊂ R 2 be a bounded, connected domain. Using the L2 inner products (9.22, 23)
on scalar and vector ﬁelds, write out the Fredholm Alternative constraints for the solvability of the boundary value problem ∇ · v = f in Ω, subject to the homogeneous boundary
conditions v · n = 0 on ∂Ω.
9.1.26. Let Ω ⊂ R 2 be a bounded simply connected domain. Using the L2 inner products
(9.22, 23) on scalar and vector ﬁelds on a domain Ω ⊂ R 2 , write out the Fredholm Alternative constraints for the solvability of the boundary value problem ∇u = f in Ω, subject to
the homogeneous boundary conditions u = 0 on ∂ Ω.

9.2 Self–Adjoint and Positive Deﬁnite Linear Functions
In ﬁnite-dimensional linear algebra, there are two particularly important classes of
matrices: symmetric, equal to their own transpose, and positive deﬁnite, as prescribed
by Deﬁnition B.12. The goal of this section is to adapt both concepts to more general
linear operators, paying particular attention to the case of linear diﬀerential operators.
The resulting classes of self-adjoint and positive (semi-)deﬁnite diﬀerential operators are
ubiquitous in applications of ordinary and partial diﬀerential equations.

354

9 A General Framework for Linear Partial Diﬀerential Equations

Self–Adjointness
Throughout this section, U will be a ﬁxed inner product space. We have already seen that
the transpose of a matrix is a very special case of the adjoint operation. Thus, the natural
analogue of a symmetric matrix is a linear operator that equals its own adjoint.
Deﬁnition 9.14. A linear operator S: U → U is called self-adjoint if S ∗ = S.
Thus, according to (9.2), S is self-adjoint if and only if
 S[ u ] , u
  =  u , S[ u
]

for all

u, u
 ∈ U.

(9.45)

Example 9.15. In the ﬁnite-dimensional case, a linear function S: R n → R n is
realized by matrix multiplication: S[ u ] = K u, where K is a square matrix of size n × n.
If we use the ordinary dot product on R n , then, according to Example 9.2, the adjoint
function S ∗ : R n → R n is given by multiplication by the transposed matrix: S ∗ [ u ] = K T u.
Thus, a linear function is self-adjoint with respect to the dot product if and only if it is
represented by a symmetric matrix: K T = K.
 provided
  = uT C u
On the other hand, if we adopt the weighted inner product  u , u
by the symmetric positive deﬁnite matrix C > 0, then, according to (9.6), the adjoint
function S ∗ has matrix representative C −1 K T C, and hence S is self-adjoint under the
weighted inner product if and only if the matrix K satisﬁes K = C −1 K T C.
Example 9.16. In Example 9.7, we argued that the second-order derivative operator
S = D2 is self-adjoint with respect to the L2 inner product, when subject to suitable homogeneous boundary conditions. A direct veriﬁcation of this result is instructive. According
to the general adjoint equation (9.2), we need to equate
 b

S[ u ] u
 dx =  S[ u ] , u
  =  u , S ∗[ u
]  =

a

 b

u S ∗[ u
 ] dx.

(9.46)

a

As before, the computation relies on (in this case two) integration by parts:
 b
 S[ u ] , u
 =
a

 b
b

d2 u
du d u
du
u

dx
u
 dx =
−
2
dx
dx
a dx dx
x=a
 b

 b
du

du

d2 u
u
−u
=
+
u
dx.
dx
dx x = a
dx2
a

Comparing with (9.46), we conclude that S ∗ = D2 = S, provided the boundary terms
vanish:
 b


 

du

du
u
−u
= u (b) u
(b) − u(b) u
  (b) − u (a) u
(a) − u(a) u
  (a) = 0.
dx
dx x = a
(9.47)
This requires that we impose suitable boundary conditions at the endpoints, which will
serve to characterize the underlying vector space U on which S = D2 acts. One possibility
is to set U = { u(a) = u(b) = 0 }, thereby imposing homogeneous Dirichlet boundary conditions. Since u
 ∈ U also, u
(a) = u
(b) = 0, and hence (9.47) holds, proving self-adjointness.
Alternatively, one can impose homogeneous Neumann, mixed, or periodic boundary conditions to specify the space U and similarly establish self-adjointness of S = D2 .

9.2 Self–Adjoint and Positive Deﬁnite Linear Functions

355

Positive Deﬁniteness
Let us turn to the characterization of positive deﬁnite, and, slightly less stringently, positive
semi-deﬁnite linear operators. These serve to extend the notions of positive deﬁnite and
semi-deﬁnite matrices to linear diﬀerential operators deﬁning boundary value problems.
Deﬁnition 9.17. A linear operator S: U → U on an inner product space is called
positive deﬁnite, written S > 0, if
 u , S[ u ]  > 0

for all

u = 0.

(9.48)

The operator S is positive semi-deﬁnite, written S ≥ 0, if
 u , S[ u ]  ≥ 0

for all

u.

(9.49)

Observe that, on the ﬁnite-dimensional space U = R n equipped with the dot product,
the linear function S[ u ] = K u is positive (semi-)deﬁnite if and only if K is a positive
(semi-)deﬁnite matrix, as per Deﬁnition B.12. (However, changing the inner product on R n
will result in an alternative notion of positive deﬁniteness for the matrix K; see Exercise
9.2.5.) In the inﬁnite-dimensional situations involving diﬀerential operators, the domain
of the operator may be only a dense subspace of the full inner product space U , and
one imposes the positivity condition (9.48) or (9.49) only on those functions u lying in
the domain of S. Fortunately, this technicality has no serious eﬀect on the subsequent
development.
Example 9.18. Consider the operator S = − D2 acting on the space U consisting of
all C2 functions deﬁned on a bounded interval [ a, b ] and subject to homogeneous Dirichlet
boundary conditions u(a) = u(b) = 0. To establish positive deﬁniteness, we evaluate
 b
 S[ u ] , u  =
a

d2 u
u
−
dx2



b

du
u
dx = −
+
dx
x=a

 b
a

du
dx

2

 b
dx =
a

du
dx

2
dx,

where we integrated by parts and then used the boundary conditions to eliminate the
boundary terms. The ﬁnal expression is clearly ≥ 0, and hence S is at least positive semideﬁnite. Moreover, since u (x) is continuous, the only way the ﬁnal integral could vanish is if
u (x) ≡ 0, which means u(x) ≡ c is constant. However, the only constant function satisfying
the homogeneous Dirichlet boundary conditions is u(x) ≡ 0. Thus,  S[ u ] , u  > 0 for all
0 = u ∈ U , which implies S > 0. A similar argument implies positive deﬁniteness when the
functions are subject to the mixed boundary conditions u(a) = u (b) = 0. On the other
hand, any constant function satisﬁes the Neumann boundary conditions u (a) = u (b) = 0,
and hence in this case S ≥ 0 is only positive semi-deﬁnite.
Proposition 9.19. If S > 0, then ker S = {0}. As a consequence, a positive deﬁnite
linear system S[ u ] = f with f in the range of S, so f ∈ rng S, must have a unique solution.
Proof : If S[ u ] = 0, then  u , S[ u ]  = 0, which, according to (9.48), is possible only if
u = 0. The second statement follows directly from Theorem 1.6.
Q.E.D.
Thus, in the ﬁnite-dimensional case, positive deﬁniteness implies that the coeﬃcient
matrix of S[ u ] = K u is nonsingular, and hence existence of a solution is automatic. In
the inﬁnite-dimensional cases of boundary value problems, existence of solutions usually
requires some further analysis, [63].

356

9 A General Framework for Linear Partial Diﬀerential Equations

The most common means of producing self-adjoint, positive (semi-)deﬁnite linear operators is provided by the following general construction. From here on, in order to distinguish the possibly diﬀerent norms resulting from the inner products on the domain and
target spaces of a linear operator L: U → V , we employ, respectively, the following double
and triple bar notation:
 u  =  u , u ,

u ∈ U,

| v | =  v , v ,

v ∈V.

(9.50)

Theorem 9.20. Let L: U → V be a linear map between inner product spaces with
adjoint L∗ : V → U . Then the composite map
S = L∗ ◦ L : U −→ U
is always self-adjoint, S = S ∗ , and positive semi-deﬁnite, S ≥ 0, with ker S = ker L.
Moreover, S > 0 is positive deﬁnite if and only if ker L = {0}.
Proof : First, by Propositions 9.5 and 9.6,
S ∗ = (L∗ ◦ L)∗ = L∗ ◦ (L∗ )∗ = L∗ ◦ L = S,
proving self-adjointness. Furthermore,
 u , S[ u ]  =  u , L∗ [ L[ u ] ]  =  L[ u ] , L[ u ]  = | L[ u ] |2 ≥ 0

(9.51)

for all u, proving positive semi-deﬁniteness. Moreover, the result is > 0 as long as L[ u ] = 0.
Thus, if ker L = { u | L[ u ] = 0 } = {0}, then  u , S[ u ]  > 0 for all u = 0, and hence S
is positive deﬁnite. Finally, the same computation proves that ker S = ker L. Indeed, if
L[ u ] = 0, then S[ u ] = L∗ [ L[ u ] ] = L∗ [ 0 ] = 0. On the other hand, if S[ u ] = 0, then
0 =  u , S[ u ]  = | L[ u ] |2 , and hence L[ u ] = 0.
Q.E.D.
We are particularly interested in linear systems that are based on the construction of
Theorem 9.20, namely
S[ u ] = L∗ [ L[ u ] ] = f.
(9.52)
We will refer to the system (9.52) as positive deﬁnite or positive semi-deﬁnite according
to the status of its deﬁning operator S. Thus, the system is positive deﬁnite if and only
if ker S = ker L = {0}, i.e., the only solution to the homogeneous system S[ z ] = 0 is the
trivial solution z = 0. In this case, the solution to (9.52) (provided it exists) is unique. On
the other hand, if there are nonzero solutions to S[ z ] = 0, then (9.52) is only positive semideﬁnite, and does not admit a unique solution. Moreover, unless the Fredholm Alternative
constraints (9.37) hold, then there are no solutions. By Theorem 9.20, we can identify
coker S = ker S ∗ = ker S = ker L,

(9.53)

which thus implies the following:
Theorem 9.21. Let S = L∗ ◦ L. If the linear system S[ u ] = f has a solution, then
 z , f  = 0 for all z ∈ ker L. Moreover, if S[ u ] = f and S[ u
 ] = f are two solutions to the
same linear system, then u
 = u + z, where z ∈ ker L is any solution to L[ z ] = 0.
Example 9.22. In the ﬁnite-dimensional case, any linear function L: R n → R m is
represented by matrix multiplication: L[ u ] = A u. For the dot product on both the domain
and target spaces, L∗ [ v ] = AT v, and so the self-adjoint combination S = L∗ ◦ L: R n → R n
is represented by the n × n symmetric matrix K = ATA. According to Theorem 9.20, the

9.2 Self–Adjoint and Positive Deﬁnite Linear Functions

357

matrix K is always positive semi-deﬁnite, and is positive deﬁnite if and only if the only
solution to the homogeneous linear system A z = 0 is the trivial solution z = 0. In the
positive semi-deﬁnite case, the Fredholm Alternative of Theorem 9.21 states that the linear
system K u = f has a solution if and only if z · f = 0 for all z ∈ ker A. (As noted before,
existence of solutions in the ﬁnite-dimensional case is not an issue.) Moreover, if u is any
 = u + z for any z ∈ ker A.
solution, so is u
More generally, if we adopt the weighted inner products (9.4) on the domain and
target spaces represented by the respective positive deﬁnite matrices M > 0 and C > 0,
then the adjoint map L∗ has matrix representative M −1 AT C, and hence S = L∗ ◦ L is
given by multiplication by the (not necessarily symmetric) n × n matrix K = M −1 AT C A.
Again, K ≥ 0 in all cases, and K > 0 if and only if ker A = {0}. Now, the Fredholm
Alternative states that the linear system K u = M −1 AT C Au = f has a solution if and
only if  z , f  = zT M f = 0 for all z ∈ ker A. See [89, 112] for applications of this
construction in mechanics, electrical networks, and the stability of structures.
Example 9.23. Consider next the diﬀerentiation operator D[ u ] = u . According
to Example 9.3, if we impose suitable homogeneous boundary conditions on the space of
allowable functions — Dirichlet, Neumann, mixed, or periodic — and use the L2 inner
products on both domain and target space, then D∗ [ v ] = − v  . Therefore, the self-adjoint
operator of Theorem 9.20 is given by S = D∗ ◦ D = − D2 .
According to Theorem 9.20, the resulting boundary value problem
S[ u ] = − u = f
is always positive semi-deﬁnite, and is positive deﬁnite if and only if ker D = {0}, i.e.,
the only function that satisﬁes D[ u ] = u = 0 along with the boundary conditions is the
zero function. Consider ﬁrst the Dirichlet boundary conditions u(a) = u(b) = 0. On
a connected interval, u = 0 if and only if u = c is a constant function. However, the
boundary conditions require that c = 0, and hence only the zero function appears in the
kernel. We conclude that the Dirichlet boundary value problem is positive deﬁnite, and
its solution unique. A similar argument applies to the mixed boundary conditions, e.g.,
u(a) = u (b) = 0, since the condition at x = a is enough to ensure that the constant function
must be zero. On the other hand, any constant function satisﬁes the Neumann boundary
conditions u (a) = u (b) = 0, and hence in this case, ker D consists of all constant functions.
Therefore, the Neumann boundary value problem is only positive semi-deﬁnite. And, as
we saw, the solution, when it exists, is not unique, since we can add any constant function
to a solution and obtain another solution. A similar argument proves that the periodic
boundary value problem, with u(a) = u(b), u (a) = u (b), is also positive semi-deﬁnite,
with the same kinds of existence and uniqueness properties.
More generally, if we use weighted inner products (9.16) on the domain and target
spaces, then, again subject to suitable boundary conditions, the adjoint is given by (9.19),
and so the self-adjoint boundary value problem S[ u ] = D∗ ◦ D[ u ] = f is based on the more
general diﬀerential equation


d
du
1
κ(x)
= f (x).
(9.54)
S[ u ] = −
ρ(x) dx
dx
Such boundary value problems model the deformations of a nonuniform elastic bar with
density ρ(x) and stiﬀness κ(x), when subject to the external forcing function f (x). Again,
the positive deﬁniteness of the problem depends on whether ker D = {0}, and so the exact

358

9 A General Framework for Linear Partial Diﬀerential Equations

same classiﬁcation holds as in the unweighted case: the Dirichlet and mixed boundary
value problems are positive deﬁnite and have a unique solution, whereas the Neumann and
periodic boundary value problems are only positive semi-deﬁnite, and the existence of a
solution requires the Fredholm conditions to be satisﬁed.
Self-adjointness underlies the symmetry of the associated Green’s function. As a function of x, the Green’s function Gξ (x) = G(x; ξ) satisﬁes the boundary value problem with
delta function forcing concentrated at position x = ξ:


∂G
1 ∂
S[ Gξ ] = δξ ,
κ(x)
= δ(x − ξ),
(9.55)
or, explicitly,
−
ρ(x) ∂x
∂x
along with the required homogeneous boundary conditions. Suppose ﬁrst that we are using
the L2 inner product on the interval [ a, b ], so that ρ(x) ≡ 1. Using the deﬁnition of the
delta function δξ (x) = δ(x − ξ) and the self-adjointness of S, we have, for any a < x, ξ < b,
 b
Gξ (y) δx (y) dy =  Gξ , δx  =  Gξ , S[ Gx ] 

G(x; ξ) = Gξ (x) =
a

 b

=  S[ Gξ ] , Gx  =  δξ , Gx  =

(9.56)
δξ (y) Gx (y) dy = Gx (ξ) = G(ξ; x).

a

This establishes† the symmetry equation
G(x; ξ) = G(ξ; x)

(9.57)

for the Green’s function of a self-adjoint boundary value problem under the L2 inner product. This can be regarded as the diﬀerential operator version of the fact that the inverse
of a symmetric matrix is also symmetric.
On the other hand, if we adopt a weighted inner product
 b
u(y) u
(y) ρ(y) dy,
u,u
 =
a

then the preceding argument must be slightly modiﬁed:
 b
ρ(y) Gξ (y) δx (y) dy =  Gξ , δx  =  Gξ , S[ Gx ] 
ρ(x) G(x; ξ) = ρ(x) Gξ (x) =
a

 b

=  S[ Gξ ] , Gx  =  δξ , Gx  =

δξ (y) Gx (y) ρ(y) dy = ρ(ξ) Gx (ξ) = ρ(ξ) G(ξ; x),
a

and so the Green’s function associated with a weighted self-adjoint boundary value problem
satisﬁes a “weighted symmetry condition”
ρ(x) G(x; ξ) = ρ(ξ) G(ξ; x).

(9.58)

Remark : Equation (9.58) implies that the modiﬁed Green’s function
 ξ) = G(x; ξ)
G(x;
ρ(ξ)
†

is genuinely symmetric:

 ξ) = G(ξ;
 x).
G(x;

Symmetry at the endpoints is a consequence of continuity.

(9.59)

9.2 Self–Adjoint and Positive Deﬁnite Linear Functions

359

The modiﬁed Green’s function also has the advantage of recasting the superposition formula
for the solution to the boundary value problem S[ u ] = f as the appropriate weighted inner
product:
 b

 b
G(x; ξ) f (ξ) dξ =

u(x) =
a

 (ξ) = G(x;
 ξ).
 ξ) f (ξ) ρ(ξ) dξ =  G
 , f , where G
G(x;
x
x

a

Two–Dimensional Boundary Value Problems
Let us next apply the self-adjoint formalism to study boundary value problems on a
bounded, connected, two-dimensional domain Ω ⊂ R 2 . We take L = ∇ to be the gradient operator, mapping a scalar ﬁeld u to a vector ﬁeld v = ∇u. We impose a suitable
set of homogeneous boundary conditions, i.e., Dirichlet, Neumann, or mixed. According to
the calculation in Section 9.1, if we adopt the basic L2 inner products (9.22, 23) between
scalar and vector ﬁelds, then the adjoint of the gradient is the negative of the divergence:
∇∗ v = − ∇ · v. Therefore, the self-adjoint combination of Theorem 9.20 yields
∇∗ ◦ ∇[ u ] = − ∇ · (∇u) = − Δu,
where Δ is the Laplacian operator. In this manner, we are able to write the two-dimensional
Poisson equation in self-adjoint form
− Δu = −∇ · (∇u) = ∇∗ ◦ ∇u = f,

(9.60)

as always subject to the selected boundary conditions.
According to Theorem 9.20, − Δ = ∇∗ ◦ ∇ is positive deﬁnite if and only if the kernel
of the gradient operator — restricted to the appropriate space of scalar ﬁelds — is trivial:
ker ∇ = {0}. Since we are assuming that the domain Ω is connected, Lemma 6.16 tells us
that the only functions that could show up in ker ∇, and thus prevent positive deﬁniteness, are the constants. The boundary conditions will tell us whether this occurs. The
only constant function that satisﬁes either homogeneous Dirichlet or homogeneous mixed
boundary conditions is the zero function, and thus, just as in the one-dimensional case,
the boundary value problem for the Poisson equation subject to Dirichlet or mixed boundary conditions is positive deﬁnite. In particular, this means that its solution is uniquely
deﬁned. On the other hand, any constant function satisﬁes the homogeneous Neumann
boundary condition ∂u/∂n = 0, and hence such boundary value problems are only positive
semi-deﬁnite. Existence of a solution relies on the Fredholm Alternative, as we discussed
in Example 9.13; moreover, when it exists, the solution is no longer unique, because one
can add in any constant without aﬀecting either the equation or the boundary conditions.
More generally, if we impose weighted inner products (9.32) on our spaces of scalar and
vector ﬁelds, then, recalling (9.34), the corresponding self-adjoint boundary value problem
takes the more general form




∂
1
∂
1
∂u
∂u
∇∗ ◦ ∇u = −
κ1 (x, y)
−
κ2 (x, y)
= f (x, y), (9.61)
ρ(x, y) ∂x
∂x
ρ(x, y) ∂y
∂y
along with the chosen boundary conditions on ∂Ω. Again, the Dirichlet and mixed boundary value problems are positive deﬁnite, with unique solutions, while the (suitably weighted)
Neumann problem is only positive semi-deﬁnite.

360

9 A General Framework for Linear Partial Diﬀerential Equations

The partial diﬀerential equation (9.61) arises in various physical contexts. For example, consider a steady-state ﬂuid ﬂow moving in a domain Ω ⊂ R 2 described by a vector
ﬁeld v. The ﬂow is called irrotational if it has zero curl, ∇ × v = 0, and hence, assuming
that Ω is simply connected, is a gradient v = ∇u, where u(x, y) is known as the ﬂuid
velocity potential . The constitutive assumptions connect the ﬂuid velocity with its rate of
ﬂow w = κ v, where κ(x, y) > 0 is the scalar density of the ﬂuid. Conservation of mass
provides the ﬁnal equation, namely ∇ · w + f = 0, where f (x, y) represents ﬂuid sources
(f > 0) or sinks (f < 0). Therefore, the basic equilibrium equations take the form




∂u
∂
∂u
∂
κ(x, y)
−
κ(x, y)
= f (x, y), (9.62)
−∇ · (κ ∇u) = f,
or
−
∂x
∂x
∂y
∂y
which is (9.61) with ρ → 1 and κ1 , κ2 → κ. The case of a homogeneous (constant density)
ﬂuid thus reduces to the Poisson equation (4.84), with f replaced by f /κ.
Symmetry of the Green’s function for the Poisson equation and the more general
boundary value problems (9.61, 62) follows by an evident adaptation of the one-dimensional
argument presented above. Details are left as Exercise 9.2.17.

Exercises
9.2.1. Which of the following matrices
linear
functions
S:R 2 → 
R 2 relative
 deﬁneself-adjoint



1 0
0 3
1
0
3 2
, (b)
, (c)
, (d)
.
to the dot product? (a)
0 1
2 2
2 −5
2 1
9.2.2. Answer Exercise 9.2.1 for the inner products
  = 2u u


(i )  u , u
1 1 + 3 u2 u2 ;



  = uT C u
 , where C =
(ii )  u , u

2
−1



−1
.
3

9.2.3. True or false: Given an inner product  u , v  on R n :
(a) The inverse of a nonsingular self-adjoint n × n matrix is self-adjoint.
(b) The inverse of a nonsingular positive deﬁnite n × n matrix is positive deﬁnite.
9.2.4. Prove that K > 0 is a positive deﬁnite n × n matrix if and only if J = K T + K is a
symmetric positive deﬁnite matrix.
♦ 9.2.5. (a) Prove that the n × n matrix K deﬁnes a self-adjoint linear function on R n with re  = uT C u
 for C a symmetric positive deﬁnite matrix if
spect to the inner product  u , u
and only if the matrix J = C K is symmetric, and hence deﬁnes a self-adjoint linear
function with respect to the dot product. (b) Prove that K > 0 under the given inner
product if and only if J > 0 under the dot product.
9.2.6. Let D[ u ] = u be the derivative operator acting on the vector space of C2 scalar functions u(x) deﬁned for 0 ≤ x ≤ 1 and satisfying the boundary conditions u(0) = 0, u(1) = 0.
1

 (x) ex dx on both its domain and
u(x) u
target spaces, determine the corresponding adjoint operator D∗ .
(b) Let S = D∗ ◦ D. Write down and solve the boundary value problem S[ u ] = 2 ex .
 =
(a) Given the weighted inner product  u , u

0

9.2.7. Let c(x) ∈ C0 [ a, b ] be a continuous function. Prove that the linear multiplication
operator S[ u ] = c(x) u(x) is self-adjoint with respect to the L2 inner product. What sort of
boundary conditions need to be imposed?
9.2.8. True or false: The Neumann boundary value problem − u + u = x, u (0) = u (π) = 0,
admits a unique solution.

9.2 Self–Adjoint and Positive Deﬁnite Linear Functions

361

du
is self-adjoint with respect to
dx
u(x) v(x) dx on the space of continuously

9.2.9. Prove that the complex diﬀerential operator L[ u ] = i
the L2 Hermitian inner product  u , v  =

 π

−π

diﬀerentiable complex-valued 2 π–periodic functions: u(x + 2 π) = u(x).
9.2.10. Let L = D2 . Using the L2 inner products on both the domain and target spaces,
write down a set of homogeneous boundary conditions that makes L∗ = D2 . Then set
S = L∗ ◦ L = D4 . Do your boundary conditions lead to a boundary value problem
S[ u ] = f that is (i ) positive deﬁnite; (ii ) positive semi-deﬁnite; or (iii ) neither?
9.2.11. Let β be a real constant. True or false: The second derivative operator S[ u ] = u is
self-adjoint with respect to the L2 inner product on the space of functions
U=





u(x) ∈ C2 [ 0, 1 ]  u(0) = 0, u (1) + β u(1) = 0



subject to Dirichlet boundary conditions at the left-hand endpoint and Robin boundary
conditions at the right-hand endpoint.
♥ 9.2.12. Let β be a real constant. Consider the diﬀerential operator S[ u ] = − u acting on the
space of functions
U=





u(x) ∈ C2 [ 0, 1 ]  u(0) = 0, u (1) + β u(1) = 0



subject to Dirichlet boundary conditions at the left-hand endpoint and Robin boundary
conditions at the right-hand endpoint. Prove that S > 0 is positive deﬁnite with respect to
the L2 inner product if and only if β > −1. Hint: Use the analysis following (4.48).
♥ 9.2.13. The equilibrium equations for a toroidal membrane (an inner tube) lead to the Poisson
equation − uxx − uyy = f (x, y) on a rectangle 0 < x < a, 0 < y < b, subject to periodic
boundary conditions
u(0, y) = u(a, y),
ux (0, y) = ux (a, y).
u(x, 0) = u(x, b),
uy (x, 0) = uy (x, b),
(a) Prove that the toroidal boundary value problem is self-adjoint. (b) Is it positive deﬁnite, positive semi-deﬁnite, or neither? (c) Are there any conditions that must be imposed
on the forcing function f (x, y) in order that a solution exist?
♦ 9.2.14. Find the adjoint of the gradient operator ∇ with respect to the L2 inner product
(9.22) between scalar ﬁelds, and the following weighted inner product between (column)
vector ﬁelds v = ( v1 (x, y), v2 (x, y) )T , v = ( v 1 (x, y), v 2 (x, y) )T :
 v , v  =





Ω

v(x, y)T C(x, y) v(x, y) dx dy,


α(x, y) β(x, y)
> 0 is symmetric, positive deﬁnite at
β(x, y) γ(x, y)
all points (x, y) ∈ Ω. What sort of boundary conditions do you need to impose? Write out
the corresponding boundary value problem for the equilibrium equation ∇∗ ◦ ∇u = f .

where the 2 × 2 matrix C(x, y) =

9.2.15. Let Ω ⊂ R 2 be a bounded domain. Construct a set of homogeneous boundary conditions on ∂Ω that make the biharmonic equation Δ2 u = f : (a) self-adjoint, (b) positive
deﬁnite, (c) positive semi-deﬁnite, but not positive deﬁnite.
 [G
 ] = δ satisﬁed by the modiﬁed Green’s
9.2.16. Write down the boundary value problem S
ξ
ξ
ξ


 , which may
function Gξ (x) = G(x; ξ) given in (9.59). Is the underlying linear operator S
ξ
depend on ξ, self-adjoint with respect to a suitable inner product?

♦ 9.2.17. Prove symmetry of the Green’s function, G(ξ ; x) = G(x; ξ ), for the Poisson equation
on a bounded domain Ω ⊂ R 2 subject to homogeneous Dirichlet boundary conditions.
Hint: Look at how we established (9.56).
9.2.18. Generalize Exercise 9.2.17 to the partial diﬀerential equation (9.61).

362

9 A General Framework for Linear Partial Diﬀerential Equations

9.3 Minimization Principles
One of the most important features of positive deﬁnite linear problems is that their solution
can be characterized by a quadratic minimization priniciple. In many physical contexts,
equilibrium conﬁguration(s) serve to minimize the potential energy of the system. Think
of a small ball rolling around in a bowl. After frictional eﬀects have stopped its motion,
the ball will be left sitting in equilibrium at the bottom of the bowl — the position that
minimizes the gravitational potential energy. Minimization priniciples are employed in
functional analytic proofs of existence of solutions, as well as providing a foundation for
the powerful ﬁnite element numerical method to be studied in Chapter 10.
The basic theorem on quadratic minimization principles is as follows.
Theorem 9.24. Let S: U → U be a self-adjoint and positive deﬁnite linear operator
on an inner product space U . Suppose that the linear system
S[ u ] = f

(9.63)

admits a (necessarily unique) solution u . Then u minimizes the value of the associated
quadratic function(al)
Q[ u ] = 12  u , S[ u ]  −  f , u ,
(9.64)
meaning that Q[ u ] < Q[ u ] for all admissible u = u in U .
Proof : We are given that S[ u ] = f , and so, for any u ∈ U ,
Q[ u ] = 12  u , S[ u ]  −  u , S[ u ]  = 12  u − u , S[ u − u ]  − 12  u , S[ u ] ,

(9.65)

where we used linearity, along with our assumption that S is self-adjoint, to identify the
terms  u , S[ u ]  =  u , S[ u ] . Since S > 0, the ﬁrst term on the right-hand side of (9.65)
is always ≥ 0; moreover it equals 0 if and only if u = u . On the other hand, the second
term does not depend on u at all. Thus, to minimize Q[ u ], we must make the ﬁrst term
as small as possible, which is accomplished by setting u = u .
Q.E.D.
Example 9.25. Consider the the problem of minimizing a quadratic function
Q(u1 , . . . , un ) =

n
n

1 
kij ui uj −
fi ui + c,
2 i,j = 1
i=1

(9.66)

T

depending on n variables u = ( u1 , u2 , . . . , un ) ∈ R n , with ﬁxed real coeﬃcients kij , fi ,
and c. Since ui uj = uj ui , we can assume, without loss of generality, that the coeﬃcients
of the quadratic terms are symmetric: kij = kji . We rewrite (9.66) in matrix notation as
Q(u) = 12 u · K u − f · u + c,

(9.67)

which, apart from the inessential constant term, agrees with (9.64) once we set S[ u ] = K u
 = u·u
 as the inner product on R n . Thus, according to
and use the dot product  u , u
Theorem 9.24, if K is a symmetric positive deﬁnite matrix, then the quadratic function
(9.67) has a unique minimizer u = (u1 , . . . , un )T , which is the solution to the linear system
K u = f .
If the positive deﬁnite linear operator in Theorem 9.24 comes from the self-adjoint
construction of Theorem 9.20, so S = L∗ ◦ L, then, by (9.51), the quadratic term can be
re-expressed as  u , S[ u ]  = | L[ u ] |2 , using our notational convention (9.50) for the norm
on the target space V of L. We can thus rephrase the minimization principle as follows.

9.3 Minimization Principles

363

Theorem 9.26. Suppose L: U → V is a linear operator between inner product spaces
with adjoint L∗ : V → U . Assume that ker L = {0}, and let S = L∗ ◦ L: U → U be the
associated positive deﬁnite linear operator. If f ∈ rng S, then the quadratic function
Q[ u ] = 12 | L[ u ] |2 −  f , u 

(9.68)

has a unique minimizer u , which is the solution to the linear system S[ u ] = f .
Warning: In (9.68), the ﬁrst term | L[ u ] |2 is computed using the norm based on the
inner product on V , while the second term  f , u  employs the inner product on U .
One of the most important applications of minimization is the method of least squares,
which is extensively applied in data analysis and approximation theory. We refer the
interested reader to [89] for developments in this direction. Here we will concentrate on
applications to diﬀerential equations.
Example 9.27. Consider the boundary value problem
− u = f (x),

(9.69)
u(a) = 0,
u(b) = 0.
The underlying diﬀerential operator S = D∗ ◦ D = − D2 , when acting on the space of
functions satisfying the homogeneous Dirichlet boundary conditions, is self-adjoint and, in
fact, positive deﬁnite, since ker D = {0}. Explicitly, positive deﬁniteness requires
 b
 b


2

 S[ u ] , u  =
u (x) dx > 0
(9.70)
− u (x) u(x) dx =
a

a

for all nonzero u(x) ≡ 0 with u(a) = u(b) = 0. Notice how we used an integration by parts,
invoking the boundary conditions to eliminate the boundary contributions, to expose the
positivity of the integral. The associated quadratic functional is, using (9.68),
 b

1  2
Q[ u ] = 21 | u |2 −  f , u  =
2 u (x) − f (x) u(x) dx.
a

2

Its minimum value, taken over all C functions that satisfy the homogeneous Dirichlet
boundary conditions, occurs precisely when u = u is the solution to the boundary value
problem.
Sturm–Liouville Boundary Value Problems
The most important class of boundary value problems governed by second-order ordinary diﬀerential equations was ﬁrst systematically investigated by the nineteenth-century
French mathematicians Jacques Sturm and Joseph Liouville. A Sturm–Liouville boundary
value problem is based on a second-order ordinary diﬀerential equation of the form


d
du
d2 u
du
S[ u ] = −
p(x)
+ q(x) u = − p(x) 2 − p (x)
+ q(x) u = f (x),
(9.71)
dx
dx
dx
dx
on a bounded interval a ≤ x ≤ b, supplemented by Dirichlet, Neumann, mixed, or periodic
boundary conditions. To avoid singular points of the diﬀerential equation (although we
will later discover that most cases of interest have one or more singular points), we assume
here that p(x) > 0 and, to ensure positive deﬁniteness, q(x) > 0 for all a ≤ x ≤ b. (The
case q(x) ≡ 0 can also be positive deﬁnite, when subject to suitable boundary conditions,
but is treated diﬀerently, in accordance with the weighted inner product case appearing in
Example 9.23.)

364

9 A General Framework for Linear Partial Diﬀerential Equations

Sturm–Liouville equations and boundary value problems appear in a remarkably broad
range of applications, and particularly in the analysis of partial diﬀerential equations by
the method of separation of variables. Moreover, most of the important special functions,
including Airy functions, Bessel functions, Legendre functions, hypergeometric functions,
and so on, naturally appear as solutions to particular Sturm–Liouville equations, [85, 86].
In the ﬁnal two chapters, the analysis of basic linear partial diﬀerential equations in curvilinear coordinates, in both two and three dimensions, will require us to solve several particular
examples, including the Bessel, Legendre, and Laguerre equations. For now, though, we
concentrate on understanding how Sturm–Liouville boundary value problems ﬁt into our
self-adjoint and positive deﬁnite framework.
Our starting point is the linear operator
 
u
(9.72)
L[ u ] =
u
T

that maps a scalar function u(x) ∈ U to a vector-valued function v(x) = ( v1 (x), v2 (x) ) ∈
V , whose components are v1 = u , v2 = u. To compute the adjoint of L: U → V , we use
the standard L2 inner product (9.7) on U , but adopt the following weighted inner product
on V :
 
 
 b


v1
v1
=
  =
, v
. (9.73)
 v , v
p(x) v1 (x) v1 (x) + q(x) v2 (x) v2 (x) dx, v =
v2
v2
a
The positivity assumptions on the weight functions p, q ensure that the latter is a bona
ﬁde inner product. As usual, the adjoint computation relies on integration by parts. Here,
we only need to manipulate the ﬁrst summand:
 b
(p u v1 + q u v2 ) dx
 L[ u ] , v  =
a

 b

= p(b) u(b) v1 (b) − p(a) u(a) v1 (a) +



u − (p v1 ) + q v2 dx.

a

The boundary terms will disappear, provided that, at each endpoint, either u or v1 vanishes.
Since for the linear operator v = L[ u ] given by (9.72), we can identify v1 = u , we conclude
that any of our usual boundary conditions — Dirichlet, mixed, or Neumann — remain valid
here. Under any of these conditions,
 b
 L[ u ] , v  =
u [ − (p v1 ) + q v2 ] dx =  u , L∗ [ v ] ,
a

and so the adjoint operator is given by
d(p v1 )
+ q v2 = − p v1 − p v1 + q v2 .
L∗ [ v ] = −
dx
The canonical self-adjoint combination


 
du
d
u
p
+ qu
=−
S[ u ] = L∗ ◦ L[ u ] = L∗
u
dx
dx

(9.74)

then reproduces the Sturm–Liouville diﬀerential operator (9.71). Moreover, since ker L =
{0} is trivial (why?), the boundary value problem is positive deﬁnite for all boundary
conditions, not only Dirichlet and mixed, but also Neumann!
A proof of the following general existence theorem can be found in [63].

9.3 Minimization Principles

365

Theorem 9.28. Let p(x) > 0 and q(x) > 0 for a ≤ x ≤ b. Then, for any choice of
boundary conditions (including Neumann), the Sturm–Liouville boundary value problem
(9.71) admits a unique solution.
Theorem 9.26 tells us that the solution to the Sturm–Liouville boundary value problem
(9.71) can be characterized as the unique minimizer of the quadratic functional
 b
Q[ u ] = 12 | L[ u ] |2 −  f , u  =

a

1


2
2
1
2 p(x) u (x) + 2 q(x) u(x) − f (x) u(x)



dx

(9.75)

among all C2 functions satisfying the prescribed homogeneous boundary conditions.
Example 9.29. Let ω > 0. Consider the constant-coeﬃcient Sturm–Liouville problem
− u + ω 2 u = f (x),

u(0) = u(1) = 0,

which we studied earlier in Example 6.10. Theorem 9.28 guarantees the existence of a
unique solution. The solution achieves the minimum possible value for the quadratic functional
 1
 1 2 1 2 2

Q[ u ] =
u + 2 ω u − f u dx
2
0

2

among all C functions satisfying the given boundary conditions.
More generally, suppose we adopt a weighted inner product
 b
u(x) u
(x) ρ(x) dx

u,u
 =

(9.76)

a

on the domain space U , where ρ(x) > 0 on [ a, b ]. The same integration by parts computation proves that, when subject to the homogeneous boundary conditions,


d(p v1 )
1
p
q
p
∗
−
+ q v2 = − v1 −
v1 + v2 ,
L [v] =
ρ
dx
ρ
ρ
ρ
and so the weighted Sturm–Liouville diﬀerential operator is




d
du
1
∗
◦
−
p
+ qu .
S[ u ] = L L[ u ] =
ρ
dx
dx

(9.77)

The corresponding weighted Sturm–Liouville equation S[ u ] = f has the form




1
d
du
p(x) d2 u p (x) du q(x)
S[ u ] =
−
p(x)
+ q(x) u = −
+
u = f (x),
−
ρ(x)
dx
dx
ρ(x) dx2
ρ(x) dx ρ(x)
(9.78)
which is, in fact, identical to the ordinary Sturm–Liouville equation (9.71) after we replace
f by ρ f . Be that as it may, the weighted generalization will become important when we
study the associated eigenvalue problems.
Example 9.30. Let m > 0 be a ﬁxed positive number. Consider the diﬀerential
equation
1
m2
B[ u ] = − u − u + 2 u = f (x),
(9.79)
x
x

366

9 A General Framework for Linear Partial Diﬀerential Equations

where B is known as the Bessel diﬀerential operator of order m. To place it in weighted
Sturm–Liouville form (9.78), we must ﬁnd p(x), q(x), and ρ(x) that satisfy
p (x)
1
= ,
ρ(x)
x

p(x)
= 1,
ρ(x)

q(x)
m2
= 2 .
ρ(x)
x

Dividing the second equation by the ﬁrst, we see that p (x)/p(x) = 1/x, and hence we can
set
m2
,
ρ(x) = x.
p(x) = x,
q(x) =
x
Thus, when subject to homogeneous Dirichlet, mixed, or even Neumann boundary conditions on an interval 0 < a ≤ x ≤ b, the Bessel operator B is positive deﬁnite and self-adjoint
with respect to the weighted inner product
 b
u(x) u
(x) x dx.

u,u
 =

(9.80)

a

Exercises
9.3.1. Consider the boundary value problem − u = x, u(0) = u(1) = 0. (i ) Find the solution.
(ii ) Write down a minimization principle that characterizes the solution. (iii ) What is
the value of the minimized quadratic functional on the solution? (iv ) Write down at least
two other functions that satisfy the boundary conditions and check that they produce larger
values for the energy.
9.3.2. Answer
for the boundary value problems
 Exercise 9.3.1

d
1
du
(a)
= x2 , u(−1) = u(1) = 0; (b) − (ex u ) = e− x , u(0) = u (1) = 0;
dx 1 + x2 dx
(c) x2 u + 2 x u = 3 x2 , u (1) = u(2) = 0; (d) x u + 3 u = 1, u(−2) = u(−1) = 0.
9.3.3. Let Q[ u ] =
2

1
0

 2
1
2 (u ) − 5 u

dx. (a) Find the function u (x) that minimizes Q[ u ]

among all C functions that satisfy u(0) = u(1) = 0.
(b) Test your answer by computing Q[ u ] and then comparing with the value of Q[ u ] when
u(x) = (i ) x − x2 , (ii ) 32 x − 32 x3 , (iii ) 23 sin πx, (iv ) x2 − x4 .
9.3.4. For each of the following functionals and associated boundary conditions: (i ) write down
a boundary value problem satisﬁed by the minimizing function, and (ii ) ﬁnd the minimizing
function u (x):
(a)
(b)
(c)
(d)
(e)

1

 2
1
u(0) = u(1) = 0,
2 (u ) − 3 u dx,
0
1
 2
1
u(0) = u(1) = 0,
2 (x + 1) (u ) − 5 u dx,
0
3
x (u )2 + 2 u dx, u(1) = u(3) = 0,
1
1
x
1 x  2
u(0) = u(1) = 0,
2 e (u ) − (1 + e ) u dx,
0
2

2
1

(x + 1) (u ) + x u
dx,
−1
(x2 + 1)2

u(−1) = u(1) = 0.

9.3 Minimization Principles

367

9.3.5. Which of the following quadratic functionals possess a unique minimizer among all C2
functions satisfying the indicated boundary conditions? Find the minimizer if it exists.
(a)
(b)
(c)
(d)
(e)

2

 2
1
u(1) = u(2) = 0;
2 x (u ) + 2 (x − 1) u dx,
1
π 
1
x (u )2 − u cos x dx, u(− π) = u(π) = 0;
−π 2
1   2
(u ) cos x − u sin x dx, u(−1) = u (1) = 0;
−1
2 
(1 − x2 ) (u )2 − u dx, u(−2) = u(2) = 0;
−2
1
(x + 1) (u )2 − u dx, u (0) = u (1) = 0.
0

9.3.6. Let D[ u ] = u be the derivative operator acting on the vector space of C2 scalar functions u(x) deﬁned for 0 ≤ x ≤ 1 and satisfying the boundary conditions u(0) = 0, u (1) = 0.
1

 (x) ex dx on both its domain and
u(x) u
target spaces, determine the corresponding adjoint operator D∗ .
(b) Let S = D∗ ◦ D. Write down and solve the boundary value problem S[ u ] = 3 ex .
 =
(a) Given the weighted inner product  u , u

0

(c) Write down a minimization principle that characterizes the solution you found in part
(b), or explain why none exists.
9.3.7. Solve the Sturm–Liouville boundary value problem − 4 u + 9 u = 1, u(0) = 0, u(2) = 0.
Is your solution unique?
9.3.8. Answer Exercise 9.3.7 for the Neumann boundary conditions u (0) = 0, u (2) = 0.
9.3.9. (i ) Write the following diﬀerential equations in Sturm–Liouville form. (ii ) If possible,
write down a minimization principle that characterizes the solutions to the Dirichlet boundary value problem on the interval [ 1, 2 ]. (a) − ex u − ex u = e2 x , (b) − x u − u + 2 u = 1,
(c) − u − 2 u + u = ex , (d) − x2 u + 2 x u + 3 u = 1, (e) x u + (1 − x) u + u = 0.
9.3.10. True or false: The Sturm–Liouville operator (9.71) is self-adjoint and positive deﬁnite
when subject to periodic boundary conditions u(a) = u(b), u (a) = u (b).
9.3.11. Does the quadratic functional Q[ u ] =

1
0

 2
1
2 (u ) −





x − 12 u dx have a minimum

value when u(x) is subject to the homogeneous Neumann boundary value conditions
u (0) = u (1) = 0? If so, determine the minimum value and ﬁnd all minimizing functions.
♥ 9.3.12. (a) Determine the adjoint of the diﬀerential operator L[ u ] = u + 2 x u with respect to
the L2 inner products on [ 0, 1 ] when subject to the ﬁxed boundary conditions
u(0) = u(1) = 0. (b) Is the self-adjoint operator S = L∗ ◦ L positive deﬁnite? Explain your
answer. (c) Write out the boundary value problem represented by S[ u ] = f . (d) Find the
2
solution to the boundary value problem when f (x) = ex . Hint: To integrate the
diﬀerential equation, work with the factored form of the diﬀerential operator. (e) Discuss
what happens if you instead impose the Neumann boundary conditions u (0) = u (1) = 0.
9.3.13. Discuss the self-adjointness and positive deﬁniteness of boundary value problems associated with the Bessel operator (9.79) of order m = 0.
9.3.14. Let u (x) be the solution to the self-adjoint positive deﬁnite boundary value problem
S[ u ] = f . Prove that if f (x) ≡ 0, then the minimum of the associated quadratic functional
is strictly negative: Q[ u ] < 0.
9.3.15. Find a function u(x) such that
the claimed positivity in (9.70)?

1

0

u (x) u(x) dx > 0. How do you reconcile this with

368

9 A General Framework for Linear Partial Diﬀerential Equations

9.3.16. Does the inequality (9.70) hold when u(x) ≡ 0 is subject to the Neumann boundary
conditions u (a) = u (b) = 0?
9.3.17. True or false: When subject to homogeneous Dirichlet boundary conditions on an interval [ a, b ], every nonsingular second-order linear ordinary diﬀerential equation
a(x) u + b(x) u + c(x) u = f (x) is (a) self-adjoint, (b) positive deﬁnite, (c) positive
semi-deﬁnite, with respect to some weighted inner product (9.76).

The Dirichlet Principle
Let us now apply these ideas to boundary value problems governed by the Poisson equation
(9.81)
− Δu = ∇∗ ◦ ∇u = f.
In the positive deﬁnite cases in which the partial diﬀerential equation is supplemented
by either homogeneous Dirichlet or homogeneous mixed boundary conditions, our general
Minimization Theorem 9.24 implies that the solution can be characterized by the justly
famous Dirichlet Principle.
Theorem 9.31. The function u(x, y) that minimizes the Dirichlet integral

1 2
1 2
Q[ u ] = 12 | ∇u |2 −  f , u  =
2 ux + 2 uy − f u dx dy

(9.82)

Ω

among all C2 functions that satisfy the prescribed homogeneous Dirichlet or mixed boundary conditions is the solution to the corresponding boundary value problem for the Poisson
equation − Δu = f .
The fact that a minimizer to the Dirichlet integral (9.82) satisﬁes the Poisson equation
is an immediate consequence of our general Minimization Theorem 9.26. On the other
hand, proving the existence of a C2 minimizing function is a nontrivial issue. Indeed,
the need for a rigorous existence proof was not immediately recognized: arguing from the
ﬁnite-dimensional situation, Dirichlet deemed existence to be self-evident, but it was not
until 50 years later that Hilbert supplied the ﬁrst rigorous proof — which was one of his
primary motivations for introducing the mathematical machinery of Hilbert space.
The Dirichlet principle (9.82) was derived under the assumption that the boundary
conditions are homogeneous — either pure Dirichlet or mixed. As it turns out, the minimization principle, as stated, also applies to the inhomogeneous Dirichlet boundary value
problem. However, the minimizing functional that characterizes the solution to a mixed
boundary value problem with inhomogeneous Neumann conditions on part of the boundary
acquires an additional boundary term.
Theorem 9.32. The solution u(x, y) to the boundary value problem
∂u
(9.83)
= k on N = ∂Ω \ D,
∂n
with D = ∅, is characterized as the unique function that minimizes the modiﬁed Dirichlet
integral


1 2
1 2
 u] =
Q[
u
+
u
−
f
u
dx
dy
−
k u ds
(9.84)
2 x
2 y
− Δu = f

in

Ω,

u = h on D ⊂ ∂Ω,

Ω

N

among all C2 functions that satisfy the prescribed boundary conditions.

9.3 Minimization Principles

369

In particular, the inhomogeneous Dirichlet problem has N = ∅, in which case the
extra boundary integral does not appear.
Proof : Write u(x, y) = u
(x, y)+v(x, y), where v is any function that satisﬁes the given
boundary conditions: v = h on D, while ∂v/∂n = k on N . (We speciﬁcally do not require
that v satisfy the Poisson equation.) Their diﬀerence u
 = u − v satisﬁes the corresponding
homogeneous boundary conditions, along with the modiﬁed Poisson equation
− Δ
u = f ≡ f + Δv

in

u
=0

Ω,

on D,

∂
u
= 0 on
∂n

N.

Theorem 9.31 implies that u
 minimizes the Dirichlet functional

1 2
 u
u |2 −  f, u
  =
x + 12 u
2y − f u
 dx dy
Q[
 ] = 12 | ∇
2u
Ω

among all functions satisfying the homogeneous boundary conditions. We compute
 u
 u − v ] = 1 | ∇(u − v) |2 −  f + Δv , u − v 
Q[
 ] = Q[
2
= 12 | ∇u |2 −  ∇u , ∇v  + 12 | ∇v |2 −  f , u  −  Δv , u  +  f + Δv , v 

= Q[ u ] −
(∇u · ∇v + u Δv) dx dy + C0 ,
Ω

where
C0 = 12 | ∇v |2 +  f + Δv , v 
does not depend on u. We then apply formula (6.83) to evaluate the middle terms:


&

∂v
∂v
ds =
ds +
(∇u · ∇v + u Δv) dx dy =
u
h
u k ds.
∂n
∂n
Ω
∂Ω
D
N
Thus,
 u
Q[
 ] = Q[ u ] −



 u] + C ,
k u ds + C1 = Q[
1
N



∂v
ds is ﬁxed by the boundary conditions and the
∂n
D
choice of v, and so its value does not change when the function u is varied. We conclude
 u
 u ].
that u
 minimizes Q[
 ] if and only if u = u
 + v minimizes Q[
Q.E.D.
where the ﬁnal term C1 = C0 +

h

Exercises
♥ 9.3.18. (a) Show that the function u(x, y) = 12 (− x y + x y 2 + x2 y − x2 y 2 ) solves the homogeneous Dirichlet boundary value problem for the Poisson equation − Δu = x2 + y 2 − x − y
on the unit square S = { 0 ≤ x ≤ 1, 0 ≤ y ≤ 1 }. (b) Write down the Dirichlet integral
(9.82) for this boundary value problem. What is its value for your solution? (c) Write
down three other functions that satisfy the homogeneous Dirichlet boundary conditions on
S, and check that all three have larger Dirichlet integrals.
9.3.19. (a) Suppose u(x, y) solves the boundary value problem − Δu = f in Ω and u = 0 on
∂Ω, with f (x, y) ≡ 0. Prove that its Dirichlet integral (9.82) is strictly negative: Q[ u ] < 0.
(b) Does this result hold for the inhomogeneous boundary value problem u = h on ∂ Ω?

370

9 A General Framework for Linear Partial Diﬀerential Equations

♥ 9.3.20. Consider the boundary value problem − Δu = 1, x2 + y 2 < 1, u = 0, x2 + y 2 = 1.
(a) Find all solutions. (b) Formulate the Dirichlet minimization principle for this problem.
Carefully indicate the function space over which you are minimizing. Make sure your solution belongs to the function space. (c) Which of the following functions belong to your
function space? (i ) 1−x2 −y 2 , (ii ) 1 − 12 x2 − 12 y 2 , (iii ) x−x3 −x y 2 , (iv ) x4 −x2 y 2 +y 4 ,
2

2

(v ) 12 e− x −y − 12 e−1 . (d) For each function in part (c) that does belong to your function
space, verify that its Dirichlet integral is larger than your solution’s value.

9.3.21. Suppose λ > 0. Under what conditions does the inhomogeneous Neumann problem
− Δu + λ u = f in Ω, ∂u/∂n = k on ∂Ω, for the Helmholtz equation have a solution?
Is the solution unique? Hint: Is the boundary value problem positive deﬁnite?
♦ 9.3.22. Suppose κ(x) > 0 for all a ≤ x ≤ b.
(a) Prove that the solution u (x) to the inhomogeneous Dirichlet boundary value problem
−

d
dx



κ(x)

du
dx



minimizes the functional Q[ u ] =

= f (x),
b
a

u(a) = α,

u(b) = β,


2
1
2 κ(x) u (x) − f (x) u(x)

dx.

Hint: Mimic the proof of Theorem 9.32.
(b) Construct a minimization principle for the mixed boundary value problem
−

d
dx



κ(x)

du
dx



= f (x),

u(a) = α,

u (b) = β.

9.3.23. Use the result of⎡ Exercise 9.3.22 to⎤ ﬁnd the C2 function u (x) that minimizes the


2 x du 2
⎣
+ x2 u ⎦ dx when subject to the boundary conditions
integral Q[ u ] =
1
2 dx
u(1) = 0, u(2) = 1.
9.3.24. Find the function u(x) that minimizes the integral Q[ u ] =

2
1

[ x(u )2 + x2 u ] dx subject

to the boundary conditions u(1) = 1, u (2) = 0. Hint: Use Exercise 9.3.22(b).
9.3.25. Prove that the functional Q[ u ] =

1
0

(u )2 dx, when subject to the mixed boundary

conditions u(0) = 0, u (1) = 1, has no minimizer.
♥ 9.3.26. Let p1 (x, y), p2 (x, y), q(x, y) > 0 be strictly positive functions on a closed, bounded,
connected domain Ω ⊂ R 2 . Consider the boundary value problem for the second-order
partial diﬀerential equation
−

∂
∂x



p1 (x, y)

∂u
∂x



−

∂
∂y



p2 (x, y)

∂u
∂y



+ q(x, y) u = f (x, y),

(x, y) ∈ Ω,

(9.85)

subject to homogeneous Dirichlet boundary conditions u = 0 on ∂Ω.
(a) True or false: Equation (9.85) is an elliptic partial diﬀerential equation. (b) Write
the boundary value problem in self-adjoint form L∗ ◦ L[ u ] = f . Hint: Regard (9.85) as a
“two-dimensional Sturm–Liouville equation”. (c) Prove that this boundary value problem
is positive deﬁnite, and then ﬁnd a minimization principle that characterizes the solution.
(d) Find suitable homogeneous Neumann–type boundary conditions involving the values of
the derivatives of u on ∂Ω that make the resulting boundary value problem for (9.85) selfadjoint. Is your boundary value problem positive deﬁnite? Why or why not?

9.4 Eigenvalues and Eigenfunctions

371

9.4 Eigenvalues and Eigenfunctions
We have already come to appreciate the value of eigenfunctions for constructing separable
solutions to dynamical partial diﬀerential equations such as the one-dimensional heat and
wave equations. In both cases, the eigenfunctions are trigonometric, and are used to
write the solution to the initial value problem in the form of a Fourier series. The most
important feature is that the Fourier eigenfunctions are orthogonal with respect to the
underlying L2 inner product. As we remarked earlier, orthogonality is not an accident.
Rather, it is a direct consequence of the self-adjointness of the linear diﬀerential operator
prescribing the eigenvalue equation. The goal of this section is, in preparation for extending
the eigenfunction method to higher-dimensional and more general dynamical problems,
to establish the orthogonality property of eigenfunctions in general, discuss how positive
(semi-)deﬁniteness aﬀects the eigenvalues, and present the basic theory of eigenfunction
series expansions, thereby signiﬁcantly generalizing basic Fourier series. As an application,
we deduce a general formula for the Green’s function of a positive deﬁnite boundary value
problem as an inﬁnite series in the eigenfunctions, and use this to formulate a condition that
guarantees completeness of the eigenfunctions. Along the way, we also need to introduce an
important minimization principle, the Rayleigh quotient, that characterizes the eigenvalues
of a positive deﬁnite linear system.
We begin with the eigenvalue problem
S[ v ] = λ v

(9.86)

for a linear operator S: U → U on† a real or complex vector space U . Clearly, v = 0 solves
the eigenvalue equation no matter what the scalar λ is. If the homogeneous linear system
(9.86) admits a nonzero solution 0 = v ∈ U , then λ ∈ C is called an eigenvalue of the
operator S and v a corresponding eigenvector or eigenfunction, depending on the context.
If λ is an eigenvalue, then the corresponding eigenspace is the subspace
Vλ = ker(S − λ I ) = { v | S[ v ] = λ v } ⊂ U,

(9.87)

consisting of all the eigenvectors/eigenfunctions along with the zero element. To avoid
technical diﬃculties, we will work under the assumption that all the eigenspaces are ﬁnitedimensional, and we call 1 ≤ dim Vλ < ∞ the geometric multiplicity of the eigenvalue
λ. Finite-dimensionality is almost always valid, and indeed, will be later established for
regular boundary value problems on bounded domains.
Self–Adjoint Operators
In the applications considered here, the vector space U comes equipped with an inner
product, and S is a self-adjoint linear operator. In such instances, one can readily establish
the basic orthogonality property of the eigenvectors/eigenfunctions.
Theorem 9.33. If S = S ∗ is a self-adjoint linear operator on an inner product space
U , then all its eigenvalues are real. Moreover, the eigenvectors/eigenfunctions associated
with diﬀerent eigenvalues are automatically orthogonal.
†
As discussed earlier, in the inﬁnite-dimensional case, the diﬀerential operator S might be
only deﬁned on a dense subspace of U consisting of suﬃciently smooth functions.

372

9 A General Framework for Linear Partial Diﬀerential Equations

Proof : To prove the ﬁrst part of the theorem, suppose λ is a complex eigenvalue,
so that S[ v ] = λ v for some complex eigenvector/eigenfunction v = 0. Then, using
the sesquilinearity (B.19) of the underlying Hermitian inner product‡ and self-adjointness
(9.45) of S, we ﬁnd
λ  v 2 =  λ v , v  =  S[ v ] , v  =  v , S[ v ]  =  v , λ v  = λ  v 2 .
Since v = 0, this immediately implies that λ = λ, its complex conjugate, and hence λ must
necessarily be real.
To prove orthogonality, suppose S[ u ] = λ u and S[ v ] = μ v. Again by self-adjointness,
λ  u , v  =  λ u , v  =  S[ u ] , v  =  u , S[ v ]  =  u , μ v  = μ  u , v ,
where the ﬁnal equality relies on the fact that the eigenvalue μ is real. Therefore, the
assumption that λ = μ immediately implies orthogonality:  u , v  = 0.
Q.E.D.
Thus, the eigenvalues of self-adjoint linear operators are necessarily real. If, in addition, the operator is positive deﬁnite, then its eigenvalues must, in fact, be positive.
Theorem 9.34. If S > 0 is a self-adjoint positive deﬁnite linear operator, then all its
eigenvalues are strictly positive: λ > 0. If S ≥ 0 is self-adjoint and positive semi-deﬁnite,
then its eigenvalues are nonnegative: λ ≥ 0.
Proof : Self-adjointness assures us that all of the eigenvalues are real. Suppose S[ u ] =
λ u with u = 0 a real eigenfunction. Then
λ  u 2 = λ  u , u  =  λ u , u  =  S[ u ] , u  > 0,
by positive deﬁniteness. Since  u 2 > 0, this immediately implies that λ > 0. The same
argument implies that λ ≥ 0 in the positive semi-deﬁnite case.
Q.E.D.
All the linear operators to be considered in this text are real, and, at the very least,
self-adjoint, and often either positive deﬁnite or semi-deﬁnite. Thus, we will restrict our
attention from here on (at least until we reach the Schrödinger equation in the ﬁnal subsection) to real operators deﬁned on real vector spaces, knowing a priori that we are not
overlooking any eigenvalues or eigenfunctions by this restriction.
Example 9.35. In ﬁnite dimensions, if we equip U = R n with the dot product,
then any self-adjoint linear function is given by multiplication by an n × n symmetric
matrix: S[ u ] = K u, where K T = K. Theorem 9.33 implies the well-known result that
a symmetric matrix has only real eigenvalues. Moreover, the eigenvectors associated with
diﬀerent eigenvalues are mutually orthogonal.
In fact, it can be proved that, in general, the eigenvectors of a symmetric matrix are
complete, [89]. In other words, there exists an orthogonal basis v1 , . . . , vn of R n consisting
of eigenvectors of K, so K vj = λj vj for j = 1, . . . , n. If the eigenvalues λ1 , . . . , λn are
all simple, so λi = λj for i = j, then the basis eigenvectors are automatically orthogonal.
When K has repeated eigenvalues, this requires selecting an orthogonal basis of each of
the associated eigenspaces Vλ = ker(K − λ I ), e.g., using the Gram–Schmidt process.
‡

We are temporarily working in the vector space of complex-valued functions. Once we
establish reality of the eigenvalues and eigenfunctions, we can shift our focus back to the real
function space.

9.4 Eigenvalues and Eigenfunctions

373

Completeness implies that the number of linearly independent eigenvectors associated with
an eigenvalue, i.e., its geometric multiplicity, is the same as the eigenvalue’s algebraic
multiplicity. If, furthermore, the matrix K > 0 is symmetric and positive deﬁnite, then
Theorem 9.34 implies that all its eigenvalues are positive: λj > 0. In this case, thanks
to completeness, the converse is also valid: a symmetric matrix is positive deﬁnite if and
only if it has all positive eigenvalues. These results can all be immediately generalized to
self-adjoint matrices under general inner products on R n .
Example 9.36. Consider the Dirichlet eigenvalue problem
−

d2 v
= λ v,
dx2

v(0) = 0,

v() = 0,

for the diﬀerential operator S = − D2 on an interval of length  > 0. As we know — see,
for instance, Section 4.1 — the eigenvalues and eigenfunctions are
" n π #2
n πx
,
n = 1, 2, 3, . . . .
,
vn (x) = sin
λn =


We now understand this example in our general framework. The fact that the eigenvalues
are real and positive follows from the fact that the boundary value problem is deﬁned by
the self-adjoint positive deﬁnite operator
S[ u ] = D∗ ◦ D[ u ] = − D2 [ u ] = − u ,
acting on the vector space U = { u(0) = u() = 0 }, equipped with the L2 inner product:

u,v =
u(x) v(x) dx.
0

The orthogonality of the Fourier sine functions,

n πx
m πx
sin
dx = 0
sin
 vm , vn  =


0

for

m = n,

is also an automatic consequence of their status as eigenfunctions of this self-adjoint boundary value problem.
Example 9.37. Similarly, the periodic boundary value problem
− v  = λ v,

v(− π) = v(π),

v  (− π) = v  (π),

(9.88)

has eigenvalues λ0 = 0, with eigenfunction v0 (x) ≡ 1, and λn = n2 , for n = 1, 2, 3, . . ., each
possessing two independent eigenfunctions: vn (x) = cos n x and vn (x) = sin n x. In this
case, a zero eigenvalue appears because S = D∗ ◦ D = − D2 is only positive semi-deﬁnite
on the space of periodic functions. Theorem 9.33 implies the all-important orthogonality of
the Fourier eigenfunctions corresponding to diﬀerent eigenvalues:  vm , vn  =  vm , 
vn  =
v n  = 0 for m = n, under the L2 inner product on [ − π, π ]. However, since they have

vm , 
the same eigenvalue, the orthogonality of vn (x) = cos n x and vn (x) = sin n x, while true,
is not ensured and must be checked by hand.
Example 9.38. On the other hand, the self-adjoint boundary value problem
−

d2 u
= λ u,
dx2

lim u(x) = 0,

x→∞

lim

x → −∞

u(x) = 0,

(9.89)

374

9 A General Framework for Linear Partial Diﬀerential Equations

on the real line has no eigenvalues: no matter what the value of λ, the only solution
decaying to 0 at both ± ∞ is the zero solution. Indeed, exponential solutions that decay at
one end become inﬁnitely large at the other. The trigonometric functions u(x) = cos ω x
and sin ω x satisfy the diﬀerential equation when λ = ω 2 > 0, but do not go to zero as
| x | → ∞, and so do not qualify as bona ﬁde eigenfunctions. Rather, because they are
bounded on the entire line, they represent the “continuous spectrum” of the underlying
self-adjoint diﬀerential operator, [95]. In this particular context, the continuous spectrum
leads directly to the Fourier transform.
Example 9.39. The eigenvalue problem for the Bessel diﬀerential operator of order
m, given in (9.79), is governed by the following diﬀerential equation:
S[ u ] = − u −

1  m2
u + 2 u = λ u,
x
x

(9.90)

or, equivalently,
d2 u
du
+ (λ x2 − m2 ) u = 0,
+x
dx2
dx
supplemented by appropriate homogeneous boundary conditions at the endpoints of the
interval 0 ≤ a < b. Its eigenfunctions are not elementary, but, as we will learn in Chapter 11, can be expressed in terms of Bessel functions. Nevertheless, no matter what their
eventual formula, Theorem 9.33 guarantees the orthogonality of any two eigenfunctions
 under the weighted inner product (9.80):
v, v associated with distinct eigenvalues λ = λ
 b
v,
v =
v(x) 
v (x) x dx = 0.
x2

a

Example 9.40. According to equation (9.60), on a bounded domain Ω ⊂ R 2 , the
(negative) Laplacian − Δ forms a self-adjoint positive (semi-)deﬁnite operator under the
L2 inner product (9.22) when subject to one of the usual sets of homogeneous boundary
conditions. Let us, for speciﬁcity, concentrate on the Dirichlet case. The eigenfunctions of
the Laplacian are the nonzero solutions to the following boundary value problem:
− Δv = λ v

in

Ω,

u=0

on ∂Ω.

(9.91)

The underlying partial diﬀerential equation, namely
∂2v
∂2v
+ 2 + λ v = 0,
2
∂x
∂y
is known as the Helmholtz equation, named after the inﬂuential and wide-ranging German
applied mathematician Hermann von Helmholtz. As we will see, the Helmholtz equation
plays a central role in the solution of the two-dimensional heat, wave, and Schrödinger
equations.
Only in a few special cases, e.g., rectangles and circular disks, can the eigenfunctions and eigenvalues be determined exactly; see Chapter 11 for details. Nevertheless,
Theorem 9.34 guarantees that, for all domains, the eigenvalues are always nonnegative,
λ ≥ 0, with λ0 = 0 being an eigenvalue only in positive semi-deﬁnite cases, e.g., Neumann boundary conditions. Moreover, Theorem 9.33 ensures the orthogonality of any two
eigenfunctions,

 v , v  =
v(x, y) 
v (x, y) dx dy = 0,
Ω

.
that are associated with distinct eigenvalues λ = λ

9.4 Eigenvalues and Eigenfunctions

375

The Rayleigh Quotient
We have already learned how to characterize the solutions of positive deﬁnite boundary
value problems by a minimization principle. One can also characterize their eigenvalues
by a minimization principle, named after the proliﬁc nineteenth-century English applied
mathematician Lord Rayleigh (John Strutt).
Deﬁnition 9.41. Let S: U → U be a self-adjoint linear operator on an inner product
space. The Rayleigh quotient of S is deﬁned as
R[ u ] =

 u , S[ u ] 
 u 2

for

0 = u ∈ U.

(9.92)

We are, in fact, primarily interested in the Rayleigh quotient of positive deﬁnite operators, for which R[ u ] > 0 for all u = 0. If S = L∗ ◦ L, then, using (9.51), we can rewrite
the Rayleigh quotient in the alternative form
R[ u ] =

| L[ u ] |2
,
 u 2

(9.93)

keeping in mind our notational convention (9.50) for the respective norms on U and V .
Theorem 9.42. Let S be a self-adjoint linear operator. Then the minimum value of
its Rayleigh quotient,
(9.94)
λ = min { R[ u ] | u = 0 } ,
is the smallest eigenvalue of the operator S. Moreover, any 0 = v ∈ U that achieves this
minimum value, R[ v ] = λ , is an associated eigenvector/eigenfunction: S[ v ] = λ v .
Proof : Suppose that v ∈ U is a minimizing element, and
λ = R[ v ] =

 v , S[ v ] 
 v 2

(9.95)

the minimum value. Given any u ∈ U , deﬁne the scalar function†
 v + t u , S[ v + t u ] 
 v + t u 2
 v , S[ v ]  + 2 t  u , S[ v ]  + t2  u , S[ u ] 
= 
,
 v 2 + 2 t  u , v  + t2  u 2

g(t) = R[ v + t u ] =

where we used the self-adjointness of S and the fact that we are working in a real inner
product space to identify the terms
 u , S[ v ]  =  S[ u ] , v  =  v , S[ u ] .
Since
g(0) = R[ v ] ≤ R[ v + t u ] = g(t),
the function g(t) will attain its minimum value at t = 0. Elementary calculus tells us that
0 = g  (0) = 2

†

 u , S[ v ]   v 2 −  v , S[ v ]   u , v 
.
 v 4

g(t) is not deﬁned if v + t u = 0, but this does not aﬀect the argument.

376

9 A General Framework for Linear Partial Diﬀerential Equations

Therefore, using (9.95) to replace  v , S[ v ]  by λ  v 2 , we must have
 u , S[ v ]  − λ  u , v  =  u , S[ v ] − λ v  = 0.

(9.96)

The only way the inner product in (9.96) can vanish for all possible u ∈ U is if
S[ v ] = λ v ,

(9.97)

which means that 0 = v is an eigenfunction and λ its associated eigenvalue.
On the other hand, if v is any eigenfunction, so S[ v ] = λ v, where, by self-adjointness,
the eigenvalue λ is necessarily real, then the value of its Rayleigh quotient is
R[ v ] =

 v , λv 
 v , S[ v ] 
=
= λ.
 v 2
 v 2

(9.98)

Since λ was, by deﬁnition, the smallest possible value of the Rayleigh quotient, it thus
must necessarily be the smallest eigenvalue.
Q.E.D.
Remark : The existence of a minimizing function is not addressed in this result, and,
indeed, there may be no minimum eigenvalue; the inﬁmum of the set of eigenvalues could be
− ∞ or, even if ﬁnite, not an eigenvalue. However, for the positive deﬁnite boundary value
problems considered here, the eigenvalues are all strictly positive, and one can, with some
additional analysis, [44], prove the existence of a minimizing eigenfunction, and hence a
smallest positive eigenvalue.
We label the eigenvalues in increasing order, so that, assuming positive deﬁniteness,
0 < λ1 ≤ λ2 ≤ λ3 ≤ · · · , with λ1 the minimum eigenvalue and hence the minimum value
of the Rayleigh quotient. To characterize the other eigenvalues, we need to restrict the
class of functions over which one minimizes. Indeed, since the nth eigenfunction vn must
be orthogonal to all its predecessors v1 , . . . , vn−1 , it makes sense to try minimizing the
Rayleigh quotient over such elements.
Theorem 9.43. Let v1 , . . . , vn−1 be eigenfunctions corresponding to the ﬁrst n − 1
eigenvalues 0 < λ1 ≤ · · · ≤ λn−1 of the positive deﬁnite self-adjoint linear operator S.
Let
.
Un−1 = u  u , v1  = · · · =  u , vn−1  = 0 ⊂ U
(9.99)
be the set of functions that are orthogonal to the indicated eigenfunctions. Then the
minimum value of the Rayleigh quotient function restricted to the subspace Un−1 is the
nth eigenvalue of S, that is,
.
λn = min R[ u ] 0 = u ∈ Un−1 ,
(9.100)
and any minimizer is an associated eigenfunction vn .
Proof : We follow the preceding proof, but now restrict v and u to belong to the subspace Un−1 . Observe that S[ u ] ∈ Un−1 whenever u ∈ Un−1 , because, by self-adjointness,
 S[ u ] , vj  =  u , S[ vj ]  = λj  u , vj  = 0

for

j = 1, . . . , n − 1.

Thus, if 0 = v ∈ Un−1 minimizes the Rayleigh quotient, then (9.96) holds for arbitrary
u ∈ Un−1 . In particular, choosing u = S[ v ] − λ v , we conclude that v satisﬁes the
eigenvalue equation (9.97), and hence must be an eigenfunction that is orthogonal to the
ﬁrst n − 1 eigenfunctions. This means that λ = λn must be the next-lowest eigenvalue
and v = vn one of its associated eigenfunctions.
Q.E.D.

9.4 Eigenvalues and Eigenfunctions

377

Example 9.44. Return to the Dirichlet eigenvalue problem on the interval [ 0,  ] for
the self-adjoint (under the L2 inner product) diﬀerential operator − D2 = D∗ ◦ D discussed
in Example 9.36. Its Rayleigh quotient can be written as


u(x) u (x) dx
u (x)2 dx

 u , −u 
0
0
| u |2
R[ u ] =
=−
,
= 
=

2
u
 u 2
2
2
u(x) dx
u(x) dx
0

0

where the second expression, based on the alternative form (9.93), can be readily deduced
from the ﬁrst via an integration by parts. (Here, both domain and target space of L = D
use the same L2 norm.) According to Theorem 9.42, the minimum value of R[ u ] over
all nonzero functions u(x) ≡ 0 satisfying the boundary conditions u(0) = u() = 0 is the
lowest eigenvalue, namely
6
7
π2
λ1 = 2 = min R[ u ] u(0) = u() = 0, u(x) ≡ 0 ,

which is achieved if and only if u(x) is a nonzero constant multiple of sin(πx/), the
corresponding eigenfunction. The reader is invited to numerically test this result by ﬁxing
a value of , and then evaluating R[ u ] on various functions u(x) satisfying the boundary
conditions to check that the numerical value is always larger than π 2 /2 , the smallest
eigenvalue. The second eigenvalue can be found by minimizing over all nonzero functions
that are orthogonal to the ﬁrst eigenfunction:
8


4 π2
π
λ2 = 2 = min R[ u ] u(0) = u() = 0,
u(x) sin x dx = 0, u(x) ≡ 0 ,


0
and similarly for the higher eigenvalues.
Example 9.45. Consider the Helmholtz eigenvalue problem (9.91) on a bounded domain Ω ⊂ R 2 , subject to Dirichlet boundary conditions. The associated Rayleigh quotient
(9.93) can be written in the form
  $  2  2 %
∂u
∂u
dx dy
+
2
∂x
∂y
| ∇u |
Ω
=
.
(9.101)
R[ u ] =

 u 2
u(x, y)2 dx dy
Ω

Its minimum value among all nonzero functions u(x, y) ≡ 0 subject to the boundary conditions u = 0 on ∂Ω is the smallest eigenvalue λ1 , and the minimizing function is any nonzero
constant multiple of the associated eigenfunction v1 (x, y). To obtain a higher eigenvalue
λn , one minimizes R[ u ], where u(x, y) ≡ 0 again satisﬁes the boundary conditions and, in
addition, is orthogonal to the preceding n − 1 eigenfunctions:

0 =  u , vk  =
u(x, y) vk (x, y) dx dy,
for
k = 1, . . . , n − 1.
Ω

It can be proved, [34, 44], that, as long as the domain is bounded with, as always, a
reasonably nice boundary, there is a solution to each of these minimization problems, and
hence the Helmholtz equation admits an inﬁnite sequence of positive eigenvalues 0 < λ1 ≤
λ2 ≤ λ3 ≤ · · · , with λn → ∞ becoming arbitrarily large as n → ∞; see also Theorem 9.47
below.

378

9 A General Framework for Linear Partial Diﬀerential Equations

Eigenfunction Series
For our applications to dynamical partial diﬀerential equations, we will be particularly
interested in expanding more general functions in terms of the orthogonal eigenfunctions,
the simplest case being the classical Fourier series. To ﬁx notation, we will proceed as
if we were treating a one-dimensional boundary value problem, although the formulas
are equally valid for higher-dimensional problems, e.g., those governed by the Helmholtz
equation. Thus, we consider an eigenvalue problem of the form S[ v ] = λ v, where S is
a positive deﬁnite or semi-deﬁnite operator that is self-adjoint relative to a weighted L2
inner product
 b
(9.102)
 v , v  =
v(x) 
v(x) ρ(x) dx,
a

with ρ(x) > 0 on the bounded interval a ≤ x ≤ b.
Let 0 ≤ λ1 ≤ λ2 ≤ λ3 ≤ · · · be the eigenvalues, and v1 , v2 , v3 , . . . , the corresponding
eigenfunctions. Theorem 9.33 assures us that those corresponding to diﬀerent eigenvalues
are mutually orthogonal:
j = k.
 vj , vk  = 0,
(9.103)
Orthogonality is not automatic if vj and vk belong to the same eigenvalue, but it can be
ensured by selecting an orthogonal basis of each eigenspace Vλ , if necessary by applying
the Gram–Schmidt orthogonalization process, [89].
Let f ∈ U be an arbitrary function in our inner product space. The eigenfunction
series of f is, by deﬁnition, its generalized Fourier series:
f ∼


k

ck vk ,

where the coeﬃcient

ck =

 f , vk 
 vk 2

(9.104)

is found by formally taking the inner product of both sides of (9.104) with the eigenfunction
vk and invoking their mutual orthogonality. (Note that our earlier eigenfunction series
formula (3.108) assumed orthonormality; here, it will be convenient to not necessarily
impose the condition  vk  = 1.) For example, in the case covered by Example 9.36,
(9.104) becomes the usual Fourier sine series for the function f , whereas for Example 9.37,
it represents its full periodic Fourier series. In a similar fashion, Example 9.40 leads
to series in the eigenfunctions of the Laplacian operator on a bounded domain subject
to appropriate homogeneous boundary conditions; explicit examples of the latter can be
found in Chapters 11 and 12.
As we learned in Section 3.5, convergence (in norm) of the series (9.104) requires completeness of the eigenfunctions. (Pointwise and uniform convergence are then implied by
more restrictive hypotheses on the function and the domain, e.g., f ∈ C1 .) In the ﬁnitedimensional context, when S: R n → R n is given by matrix multiplication, S[ u ] = K u,
there are only ﬁnitely many eigenvectors, and so the summation (9.104) has only ﬁnitely
many terms. There are, hence, no convergence considerations, and completeness is automatic. For boundary value problems in inﬁnite-dimensional function space, the completeness of the resulting eigensolutions is a more delicate issue. In Example 9.36, the
eigenvalue problem for S = − D2 subject to homogeneous Dirichlet boundary conditions
on a bounded interval leads to the Fourier sine eigenfunctions, which we know to be complete. On the other hand, the corresponding eigenvalue problem on the real line, treated in
Example 9.38, has no eigenfunctions, and so completeness is out of the question. As we will

9.4 Eigenvalues and Eigenfunctions

379

see, the eigenfunctions associated with regular boundary value problems on bounded domains are automatically complete, whereas singular problems and problems on unbounded
domains require additional analysis.
Whether or not the eigenfunctions are complete, we always have Bessel’s inequality †
(3.117):

c2k  vk 2 ≤  f 2 .
(9.105)
k

Theorem 3.43 says that the eigenfunctions are complete if and only if Bessel’s inequality
is an equality, which is then the Plancherel formula for the eigenfunction expansion.
Green’s Functions and Completeness
We now combine two of our principal themes. Remarkably, the key to the completeness
of eigenfunctions for boundary value problems lies in the eigenfunction expansion of the
Green’s function! Assume that S is both self-adjoint and positive deﬁnite. Thus, by
Theorem 9.34, all its eigenvalues are positive. We index them in increasing order:
0 < λ1 ≤ λ2 ≤ λ3 ≤ · · · ,

(9.106)

where each eigenvalue is repeated according to its multiplicity.
By positive deﬁniteness, the boundary value problem S[ u ] = f has a unique solution.‡
Therefore, it admits a Green’s function Gξ (x) = G(x; ξ), which satisﬁes the boundary value
problem
S[ Gξ ] = δξ ,
(9.107)
with a delta function impulse on the right-hand side. For each ﬁxed ξ, let us write down
the eigenfunction series (9.104) for the Green’s function:
G(x; ξ) =

∞


ck (ξ) vk (x),

where the coeﬃcient

k=1

ck (ξ) =

 Gξ , vk 
 vk 2

(9.108)

depends on the impulse point ξ. Since S[ vk ] = λk vk , the coeﬃcients can be explicitly
evaluated by means of the following calculation:
λk ck (ξ)  vk 2 =  Gξ , λk vk  =  Gξ , S[ vk ] 
 b
δ(x − ξ) vk (x) ρ(x) dx = vk (ξ) ρ(ξ),
=  S[ Gξ ] , vk  =  δξ , vk  =
a

where ρ(x) is the weight function of our inner product (9.102), and we invoked the selfadjointness of S. Solving for
v (ξ) ρ(ξ)
(9.109)
ck (ξ) = k
λk  vk 2
†

Formula (3.117) assumed orthonormality of the functions; here we are stating the analogous
result for orthogonal elements. Moreover, here, the eigenfunctions and hence the coeﬃcients ck
are all real, so we don’t need absolute value signs.
‡

As usual, we are assuming existence of the solution; Proposition 9.19 guarantees uniqueness.

380

9 A General Framework for Linear Partial Diﬀerential Equations

and then substituting back into (9.108), we deduce the explicit eigenfunction series
∞

v (x) v (ξ) ρ(ξ)

G(x; ξ) ∼

k

k

(9.110)

λk  vk 2

k=1

for the Green’s function. Observe that this expression is compatible with the weighted
symmetry equation (9.58).
Example 9.46. According to Example 6.9, the Green’s function for the L2 selfadjoint boundary value problem
− u = f (x),
is


G(x; ξ) =

u(0) = 0 = u(1),
x(1 − ξ),

x ≤ ξ,

ξ(1 − x),

x ≥ ξ.

(9.111)

On the other hand, the eigenfunctions for
− v  = λ v,

v(0) = 0 = v(1),

are vk (x) = sin k πx, with corresponding eigenvalues λk = k 2 π 2 , for k = 1, 2, 3, . . . . Since
 1
 vk 2 =
0

sin2 k πx dx = 12 ,

formula (9.110) implies the eigenfunction expansion
G(x; ξ) =

∞

2 sin k πx sin k πξ

k2 π2

k=1

.

(9.112)

This result can be checked by a direct computation of the Fourier sine series of (9.111).
Let us now apply Bessel’s inequality (9.105) to the eigenfunction series (9.108) for the
Green’s function; using (9.109), the result is
n


ck (ξ)  vk  =
2

2

k=1

n

v (ξ)2 ρ(ξ)2
k

k=1

λ2k  vk 2

 b
≤  Gξ  =
2

G(x; ξ)2 ρ(x) dx.

(9.113)

a

We divide by ρ(ξ) > 0, and then integrate both sides of the resulting inequality from a to
b. On the left-hand side, the integrated summands are
 b
a

vk (ξ)2 ρ(ξ)
1
2
2 dξ = 2
λk  vk 
λk  vk 2

 b
vk (ξ)2 ρ(ξ) dξ =
a

1
.
λ2k

Substituting back into (9.113) establishes the interesting inequality
 b b
n

1
ρ(x)
dx dξ.
≤
G(x; ξ)2
λ2k
ρ(ξ)
a a

(9.114)

k=1

To make the right-hand side look less strange, we can replace G(x; ξ) by the symmetric

9.4 Eigenvalues and Eigenfunctions

381

 ξ) = G(x; ξ)/ρ(ξ) = G(ξ;
 x), cf. (9.59), whence
modiﬁed Green’s function G(x;
 b b
G(x; ξ)2
a

a

ρ(x)
dx dξ =
ρ(ξ)

 b b
a

 2 ,
 ξ)2 ρ(x) ρ(ξ) dx dξ ≡  G
G(x;

(9.115)

a

which we can interpret as a “double weighted L2 norm” of the modiﬁed Green’s function
 ξ). Since the summands in (9.114) are all positive, we can let n → ∞, and conclude
G(x;
that
∞

1
 2 .
≤ G
(9.116)
λ2k
k=1

Thus, assuming that the right-hand side of this inequality is ﬁnite, the summation on the
left converges. This implies that its summands must go to zero: λ−2
k → 0 as k → ∞. We
have thus proved the ﬁrst statement of the following important result.
 2 < ∞, then the eigenvalues of the positive deﬁnite selfTheorem 9.47. If  G
adjoint operator S are unbounded: 0 < λk → ∞ as k → ∞. Moreover, the associated
orthogonal eigenfunctions v1 , v2 , v3 , . . . , are complete.
Proof : Our remaining task is to prove completeness — that is, that the eigenfunction
series (9.104) of any function f ∈ U converges in norm. For n = 2, 3, 4, . . . , consider the
function
gn−1 = f −

n−1


ck vk ,

k=1

i.e., the diﬀerence between the function f and the (n − 1)st partial sum of its eigenfunction
series. Completeness requires that
 gn−1  −→ 0

as

n → ∞.

(9.117)

We can assume that gn−1 = 0, since otherwise, the eigenfunction series terminates, with
0 = gn−1 = gn = gn+1 = · · · (why?), and so (9.117) holds trivially.
First, note that, for any j = 1, . . . , n − 1,
 gn−1 , vj  =  f , vj  −

n−1


ck  vk , vj  =  f , vj  − cj  vj 2 = 0,

k=1

by the orthogonality of the eigenfunctions combined with the formula (9.104) for the coeﬃcient cj . Thus, gn−1 ∈ Vn−1 , the subspace (9.99) of functions orthogonal to the ﬁrst
n − 1 eigenfunctions used in the Rayleigh Minimization Theorem 9.43. Since, according to
(9.100), λn is the minimum value of the Rayleigh quotient among all nonzero elements of
Vn−1 , we must have
λn ≤ R[ gn−1 ] =

 gn−1 , S[ gn−1 ] 
,
 gn−1 2

382

9 A General Framework for Linear Partial Diﬀerential Equations

and hence
λn  gn−1 2 ≤  gn−1 , S[ gn−1 ] 
%

$
n−1
n−1


= f−
ck vk , S f −
ck vk

f−

=


f−

=

k=1

k=1

n−1


n−1


ck vk , S[ f ] −

ck S[ vk ]

k=1

k=1

n−1


n−1


ck vk , S[ f ] −

k=1

=  f , S[ f ]  −
=  f , S[ f ]  −



ck λk vk

k=1
n−1

k=1
n−1

k=1

λk ck  f , vk  −

n−1


n−1


ck  vk , S[ f ]  +

k=1

λk c2k  vk 2

k=1

 f , vk 
.
 vk 2
2

λk

In the ﬁnal equality, we used the self-adjointness of S to identify
 vk , S[ f ]  =  S[ vk ] , f  = λk  vk , f  = λk  f , vk ,
coupled with the formula in (9.104) for the coeﬃcients ck . Since the summands in the ﬁnal
expression are all positive, we conclude that
 gn−1 2 ≤

 f , S[ f ] 
.
λn

Since we already know that λn → ∞, the right-hand side of the ﬁnal inequality goes to 0
as n → ∞. This implies (9.117) and hence establishes completeness.
Q.E.D.
One important corollary of this theorem is that, since each eigenvalue is repeated
according to its geometric multiplicity, the multiplicity cannot be inﬁnite (why?), and
hence each eigenspace of such an S is necessarily ﬁnite-dimensional.
Example 9.48. For the eigenvalue problem considered in Example 9.46, since ρ(x) ≡
 ξ) is
1, the double norm of the (modiﬁed) Green’s function G(x; ξ) = G(x;
 1 1
 G 2 =

 1 ξ
x2 (1 − ξ)2 dx dξ =

G(x; ξ)2 dx dξ = 2
0

0

0

0

1
< ∞.
90

Thus, Theorem 9.47 re-establishes the completeness of the sine eigenfunctions, meaning
that the eigenfunction series, which is just the ordinary Fourier sine series on [ 0, 1 ], converges in norm.
Indeed, for any regular Sturm–Liouville boundary value problem on a bounded interval, the (modiﬁed) Green’s function is automatically continuous, and hence its double
weighted norm is ﬁnite. Thus, Theorem 9.47 implies the completeness of the Sturm–
Liouville eigenfunctions. In Chapters 11 and 12, we will extend this result to some important singular boundary value problems.

9.4 Eigenvalues and Eigenfunctions

383

Example 9.49. The completeness result of Theorem 9.47 doesn’t directly apply to
the periodic boundary value problem of Example 9.37, because it is not positive deﬁnite,
and hence there is no Green’s function. However, we can convert it into a positive deﬁnite
problem by a simple trick. As you are asked to prove in Exercise 9.4.4, if S ≥ 0 is any
positive semi-deﬁnite operator and μ > 0 any positive constant, then S = S +μ I is positive
deﬁnite, where I [ u ] = u is the identity operator. Thus, we replace the original periodic
boundary value problem (9.88) by the following modiﬁcation:
− v  + μ v = λ v,

v(− π) = v(π),

v  (− π) = v  (π).

(9.118)

This does not alter the eigenfunctions, while adding μ to each of the eigenvalues, and
hence the modiﬁed problem has eigenvalues λ0 = μ, with eigenfunction v0 (x) ≡ 1, and
λn = n2 + μ, with two independent eigenfunctions: vn (x) = cos n x and vn (x) = sin n x.
The Green’s function for the periodic boundary value problem
− v  + μ v = δ(x − ξ),

v(− π) = v(π),

v  (− π) = v  (π),

where μ > 0 is a ﬁxed constant, is derived along the same lines as in Example 6.10. Setting
μ = ω 2 , the result is
cosh ω π − | x − ξ |
.
(9.119)
G(x; ξ) =
2 ω sinh πω
Its double L2 norm is clearly ﬁnite, and, although unnecessary, can even be computed:
 π  π
π 2 πω + sinh 2 πω
2
G =
< ∞.
G(x; ξ)2 dx dξ =
4 ω 3 sinh2 πω
−π −π
As a result, Theorem 9.47 reconﬁrms the completeness of the trigonometric eigenfunctions.
Example 9.50. According to (6.120), the Green’s function G(x; ξ) for the Dirichlet
boundary value problem for the Poisson equation on a domain Ω ⊂ R 2 is the sum of a
logarithmic potential (6.106) and a harmonic function. Thus G(x; ξ)2 is a sum of three
terms: the ﬁrst two, involving (log r)2 and log r with r =  x − ξ , have mild singularities
when x = ξ, while the last term is smooth (indeed analytic) everywhere. Using this
information, it is not hard to prove that its double L2 norm

 
G(x, y; ξ, η)2 dx dy dξ dη < ∞
 G 2 =
Ω

Ω

is ﬁnite. Indeed, the only problematic point is the logarithmic singularity at x = ξ, but a
polar coordinate computation, similar to that used in the proof of Theorem 6.17, shows that
such logarithmic singularities still have ﬁnite integrals. Therefore, Theorem 9.47 implies
that the Helmholtz eigenvalues λn → ∞, and the corresponding Helmholtz eigenfunctions
vn (x, y) form a complete orthogonal system.
Remark : In problems involving unbounded domains, such as the Schrödinger equation
for the hydrogen atom to be discussed in Section 12.7, the eigenfunctions are typically not
complete, and one needs to introduce additional solutions corresponding to what is known
as the continuous spectrum of the operator. Functions are now represented by combinations
of discrete Fourier-like sums over the eigenfunctions (the bound states in the quantummechanical system) plus a Fourier integral-like term involving the continuous spectrum
(the scattering states), [66, 72]. A full discussion of completeness and convergence in such
cases must be deferred to an advanced course in analysis, [95].

384

9 A General Framework for Linear Partial Diﬀerential Equations

Exercises
9.4.1. Find the eigenvalues and an orthonormal eigenvector basis for the following symmetric
matrices:
⎛
⎞
⎛
⎞






1 0 4
6 −4
1
2
6
5 −2
2 −1
⎜
⎟
⎜
, (b)
, (c)
, (d) ⎝ 0 1 3 ⎠, (e) ⎝ −4
6 −1 ⎟
(a)
⎠.
6 −7
−2
5
−1
5
4 3 1
1 −1 11
9.4.2. Determine whether the following symmetric matrices are positive deﬁnite by computing
their eigenvalues.
⎛
⎞
⎛
⎞




1 −1
0
4 −1 −2
2 −2
−2 3
2 −1 ⎟
4 −1 ⎟
(b)
, (c) ⎜
(d) ⎜
(a)
⎝ −1
⎠,
⎝ −1
⎠.
−2
3
3 6
0 −1
1
−2 −1
4




0 1
. (a) Show that S: R 2 → R 2 is positive semi−1 0
deﬁnite under the dot product. (b) Find the eigenvalues of S. (c) Explain why your result
in part (b) does not contradict Theorem 9.34.

9.4.3. Suppose S[ u ] = K u, where K =

♦ 9.4.4. Suppose that S: U → U is a positive semi-deﬁnite linear operator. Let I : U → U be the
identity operator, so I [ u ] = u. (a) Prove that, for any positive scalar μ > 0, the operator
Sμ = S + μ I is positive deﬁnite. (b) Show that S and Sμ have the same eigenfunctions.
Do they have the same eigenvalues? If not, how are their eigenvalues related?
1 2

9.4.5. Find the minimum value of R[ v ] =

v dx

0

1 2
v dx
0

on the space of C2 functions v(x) deﬁned

on 0 ≤ x ≤ 1 that are subject to one of the following pairs of boundary conditions:
(a) v(0) = v(1) = 0,

(b) v(0) = v  (1) = 0,
e

9.4.6. Find the minimum value of R[ v ] =

1

(c) v  (0) = v  (1) = 0.

x2 v 2 dx

e 2
v dx
1

on the space of C2 functions deﬁned on

[ 1, e ] subject to the boundary conditions v(1) = v(e) = 0.
9.4.7. Show that the Rayleigh quotient R[ v ] has the same value for all nonzero scalar multiples
of an element 0 = v ∈ U , i.e., R[ c v ] = R[ v ] for all c = 0.
9.4.8. Prove that the minimum value of the Rayleigh quotient of a positive semi-deﬁnite, but
not positive deﬁnite, operator is 0.
♥ 9.4.9. (a) Find the eigenfunctions and eigenvalues for the boundary value problem
u(1) = u(e) = 0.
− x2 u − x u = λ u,
(b) Under which inner product are the eigenfunctions orthogonal? Justify your answer by
direct computation.
(c) Write down the eigenfunction expansion of a function f (x) deﬁned for 1 ≤ x ≤ e.
(d) Find the Green’s function for
− x2 u − x u = f (x),
u(1) = u(e) = 0,
both in closed form and as a series in the eigenfunctions you found in part (a).
(e) Is your Green’s function symmetric? Discuss.
(f ) Prove the completeness of the eigenfunctions.
9.4.10. Discuss completeness of the eigenfunctions of the boundary value problem
− x2 u − 2 x u = λ u,
| u(0) | < ∞,
u(1) = 0.

9.5 A General Framework for Dynamics

385

9.4.11. Consider the eigenvalue problem − u = λ u, u(0) = 0, u (1) = 0. (a) Is the problem
self-adjoint? positive deﬁnite? Which inner product are you referring to? (b) Find all
eigenvalues and eigenfunctions. (c) Write down the explicit formula for the eigenfunction
expansion of a function f (x) deﬁned on [ 0, 1 ]. (d) Find the Green’s function and use it to
prove completeness of the eigenfunctions.
♥ 9.4.12. (a) Find the eigenfunctions and eigenvalues for the Chebyshev boundary value problem
(x2 − 1)u + x u = λ u,
u(−1) = u(1) = 0.
Hint: Let x = cos θ. (b) Under what inner product are the eigenfunctions orthogonal?
Justify your answer by direct computation. (c) Find the Green’s function for

(x2 − 1)u + x u = f (x),
u(−1) = u(1) = 0,
both in closed form and as a series in the eigenfunctions you found in part (a).
(d) Discuss completeness of the eigenfunctions.
9.4.13. Consider the diﬀerential operator S[ u ] = − u + u on the space of C2 functions u(x)
deﬁned for all x and subject to the boundary conditions lim u(x) = lim u(x) = 0.
x→∞

x → −∞

(a) Find the Green’s function G(x; ξ). (b) Compute its double L2 norm:  G 2 . What
does this indicate about the completeness of the eigenfunctions of S? (c) Justify your conclusion in part (b) by determining the eigenfunctions.
9.4.14. Find all (real and complex) eigenvalues of the ﬁrst-derivative operator D = d/dx on
the interval [ 0, 1 ] subject to the single periodic boundary condition v(0) = v(1). Are the
corresponding eigenfunctions orthogonal? For which inner product?
♥ 9.4.15. Consider the Dirichlet boundary value problem
− Δu = h(x, y), u(x, 0) = 0, u(x, 1) = 0, u(0, y) = 0, u(1, y) = 0, 0 < x, y < 1,
for the Poisson equation on the unit square. (a) Find the eigenfunction series expansion
for the Green’s function of this problem. (b) Does your series coincide with that derived
in Exercise 6.3.22? Explain any discrepancies. (c) For the impulse points (ξ, η) = (.5, .5)
and (.7, .8), graph the result of summing the ﬁrst 9, 25, and 100 terms in your series, and
discuss what you observe in light of what you expect the Green’s function to look like.
9.4.16. Find the eigenfunction series expansion for the Green’s function of the following mixed
boundary value problems:
(a) − Δu = h(x, y), u(x, 0) = 0, u(x, 1) = 0, ux (0, y) = 0, ux (1, y) = 0, 0 < x, y < 1;
(b) − Δu = h(x, y), u(x, 0) = 0, uy (x, 1) = 0, u(0, y) = 0, ux (1, y) = 0, 0 < x, y < 1.
9.4.17. Find the eigenfunction series expansion for the Green’s function of the following
Helmholtz boundary value problem:
− Δu + u = h(x, y),
u(x, 0) = u(x, π) = u(0, y) = u(π, y) = 0,
0 < x, y < π.
♦ 9.4.18. If the eigenvalues of a self-adjoint linear operator satisfy λn → ∞ as n → ∞, explain
why each eigenspace is necessarily ﬁnite-dimensional.
9.4.19. True or false: If S: R n → R n is any linear function, then one can ﬁnd an inner product
on R n that makes S self-adjoint.

9.5 A General Framework for Dynamics
In this ﬁnal section, we show how to use general eigenfunction expansions to analyze
three important classes of linear dynamical systems: parabolic diﬀusion equations such
as the heat equation, hyperbolic vibration equations such as the wave equation, and the
Schrödinger equation, a complex evolution equation that governs the dynamical processes

386

9 A General Framework for Linear Partial Diﬀerential Equations

of quantum mechanics. In all three cases we can, assuming completeness, write the general
solution to the initial-boundary value problem as a convergent eigenfunction series with
time-dependent coeﬃcients, and thereby establish several general properties governing their
dynamics.

Evolution Equations
In all cases, our starting point is the basic equilibrium equation, which is a linear system
of the form
S[ u ] = f,
(9.120)
where f represents an external forcing. The linear operator S is assumed to be of the usual
self-adjoint form
S = L∗ ◦ L,
(9.121)
which is either positive deﬁnite, when ker L = {0}, or positive semi-deﬁnite, the latter
case being characterized by the existence of null eigenfunctions 0 = v ∈ ker L = ker S. In
ﬁnite dimensions, (9.120) represents a linear algebraic system consisting of n equations in n
unknowns with positive (semi-)deﬁnite coeﬃcient matrix. In inﬁnite-dimensional function
space, it represents a self-adjoint positive (semi-)deﬁnite boundary value problem for the
unknown function u.
With the equilibrium operator in hand, there are two principal classical dynamical
systems of importance as physical models. The ﬁrst are the (unforced) diﬀusion processes
modeled by an evolution equation of the form
∂u
= − S[ u ] = − L∗ ◦ L[ u ].
∂t

(9.122)

In the discrete case, this represents a ﬁrst-order system of ordinary diﬀerential equations,
known as a linear gradient ﬂow . In the continuous case, S is a linear diﬀerential operator
equipped with homogeneous boundary conditions, and (9.122) represents a linear partial
diﬀerential equation for the time-varying function u = u(t, x), the heat equation being
the prototypical example. (As in the preceding section, the notation employed below
indicates that we are working in a single space dimension, but the methods and results
apply equally well to higher-dimensional problems.) The addition of external forcing to
the diﬀusion process is treated in Exercise 9.5.6.
The basic separation of variables solution technique was already outlined in Section 3.1.
To recap, the separable solutions are of exponential form
u(t, x) = e− λ t v(x),

(9.123)

where v ∈ U is a ﬁxed function. Since the operator S is linear and does not involve t
diﬀerentiation, we ﬁnd
∂u
= − λ e− λ t v,
∂t

while

S[ u ] = e− λ t S[ v ].

Substituting back into (9.122) and canceling the common exponential factors, we are led
to the eigenvalue problem
S[ v ] = λ v.
(9.124)

9.5 A General Framework for Dynamics

387

Thus, (9.123) deﬁnes a solution if and only if v is an eigenfunction for the linear operator
S, with λ the corresponding eigenvalue.
We let vk (x), k = 1, 2, . . . , be the orthogonal eigenfunctions and 0 ≤ λ1 ≤ λ2 ≤
λ3 ≤ · · · → ∞ the corresponding eigenvalues. Assuming completeness, the solution to
the initial value problem
u(0, x) = f (x)

(9.125)

can be expanded in terms of the eigensolutions:
u(t, x) =

∞


e− λk t ck vk (x),

where

ck =

k=1

 f , vk 
 vk 2

(9.126)

are the eigenfunction coeﬃcients of the initial data. In particular, the fundamental solution
of the diﬀusion equation is deﬁned as the solution u = F (t, x; ξ) to the initial value problem
u(0, x) = δξ (x)

(9.127)

induced by an initial delta impulse at the point ξ. Its eigenfunction coeﬃcients are
ck =

 b

 δξ , vk 
1
=
2
 vk 
 vk 2

δ(x − ξ) vk (x) ρ(x) dx =
a

vk (ξ) ρ(ξ)
.
 vk 2

Thus,
∞


F (t, x; ξ) =

e− λk t

k=1

vk (x) vk (ξ) ρ(ξ)
,
 vk 2

(9.128)

where the denominator denotes the appropriately weighted L2 norm of the eigenfunction:
 b
 vk  =
2

vk (x)2 ρ(x) dx.
a

As with the one-dimensional heat equation, if the equilibrium operator is positive definite, S > 0, then all the eigenvalues are strictly positive, and hence, generically, solutions
decay to 0 at the exponential rate prescribed by the smallest eigenvalue, which can be
characterized as the minimum value of the Rayleigh quotient. On the other hand, if S
is only positive semi-deﬁnite, then the solution will tend to a null eigenmode, that is, an
element of ker S = ker L, as its asymptotic equilibrium state. If dim ker S = p, the ﬁrst p
eigenvalues are all 0 = λ1 = · · · = λp < λp+1 , and the solution
u(t, x) −→

p


ck vk (x)

as

t −→ ∞

k=1

will tend to its eventual equilibrium conﬁguration at an exponential rate determined by
the smallest positive eigenvalue λp+1 > 0. In almost all applications, p = 1 and there is a
single, constant null eigenfunction. The Neumann and periodic boundary value problems
for the heat equation are prototypical examples.

388

9 A General Framework for Linear Partial Diﬀerential Equations

Exercises
9.5.1. Find the eigenfunction series of the fundamental solution for the heat equation
ut = γ uxx on the interval 0 ≤ x ≤ 1 subject to homogeneous Dirichlet boundary conditions.
9.5.2. Solve Exercise 9.5.1 for (a) the mixed boundary conditions u(t, 0) = ux (t, 1) = 0;
(b) homogeneous Neumann boundary conditions.
9.5.3. Let D[ u ] = u be the derivative operator acting on the vector space of C1 scalar functions u(x) deﬁned for 0 ≤ x ≤ 1 and satisfying the boundary conditions u(0) = u (1) = 0.
(a) Given the L2 inner product on its domain space and the weighted inner product
1
 v , v  =
v(x) v(x) x dx on its target space, determine the adjoint operator D∗ .
0

(b) Let S = D∗ ◦ D. Write out the diﬀusion equation ut = − S[ u ] explicitly, as a partial
diﬀerential equation plus boundary conditions.
(c) Given the initial condition u(0, x) = x − x2 , what is the asymptotic equilibrium
u (x) = lim u(t, x) of the resulting solution to the diﬀusion equation?
t→∞

9.5.4. Write down an eigenfunction series for the solution u(t, x) to the initial value problem
u(0, x) = f (x) for the fourth-order evolution equation ut = − uxxxx subject to the boundary
conditions u(t, 0) = uxx (t, 0) = u(t, 1) = uxx (t, 1) = 0. Does your solution tend to an
equilibrium state? If so, at what rate?
9.5.5. Answer Exercise 9.5.4 for the boundary conditions
ux (t, 0) = uxxx (t, 0) = ux (t, 1) = uxxx (t, 1) = 0.
♦ 9.5.6. Explain how to solve the forced diﬀusion equation ut = − S[ u ] + f , subject to homogeneous boundary conditions, when f (x) does not depend on time t. Does the solution tend
to equilibrium as t → ∞? If so, what is the rate of decay, and what is the equilibrium?
9.5.7. Show that if u(t, x) solves the diﬀusion equation (9.122), then  u(t, · )  ≥  u(s, · ) 
whenever t ≤ s.
♦ 9.5.8. Let S > 0 be a positive deﬁnite operator. Suppose F (t, x; ξ) is the fundamental
solution for the diﬀusion equation (9.122). Prove that G(x; ξ) =

∞

0

F (t, x; ξ) dt is

the Green’s function for the corresponding equilibrium equation S[ u ] = f .

Vibration Equations
The second important class of dynamical systems comprises the second-order (in time)
vibration equations
∂2u
= − S[ u ],
(9.129)
∂t2
which we initially analyze in the absence of external forcing. Vibrational systems arise as
a consequence of Newton’s equations of motion in the absence of frictional forces. Their
continuum versions model the propagation of waves in solids and ﬂuids, electromagnetic
waves, plasma waves, and many other related physical systems.
For a general vibration equation, the separable solutions are of trigonometric form
u(t, x) = cos(ω t) v(x)

or

sin(ω t) v(x).

(9.130)

9.5 A General Framework for Dynamics

389

Substituting either ansatz back into (9.129) results in the same eigenvalue problem (9.124)
for v(x) with eigenvalue λ = ω 2 equal to the square of the vibrational frequency. We
conclude that the normal modes or eigensolutions take the form
u
k (t, x) = sin(ωk t) vk (x),

uk (t, x) = cos(ωk t) vk (x),

provided λk = ωk2 > 0 is a nonzero eigenvalue and vk an associated eigenfunction. Thus, the
natural vibrational frequencies of the system are the square roots of the nonzero eigenvalues,
a fact that we already observed in the context of the one-dimensional wave equation.
In the positive deﬁnite case, the eigenvalues are all strictly positive, and so the general
solution is built up as a linear combination of vibrational eigenmodes:
u(t, x) =
=

∞


k=1
∞




k (t, x)
ck uk (t, x) + dk u




ck cos(ωk t) + dk sin(ωk t) vk (x) =

k=1

∞


(9.131)
rk cos(ωk t − δk ) vk ,

k=1

where (rk , δk ) are the polar coordinates of (ck , dk ):
ck = rk cos δk ,

dk = rk sin δk .

(9.132)

The initial conditions
g(x) = u(0, x) =

∞


ck vk (x),

h(x) = ut (0, x) =

k=1

∞


dk ωk vk (x),

(9.133)

k=1

are used to specify the coeﬃcients:
ck =

 g , vk 
,
 vk 2

dk =

 h , vk 
.
ωk  vk 2

(9.134)

In the unstable, positive semi-deﬁnite cases, any null eigenfunction v0 ∈ ker S = ker L
contributes two aperiodic eigensolutions:
u0 (t, x) = v0 (x),

u
0 (t, x) = t v0 (x),

as can be readily checked. The ﬁrst is constant in time, while the second is an unstable,
linearly growing mode, which is excited if and only if the initial velocity is not orthogonal
to the null eigenfunction:  h , v0  = 0.
If, as occurred in the one-dimensional wave equation, the natural frequencies happen
to be integer multiples of a common frequency, ωk = nk ω for nk ∈ N, then the solution
(9.131) is a periodic function of t with period p = 2 π/ω . On the other hand, in most
cases the frequencies are not rationally related, and the solution is only quasiperiodic.
Although it is the sum of individually periodic modes, it is not periodic, and never exactly
reproduces its initial behavior; see the illustrative Example 2.20 for additional details.
Forcing and Resonance
Periodically forcing an undamped mechanical structure, modeled by a vibrational system
of ordinary diﬀerential equations, at a frequency that is distinct from its natural vibrational
frequencies, leads, in general, to a quasiperiodic response. The solution is a sum of the

390

9 A General Framework for Linear Partial Diﬀerential Equations

unforced vibrational modes superimposed with an additional component that vibrates at
the forcing frequency. However, if forced at one of its natural frequencies, the system may
experience a catastrophic resonance. See [89; §9.6] for details.
The same type of quasiperiodic/resonant response is also observed in the partial differential equations governing the vibrations of continuous media. Consider the forced
vibrational equation
∂2u
= − S[ u ] + F (t, x),
(9.135)
∂t2
subject to speciﬁed homogeneous boundary conditions. The external forcing function
F (t, x) may depend on both time t and position x. We will be particularly interested
in a periodically varying external force of the form
F (t, x) = cos(ω t) h(x),

(9.136)

where ω is the forcing frequency, while the forcing proﬁle h(x) is unvarying.
As always, the solution to an inhomogeneous linear equation can be written as a
combination,
(9.137)
u(t, x) = u (t, x) + z(t, x),
of a particular solution u (t, x) to the inhomogeneous forced equation combined with the
general solution z(t, x) to the homogeneous equation, namely
∂2z
= − S[ z ].
(9.138)
∂t2
The boundary and initial conditions will serve to uniquely prescribe the solution u(t, x),
but there is some ﬂexibility in its two constituents (9.137). For instance, we may ask
that the particular solution u satisfy the homogeneous boundary conditions along with
zero (homogeneous) initial conditions and thus represent the pure response of the system
to the forcing. The homogeneous solution z(t, x) will then reﬂect the eﬀect of the initial
and boundary conditions unadulterated by the external forcing. The ﬁnal solution is the
combined sum of the two individual responses.
In the case of periodic forcing (9.136), we look for a particular solution
u (t, x) = cos(ω t) v (x)

(9.139)

that vibrates at the forcing frequency. Substituting the ansatz (9.139) into the equation (9.135) and canceling the common cosine factors, we discover that v (x) must satisfy
the boundary value problem prescribed by a forced diﬀerential equation
S[ v ] − ω 2 v = h(x),

(9.140)

supplemented by the relevant homogeneous boundary conditions: Dirichlet, Neumann,
mixed, or periodic.
At this juncture, there are two possibilities. If the unforced homogeneous boundary
value problem
S[ v ] − ω 2 v = 0
(9.141)
has only the trivial solution v ≡ 0, then, according to the Fredholm Alternative Theorem 9.10, a solution to the forced boundary value problem will exist† for any form of the
†
Existence is immediate in ﬁnite-dimensional systems. For boundary value problems, this
relies on an analytic existence theorem, e.g., Theorem 9.28.

9.5 A General Framework for Dynamics

391

forcing function h(x). In other words, if ω 2 is not an eigenvalue, then the particular solution (9.139) will vibrate with the forcing frequency, and the general solution will be a
periodic or quasiperiodic combination (9.137) of the natural vibrational modes along with
the vibrational response to the periodic forcing.
On the other hand, if ω 2 = λk is an eigenvalue, and so ω = ωk coincides with one of the
natural vibrational frequencies of the homogeneous problem, then (9.141) admits nontrivial
solutions, namely the eigenfunction† vk (x). In such cases, the Fredholm Alternative tells
us that the boundary value problem (9.140) admits a solution if and only if the forcing
function is orthogonal to the eigenfunction:
 h , vk  = 0.

(9.142)

If this holds, then the resulting particular solution (9.139) still vibrates with the forcing
frequency, and resonance doesn’t occur.
If we force in a resonant manner — meaning that the Fredholm condition (9.142) does
not hold — then the solution will be a resonantly growing vibration of the form
u (t, x) = a t sin(ωk t) vk (x) + cos(ωk t) v (x),

(9.143)

in which a is a constant to be speciﬁed as follows. By direct calculation,

∂ 2 u
+ S[ u ] = a t sin(ωk t) S[ vk ] − ωk2 vk (x)
2
∂t

+ cos(ωk t) S[ v ] − ωk2 v (x) + 2 a ωk vk (x) .
The ﬁrst term vanishes, since vk (x) is an eigenfunction with eigenvalue λk = ωk2 . Therefore,
(9.143) satisﬁes the forced boundary value problem if and only if v (x) satisﬁes the forced
boundary value problem
S[ v ] − ωk2 v (x) = h(x) − 2 a ωk vk (x).

(9.144)

Again, the Fredholm Alternative implies that (9.144) admits a solution v (x) if and only if
0 =  h − 2 a ωk vk , vk  =  h , vk −2 a ωk  vk 2 ,

and hence

a=

 h , vk 
, (9.145)
2 ωk  vk 2

which serves to ﬁx the value of the constant in the resonant solution ansatz (9.143).
In a real-world situation, such large resonant (or even near resonant) vibrations will, if
unchecked, eventually either leads to a catastrophic breakdown of the system or to a transition into the nonlinear regime.
Example 9.51. As a speciﬁc example, consider the initial-boundary value problem
modeling the forced vibrations of a uniform string of unit length that is ﬁxed at both ends:
utt = c2 uxx + cos(ω t) h(x),
u(t, 0) = 0 = u(t, 1),

u(0, x) = f (x),

ut (0, x) = g(x).

(9.146)

†
For simplicity, we assume that the eigenvalue λk is simple, and so there is a unique, up to
constant multiple, eigenfunction vk . Modiﬁcations for multiple eigenvalues proceed analogously.

392

9 A General Framework for Linear Partial Diﬀerential Equations

The particular solution u (t, x) will have the nonresonant form (9.139), provided there
exists a solution v (x) to the boundary value problem
S[ v ] − ω 2 v = − c2 v − ω 2 v = h(x),

v (0) = 0 = v (1).

(9.147)

The natural frequencies and associated eigenfunctions of the unforced Dirichlet boundary
value problem are
ωk = k c π,

vk (x) = sin k πx,

k = 1, 2, 3, . . . .

Thus, the boundary value problem (9.147) will admit a solution, and hence the forcing is
not resonant, if either ω = ωk is not a natural frequency or ω = ωk for some k but the
forcing proﬁle is orthogonal to the associated eigenfunction:
0 =  h , vk  =

 1
h(x) sin k πx dx.

(9.148)

0

Otherwise, the system will undergo a resonant response.
For example, under periodic forcing of frequency ω with trigonometric sine proﬁle
h(x) ≡ sin k πx, for k a positive integer, the particular solution to (9.147) is
v (x) =

sin k πx
,
k 2 π 2 c2 − ω 2

so that

u (t, x) =

cos ω t sin k πx
,
k 2 π 2 c2 − ω 2

(9.149)

which is valid provided ω = ωk = k πc. Observe that we may allow the forcing frequency
to coincide with any of the other natural frequencies, ω = ωn for n = k, because the sine
proﬁles are mutually orthogonal, and so the nonresonance condition (9.148) holds. On the
other hand, if ω = ωk = k πc, then the particular solution
u (t, x) =

t sin k πc t sin k πx
2 k πc

(9.150)

is resonant and grows linearly in time.
To obtain the full solution to the initial-boundary value problem, we write u = u + z,
where z(t, x) must satisfy
ztt − c2 zxx = 0,

z(t, 0) = 0 = z(t, 1),

along with the modiﬁed initial conditions
z(0, x) = f (x) −

sin k πx
k 2 π 2 c2 − ω 2

,

∂z
(0, x) = g(x),
∂t

stemming from the fact that the particular solution (9.149) has a nontrivial initial displacement. (In the resonant case (9.150), there is no extra term in the initial data.) Note that
the closer ω is to the resonant frequency, the larger the modiﬁcation of the initial data,
and hence the larger the response of the system to the periodic forcing. As before, the
solution z(t, x) to the homogeneous equation can be written as a Fourier sine series (4.68).
The ﬁnal formulas are left for the reader to write out in detail; see Exercise 9.5.14.

9.5 A General Framework for Dynamics

393

Exercises
9.5.9. Which of the following forcing functions F (t, x) excites resonance in the wave equation
utt = uxx + F (t, x) when subject to homogeneous Dirichlet boundary conditions on the
interval 0 ≤ x ≤ 1? (a) sin 3 t, (b) sin 3 πt, (c) sin 32 πt, (d) sin πt sin πx,
(e) sin πt sin 2 πx, (f ) sin 2 πt cos πx, (g) x (1 − x) sin 2 πt.
9.5.10. Answer Exercise 9.5.9 when the solution is subject to the mixed boundary conditions
u(t, 0) = ux (t, 1) = 0.
♥ 9.5.11. Let ω > 0. Find the solution to the initial-boundary value problem
utt = uxx + cos ω t, u(t, 0) = 0 = u(t, 1), u(0, x) = 0 = ut (0, x).
9.5.12. Answer Exercise 9.5.11 for homogeneous Neumann boundary conditions.
9.5.13. A piano wire of length 1 m and wave speed c = 2 m/sec can support a maximal
deﬂection of 5 cm before breaking. Suppose the wire starts at rest, with both ends ﬁxed,
1
and then is subject to a uniform periodic force F (t, x) = 10
cos ω t sin πx. What range of
frequencies will cause the wire to break?
♦ 9.5.14. Write out the eigenfunction series solution to the initial-boundary value problem in
Example 9.51 with h(x) = sin k πx.
9.5.15. How should the solution formulas (9.131, 134) be modiﬁed when there are unstable
modes? Write down explicit conditions on the initial data that prevent an instability from
being excited.
♦ 9.5.16. Explain how to convert the homogeneous wave equation with inhomogeneous Dirichlet
boundary conditions u(t, 0) = α(t), u(t, ) = β(t), into a homogeneous boundary value
problem for the forced wave equation. Hint: Mimic (4.46).
♥ 9.5.17. Two children hold a jump rope taut, while one of them periodically shakes their end of
the rope. Use the inhomogeneous boundary value problem
∂2u
∂2u
=
,
u(t, 0) = 0,
u(t, 1) = sin ω t,
2
∂t
∂x2
to model the motion of the rope, adopting units in which the wave speed c = 1.
(a) What are the resonant frequencies of this system?
(b) Apply the method of Exercise 9.5.16 to ﬁnd a particular solution to the boundary value
problem when ω is a nonresonant frequency.
(c) Suppose the rope starts at rest. Find a series solution to the corresponding initialboundary value problem when ω is a nonresonant frequency.
(d) Answer parts (b,c) when ω is a resonant frequency. Hint: Use the ansatz (9.143).
9.5.18. Explain how to solve the periodically forced telegrapher’s equation
utt + a ut = c2 uxx + h(x) cos ω t
on the interval 0 ≤ x ≤ 1 when subject to homogeneous Dirichlet boundary conditions.
At which frequencies does the forcing function excite a resonant response?
Hint: First solve Exercise 4.2.9.
9.5.19. The fourth-order evolution equation utt = − c2 uxxxx , subject to the boundary
conditions u(t, 0) = uxx (t, 0) = u(t, 1) = uxx (t, 1) = 0, models the transverse vibrations
of a simply supported uniform thin elastic beam, in which c > 0 represents the wave speed.
Write down an eigenfunction series for the solution to the initial value problem
u(0, x) = f (x), ut (0, x) = 0. Is the solution
(i ) periodic, (ii ) quasiperiodic, (iii ) chaotic, (iv ) none of the above?

394

9 A General Framework for Linear Partial Diﬀerential Equations

The Schrödinger Equation
The fundamental dynamical system that governs all quantum-mechanical systems is known
as the Schrödinger equation, ﬁrst written down by the great twentieth-century German
physicist Erwin Schrödinger, one of the preeminent founders of modern quantum physics.
His original series of papers in which, by ﬁts and starts, he arrives at his fundamental
equation, makes for fascinating reading, [101].
Unlike classical mechanics, quantum mechanics is a completely linear theory, governed by linear systems of partial diﬀerential equations. The abstract form of the linear
Schrödinger equation is
∂ψ
= S[ ψ ],
(9.151)
i
∂t
where√ S is a linear operator of the usual self-adjoint form (9.121). In this equation,
i = −1, while  is Planck’s constant (7.69). The operator S is known as the Hamiltonian for the quantum-mechanical system, and, typically, represents the quantum energy
operator. For physical systems such as atoms and nuclei, the relevant Hamiltonian operator is constructed from the classical energy through the rather mysterious process of
“quantization”.
At each time t, the solution ψ(t, x) to the Schrödinger equation represents the wave
function of the quantum system, and so should be a complex-valued square-integrable
function having unit L2 norm:  ψ  = 1. (The reader may wish to revisit Sections 3.5
and 7.1 for a discussion of the basics of quantum mechanics and Hilbert space.) We
interpret the wave function as a probability density on the possible quantum states, and so
the Schrödinger equation governs the dynamical evolution of quantum probabilities. The
interested reader should consult a basic text on quantum mechanics, e.g., [66, 72, 115],
for full details on both the physics and underlying mathematics.
Proposition 9.52. If ψ(t, x) is a solution to the Schrödinger equation, its Hermitian
L2 norm  ψ(t, ·)  is ﬁxed for all time.
Proof : Since the solution is complex-valued, we use the sesquilinearity of the underlying Hermitian inner product, as in (B.19), to compute
d
 ψ(t, ·) 2 =
dt

4

∂ψ
,ψ
∂t

5

4
+

ψ,

∂ψ
∂t

5

4

i
S[ ψ ] , ψ


5

4

5

−

=−

i
i
 S[ ψ ] , ψ  +  ψ , S[ ψ ]  = 0,



+

ψ, −

i
S[ ψ ]


=

which vanishes because S is self-adjoint. This implies that  ψ(t, ·) 2 is constant. Q.E.D.
As a result, if the initial data ψ(t0 , x) = ψ0 (x) is a quantum-mechanical wave function,
meaning that  ψ0  = 1, then, at each time t, the solution ψ(t, x) to the Schrödinger
equation also has norm 1, and hence remains a wave function throughout the evolutionary
process.
Apart from the extra factor of i , the Schrödinger equation looks like a diﬀusion
equation (9.122). This inspires us to seek separable solutions with an exponential ansatz:
ψ(t, x) = eα t v(x).

9.5 A General Framework for Dynamics

395

Substituting this expression into the Schrödinger equation (9.151) and canceling the common exponential factors reduces us to the usual eigenvalue problem
S[ v ] = λ v,

λ = i  α.

with eigenvalue

By self-adjointness, the eigenvalues are necessarily real. Let vk denote the normalized
eigenfunction, so  vk  = 1, associated with the k th eigenvalue λk . The corresponding
eigensolution of the Schrödinger equation is the complex-valued function
ψk (t, x) = e− i λk t/ vk (x).
Observe that, in contrast to the exponentially decaying solutions to the diﬀusion equation,
the eigensolutions to the Schrödinger equation are periodic, with vibrational frequencies
ωk = − λk / proportional to the eigenvalues. (Along with constant solutions corresponding
to the null eigenmodes, if any.) The general solution is a (quasi)periodic series in the
fundamental eigensolutions,


ψ(t, x) =
ck ψk (t, x) =
ck e− i λk t/ vk (x),
(9.152)
k

k

whose coeﬃcients are prescribed by the initial conditions. The periodicity of the summands
has the additional implication that, again unlike the diﬀusion equation, the Schrödinger
equation can be run backwards in time, i.e., it remains well-posed in the past. Consequently,
we can determine both the past and future behavior of a quantum system from its present
conﬁguration.
The eigenvalues represent the energy levels of the system described by the Schrödinger
equation and can be experimentally detected by exciting the system. For instance, when
an excited electron orbiting a nucleus jumps back to a lower energy level, it emits a photon
whose observed electromagnetic spectral line corresponds to the diﬀerence between the
energies of the two quantum levels. This motivates the use of the term spectrum to describe
the eigenvalues of a linear Hamiltonian operator.
Example 9.53. The simplest version of the Schrödinger equation is based on the
derivative operator L = D, leading to the self-adjoint combination S = L∗ ◦ L = − D2
when subject to appropriate boundary conditions. In this case, the Schrödinger equation
(9.151) reduces to the second-order partial diﬀerential equation
i

∂ψ
∂2ψ
.
=−
∂t
∂x2

(9.153)

If we impose the Dirichlet boundary conditions ψ(t, 0) = ψ(t, ) = 0, then the Schrödinger
equation (9.153) governs the dynamics of a quantum particle that is conﬁned to the interval
0 < x < ; the boundary conditions imply that there is zero probability of the particle
escaping from the interval.
According to Section 4.1, the eigenfunctions of the Dirichlet eigenvalue problem
v  + λ v = 0,

v(0) = v() = 0,

are

vk (x) =

2
kπ
sin
x,



with eigenvalue

λk =

k2 π2
,
2

for k = 1, 2, . . . ,

396

9 A General Framework for Linear Partial Diﬀerential Equations

where the initial factor ensures that vk has unit L2 norm, and hence is a bona ﬁde wave
function. The corresponding oscillatory eigenmodes are

ψk (t, x) =

2
exp



−i

k2 π2
t
 2


sin

kπ
x.


(9.154)

Since the temporal frequencies ωk = − k 2 π 2 /( 2 ) depend nonlinearly on the wave number
k π/, the Schrödinger equation, is, in fact, dispersive, sharing many similarities with the
third-order linear equation (8.90); see, for instance, Exercises 9.5.25, 27.

Exercises
9.5.20. (a) Solve the following initial boundary value problem:
i  ψt = − ψxx ,

ψ(t, 0) = ψ(t, 1) = 0,

ψ(0, x) = 1.

(b) Using your solution formula, verify that  ψ(t, · )  = 1 for all t.
√
9.5.21. Answer Exercise 9.5.20 for the initial condition ψ(0, x) = 30 x(1 − x).
9.5.22. Answer Exercise 9.5.20 when the solution is subject to Neumann boundary conditions
ψx (t, 0) = ψx (t, 1) = 0.
9.5.23. Write down the eigenseries solution for the Schrödinger equation on a bounded interval
[ 0, ] when subject to homogeneous Neumann boundary conditions.
9.5.24. Given the solution formula (9.152), and assuming completeness of the eigenfunctions,
prove that  ψ(t, · ) 2 =
| ck |2 for all t.
k

♦ 9.5.25. Write down the dispersion relation, phase velocity, and group velocity for the one-dimensional Schrödinger equation (9.153).
9.5.26. Show that the real and imaginary parts of the solution ψ(t, x) = u(t, x) + i v(t, x) to the
one-dimensional Schrödinger equation (9.153) are solutions to the beam equation of Exercise 9.5.19. What is the wave speed?
♦ 9.5.27. The Talbot eﬀect for the linear Schrödinger equation: Let u(t, x) solve the periodic initialboundary value problem
i ut = uxx ,

u(t, − π) = u(t, π),

ux (t, − π) = ux (t, π),

with initial data u(0, x) = σ(x) given by the unit step function. Prove that when t = π p/q,
where p, q are integers, the solution u(t, x) is constant on each interval  π j/q < x <
 π(j + 1)/q for integers j ∈ Z. Hint: Use Exercise 6.1.29(d).
9.5.28. The wave function ψ(t, x) of a one-dimensional free quantum particle of mass m satisﬁes the Schrödinger equation i ψt = −  ψxx /(2 m) on the real line − ∞ < x < ∞. Assuming that ψ and its x derivatives decay reasonably rapidly to zero as | x | → ∞, prove that
the particle’s expected position  x  =
Hint: Prove that

d2  x 
= 0.
dt2

∞

−∞

x | ψ(t, x) |2 dx moves on a straight line.

9.5 A General Framework for Dynamics

397

♥ 9.5.29. Consider the periodically forced Schrödinger equation i  ψt = − ψxx + e i ω t on the
interval 0 ≤ x ≤ 1, subject to homogeneous Dirichlet boundary conditions. (a) At which
frequencies ω does the forcing function excite a resonant response? (b) Find the solution
to the general initial value problem for a nonresonant forcing frequency. (c) Find the solution to the general initial value problem for a resonant forcing frequency. What are the
conditions on  that ensure that the resulting solution remains a wave function?
♥ 9.5.30. The Schrödinger equation for the harmonic oscillator is i  ψt = ψxx − x2 ψ. Write this
equation in the self-adjoint form (9.151) under a suitable choice of boundary conditions.
Write down the self-adjoint boundary value problem for the eigenfunctions.
Remark : The eigenfunctions are not elementary functions. After studying Section 11.3, you
may wish to return here to investigate its solutions.

Chapter 10

Finite Elements and Weak Solutions

In Chapter 5, we studied the oldest, and in many ways the simplest, class of numerical
algorithms for approximating the solutions to partial diﬀerential equations: those based on
ﬁnite diﬀerence approximations. In the present chapter, we introduce the second of the two
major numerical paradigms: the ﬁnite element method. Finite elements are of more recent
vintage, having ﬁrst appeared soon after the Second World War; historical details can be
found in [113]. As a consequence of their ability to adapt to complicated geometries, ﬁnite
elements have, in many situations, become the method of choice for solving equilibrium
boundary value problems governed by elliptic partial diﬀerential equations. Finite elements
can also be adapted to dynamical problems, but lack of space prevents us from pursuing
such extensions in this text.
Finite elements rely on a more sophisticated understanding of the partial diﬀerential equation, in that, unlike ﬁnite diﬀerences, they are not obtained by simply replacing
derivatives by their numerical approximations. Rather, they are initially founded on an associated minimization principle that, as we learned in Chapter 9, characterizes the unique
solution to a positive deﬁnite boundary value problem. The basic idea is to restrict the
minimizing functional to an appropriately chosen ﬁnite-dimensional subspace of functions.
Such a restriction produces a ﬁnite-dimensional minimization problem, which can then
be solved by numerical linear algebra. When properly formulated, the restricted ﬁnitedimensional minimization problem will have a solution that well approximates the true
minimizer, and hence the solution to the original boundary value problem. To gain familiarity with the underlying principles, we will ﬁrst illustrate the basic constructions in
the context of boundary value problems for ordinary diﬀerential equations. The following
section extends ﬁnite element analysis to boundary value problems associated with the
two-dimensional Laplace and Poisson equations, thereby revealing the key features used
in applications to the numerical solution of multidimensional equilibrium boundary value
problems.
An alternative approach to the ﬁnite element method, one that can be applied even in
situations in which no minimum principle is available, is founded on the concept of a weak
solution to the diﬀerential equation, a construction of independent analytical importance.
The term “weak” refers to the fact that one is able to relax the diﬀerentiability requirements
imposed on classical solutions. Indeed, as we will show, discontinuous shock wave solutions
as well as the nonsmooth, and hence nonclassical, solutions to the wave equation that we
encountered in Chapters 2 and 4 can all be rigorously characterized through the weak
solution formulation. For the ﬁnite element approximation, rather than impose the weak
solution criterion on the entire inﬁnite-dimensional function space, one again restricts to a
P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_10, © Springer International Publishing Switzerland 2014

399

400

10 Finite Elements and Weak Solutions

suitably chosen ﬁnite-dimensional subspace. For positive deﬁnite boundary value problems,
which necessarily admit a minimization principle, the weak solution approach leads to the
same ﬁnite element equations.
A rigorous justiﬁcation and proof of convergence of the ﬁnite element approximations
requires further analysis, and we refer the interested reader to more specialized texts,
such as [6, 113, 126]. In this chapter, we shall focus our eﬀort on understanding how to
formulate and implement the ﬁnite element method in practical contexts.

10.1 Minimization and Finite Elements
To explain the principal ideas underpinning the ﬁnite element method, we return to the
abstract framework for boundary value problems that was developed in Chapter 9. Recall
Theorem 9.26, which characterizes the unique solution to a positive deﬁnite linear system
as the minimizer, u ∈ U , of an associated quadratic functional Q: U → R. For boundary
value problems governed by diﬀerential equations, U is an inﬁnite-dimensional function
space containing all suﬃciently smooth functions that satisfy the prescribed homogeneous
boundary conditions. (Modiﬁcations to deal with inhomogeneous boundary conditions will
be discussed in due course.)
This framework sets the stage for the ﬁrst key idea of the ﬁnite element method. Instead of trying to minimize the functional Q[ u ] over the entire inﬁnite-dimensional function
space, we will seek to minimize it over a ﬁnite-dimensional subspace W ⊂ U . The eﬀect
is to reduce a problem in analysis — a boundary value problem for a diﬀerential equation
— to a problem in linear algebra, and hence one that a computer is capable of solving.
On the surface, the idea seems crazy: how could one expect to come close to ﬁnding the
minimizer in a gigantic inﬁnite-dimensional function space by restricting the search to a
mere ﬁnite-dimensional subspace? But this is where the magic of inﬁnite dimensions comes
into play. One can, in fact, approximate all (reasonable) functions arbitrarily closely by
functions belonging to ﬁnite-dimensional subspaces. Indeed, you are already familiar with
two examples: Fourier series, where one approximates rather general periodic functions by
trigonometric polynomials, and interpolation theory, in which one approximates functions
by ordinary polynomials, or, more sophisticatedly, by splines, [89, 102]. Thus, the ﬁnite
element idea perhaps is not as outlandish as it might initially seem.
To be a bit more explicit, let us begin with a linear operator L: U → V between real
inner product spaces, where, as in Section 9.1,  u , u
  is used to denote the inner product
in U , and  v , v  the inner product in V . To ensure uniqueness of solutions, we always
assume that L has trivial kernel: ker L = {0}. According to Theorem 9.26, the element
u ∈ U that minimizes the quadratic function(al)
Q[ u ] = 12 | L[ u ] |2 −  f , u ,

(10.1)

where | · | denotes the norm in V , is the solution to the linear system
S[ u ] = f,

where

S = L∗ ◦ L,

(10.2)

with L∗ : V → U denoting the adjoint operator. The hypothesis that L has trivial kernel
implies that S is a self-adjoint positive deﬁnite linear operator, which implies that the
solution to (10.2), and hence the minimizer of Q[ u ], is unique. In our applications, L
is a linear diﬀerential operator between function spaces, e.g., the gradient, while Q[ u ]
represents a quadratic functional, e.g., the Dirichlet principle, and the associated linear

10.1 Minimization and Finite Elements

401

system (10.2) forms a positive deﬁnite boundary value problem, e.g., the Poisson equation
along with suitable boundary conditions.
To form a ﬁnite element approximation to the solution u ∈ U , rather than try to
minimize Q[ u ] on the entire function space U , we now seek to minimize it on a suitably
chosen ﬁnite-dimensional subspace W ⊂ U . We will specify W by selecting a set of linearly
independent functions ϕ1 , . . . , ϕn ∈ U , and letting W be their span. Thus, ϕ1 , . . . , ϕn form
a basis of W , whereby dim W = n, and the general element of W is a (uniquely determined)
linear combination
w(x) = c1 ϕ1 (x) + · · · + cn ϕn (x)
(10.3)
of the basis functions. Our goal is to minimize Q[ w ] over all possible w ∈ W ; in other
words, we need to determine the coeﬃcients c1 , . . . , cn ∈ R such that
Q[ w ] = Q[ c1 ϕ1 + · · · + cn ϕn ]

(10.4)

is as small as possible. Substituting (10.3) back into (10.1) and then expanding, using
the linearity of L and then the bilinearity of the inner product, we ﬁnd that the resulting
expression is the quadratic function
P (c) =

n
n

1 
kij ci cj −
bi ci = 12 cT K c − cT b,
2 i,j = 1
i=1

(10.5)

in which
T
• c = ( c1 , c2 , . . . , cn ) ∈ R n is the vector of unknown coeﬃcients in (10.3);
• K = (kij ) is the symmetric n × n matrix with entries
kij =  L[ ϕi ] , L[ ϕj ] ,

i, j = 1, . . . , n;

(10.6)

T

• b = ( b1 , b2 , . . . , bn ) is the vector with entries
bi =  f , ϕi ,

i = 1, . . . , n.

(10.7)

Note that formula (10.6) uses the inner product on the target space V , whereas (10.7)
relies on the inner product on the domain space U .
Thus, once we specify the basis functions ϕi , the coeﬃcients kij and bi are all known
quantities. We have eﬀectively reduced our original problem to the ﬁnite-dimensional
problem of minimizing the quadratic function (10.5) over all possible vectors c ∈ R n . The
symmetric matrix K is, in fact, positive deﬁnite, since, by the preceding computation,
cT K c =

n


kij ci cj = | L[ c1 ϕ1 (x) + · · · + cn ϕn ] |2 = | L[ w ] |2 > 0,

(10.8)

i,j = 1

as long as L[ w ] = 0. Moreover, our initial assumption tells us that L[ w ] = 0 if and
only if w = 0, which, by linear independence, occurs only when c = 0. Thus, (10.8) is
indeed positive for all c = 0. We can now invoke the ﬁnite-dimensional minimization result
contained in Example 9.25 to conclude that the unique minimizer to (10.5) is obtained by
solving the associated linear system
K c = b,

whereby

c = K −1 b.

(10.9)

402

10 Finite Elements and Weak Solutions

Remark : When of moderate size, the linear system (10.9) can be solved by basic
Gaussian Elimination. When the size (i.e., the dimension, n, of the subspace W ) becomes
too large, as is often the case in dealing with partial diﬀerential equations, it is better to
rely on an iterative linear system solver, e.g., Gauss–Seidel or Successive Over–Relaxation
(SOR); see [89, 118] for details.
This summarizes the basic abstract setting for the ﬁnite element method. The key
issue, then, is how to eﬀectively choose the ﬁnite-dimensional subspace W . Two candidates
that might spring to mind are the space of polynomials of degree ≤ n and the space of
trigonometric polynomials (truncated Fourier series) of degree ≤ n. However, for a variety
of reasons, neither is well suited to the ﬁnite element method. One constraint is that
the functions in W must satisfy the relevant boundary conditions — otherwise, W would
not be a subspace of U . More importantly, in order to obtain suﬃcient accuracy of the
approximate solution, the linear algebraic system (10.9) will typically — especially when
dealing with partial diﬀerential equations — be quite large, and hence it is desirable that
the coeﬃcient matrix K be as sparse as possible, i.e., have lots of zero entries. Otherwise,
computing the solution may well be too time-consuming to be of much practical value.
With this in mind, the second innovative contribution of the ﬁnite element method is to
ﬁrst (paradoxically) enlarge the space U of allowable functions upon which to minimize the
quadratic functional Q[ u ]. The governing diﬀerential equation requires its (classical) solutions to have a certain degree of smoothness, whereas the associated minimization principle
typically requires that they possess only half as many derivatives. Thus, for second-order
boundary value problems, the diﬀerential equation requires continuous second-order derivatives, while the quadratic functional Q[ u ] involves only ﬁrst-order derivatives. It fact, it
can be rigorously shown that, under rather mild hypotheses, the functional retains the
same minimizing solution, even when one allows functions that fail to qualify as classical
solutions to the diﬀerential equation. We will proceed to develop the method in the context
of particular, fairly elementary examples.

Exercises
10.1.1. Let U = { u(x) ∈ C2 [ 0, π ] | u(0) = u(π) = 0 } and V = { v(x) ∈ C1 [ 0, π ] } both
be equipped with the L2 inner product. Let L: U → V be given by L[ u ] = D[ u ] = u ,
and f (x) = x − 1. (a) Write out the quadratic functional Q[ u ] given by (10.1). (b) Write
out the associated boundary value problem (10.2). (c) Find the function u (x) ∈ U that
minimizes Q[ u ]. What is the value of Q[ u ]? (d) Let W ⊂ U be the subspace spanned
by sin x and sin 2 x. Write out the corresponding ﬁnite-dimensional minimization problem
(10.8). (e) Find the function w (x) ∈ W that minimizes Q[ w ]. Is Q[ w ] ≥ Q[ u ]? If not,
why not? How close is your ﬁnite element minimizer w (x) to the actual minimizer u (x)?
10.1.2. Let U = { u(x) ∈ C2 [ 0, 1 ] | u(0) = u(1) = 0 } and V = { v(x) ∈ C1 [ 0, 1 ] } both have
the L2 inner product. Let L: U → V be given by L[ u ] = u (x) − u(x), and f (x) = 1
for all x. (a) Write out the quadratic functional Q[ u ] given by (10.1). (b) Write out the
associated boundary value problem (10.2). (c) Find the function u (x) ∈ U that minimizes
Q[ u ]. What is the value of Q[ u ]? (d) Let W ⊂ U be the subspace containing all cubic
polynomials p(x) that satisfy the boundary conditions: p(0) = p(1) = 0. Find a basis of
W and then write out the corresponding ﬁnite-dimensional minimization problem (10.8).
(e) Find the polynomial p (x) ∈ W that minimizes Q[ p ] for p ∈ W . Is Q[ p ] ≥ Q[ u ]? If
not, why not? How close is your ﬁnite element minimizer p (x) to the minimizer u (x)?

10.2 Finite Elements for Ordinary Diﬀerential Equations

403

10.1.3. Let U = { u(x) ∈ C2 [ 1, 2 ] | u(1) = u(2) = 0 }, V = { (v1 (x), v2 (x))T | v1 , v2 ∈C1 [ 1, 2 ] },

x u (x)
2
√
,
both be endowed with the L inner product. Let L: U → V be given by L[ u ] =
2 u(x)
and let f (x) = 2 for all 1 ≤ x ≤ 2. (a) Write out the quadratic functional Q[ u ] given by
(10.1). (b) Write out the associated boundary value problem (10.2). (c) Find the function u (x) ∈ U that minimizes Q[ u ]. What is the value of Q[ u ]? (d) Let W ⊂ U be
the subspace containing all cubic polynomials p(x) that satisfy the boundary conditions
p(1) = p(2) = 0. Find a basis of W and then write out the corresponding ﬁnite-dimensional
minimization problem (10.8). (e) Find the polynomial p (x) ∈ W that minimizes Q[ p ] for
p ∈ W . Is Q[ p ] ≥ Q[ u ]? If not, why not? How close is your ﬁnite element minimizer
p (x) to the actual minimizer u (x)?
♥ 10.1.4. (a) Find the solution to the boundary value problem − u = x2 − x, u(−1) = u(1) = 0.
(b) Write down a quadratic functional Q[ u ] that is minimized by your solution.
(c) Let W be the subspace spanned by the two functions (1−x2 ), x (1−x2 ). Find the function w (x) ∈ W that minimizes the restriction of your quadratic functional to W . Compare
w with your solution from part (a). (d) Answer part (c) for the subspace W spanned by
sin π x, sin 2 π x. Which of the two approximations is the better?
♥ 10.1.5. (a) Find the function u (x) that minimizes Q[ u ] =
2

1
0


2
1
2 (x + 1) u (x) − u(x)

dx over

the vector space U consisting of C functions satisfying u(0) = u(1) = 0. (b) Let W3 ⊂ U
be the subspace consisting of all cubic polynomials w(x) that satisfy the same boundary
conditions. Find the function w (x) that minimizes the restriction Q[ w ] for w ∈ W3 .
Compare w (x) and u (x): how close are they in the L2 norm? What is the maximal
discrepancy | w (x) − u (x) | for 0 ≤ x ≤ 1? (c) Suppose you enlarge your ﬁnite-dimensional subspace W4 ⊂ U to contain all quartic polynomials that satisfy the boundary
conditions. Is your new ﬁnite element approximation better? Discuss.
♥ 10.1.6. (a) Find the function u (x) that minimizes Q[ u ] =
2

1
0

2
1 x 
2 e u (x) − 3 u(x)

dx over the


space U consisting of C functions satisfying the boundary conditions u(0) = u (1) = 0.
(b) Let W ⊂ U be the subspace containing all cubic polynomials w(x) that satisfy the
boundary conditions. Find the polynomial w (x) that minimizes the restriction Q[ w ] for
w ∈ W . Compare w (x) and u (x): how close are they in the L2 norm? What is the
maximal discrepancy | w (x) − u (x) | for 0 ≤ x ≤ 1?
10.1.7. Consider the Dirichlet boundary value problem
− Δu = x (1 − x) + y (1 − y),
u(x, 0) = u(x, 1) = u(0, y) = u(1, y) = 0,
on the unit square { 0 < x, y < 1 }.
(a) Find the exact solution u (x, y). Hint: It is a polynomial.
(b) Write down a minimization principle Q[ u ] that characterizes the solution. Be careful to
specify the function space U over which the minimization takes place.
(c) Let W ⊂ U be the subspace spanned by the four functions sin π x sin π y, sin 2 π x sin π y,
sin π x sin 2 π y, and sin 2 π x sin 2 π y. Find the function w ∈ W that minimizes the
restriction of Q[ w ] to w ∈ W . How close is w to the solution you found in part (a)?
♦ 10.1.8. Justify the identiﬁcation of (10.4) with the quadratic function (10.5).

10.2 Finite Elements for Ordinary Diﬀerential Equations
To understand the preceding abstract formulation in concrete terms, let us focus our attention on boundary value problems governed by a second-order ordinary diﬀerential equation.

404

10 Finite Elements and Weak Solutions

Figure 10.1.

A continuous piecewise aﬃne function.

For example, we might be interested in solving a Sturm–Liouville problem (9.71) subject
to, say, homogeneous Dirichlet boundary conditions. Once we understand how the ﬁnite
element constructions work in this relatively simple context, we will be in a good position
to extend the techniques to much more general linear boundary value problems governed
by elliptic partial diﬀerential equations.
For such one-dimensional boundary value problems, a popular and eﬀective choice
of the ﬁnite-dimensional subspace W is to employ continuous, piecewise aﬃne functions.
Recall that a function is aﬃne if its graph is a straight line: f (x) = a x + b. (The function
is linear , in accordance with Deﬁnition B.32, if and only if b = 0.) A function is called
piecewise aﬃne if its graph consists of a ﬁnite number of straight line segments; a typical
example is plotted in Figure 10.1. Continuity requires that the individual segments be
connected together end to end.
Given a boundary value problem on a bounded interval [ a, b ], let us ﬁx a ﬁnite collection of nodes
a = x0 < x1 < x2 < · · · < xn−1 < xn = b.
The formulas simplify if one uses equally spaced nodes, but this is not necessary for the
construction to be carried out. Let W denote the vector space consisting of all continuous functions w(x) that are deﬁned on the interval a ≤ x ≤ b, satisfy the homogeneous
boundary conditions, and are aﬃne when restricted to each subinterval [ xj , xj+1 ]. On each
subinterval, we write
w(x) = cj + bj (x − xj ),

for

xj ≤ x ≤ xj+1,

j = 0, . . . , n − 1,

for certain constants cj , bj . Continuity of w(x) requires
−
cj = w(x+
j ) = w(xj ) = cj−1 + bj−1 hj−1 ,

j = 1, . . . , n − 1,

(10.10)

where hj−1 = xj − xj−1 denotes the length of the j th subinterval. The homogeneous
Dirichlet boundary conditions at the endpoints require
w(a) = c0 = 0,

w(b) = cn−1 + bn−1 hn−1 = 0.

(10.11)

Observe that the function w(x) involves a total of 2 n unspeciﬁed coeﬃcients c0 , . . . , cn−1 ,
b0 , . . . , bn−1 . The continuity conditions (10.10) and the second boundary condition (10.11)
uniquely determine the bj . The ﬁrst boundary condition speciﬁes c0 , while the remaining
n − 1 coeﬃcients c1 = w(x1 ), . . . , cn−1 = w(xn−1 ) are arbitrary, specifying the values of
w(x) at the interior nodes. We conclude that the ﬁnite element subspace W has dimension
n − 1, the number of interior nodes.

10.2 Finite Elements for Ordinary Diﬀerential Equations

405

1

1
2
3
4
5
Figure 10.2. A hat function.

6

7

Remark : Every function w(x) in our subspace has piecewise constant ﬁrst derivative
w (x). However, the jump discontinuities in w (x) imply that its second derivative w (x)
may well include delta function impulses at the nodes, and hence w(x) is far from being a
solution to the diﬀerential equation. Nevertheless, in practice, the ﬁnite element minimizer
w (x) ∈ W will (under suitable assumptions) provide a reasonable approximation to the
actual solution u (x).
The most convenient basis for W consists of the hat functions, which are continuous,
piecewise aﬃne functions satisfying

1,
j = k,
ϕj (xk ) =
for
j = 1, . . . , n − 1,
k = 0, . . . , n.
(10.12)
0,
j = k,
The graph of a typical hat function appears in Figure 10.2. The explicit formula is easily
established:
⎧ x−x
j−1
⎪
⎪
,
xj−1 ≤ x ≤ xj ,
⎪
⎪
x
−
x
⎪
j−1
⎨ j
xj+1 − x
ϕj (x) =
j = 1, . . . , n − 1.
(10.13)
,
xj ≤ x ≤ xj+1 ,
⎪
⎪
⎪
⎪ xj+1 − xj
⎪
⎩
0,
x ≤ xj−1 or x ≥ xj+1 ,
One advantage of using these basis functions is that, thanks to (10.12), the coeﬃcients in
the linear combination
w(x) = c1 ϕ1 (x) + · · · + cn ϕn (x)
coincide with its values at the nodes:
cj = w(xj ),

j = 1, . . . , n.

(10.14)

Example 10.1. Let κ(x) > 0 for 0 ≤ x ≤ . Consider the equilibrium equations


d
du
S[ u ] = −
κ(x)
= f (x),
0 < x < ,
u(0) = u() = 0,
dx
dx
for a nonuniform bar with ﬁxed ends and variable stiﬀness κ(x), that is subject to an
external forcing f (x). In order to ﬁnd a ﬁnite element approximation to the resulting

406

10 Finite Elements and Weak Solutions

displacement u(x), we begin with the minimization principle based on the quadratic functional
 

1

2
Q[ u ] =
2 κ(x) u (x) − f (x) u(x) dx,
0

which is a special case of (9.75); see also Exercise 9.3.22. We divide the interval [ 0, ] into
n equal subintervals, each of length h = /n. The resulting uniform mesh has nodes
j
,
n

xj = j h =

j = 0, . . . , n.

The corresponding ﬁnite element basis hat functions are explicitly given by
⎧
xj−1 ≤ x ≤ xj ,
⎨ (x − xj−1 )/h,
ϕj (x) =
j = 1, . . . , n − 1.
− x)/h,
xj ≤ x ≤ xj+1 ,
(x
⎩ j+1
0,
otherwise,

(10.15)

The associated linear system (10.9) has coeﬃcient matrix entries
kij =  ϕi , ϕj  =

 
0

ϕi (x) ϕj (x) κ(x) dx,

i, j = 1, . . . , n − 1.

Since the function ϕi (x) vanishes except on the interval xi−1 < x < xi+1 , while ϕj (x)
vanishes outside xj−1 < x < xj+1 , the integral will vanish unless i = j or i = j ± 1.
Moreover,
⎧
xj−1 ≤ x ≤ xj ,
⎨ 1/h,

ϕi (x) =
j = 1, . . . , n − 1.
− 1/h,
xj ≤ x ≤ xj+1 ,
⎩
0,
otherwise,
Therefore, the ﬁnite element coeﬃcient matrix assumes the tridiagonal form
⎛s + s
0

⎜ − s1
⎜
1 ⎜
K= 2 ⎜
h ⎜
⎜
⎝

1

− s1
s1 + s2
− s2

⎞

..

− s2
s2 + s3
.

..

− sn−3

− s3

..
.
.
sn−3 + sn−2
− sn−2
− sn−2 sn−2 + sn−1

⎟
⎟
⎟
⎟,
⎟
⎟
⎠

(10.16)

 xj+1

where
sj =

κ(x) dx
xj

(10.17)

is the total stiﬀness of the j th subinterval. The corresponding right-hand side has entries
 
bj =  f , ϕj  =
f (x) ϕj (x) dx
0


(10.18)
 xj+1
xj
1
(x − xj−1 )f (x) dx +
(xj+1 − x)f (x) dx .
=
h
xj−1
xj
In practice, we do not have to explicitly evaluate the integrals (10.17, 18), but may replace
them by suitably close numerical approximations. When the step size h  1 is small, then

10.2 Finite Elements for Ordinary Diﬀerential Equations

407

the integrals are taken over small intervals, and so the elementary trapezoid rule, [24, 108],
produces suﬃciently accurate approximations:
h
(10.19)
[ κ(xj ) + κ(xj+1 ) ],
bj ≈ h f (xj ).
2
The resulting ﬁnite element system K c = b is then solved for c, whose entries, according
to (10.14), coincide with the values of the ﬁnite element approximation to the solution at
the nodes: cj = w(xj ) ≈ u(xj ). Indeed, the tridiagonal Gaussian Elimination algorithm,
[89], will rapidly produce the desired solution. Since the accuracy of the ﬁnite element
solution increases with the number of nodes, this numerical scheme allows us to easily
compute very accurate approximations to the solution to the boundary value problem.
In particular, in the homogeneous case κ(x) ≡ 1, the coeﬃcient matrix (10.16) reduces
to the special form
⎛
⎞
2 −1
⎜ −1
⎟
2 −1
⎜
⎟
⎜
⎟
−1
2
−1
1⎜
⎟
..
.. ..
K= ⎜
(10.20)
⎟.
.
. .
⎟
h⎜
⎜
⎟
⎝
−1
2 −1 ⎠
−1
2
sj ≈

In this case, the j th equation in the ﬁnite element linear system is, upon dividing by h,
−

cj+1 − 2 cj + cj−1
= f (xj ).
h2

(10.21)

Since cj ≈ u(xj ), the left-hand side coincides with the standard ﬁnite diﬀerence approximation to minus the second derivative − u (xj ) at the node xj , cf. (5.5). As a result,
in this particular case the ﬁnite element and ﬁnite diﬀerence numerical solution schemes
happen to coincide.
The sparse tridiagonal nature of the ﬁnite element matrix is a consequence of the
fact that the basis functions are zero on much of the interval, or, in more mathematical
language, that they have small support, in the following sense.
Deﬁnition 10.2. The support of a function f (x), written supp f , is the closure of
the set where f (x) = 0.
Thus, a point x will belong to the support, provided f is not zero there, or at least
is not zero at nearby points. For example, the support of the hat function (10.13) is the
(small) interval [ xj−1 , xj+1 ]. The key property, ensuring sparseness, is that the integral
of the product of two functions will be zero if their supports have empty intersection, or,
slightly more generally, have only a ﬁnite number of points in common.
Example 10.3. Consider the boundary value problem
du
d
(x + 1)
= 1,
u(0) = 0,
dx
dx
The explicit solution is easily found by direct integration:
−

u(x) = − x +

log(x + 1)
.
log 2

u(1) = 0.

(10.22)

(10.23)

408

10 Finite Elements and Weak Solutions

Finite element solution to (10.22).

Figure 10.3.

It minimizes the associated quadratic functional

Q[ u ] =
0

1


2
2 (x + 1) u (x) − u(x)



dx

(10.24)

over the space of all C2 functions u(x) that satisfy the given boundary conditions. The
ﬁnite element system (10.9) has coeﬃcient matrix given by (10.16) and right-hand side
(10.18), where
 xj+1
sj =
xj

(1 + x) dx = h (1 + xj ) + 12 h2 = h + h2 j + 12 ,

 xj+1
bj =

1 dx = h.
xj

The resulting piecewise aﬃne approximation to the solution is plotted in Figure 10.3. The
ﬁrst three graphs contain, respectively, 5, 10, 20 nodes, so that h = .2, .1, .05, while the
last plots the exact solution (10.23). The maximal errors at the nodes are, respectively,
.000298, .000075, .000019, while the maximal overall errors between the exact solution and
its piecewise aﬃne ﬁnite element approximations are .00611, .00166, .00043. (One can more
closely ﬁt the solution curve by employing a cubic spline to interpolate the computed nodal
values, [89, 102], which has the eﬀect of reducing the preceding maximal overall errors by
a factor of, approximately, 20.) Thus, even when computed on rather coarse meshes, the
ﬁnite element approximation gives quite respectable results.
Remark : One can obtain a smoother, and hence more realistic, approximation to the
solution by smoothly interpolating the ﬁnite element approximations cj ≈ u(xj ) at the
nodes, e.g., by use of cubic splines, [89, 102]. Alternatively, one can require that the ﬁnite
element functions themselves be smoother, e.g., by making the ﬁnite element subspace
consist of piecewise cubic splines that satisfy the boundary conditions.

10.2 Finite Elements for Ordinary Diﬀerential Equations

409

Exercises
♣ 10.2.1. Use the ﬁnite
 element
 method to approximate the solution to the boundary value
d
−x du
problem −
e
= 1, u(0) = u(2) = 0. Carefully explain how you are setting
dx
dx
up the calculation. Plot the resulting solutions and compare your answer with the exact
solution. You should use an equally spaced mesh, but try at least three diﬀerent mesh
spacings and compare your results. By inspecting the errors in your various approximations, can you predict how many nodes would be required for six-digit accuracy of the
numerical approximation?
♠ 10.2.2. For each of the following boundary value problems: (i ) Solve the problem exactly.
(ii ) Approximate the solution using the ﬁnite element method based on ten equally spaced
nodes. (iii ) Compare the graphs of the exact solution and its piecewise aﬃne ﬁnite element
approximation. What is the maximal error in your approximation at the nodes? on the
entire interval?



1 x > 1,
d
du
u(0) = u(2) = 0; (b) −
(1 + x)
= 1, u(0) = u(1) = 0;
(a) − u =
0, x < 1,
dx
dx
d
(c) −
dx



x

2 du

dx



d
= − x, u(1) = u(3) = 0; (d) −
dx



x du

e

dx



= ex , u(−1) = u(1) = 0.

♣ 10.2.3. (a) Find the exact solution to the boundary value problem − u = 3 x, u(0) = u(1) = 0.
(b) Use the ﬁnite element method based on ﬁve equally spaced nodes to approximate the
solution. (c) Compare the graphs of the exact solution and its piecewise aﬃne ﬁnite element
approximation. (d) What is the maximal error (i ) at the nodes? (ii ) on the entire interval?
♣ 10.2.4. Use ﬁnite elements to approximate the solution to the Sturm–Liouville boundary value
problem − u +(x+1)u = x ex , u(0) = 0, u(1) = 0, using 5, 10, and 20 equally spaced nodes.
♣ 10.2.5. (a) Devise a ﬁnite element scheme for numerically approximating the solution to the
mixed boundary value problem


d
du
−
κ(x)
= f (x),
a < x < b,
u(a) = 0,
u (b) = 0.
dx
dx
(b) Test your method on the particular boundary value problem


d
du
−
(1 + x)
= 1,
0 < x < 1,
u(0) = 0,
u (1) = 0,
dx
dx
using 10 equally spaced nodes. Compare your approximation with the exact solution.
♠ 10.2.6. Consider the periodic boundary value problem
u(0) = u(2 π),
u (0) = u (2 π).
− u + u = x,
(a) Write down the analytic solution. (b) Write down a minimization principle.
(c) Divide the interval [ 0, 2 π ] into n = 5 equal subintervals, and let Wn denote the subspace consisting of all piecewise aﬃne functions that satisfy the boundary conditions. What
is the dimension of Wn ? Write down a basis. (d) Construct the ﬁnite element approximation to the solution to the boundary value problem by minimizing the functional from part
(b) on the subspace Wn . Graph the result and compare with the exact solution. What is
the maximal error on the interval? (e) Repeat part (d) for n = 10, 20, and 40 subintervals,
and discuss the convergence of your solutions.
♠ 10.2.7. Answer Exercise 10.2.6 when the ﬁnite element subspace Wn consists of all periodic
piecewise aﬃne functions of period 1, so w(x + 1) = w(x). Which approximation is better?
♣ 10.2.8. Use the method of Exercise 10.2.7 to approximate the solution to the following periodic
boundary value problem for the Mathieu equation:
− u + (1 + cos x) u = 1,
u(0) = u(2 π),
u (0) = u (2 π).

410

10 Finite Elements and Weak Solutions

♠ 10.2.9. Consider the boundary value problem solved in Example 10.3. Let Wn be the subspace consisting of all polynomials u(x) of degree ≤ n satisfying the boundary conditions
u(0) = u(1) = 0. In this project, we will try to approximate the exact solution to the
boundary value problem by minimizing the functional (10.24) on the polynomial subspace
Wn . For n = 5, 10, and 20: (a) First, determine a basis for Wn . (b) Set up the minimization problem as a system of linear equations for the coeﬃcients of the polynomial minimizer
relative to your basis. (c) Solve the polynomial minimization problem and compare your
“polynomial ﬁnite element” solution with the exact solution and the piecewise aﬃne ﬁnite
element solution graphed in Figure 10.3.
♠ 10.2.10. Consider the boundary value problem − u + λ u = x, for 0 < x < π, with u(0) = 0,
u(1) = 0. (a) For what values of λ does the system have a unique solution? (b) For which
values of λ can you ﬁnd a minimization principle that characterizes the solution? Is the
minimizer unique for all such values of λ? (c) Using n equally spaced nodes, write down
the ﬁnite element equations for approximating the solution to the boundary value problem.
Note: Although the ﬁnite element construction is supposed to work only when there is a
minimization principle, we will consider the resulting linear algebraic system for any value
of λ. (d) Select a value of λ for which the solution can be characterized by a minimization
principle and verify that the ﬁnite element approximation with n = 10 approximates the
exact solution. (e) Experiment with other values of λ. Does your ﬁnite element solution
give a good approximation to the exact solution when it exists? What happens at values of
λ for which the solution does not exist or is not unique?

10.3 Finite Elements in Two Dimensions
The same basic framework underlies the adaptation of ﬁnite element techniques for numerically approximating the solution to boundary value problems governed by elliptic
partial diﬀerential equations. In this section, we concentrate on the simplest case: the
two-dimensional Poisson equation. Having mastered this, the reader will be well equipped
to carry over the method to more general equations and higher dimensions. As before, we
concentrate on the practical design of the ﬁnite element procedure, and refer the reader
to more advanced texts, e.g., [6, 113, 126], for the analytical details and proofs of convergence. Most of the multi-dimensional complications lie not in the underlying theory, but
rather in the realm of data management and organization.
For speciﬁcity, consider the homogeneous Dirichlet boundary value problem
− Δu = f

in

Ω,

u=0

on ∂Ω,

(10.25)

on a bounded domain Ω ⊂ R 2 . According to Theorem 9.31, the solution u (x, y) is characterized as the unique minimizer of the Dirichlet functional

2
1
1 2
1 2
Q[ u ] = 2 | ∇u | −  u , f  =
(10.26)
2 ux + 2 uy − f u dx dy
Ω

2

among all C functions u(x, y) that satisfy the prescribed boundary conditions.
To construct a ﬁnite element approximation, we restrict the Dirichlet functional to a
suitably chosen ﬁnite-dimensional subspace. As in the one-dimensional version, the most
eﬀective subspaces contain functions that may lack the requisite degree of smoothness that
qualiﬁes them as candidate solutions to the partial diﬀerential equation. Nevertheless, they
will provide good approximations to the actual classical solution. Another important practical consideration, ensuring sparseness of the ﬁnite element matrix, is to employ functions

10.3 Finite Elements in Two Dimensions

Figure 10.4.

411

Triangulation of a planar domain.

that have small support, meaning that they vanish on most of the domain. Sparseness has
the beneﬁt that the solution to the linear ﬁnite element system can be relatively rapidly
calculated, usually by application of an iterative numerical scheme such as the Gauss–Seidel
or SOR methods discussed in [89, 118].
Triangulation
The ﬁrst step is to introduce a mesh consisting of a ﬁnite number of nodes xl = (xl , yl ),
l = 1, . . . , m, usually lying inside the domain Ω ⊂ R 2 . Unlike ﬁnite diﬀerence schemes, ﬁnite
element methods are not tied to a rectangular mesh, thus endowing them with considerably
more ﬂexibility in the allowable discretizations of the domain. We regard the nodes as the
vertices of a triangulation of the domain, consisting of a collection of non-overlapping small

triangles, which we denote by T1 , . . . , TN , whose union T = ν Tν approximates Ω; see
Figure 10.4 for a typical example. The nodes are split into two categories — interior nodes
and boundary nodes, the latter lying on or close to ∂Ω. A curved boundary will thus be
approximated by the polygonal boundary ∂T of the triangulation, whose vertices are the
boundary nodes. Thus, in any practical implementation of a ﬁnite element scheme, the
ﬁrst requirement is a routine that will automatically triangulate a speciﬁed domain in some
“reasonable” manner, as explained below.
As in our one-dimensional construction, the functions w(x, y) in the ﬁnite-dimensional
subspace W will be continuous and piecewise aﬃne, which means that, on each triangle,
the graph of w is a ﬂat plane and hence has the formula†
w(x, y) = αν + β ν x + γ ν y

when

(x, y) ∈ Tν ,

(10.27)

for certain constants αν , β ν , γ ν . Continuity of w requires that its values on a common edge
between two triangles must agree, and this will impose compatibility constraints on the
coeﬃcients αμ , β μ , γ μ and αν , β ν , γ ν associated with adjacent pairs of triangles Tμ and Tν .
The full graph of the piecewise aﬃne function z = w(x, y) forms a connected polyhedral
surface whose triangular faces lie above the triangles Tν ; see Figure 10.5 for an illustration.
In addition, we require that the piecewise aﬃne function w(x, y) vanish at the boundary
nodes, which implies that it vanishes on the entire polygonal boundary of the triangulation,
†

Here and subsequently, the index ν is a superscript, not a power.

412

10 Finite Elements and Weak Solutions

Figure 10.5.

Figure 10.6.

Piecewise aﬃne function.

Finite element pyramid function.

∂T , and hence (approximately) satisﬁes the homogeneous Dirichlet boundary conditions
on the curved boundary of the original domain, ∂Ω.
The next step is to choose a basis of the subspace of piecewise aﬃne functions associated with the given triangulation and subject to the imposed homogeneous Dirichlet
boundary conditions. The analogue of the one-dimensional hat function (10.12) is the pyramid function ϕl (x, y), which has the value 1 at a single node xl = (xl , yl ), and vanishes at
all the other nodes:

1,
i = l,
(10.28)
ϕl (xi , yi ) =
0,
i = l.
Because, on any triangle, the pyramid function ϕl (x, y) is uniquely determined by its values
at the vertices, it will be nonzero only on those triangles that have the node xl as one of
their vertices. Hence, as its name implies, the graph of ϕl forms a pyramid of unit height
sitting on a ﬂat plane; a typical example appears in Figure 10.6.
The pyramid functions ϕl (x, y) associated with the interior nodes xl automatically
satisfy the homogeneous Dirichlet boundary conditions on the boundary of the domain —
or, more correctly, on the polygonal boundary of the triangulated domain. Thus, the ﬁnite

10.3 Finite Elements in Two Dimensions

413

element subspace W is the span of the interior node pyramid functions, and so a general
piecewise aﬃne function w ∈ W is a linear combination thereof:
w(x, y) =

n


cl ϕl (x, y),

(10.29)

l=1

where the sum ranges over the n interior nodes of the triangulation. Owing to the original
speciﬁcation (10.28) of the pyramid functions, the coeﬃcients
cl = w(xl , yl ) ≈ u(xl , yl ),

l = 1, . . . , n,

(10.30)

are the same as the values of the ﬁnite element approximation w(x, y) at the interior
nodes. This immediately implies linear independence of the pyramid functions, since the
only linear combination that vanishes at all nodes is the trivial one c1 = · · · = cn = 0.
Determining the explicit formulas for the pyramid functions is not diﬃcult. On one of
the triangles Tν that has xl as a vertex, ϕl (x, y) will be the unique aﬃne function (10.27)
that takes the value 1 at the vertex xl and 0 at its other two vertices xi and xj . Thus, we
seek a formula for an aﬃne function or element
ωlν (x, y) = ανl + βlν x + γlν y,

(x, y) ∈ Tν ,

(10.31)

that takes the prescribed values
ωlν (xi , yi ) = ανl + βlν xi + γlν yi = 0,

ωlν (xj , yj ) = ανl + βlν xj + γlν yj = 0,

(10.32)

ωlν (xl , yl ) = ανl + βlν xl + γlν yl = 1.

Solving this linear system for the coeﬃcients — using either Cramer’s Rule or direct Gaussian Elimination — produces the explicit formulas
ανl =

xi yj − xj yi
,
Δν

βlν =

yi − yj
,
Δν

γlν =

xj − xi
,
Δν

(10.33)

where the denominator
⎛

1
⎝
Δν = det 1
1

xi
xj
xl

⎞
yi
yj ⎠ = ± 2 area Tν
yl

(10.34)

is, up to sign, twice the area of the triangle Tν ; see Exercise 10.3.5.
Example 10.4. Consider an isosceles right triangle T with vertices
x1 = (0, 0),

x2 = (1, 0),

x3 = (0, 1).

Using (10.33–34) (or solving the linear system (10.32) directly), we immediately produce
the three corresponding aﬃne elements
ω1 (x, y) = 1 − x − y,

ω2 (x, y) = x,

ω3 (x, y) = y.

(10.35)

As required, each ωl equals 1 at the vertex xl and is zero at the other two vertices.

414

10 Finite Elements and Weak Solutions

Figure 10.7.

Vertex polygons.

A pyramid function is then obtained by piecing together the individual aﬃne elements:
 ν
ωl (x, y),
if (x, y) ∈ Tν and xl is a vertex of Tν ,
ϕl (x, y) =
(10.36)
0,
otherwise.
Continuity of ϕl (x, y) is assured, since the constituent aﬃne elements have the same values
at common vertices, and hence also along common edges. The support of the pyramid
function (10.36) is the vertex polygon
supp ϕl = Pl =



Tν

(10.37)

ν

consisting of all the triangles Tν that have the node xl as a vertex. In other words,
ϕl (x, y) = 0 whenever (x, y) ∈ Pl . The node xl lies on the interior of its vertex polygon
Pl , while the vertices of Pl are all the nodes connected to xl by a single edge of the
triangulation. In Figure 10.7, the shaded regions indicate two of the vertex polygons for
the triangulation in Figure 10.4.
Example 10.5. The simplest, and most common, triangulations are based on regular
meshes. For example, suppose that the nodes lie on a square grid, and so are of the form
xi,j = (i h + a, j h + b), where (i, j) run over a collection of integer pairs, h > 0 is the
inter-node spacing, and (a, b) represents an overall oﬀset. If we choose the triangles to all
have the same orientation, as in the ﬁrst picture in Figure 10.8, then the vertex polygons
all have the same shape, consisting of six triangles of total area 3 h2 — the shaded region.
On the other hand, if we choose an alternating triangulation, as in the second picture, then
there are two types of vertex polygons. The ﬁrst, consisting of four triangles, has area 2 h2 ,
while the second, containing eight triangles, has twice the area, 4 h2 . In practice, there are
good reasons to prefer the former triangulation.
In general, to ensure convergence of the ﬁnite element solution to the true minimizer,
one should choose triangulations that satisfy the following properties:
• The three side lengths of any individual triangle should be of comparable size, and
so long, skinny triangles and obtuse triangles should be avoided.
• The areas of nearby triangles Tν should not vary too much.
• The areas of nearby vertex polygons Pl should also not vary too much.

10.3 Finite Elements in Two Dimensions

Figure 10.8.

415

Square mesh triangulations.

While the nearby triangles should be of comparable size, one might very well allow wide
variations over the entire domain, with small triangles in regions where the solution is
changing rapidly, and large triangles in less active regions.

Exercises
10.3.1. Sketch a triangulation of the following domains so that all triangles have side length
at most .5: (a) a unit square; (b) an isosceles triangle with vertices (−.5, 0), (.5, 0) and
(0, 1); (c) the square { | x |, | y | ≤ 2 } with the hole { | x |, | y | < 1 } removed;
(d) the unit disk; (e) the annulus 1 ≤  x  ≤ 2.
10.3.2. Describe the vertex polygons for a triangulation that uses regular equilateral triangles.
10.3.3. Are there any restrictions on the number of sides a vertex polygon can have?
10.3.4. Find the three ﬁnite element functions ω1 (x, y), ω2 (x, y), ω3 (x, y), associated with
(a) the triangle having vertices (1, 0), (0, 1), and (1, 1);
(b) the triangle having vertices (0, 1), (1, −1), and (−1, −1);
(c) an equilateral triangle centered at the origin having one vertex at (1, 0).
♦ 10.3.5. (a) Prove that the ⎛
area of a planar
triangle T with vertices (a, b), (c, d), (e, f ) is equal to
⎞
1 a b
⎜
⎟
1
2 | Δ |, where Δ = det ⎝ 1 c d ⎠. (b) Prove that Δ > 0 if and only if the vertices of the
1 e f
triangle are listed in counterclockwise order.
♦ 10.3.6. Give a detailed justiﬁcation of the continuity of the pyramid function (10.36).
♥ 10.3.7. An alternative to triangular elements is to employ piecewise bi-aﬃne functions, meaning ω(x, y) = α + β x + γ y + δ x y, on rectangles. (a) Suppose R is a rectangle with vertices
(x1 , y1 ), (x2 , y2 ), (x3 , y3 ), (x4 , y4 ), whose sides are parallel to the coordinate axes. Prove
that, for each l = 1, . . . , 4, there is a unique bi-aﬃne function ωl (x, y) deﬁned on R that has
the value ωl (xl , yl ) = 1 at one vertex while ωl (xi , yi ) = 0, i = l, at the other three vertices.

416

10 Finite Elements and Weak Solutions
(b) Write out the four bi-aﬃne functions ω1 (x, y), . . . , ω4 (x, y), when
(i ) R = { 0 ≤ x, y ≤ 1 }, (ii ) R = { −1 ≤ x, y ≤ 1 }. (c) Does the result in part (a) hold for
rectangles whose sides are not aligned with the axes? For general quadrilaterals?

The Finite Element Equations
We now seek to approximate the solution to the homogeneous Dirichlet boundary value
problem by restricting the Dirichlet functional (10.26) to the selected ﬁnite element subspace W . Using the general framework of Section 10.1, we substitute the formula (10.29)
for a general element of W into the quadratic Dirichlet functional (9.82). Expanding, we
obtain
   n
"
! n
 n
2



ci ∇ϕi
dx dy
Q[ w ] = Q
ci ϕi =
− f (x, y)
ci ϕi
Ω

i=1

=

n
1 

2 i,j = 1

i=1

kij ci cj −

n

i=1

i=1

(10.38)

bi ci = 12 cT K c − bT c.
T

Here K = (kij ) is a symmetric n × n matrix, while b = ( b1 , b2 , . . . , bn ) is a vector in R n ,
with respective entries

kij =  ∇ϕi , ∇ϕj  =
∇ϕi · ∇ϕj dx dy,
Ω

(10.39)
bi =  f , ϕi  =
f ϕi dx dy,
Ω

which also follow directly from the general formulas (10.6–7). Thus, the ﬁnite element
approximation (10.29) will minimize the quadratic function
P (c) = 12 cT K c − bT c

(10.40)
T

over all possible choices of coeﬃcients c = ( c1 , c2 , . . . , cn ) ∈ R n , i.e., over all possible
function values at the interior nodes. As above, the minimizer’s coeﬃcients are obtained
by solving the associated linear system
K c = b,

(10.41)

using either Gaussian Elimination or a suitable iterative linear systems solver.
To ﬁnd explicit formulas for the matrix coeﬃcients kij in (10.39), we begin by noting
that the gradient of the aﬃne element (10.31) is equal to
 ν
  ν


1
∂ωl /∂x
βl
yi − yj
(10.42)
gνl = ∇ωlν (x, y) =
=
=
,
(x, y) ∈ Tν ,
∂ωlν /∂y
γlν
Δ ν xj − xi
which is a constant vector inside the triangle Tν , while ∇ωlν = 0 outside Tν . Therefore,
# ν
if (x, y) ∈ Tν that has xl as a vertex,
gl ,
(10.43)
∇ϕl (x, y) =
0,
otherwise.

10.3 Finite Elements in Two Dimensions

417

Actually, (10.43) is not quite correct, since the gradient is not well deﬁned on the boundary
of a triangle Tν , but this will not cause us any diﬃculty in evaluating the ensuing integrals.
We will approximate integrals over the domain Ω by summing the corresponding integrals over the individual triangles — which relies on our assumption that the polygonal
boundary of the triangulation ∂T is a reasonably close approximation to the true boundary
∂Ω. In particular,
 

ν
∇ϕi · ∇ϕj dx dy ≡
kij
.
(10.44)
kij ≈
ν

Tν

ν

Now, according to (10.43), one or the other gradient in the integrand will vanish on the
entire triangle Tν unless both xi and xj are vertices. Therefore, the only terms contributing
to the sum are those triangles Tν that have both xi and xj as vertices. If i = j, there are
only two such triangles, having a common edge, while if i = j, every triangle in the ith
vertex polygon Pi contributes. The individual summands are easily evaluated, since the
gradients are constant on the triangles, and so, by (10.43),

ν
kij
=
gνi · gνj dx dy = gνi · gνj area Tν = 21 gνi · gνj | Δν | .
Tν

Let Tν have vertices xi , xj , xl . Then, by (10.34, 42, 43),
(xi − xl ) · (xj − xl )
1 (yj − yl )(yl − yi ) + (xl − xj )(xi − xl )
, i = j,
| Δν | = −
2
2
(Δν )
2 | Δν |
 xj − xl  2
1 (yj − yl )2 + (xl − xj )2
ν
kii
(10.45)
=
|
Δ
|
=
ν
2
(Δν )2
2 | Δν |
(xi − xl ) · (xj − xl ) + (xl − xj ) · (xi − xj )
ν
ν
= − kij
− kil
.
=
2 Δν

ν
kij
=

ν
ν
In this manner, each triangle Tν speciﬁes a collection of six diﬀerent coeﬃcients, kij
= kji
,
indexed by its vertices, and known as the elemental stiﬀnesses of Tν . Interestingly, the
elemental stiﬀnesses depend only on the three vertex angles in the triangle and not on
its size. Thus, similar triangles have the same elemental stiﬀnesses. Indeed, according to
Exercise 10.3.13,
ν
kii
= 12 (cot θjν + cot θlν ),

while

ν
ν
kij
= kji
= − 12 cot θlν ,

i = j,

(10.46)

where 0 < θlν < π denotes the angle in Tν at the vertex xl .
Example 10.6. The right triangle with vertices x1 = (0, 0), x2 = (1, 0), x3 = (0, 1)
has elemental stiﬀnesses
k11 = 1,

k22 = k33 = 12 ,

k12 = k21 = k13 = k31 = − 12 ,

k23 = k32 = 0.

(10.47)

The same holds for any other isosceles right triangle, provided its vertices are labeled in
the same manner. Similarly, an equilateral triangle has all 60◦ angles, and so its elemental
stiﬀnesses are
k11 = k22 = k33 = √13 ≈ .5774,
(10.48)
1
k12 = k21 = k13 = k31 = k23 = k32 = − 2√
≈ −.2887.
3

418

10 Finite Elements and Weak Solutions

Exercises
10.3.8. Write down the elemental stiﬀnesses for: (a) the triangle with vertices (0, 1), (−1, 2),
(0, −1); (b) the triangle with vertices (1, 1), (−1, 1), (0, −2); (c) a 30−60−90 degree right
triangle; (d) a right triangle with side lengths 3, 4, 5; (e) an isosceles triangle of height 3
and base 2; (f ) a “golden” isosceles triangle with angles 36◦ , 72◦ , 72◦ .
♦ 10.3.9. A rectangular mesh has nodes xi,j = (i Δx + a, j Δy + b), where Δx, Δy > 0 are,
respectively, the horizontal and vertical step sizes. Find the elemental stiﬀnesses for the
triangles associated with such a rectangular mesh.
10.3.10. True or false: Let T be a triangle, and T a triangle obtained by rotating T by 60◦ .
Then T and T have the same elemental stiﬀnesses.
xl
10.3.11. Prove that the gradient (10.42) of the aﬃne element is equal
xj
to ∇ωlν =  aνl −2 aνl , where aνl is the altitude vector that goes
ν
to the vertex xl from its opposite side, as indicated in the ﬁgure.
a
l

10.3.12. Explain why the pyramid functions are linearly independent.
♦ 10.3.13. Prove formulas (10.46).

xi

Assembling the Elements
The elemental stiﬀnesses of each triangle will contribute, through the summation (10.44),

to the ﬁnite element coeﬃcient matrix K. We begin by constructing a larger matrix K,
which we call the full ﬁnite element matrix , of size m × m, where m is the total number
of nodes in our triangulation, including both interior and boundary nodes. The rows and
 are labeled by the nodes x , . . . , x . Let K = (k ν ) be the corresponding
columns of K
ij
1
m
ν
ν
of Tν in the rows and columns
m × m matrix containing the elemental stiﬀnesses kij
indexed by its vertices, and all other entries equal to 0. Thus, Kν will have (at most) nine
nonzero entries. The resulting m × m matrices are summed together over all the triangles
T1 , . . . , TN , whereby
N

 =
Kν ,
(10.49)
K
ν =1

in accordance with (10.44).
 is too large, since its rows and columns include all
The full ﬁnite element matrix K
the nodes, whereas the ﬁnite element matrix K appearing in (10.41) refers only to the n
 by
interior nodes. The reduced n × n ﬁnite element matrix K is simply obtained from K
deleting all rows and columns indexed by boundary nodes, retaining only the elements kij
for which both xi and xj are interior nodes. For the homogeneous boundary value problem,
this is all we require. As we will subsequently see, inhomogeneous boundary conditions are

most easily handled by retaining (another part of) the full matrix K.
The easiest way to absorb the construction is by working through a particular example.
Example 10.7. A metal plate has the shape of an oval running track, consisting of
a square, with side lengths 2 m, and two semi-circular disks glued onto opposite sides, as

10.3 Finite Elements in Two Dimensions

Figure 10.9.

419

The oval plate.

5
1

4

7
5

6

9

10

8

11

14

Triangles
Figure 10.10.

12
1

13
3

13

6

12

2

4

2

3

7

11
8

9

10

Nodes
A coarse triangulation of the oval plate.

sketched in Figure 10.9. The plate is subject to a heat source, while its edges are held at a
ﬁxed temperature. The problem is to ﬁnd the equilibrium temperature distribution within
the plate. Mathematically, we must solve the planar Poisson equation, subject to Dirichlet
boundary conditions, for the equilibrium temperature u(x, y).
Let us describe how to set up the ﬁnite element approximation. We begin with a
very coarse triangulation of the plate, which will not give particularly accurate results,
but serves to illustrate how to go about assembling the ﬁnite element matrix. We divide
the rectangular part of the plate into eight right triangles, while each semicircular end
will be approximated by three equilateral triangles. The triangles are numbered from 1
to 14 as indicated in Figure 10.10. There are 13 nodes in all, numbered as in the second
ﬁgure. Only nodes 1, 2, 3 are interior, while the boundary nodes are labeled 4 through 13
 will have
in counterclockwise order starting at the top. The full ﬁnite element matrix K
size 13 × 13, its rows and columns labeled by all the nodes, while the reduced matrix K
appearing in the ﬁnite element equations (10.41) consists of the upper left 3 × 3 submatrix
 corresponding to the three interior nodes.
of K
For each ν = 1, . . . , 14, the triangle Tν will contribute its elemental stiﬀnesses, as
 through a summand K . For example, the ﬁrst
indexed by its vertices, to the matrix K
ν
triangle T1 is equilateral, and so has elemental stiﬀnesses (10.48). Its vertices are labeled
1, 5, and 6, and therefore we place the stiﬀnesses in the rows and columns numbered 1, 5, 6

420

10 Finite Elements and Weak Solutions

to form the summand
⎛

.5774
⎜ 0
⎜
⎜ 0
⎜
⎜ 0
⎜
⎜ −.2887
K1 = ⎜
⎜ −.2887
⎜
⎜ 0
⎜
⎜ 0
⎜
..
⎝
.

0
0
0
0
0
0
0
0
..
.

0
0
0
0
0
0
0
0
..
.

0
0
0
0
0
0
0
0
..
.

−.2887 −.2887 0
0
0
0
0
0
0
0
0
0
.5774 −.2887 0
−.2887
.5774 0
0
0
0
0
0
0
..
..
..
.
.
.

0
0
0
0
0
0
0
0
..
.

...
...
...
...
...
...
...
...
..
.

⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟,
⎟
⎟
⎟
⎟
⎟
⎟
⎠

where all the undisplayed entries in the full 13 × 13 matrix are 0. The next triangle T2 has
the same equilateral elemental stiﬀness matrix (10.48), but now its vertices are 1, 6, 7, and
so it will contribute
⎛

.5774
⎜ 0
⎜
⎜ 0
⎜
⎜ 0
⎜
⎜ 0
K2 = ⎜
⎜ −.2887
⎜
⎜ −.2887
⎜
⎜ 0
⎜
..
⎝
.

0
0
0
0
0
0
0
0
..
.

0
0
0
0
0
0
0
0
..
.

0
0
0
0
0
0
0
0
..
.

0
0
0
0
0
0
0
0
..
.

−.2887 −.2887 0
0
0
0
0
0
0
0
0
0
0
0
0
.5774 −.2887 0
−.2887
.5774 0
0
0
0
..
..
..
.
.
.

...
...
...
...
...
...
...
...
..
.

⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟.
⎟
⎟
⎟
⎟
⎟
⎟
⎠

Similarly for K3 , with vertices 1, 7, 8. On the other hand, T4 is an isosceles right triangle,
and so has elemental stiﬀnesses (10.47). Its vertices are labeled 1, 4, and 5, with vertex 5
at the right angle. Therefore, its contribution is
⎛

.5
⎜ 0
⎜
⎜ 0
⎜
⎜ 0
⎜
⎜ −.5
K4 = ⎜
⎜ 0
⎜
⎜ 0
⎜
⎜ 0
⎜ .
⎝ ..

0
0
0
0
0
0
0
0
..
.

0
0
0
0
0
0
0
0
..
.

0
0
0
.5
−.5
0
0
0
..
.

−.5
0
0
−.5
1.0
0
0
0
..
.

0
0
0
0
0
0
0
0
..
.

0
0
0
0
0
0
0
0
..
.

0
0
0
0
0
0
0
0
..
.

...
...
...
...
...
...
...
...
..
.

⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟.
⎟
⎟
⎟
⎟
⎟
⎟
⎠

Continuing in this manner, we assemble 14 contributions K1 , . . . , K14 , each with at most
9 nonzero entries. The full ﬁnite element matrix is their sum

10.3 Finite Elements in Two Dimensions

421

% = K + K + ··· + K
K
1
2
14
⎛

3.732

⎜ −1
⎜
⎜
⎜
0
⎜
⎜
0
⎜
⎜ −.7887
⎜
⎜
⎜ −.5774
⎜
=⎜
⎜ −.5774
⎜
⎜ −.7887
⎜
⎜
0
⎜
⎜
0
⎜
⎜
⎜
0
⎜
⎝
0

0

−1
4
−1
−1
0
0
0
0
−1
0
0
0
0

0
−1
3.732
0
0
0
0
0
0
−.7887
−.5774
−.5774
−.7887

0
−1
0
2
−.5
0
0
0
0
0
0
0
−.5

−.7887
0
0
−.5
1.577
−.2887
0
0
0
0
0
0
0

−.5774
0
0
0
−.2887
1.155
−.2887
0
0
0
0
0
0

−.7887
0
0
0
0
0
−.2887
1.577
−.5
0
0
0
0

0
−1
0
0
0
0
0
−.5
2
−.5
0
0
0

−.5774
0
0
0
0
−.2887
1.155
−.2887
0
0
0
0
0
0
0
−.7887
0
0
0
0
0
−.5
1.577
−.2887
0
0

(10.50)

0
0
−.5774
0
0
0
0
0
0
−.2887
1.155
−.2887
0

Since nodes 1, 2, 3 are interior, the reduced ﬁnite element matrix
⎛
⎞
3.732 −1
0
K = ⎝ −1
4 −1 ⎠
0 −1 3.732

0
0
−.5774
0
0
0
0
0
0
0
−.2887
1.155
−.2887

⎞

0
0 ⎟
⎟
⎟
−.7887 ⎟
⎟
−.5 ⎟
⎟
0 ⎟
⎟
⎟
0 ⎟
⎟
0 ⎟
⎟.
0 ⎟
⎟
⎟
0 ⎟
⎟
0 ⎟
⎟
⎟
0 ⎟
⎟
−.2887 ⎠
1.577

(10.51)

 Clearly, it would not be diﬃcult to directly
uses only the upper left 3 × 3 block of K.

construct K, bypassing K entirely.
For a ﬁner triangulation, the construction is similar, but the matrices become much
larger. The procedure can, of course, be automated. Fortunately, if we choose a very
regular triangulation, then we do not need to be nearly as meticulous in assembling the
stiﬀness matrices, since many of the entries are the same. The simplest case employs a
uniform square mesh, and so triangulates the domain into isosceles right triangles. This is
accomplished by laying out a relatively dense square grid over the domain Ω ⊂ R 2 . The
interior nodes are the grid points that fall inside the oval domain, while the boundary
nodes are all those grid points lying adjacent to one or more of the interior nodes, and
are near but not necessarily precisely on the boundary ∂Ω. Figure 10.11 shows the nodes
in a square grid with intermesh spacing h = .2. While a bit crude in its approximation
of the boundary of the domain, this procedure does have the advantage of making the
construction of the associated ﬁnite element matrix relatively painless.
For such a mesh, all the triangles are isosceles right triangles, with elemental stiﬀnesses
(10.47). Summing the corresponding matrices Kν over all the triangles, as in (10.49), we
 corresponding to the interior nodes all have the same
ﬁnd that the rows and columns of K

422

10 Finite Elements and Weak Solutions

Figure 10.11.

A square mesh for the oval plate.

form. Namely, if i labels an interior node, then the corresponding diagonal entry is kii = 4,
while the oﬀ-diagonal entries kij = kji , i = j, are equal to −1 when node i is adjacent to
node j on the grid, and are equal to 0 in all other cases. Node j is allowed to be a boundary
node. (Interestingly, the result does not depend on how one orients the pair of triangles
making up each square of the grid, which plays a role only in the computation of the righthand side of the ﬁnite element equation.) Observe that the same computation applies even
to our coarse triangulation. The interior node 2 belongs to all right isosceles triangles, and
the corresponding nonzero entries in (10.50) are k22 = 4 and k21 = k23 = k24 = k29 = −1,
indicating the four adjacent nodes.
Remark : The coeﬃcient matrix constructed from the ﬁnite element method on a
square (or even rectangular) grid is the same as the coeﬃcient matrix arising from a
ﬁnite diﬀerence solution to the Laplace or Poisson equation, as described in Example 5.7.
The ﬁnite element approach has the advantage of readily adapting to much more general
discretizations of the domain, and is not restricted to rectangular grids.

The Coeﬃcient Vector and the Boundary Conditions
So far, we have been concentrating on assembling the ﬁnite element coeﬃcient matrix K.
We also need to compute the forcing vector b = ( b1 , b2 , . . . , bn )T appearing on the righthand side of the fundamental linear equation (10.41). According to (10.39), the entries bi
are found by integrating the product of the forcing function and the ﬁnite element basis
function. As before, we will approximate the integral over the domain Ω by an integral
over the triangles, and so

f (x, y) ϕi(x, y) dx dy ≈

bi =
Ω

 
ν

f (x, y) ωiν (x, y) dx dy ≡
Tν



bνi .

(10.52)

ν

Typically, an exact computation of the various triangular double integrals is not so
convenient, and so we resort to a numerical approximation. Since we are assuming that
the individual triangles are small, we can get away with a very crude numerical integration
scheme. If the function f (x, y) does not vary much over the triangle Tν — which will
certainly be the case if Tν is suﬃciently small — we may approximate f (x, y) ≈ cνi for

10.3 Finite Elements in Two Dimensions

Figure 10.12.

423

Finite element tetrahedron.

(x, y) ∈ Tν by a constant. The integral (10.52) is then approximated by


ν
ν
ν
bi =
f (x, y) ωi (x, y) dx dy ≈ ci
ωiν (x, y) dx dy = 13 cνi area Tν = 16 cνi | Δν |.
Tν

Tν

(10.53)
The formula for the integral of the aﬃne element ωiν (x, y) follows from solid geometry: it
equals the volume under its graph, a tetrahedron of height 1 and base Tν , as illustrated in
Figure 10.12.
How to choose the constant cνi ? In practice, the simplest choice is to let cνi = f (xi , yi )
be the value of the function at the ith vertex. With this choice, the sum in (10.52) becomes
bi ≈



1
1
3 f (xi , yi ) area Tν = 3 f (xi , yi ) area Pi ,

(10.54)

ν

where Pi is the vertex polygon (10.37) corresponding to the node xi . In particular, for the
square mesh with the uniform choice of triangles, as in the ﬁrst plot in Figure 10.8,
area Pi = 3 h2

for all i, and so

bi ≈ f (xi , yi ) h2

(10.55)

is well approximated by just h2 times the value of the forcing function at the node. This
is the underlying reason to choose the uniform triangulation for the square mesh; the
alternating version would give unequal values for the bi over adjacent nodes, and this could
give rise to unnecessary errors in the ﬁnal approximation.
Example 10.8. For the coarsely triangulated oval plate, the reduced stiﬀness matrix
is (10.51). The Poisson equation
− Δu = 4
models a constant external heat source of magnitude 4◦ over the entire plate. If we keep the
edges of the plate ﬁxed at 0◦ , then we need to solve the ﬁnite element equation K c = b,
where K is the coeﬃcient matrix (10.51). The entries of b are, by (10.54), equal to 4 (the
right-hand side of the diﬀerential equation) times one-third the area of the corresponding
vertex polygon, which for node 2 is the square consisting of four right triangles, each of
area 12 , whereas for nodes 1 and 3 it consists of four right triangles of area 12 plus three

424

10 Finite Elements and Weak Solutions

Figure 10.13.

Finite element solutions to Poisson’s equation for an oval plate.
√

equilateral triangles, each of area 43 ; see Figure 10.10. Thus,
"
√
√ #T
T
b = 43 2 + 3 4 3 , 2, 2 + 3 4 3
= ( 4.3987, 2.6667, 4.3987 ) .
The solution to the ﬁnal linear system K c = b is easily found:
T

c = ( 1.5672, 1.4503, 1.5672 ) .
Its entries are the values of the ﬁnite element approximation at the three interior nodes.
The piecewise aﬃne ﬁnite element solution is plotted in the ﬁrst illustration in Figure 10.13.
A more accurate approximation, based on a square grid triangulation of size h = .1, appears
in the second ﬁgure. Here, the largest errors are concentrated near the poorly approximated
corners of the oval, and could be improved by a more sophisticated triangulation.

Inhomogeneous Boundary Conditions
So far, we have restricted our attention to problems with homogeneous Dirichlet boundary conditions. According to Theorem 9.32, the solution to the inhomogeneous Dirichlet
problem
− Δu = f in Ω,
u = h on ∂Ω,
is also obtained by minimizing the Dirichlet functional (9.82). However, now the minimization takes place over the set of functions that satisfy the inhomogeneous boundary
conditions. It is not diﬃcult to ﬁt this problem into the ﬁnite element scheme.
The elements corresponding to the interior nodes of our triangulation remain as before,
but now we need to include additional elements to ensure that our approximation satisﬁes
the boundary conditions. Note that if xl is a boundary node, then the corresponding
boundary element ϕl (x, y) satisﬁes (10.28), and so has the same piecewise aﬃne form
(10.36). The corresponding ﬁnite element approximation
w(x, y) =

m


cl ϕl (x, y)

(10.56)

l=1

has the same form as before, (10.29), but now the sum is over all nodes, both interior
and boundary. As before, the coeﬃcients cl = w(xl , yl ) ≈ u(xl , yl ) are the values of the

10.3 Finite Elements in Two Dimensions

425

ﬁnite element approximation at the nodes. Therefore, in order to satisfy the boundary
conditions, we require
cj = hj = h(xj , yj )

whenever xj = (xj , yj )

is a boundary node.

(10.57)

If the boundary node xj does not lie precisely on the boundary ∂Ω, then h(xj , yj ) is not
deﬁned, and so we need to approximate the value hj appropriately, e.g., using the value of
h(x, y) at a nearby boundary point (x, y) ∈ ∂Ω.
The derivation of the ﬁnite element equations proceeds as before, but now there are
additional terms arising from the nonzero boundary values. Leaving the intervening details
 denote the full m×m
to Exercise 10.3.23, the ﬁnal outcome can be written as follows. Let K
ﬁnite element matrix constructed as above. The reduced coeﬃcient matrix K is obtained
by retaining the rows and columns corresponding to only interior nodes, and so will have
 is
size n × n, where n is the number of interior nodes. The boundary coeﬃcient matrix K
the n × (m − n) matrix consisting of those entries of the interior rows that do not appear
in K, i.e., those lying in the columns indexed by the boundary nodes. For instance, in the
coarse triangulation of the oval plate, the full ﬁnite element matrix is given in (10.50), and
the upper 3 × 3 subblock is the reduced matrix (10.51). The remaining entries of the ﬁrst
three rows form the boundary coeﬃcient matrix
⎛
⎞
0 −.7887 −.5774 −.5774 −.7887
0
0
0
0
0
 = ⎝ −1
K
0
0
0
0
−1
0
0
0
0 ⎠.
0
0
0
0
0
0 −.7887 −.5774 −.5774 −.7887
(10.58)
We similarly split the coeﬃcients ci of the ﬁnite element function (10.56) into two groups.
We let c = ( c1 , c2 , . . . , cn )T ∈ R n denote the as yet unknown coeﬃcients corresponding to
T
∈
the values of the approximation at the interior nodes xi , while h = h1 , h2 , . . . , hm−n
R m−n will be the vector containing the boundary values (10.57). The solution to the ﬁnite
element approximation (10.56) is then obtained by solving the associated linear system
 h = b,
Kc+K

or, equivalently,

 h.
Kc = f = b−K

(10.59)

Example 10.9. For the oval plate discussed in Example 10.7, suppose the right-hand
semicircular edge is held at 10◦ , the left-hand semicircular edge at −10◦ , while the two
straight edges have a linearly varying temperature distribution ranging from −10◦ at the
left to 10◦ at the right, as illustrated in Figure 10.14. Our task is to compute its equilibrium
temperature, assuming no internal heat source. Thus, for the coarse triangulation we have
the boundary node values
T

T

h = ( h4 , . . . , h13 ) = ( 0, −10, −10, −10, −10, 0, 10, 10, 10, 10 ) .
Using the previously computed formulas (10.51, 58) for the interior and boundary coeﬃcient
 we approximate the solution to the Laplace equation by solving (10.59). We
matrices K, K,
are assuming that there is no external forcing function, f (x, y) ≡ 0, and hence b = 0, and
 h = ( 2.1856, 3.6, 7.6497 )T . The ﬁnite element function
so we must solve K c = f = − K
corresponding to the solution c = ( 1.0679, 1.8, 2.5320 )T is plotted in the ﬁrst illustration in
Figure 10.14. Even on such a coarse mesh, the approximation is not too bad, as evidenced
by the second illustration, which plots the ﬁnite element solution for the ﬁner square mesh
of Figure 10.11.

426

10 Finite Elements and Weak Solutions

Figure 10.14.

Solution to the Dirichlet problem for the oval plate.

Exercises
♠ 10.3.14. Consider the Dirichlet boundary value problem Δu = 0, u(x, 0) = sin x, u(x, π) = 0,
u(0, y) = 0, u(π, y) = 0, on the square S = { 0 < x, y < π }. (a) Find the exact solution.
(b) Set up and solve the ﬁnite element equations based on a square mesh with n = 2
squares on each side of S. Write out the reduced ﬁnite element matrix, the boundary coeﬃcient matrix, and the value of your approximation at the middle of the unit square. How
close is this value to the exact solution there? (c) Repeat part (b) for n = 4 squares per
side. Is the value of your approximation at the center of the unit square closer
to the

 true
solution? (d) Use a computer to ﬁnd a ﬁnite element approximation to u 12 π, 12 π using
n = 8 squares per side. Is your approximation converging to the exact solution as the mesh
becomes ﬁner and ﬁner?
♣ 10.3.15. Approximate the solution to the Dirichlet problem Δu = 0, u(x, 0) = x, u(x, 1) =
1 − x, u(0, y) = y, u(1, y) = 1 − y, by use of ﬁnite elements with mesh sizes Δx = Δy = .25
and .1. Compare your approximations with the solution you obtained in Exercise 4.3.12(d).
What is the maximal error at the nodes in each case?
♠ 10.3.16. A metal plate has the shape of an equilateral triangle with unit sides. One side is
heated to 100◦ , while the other two are kept at 0◦ . In order to approximate the equilibrium
temperature distribution, the plate is divided into smaller equilateral triangles, with n
triangles on each side, and the corresponding ﬁnite element approximation is then
computed. (a) How many triangles are in the triangulation? How many interior nodes?
How many edge nodes? (b) For n = 3, set up and solve the ﬁnite element linear system
to ﬁnd an approximation to the temperature at the center of the triangle. (c) Answer part
(b) when n = 4. (d) Use a computer to ﬁnd the ﬁnite element approximation to the temperature at the center when n = 5, 10, and 15. Are your values converging to the actual
temperature? (e) Plot the ﬁnite element approximations you constructed in the previous
parts.
10.3.17. Find the equilibrium temperature distribution in a unit equilateral triangle when one
side is heated to 100◦ , while the other two are insulated.
♠ 10.3.18. A metal plate has the shape of a 3 cm square with a 1 cm square hole cut out of the
middle. The plate is heated by ﬁxing the inner edge at temperature 100◦ while keeping
the outer edge at 0◦ . (a) Find the (approximate) equilibrium temperature using ﬁnite

10.4 Weak Solutions

427

elements with a mesh width of Δx = Δy = .5 cm. Plot your approximate solution using
a three-dimensional graphics program. (b) Let C denote the square contour lying midway
between the inner and outer square boundaries of the plate. Using your ﬁnite element
approximation, at what point(s) on C is the temperature a (i ) minimum? (ii ) maximum?
(iii ) equal to 50◦ , the average of the two boundary temperatures? (c) Repeat part (a)
using a smaller mesh width of h = .2. How much does this aﬀect your answers in part (b)?
♣ 10.3.19. Answer Exercise 10.3.18 when the plate is additionally subjected to a constant heat
source f (x, y) = 600 x + 800 y − 2400.
♠ 10.3.20. (a) Construct a ﬁnite element approximation to the solution, using a maximal mesh
size of .1, to the following boundary value problem on the unit disk:
Δu = 0,

2

2

x + y < 1,

⎧
⎨ 1,

u=⎩

x2 + y 2 = 1, y > 0,

0,
x2 + y 2 = 1, y < 0.
(b) Compare your solution with the exact solution given in Example 4.7.
♣ 10.3.21. (a) Use ﬁnite elements to approximate the solution to the boundary value problem
− Δu + u = 0,
0 < x, y < 1,
u(x, 0) = u(x, 1) = u(0, y) = 0,
u(1, y) = 1.
(b) Compare your result with the ﬁrst 5 and 10 summands in the series solution obtained
via separation of variables.
♦ 10.3.22. (a) Justify the construction of the ﬁnite element matrix for a square mesh described
in the text. (b) How would you modify the matrix for a rectangular mesh, as in Exercise
10.3.9?
♦ 10.3.23. Justify the inhomogeneous ﬁnite element construction in the text.
♥ 10.3.24. (a) Explain how to adapt the ﬁnite element method to a mixed boundary value problem with inhomogeneous Neumann conditions. (b) Apply your method to the problem
∂u
Δu = 0,
(x, 0) = x,
u(x, 1) = 0,
u(0, y) = 0,
u(1, y) = 0.
∂y
(c) Solve the boundary value problem via separation of variables. Compare the values of
your solutions at the center of the square.

10.4 Weak Solutions
An alternative route to the ﬁnite element method, which avoids the requirement of a
minimization principle, rests upon the notion of a weak solution to a diﬀerential equation
— a concept of considerable independent interest, since it includes many of the nonclassical
solutions that we encountered earlier in this book. In particular, the discontinuous shock
waves of Section 2.3 are, in fact, weak solutions to the nonlinear transport equation, as are
the continuous but only piecewise smooth solutions to the wave equation that resulted from
applying d’Alembert’s formula to nonsmooth initial data. Weak solutions have become an
incredibly powerful idea in the modern theory of partial diﬀerential equations, and we have
space to present only the very basics here. They are particularly appropriate in the study
of discontinuous and nonsmooth physical phenomena, including shock waves, cracks and
dislocations in elastic media, singularities in liquid crystals, and so on. In the mathematical
analysis of partial diﬀerential equations, it is often easier to prove the existence of a weak
solution, for which one can then try to establish suﬃcient smoothness in order that it
qualify as a classical solution. Further developments along with a range of applications can
be found in more advanced texts, including [38, 44, 61, 99, 107, 122].

428

10 Finite Elements and Weak Solutions

Weak Formulations of Linear Systems
The key idea behind the concept of a weak solution begins with a rather trivial observation:
the only element in an inner product space that is orthogonal to every other element is the
zero element.
Lemma 10.10. Let V be an inner product space with inner product†  · , · . An
element v ∈ V satisﬁes  v , v  = 0 for all v ∈ V if and only if v = 0.
Proof : In particular, v must be orthogonal to itself, so 0 =  v , v  = | v |2 ,
which immediately implies v = 0.
Q.E.D.
Thus, one method of solving a linear — or even nonlinear — equation F [ u ] = 0 is to
write it in the form
 F [ u ] , v  = 0
for all
v ∈ V,
(10.60)
where V is the target space of F : U → V . In particular, for an inhomogeneous linear
system, L[ u ] = f , with L: U → V a linear operator between inner product spaces, the
condition (10.60) takes the form
0 =  L[ u ] − f , v  =  L[ u ] , v  −  f , v 
or, equivalently,

 u , L∗[ v ]  −  f , v  = 0

for all

for all
v ∈ V,

v ∈ V,
(10.61)

where L∗ : V → U denotes the adjoint of the operator L, as deﬁned in (9.2). We will call
(10.61) the weak formulation of the original linear system.
So far we have not really done anything of substance, and, indeed, for linear systems
of algebraic equations, this more complicated characterization of solutions is of scant help.
However, this is no longer the case for diﬀerential equations, because, thanks to the integration by parts argument used to determine the adjoint operator, the solution u to the weak
form (10.61) is not restricted by the degree of smoothness required of a classical solution.
A simple example will illustrate the basic construction.
Example 10.11. On a bounded interval a ≤ x ≤ b, consider the elementary boundary value problem
d2 u
− 2 = f (x),
u(a) = u(b) = 0.
dx
The underlying vector space is U = { u(x) ∈ C2 [ a, b ] | u(a) = u(b) = 0 }. To obtain a
weak formulation, we multiply the diﬀerential equation by a test function v(x) ∈ U and
integrate:
 b


(10.62)
− u (x) − f (x) v(x) dx = 0.
a

The left-hand integral can be identiﬁed with the L2 inner product between the left-hand
side of the equation L[ u ] − f = − u − f = 0 and the test function v. According to
Lemma 10.10, condition (10.62) holds for all v(x) ∈ U if and only if u(x) ∈ U satisﬁes the
†
Shortly, as in the general framework developed in Chapter 9, V will be identiﬁed as the
target space of a linear operator L: U → V , and hence the choice of notation for its inner product.

10.4 Weak Solutions

429

boundary value problem. However, suppose that we integrate the ﬁrst term by parts once.
The boundary conditions on v imply that the boundary terms vanish, and the result is
 b

 
(10.63)
u (x) v  (x) − f (x) v(x) dx = 0.
a

A function u(x) that satisﬁes the latter integral condition for all smooth test functions v(x)
will be called a weak solution to the original boundary value problem. The key observation
is that the original diﬀerential equation, as well as the integral reformulation (10.62),
requires that u(x) be twice diﬀerentiable, whereas the weak version (10.63) requires only
that its ﬁrst derivative be deﬁned.
Of course, one need not stop at (10.63). Performing another integration by parts on
its ﬁrst term and invoking the boundary conditions on u produces
 b


(10.64)
− u(x) v  (x) − f (x) v(x) dx = 0.
a

Now u(x) need only be (piecewise) continuous in order that the integral be deﬁned —
keeping in mind that the test function v(x) is still required to be smooth. Equation (10.64)
is sometimes referred to as the fully weak formulation of the boundary value problem, while
the intermediate integral (10.63), in which the derivatives are evenly distributed among u
and v, is then known as the semi-weak formulation.
Remark : Recall also the Deﬁnition 6.5 of weak convergence, which similarly involves
integrating the standard convergence criterion against a suitable test function. Both are
part and parcel of a general weak analytical framework that plays an essential role in all
of modern advanced analysis, including partial diﬀerential equations.
The preceding example is a particular case of a general construction based on the
abstract formulation of self-adjoint linear systems in Chapter 9. Let L: U → V be a linear
map between inner product spaces, and let S = L∗ ◦ L : U → U be the associated selfadjoint operator. We further assume that ker L = {0}, which implies that S > 0 is positive
deﬁnite and, provided f ∈ rng S, the associated linear system
S[ u ] = L∗ ◦ L[ u ] = f

(10.65)

has a unique solution.
In order to construct a weak formulation of the linear system (10.65), we begin by
taking its inner product with a test function v ∈ U , whereby
0 =  S[ u ] − f , v  =  S[ u ] , v  −  f , v  =  L∗ ◦ L[ u ] , v  −  f , v .
Integration by parts, as in the preceding example, amounts to moving the adjoint operator
so that it acts on the test function v, and in this manner we obtain the weak formulation
 L[ u ] , L[ v ]  =  f , v 

for all

v ∈ U,

(10.66)

where we use our usual notation conventions regarding the inner products on U and V .
Warning: Unlike the minimization principle (10.1), the weak formulation (10.66) does
not have a factor of 12 on the left-hand side. Since, in the applications treated here, L is
a diﬀerential operator of order, say, k, the weak formulation requires only that u ∈ Ck
be k times diﬀerentiable, whereas, since S has order 2 k, the classical formulation (10.65)
requires u ∈ C2k to have twice as many derivatives.

430

10 Finite Elements and Weak Solutions

Similarly, the fully weak formulation involves an additional integration by parts, realized in the abstract framework by moving the linear operator L acting on u so as to act
on the test element v, and so
 u , L∗ ◦ L[ v ]  =  u , S[ v ]  =  f , v 

for all

v ∈ U.

(10.67)

In practice, it is often advantageous to restrict the class of test functions in order to
avoid technicalities involving smoothness and boundary behavior. This requires replacing
the simple argument used to establish Lemma 10.10 by a more sophisticated result, named
after the nineteenth-century German analyst Paul du Bois–Reymond.
Lemma 10.12. Let f (x) be a continuous function for a ≤ x ≤ b. Then
 b
f (x) v(x) dx = 0
a

for every C1 function v(x) with compact support in the open interval ( a, b ) if and only if
f (x) ≡ 0.
Proof : Suppose f (x0 ) > 0 for some a < x0 < b. Then, by continuity, f (x) > 0 for all
x in some interval a < x0 − ε < x < x0 + ε < b around x0 . Choose v(x) to be a C1 function
that is strictly positive in this interval and vanishes outside. An example is

2
| x − x0 | ≤ ε,
(x − x0 )2 − ε2 ,
(10.68)
v(x) =
0,
otherwise.
Then f (x) v(x) > 0 when | x − x0 | < ε and = 0 everywhere else. This implies
 b
 x0 +ε
f (x) v(x) dx =
f (x) v(x) dx > 0,
a

x0 −ε

which contradicts the original assumption. An analogous argument rules out f (x0 ) < 0 for
Q.E.D.
some a < x0 < b.

Finite Elements Based on Weak Solutions
To characterize weak solutions, one imposes the appropriate integral criterion on the entire
inﬁnite-dimensional space of smooth test functions. Thus, an evident approximation strategy is to restrict the criterion to a suitable ﬁnite-dimensional subspace, thereby seeking an
approximate weak solution that belongs to the subspace.
More precisely, concentrating on the self-adjoint framework discussed at the end of the
preceding subsection, we restrict the weak formulation (10.66) of the linear system (10.65)
to a ﬁnite-dimensional subspace W ⊂ U , and thus seek w ∈ W such that
 L[ w ] , L[ v ]  =  f , v 

for all

v ∈ W.

(10.69)

In this fashion, we characerize the ﬁnite element approximation to the weak solution u as
the element w ∈ W such that (10.69) holds for all v ∈ W .
To analyze this condition, as in (10.3), we now specify a basis ϕ1 , . . . , ϕn of W , and
thus can write both w and v as linear combinations thereof:
w = c1 ϕ1 + · · · + cn ϕn ,

v = d1 ϕ1 + · · · + dn ϕn .

10.4 Weak Solutions

431

Substituting these expressions into (10.69) produces the bilinear function
B(c, d) =

n

i,j = 1

kij ci dj −

n


bi di = cT K d − bT d = (K c − b)T d = 0,

(10.70)

i=1

where
kij =  L[ ϕi ] , L[ ϕj ] ,

bi =  f , ϕi ,

i, j = 1, . . . , n,

(10.71)
T

are the same as our earlier speciﬁcations (10.6, 7), and we used the fact that K = K
is a symmetric matrix to arrive at the ﬁnal expression in (10.70). The condition that
(10.69) hold for all v ∈ W is equivalent to the requirement that (10.70) hold for all
T
T
d = ( d1 , d2 , . . . , dn ) ∈ R n , which, in turn, implies that c = ( c1 , c2 , . . . , cn ) must satisfy
the linear system
K c = b.
But we immediately recognize that this is exactly the same as the ﬁnite element linear system (10.9)! We therefore conclude that for a positive deﬁnite linear system constructed as
above, the weak ﬁnite element approximation to the solution is the same as the minimizing
ﬁnite element approximation. In other words, it does not matter whether we characterize
the solutions through the minimization principle or the weak reformulation; the resulting
ﬁnite element approximations are exactly the same. There is thus no need to present any
additional examples illustrating this construction.
In general, while the weak formulation is of much wider applicability, outside of boundary value problems with well-deﬁned minimization principles, the rigorous underpinning
that guarantees that the numerical solution is close to the actual solution is harder to establish and, in fact, not always valid. Indeed, one can ﬁnd boundary value problems without
analytic solutions that have spurious ﬁnite element numerical solutions, and, conversely,
boundary value problems with solutions for which some ﬁnite element approximations do
not exist because the resulting coeﬃcient matrix is singular, [113, 126].
Shock Waves as Weak Solutions
Finally, let us return to our earlier analysis, in Section 2.3, of shock waves, but now in the
context of weak solutions. We begin by writing the nonlinear transport equation in the
conservative form
∂u
∂ 1 2
+
u = 0.
(10.72)
∂t
∂x 2
Since shock waves are discontinuous functions, they do not qualify as classical solutions.
However, they can be rigorously characterized as weak solutions, a formulation that will,
reassuringly, lead to the Rankine–Hugoniot Equal Area Rule for shock dynamics.
To construct a weak formulation of the nonlinear transport equation, we follow the
general framework, and hence begin by multiplying the equation (10.72) by a smooth test
function v(t, x) and integrating over a domain Ω ⊂ R 2 :

 
∂ 1 2
∂u
+
v(t, x) dt dx = 0.
(10.73)
u
∂t
∂x 2
Ω
As a direct consequence of the two-dimensional version of the du Bois–Reymond Lemma,
cf. Exercise 10.4.7, if u(t, x) ∈ C1 and condition (10.73) holds for all C1 functions v(t, x)

432

10 Finite Elements and Weak Solutions

Ω+
C
n−

n+

n+
n−
Ω−
Figure 10.15.

Integration domain for weak shock-wave solution.

with compact support contained in Ω, then u(t, x) is necessarily a classical solution to the
partial diﬀerential equation (10.72). The next step is to integrate by parts in order to
remove the derivatives from u, and this is accomplished by appealing to Green’s formula
(6.82), which we rewrite in the form


 
 
&
∂u1
∂v
∂v
∂u2
(u · n) v ds −
u1
+ u2
dt dx =
+
v dt dx, (10.74)
∂t
∂x
∂t
∂x
Ω
∂Ω
Ω
T

where u = ( u1 , u2 ) . In our case, we identify the integral in (10.73) with the left-hand
side of (10.74) by setting u1 = u, u2 = 12 u2 . Since v has compact support, the boundary
integral vanishes, and thus we arrive at the weak formulation of the equation.
Deﬁnition 10.13. A function u(t, x) is said to be a weak solution to the nonlinear
transport equation (10.72) on Ω ⊂ R 2 if

 
∂v
1 2 ∂v
u
+ 2u
dt dx = 0
(10.75)
∂t
∂x
Ω
for all C1 functions v(t, x) with compact support: supp v ⊂ Ω.
The key point is that, in the weak formulation (10.75), the derivatives are acting solely
on v(t, x), which we assume to be smooth, and not on our prospective solution u(t, x), which
now need not even be continuous for the integral to be well deﬁned.
Let us derive the Rankine–Hugoniot shock condition (2.53) as a consequence of the
weak formulation. Suppose u(t, x) is a weak solution, deﬁned on a domain Ω ⊂ R 2 , that
has a single jump discontinuity along a curve C parametrized by x = σ(t) that separates
Ω into two subdomains, say Ω+ and Ω− , such that its restriction to either subdomain,
denoted by u+ = u | Ω+ and u− = u | Ω− , are each classical solutions on their respective
domains, while the separating curve C = { x = σ(t) } represents a shock-wave discontinuity.
For speciﬁcity, we assume that Ω+ lies above and Ω− lies below C in the (t, x)–plane; see
Figure 10.15. Let us investigate what the preceding weak formulation implies in this
situation. We split the integral (10.75) into two parts, and then apply the integration
by parts formula (10.74) to each individual double integral, keeping in mind that, when

10.4 Weak Solutions

433

restricted to Ω+ or Ω− , the integrand is suﬃciently smooth to justify application of the
formula:

 
∂v
1 2 ∂v
+ 2u
dt dx
0=
u
∂t
∂x
Ω




 
∂v
∂v
1 2 ∂v
1 2 ∂v
=
+ 2 u+
dt dx +
+ 2 u−
dt dx
u+
u−
∂t
∂x
∂t
∂x
Ω+
Ω−

&
 
∂u+
∂ 1 2
=
+
(
u+ · n+ ) v ds −
u
v dt dx +
∂t
∂x 2 +
∂Ω+
Ω+

 
&
∂u−
∂ 1 2
+
(
u− · n− ) v ds −
u
v dt dx
+
∂t
∂x 2 −
∂Ω−
Ω−

 − · n− ) v ds.
(
u+ · n+ + u
=
C

Here


+ =
u

u+
1 2
2 u+




,

− =
u

u−
1 2
2 u−


,

while n+ , n− are the unit outwards normals on, respectively, ∂Ω+ and ∂Ω− . The ﬁnal
equality follows from the fact that the support of v is contained strictly inside Ω, and
hence vanishes on those parts of the boundaries of Ω+ and Ω− that do not lie on the
curve C. In particular, since C is the graph of x = σ(t), the unit normals along it are,
respectively,
⎛ dσ ⎞
⎛ dσ ⎞
1
1
−

,
n
=
−
n
=
n+ = 
−
+
 2 ⎝ dt ⎠
 2 ⎝ dt ⎠,
dσ
dσ
1+
1+
−1
1
dt
dt
keeping in mind our convention that Ω+ lies above and Ω− lies below C, while

 2
dσ
dt.
ds = 1 +
dt
Thus, the ﬁnal line integral reduces to

 
dσ
− 12 (u2− − u2+ ) v dt = 0.
(u− − u+ )
dt
C

(10.76)

Since (10.76) vanishes for all C1 functions v(t, x) with compact support, the du Bois–
Reymond Lemma 10.12 implies that
(u− − u+ )

dσ
= 12 (u2− − u2+ )
dt

on

C,

thereby re-establishing the Rankine–Hugoniot shock condition (2.53). The upshot is that
the shock-wave solutions produced in Section 2.3 are bona ﬁde weak solutions.
Another computation shows that the rarefaction wave (2.54) also qualiﬁes as a weak
solution. However, so does the non-physical reverse shock solution discussed in Example 2.11. Thus, although the weak formulation recovers the Rankine–Hugoniot condition,
it does not address the problem of causality, which must be additionally imposed to single

434

10 Finite Elements and Weak Solutions

out a unique, physically meaningful weak solution. Further developments of these ideas
can be found in more advanced monographs, e.g., [107, 122].

Exercises
10.4.1. Write out semi-weak and fully weak formulations for the following boundary value
problems:
(a) − u + 2 u = x − x2 , u(0) = u(1) = 0;
x 
(b) e u + u = cos x, u (0) = u (2) = 0; (c) x u + u + x u = 0, u(1) = u(2) = 0.
10.4.2. (a) Write down a weak formulation for the boundary value problem − u + 3 u = x,
u(0) = u(1) = 0. (b) Based on your weak formulation, construct a ﬁnite element
approximation to the solution, using n = 10 nodes.
10.4.3. (a) Write down a weak formulation of the transport equation ut + 3 ux = 0 on the real


line.

(b) Solve the initial value problem u(0, x) =

1 − | x |,
0,

| x | ≤ 1,
otherwise.

(c) Explain why the result of part (b) is not a classical solution to the transport equation.
Is it a weak solution according to your formulation in part (a)?
10.4.4. (a) Write down a semi-weak formulation of the wave equation utt = 4 uxx on the real
line. (b) Solve the initial value problem u(0, x) = ρ(x), ut (0, x) = 0, where the initial
displacement is a ramp function (6.25). (c) Explain why the result of part (b) is not a
classical solution to the wave equation. Does it satisfy the semi-weak formulation of part
(a)? Explain your answer.
♦ 10.4.5. (a) Starting with the nonlinear transport equation written in the alternative conservative form (2.56), ﬁnd a corresponding weak formulation.
(b) Prove that your weak formulation produces the alternative entropy condition (2.58) for
the motion of a shock discontinuity.
♦ 10.4.6. Prove that the du Bois–Reymond Lemma 10.12 remains valid even when v(x) ∈ C∞ is
required to be inﬁnitely diﬀerentiable.
2
♦ 10.4.7. The Two-dimensional du Bois–Reymond Lemma:
  Let Ω ⊂ R be a domain, and f (t, x)
a continuous function deﬁned thereon. Prove that
f (t, x) v(t, x) dt dx = 0 for every C1
Ω

function v(t, x) with compact support in Ω if and only if f (t, x) ≡ 0.
♠ 10.4.8. (a) Investigate the ability of ﬁnite elements to approximate a solution to the
non-positive-deﬁnite boundary value problem Δu + λ u = 0, 0 < x < π, 0 < y < π,
u(x, 0) = 1, u(x, π) = u(0, y) = u(π, y) = 0, when (i ) λ = 1, (ii ) λ = 2. Use separation of
variables to ﬁnd a series solution and use it to determine the accuracy of your ﬁnite element
solution in part (a).

Chapter 11

Dynamics of Planar Media

In previous chapters, we studied the equilibrium conﬁgurations of planar media — plates
and membranes — governed by the two-dimensional Laplace and Poisson equations. In
this chapter, we analyze their dynamics, modeled by the two-dimensional heat and wave
equations. The heat equation describes diﬀusion of, say, heat energy in a thin metal plate,
an animal population dispersing over a region, or a pollutant spreading out into a shallow
lake. The wave equation models small vibrations of a two-dimensional membrane such as a
drum. Since both equations ﬁt into the general framework for dynamics that we established
in Section 9.5, their solutions share many of the general qualitative and analytic properties
possessed by their respective one-dimensional counterparts.
Although the increase in dimension may tax our analytical prowess, we have, in fact,
already mastered the principal solution techniques: separation of variables, eigenfunction
series, and fundamental solutions. When applied to partial diﬀerential equations in higher
dimensions, separation of variables in curvilinear coordinates often leads to new linear,
but non-constant-coeﬃcient, ordinary diﬀerential equations, whose solutions are no longer
elementary functions. Rather, they are expressed in terms of a variety of important special
functions, which include the error and Airy functions we encountered earlier; the Bessel
functions, which play a starring role in the present chapter; and the Legendre and Ferrers
functions, spherical harmonics, and spherical Bessel functions arising in three-dimensional
problems. Special functions are ubiquitous in more advanced applications in physics, chemistry, mechanics, and mathematics, and, over the last two hundred and ﬁfty years, many
prominent mathematicians have devoted signiﬁcant eﬀort to establishing their fundamental properties, to the extent that they are now, by and large, well understood, [86]. To
acquire the requisite familiarity with special functions, in preparation for employing them
to solve higher-dimensional partial diﬀerential equations, we must ﬁrst learn basic series
solution techniques for linear second-order ordinary diﬀerential equations.

11.1 Diﬀusion in Planar Media
As we learned in Chapter 4, the equilibrium temperature u(x, y) of a thin, uniform, isotropic
plate is governed by the two-dimensional Laplace equation
Δu = uxx + uyy = 0.
Working by analogy, the dynamical diﬀusion of the plate’s temperature should be modeled
by the two-dimensional heat equation
ut = γ Δu = γ (uxx + uyy ).
P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_11, © Springer International Publishing Switzerland 2014

(11.1)
435

436

11 Dynamics of Planar Media

The coeﬃcient γ > 0, assumed constant, measures the relative speed of diﬀusion of heat
energy throughout the plate; its positivity is required on physical grounds, and also serves
to avoid ill-posedness inherent in running diﬀusion processes backwards in time. In this
model, we are assuming that the plate is uniform and isotropic, and experiences no loss of
heat or external heat sources other than at its edge — which can be arranged by covering
its top and bottom with insulation.
The solution u(t, x) = u(t, x, y) to the heat equation measures the temperature, at
time t, at each point x = (x, y) in the (bounded) domain Ω ⊂ R 2 occupied by the plate.
To uniquely specify the solution u(t, x, y), we must impose suitable initial and boundary
conditions. The initial data is the temperature of the plate
(x, y) ∈ Ω,

u(0, x, y) = f (x, y),

(11.2)

at an initial time, which for simplicity, we take to be t0 = 0. The most important boundary
conditions are as follows:
• Dirichlet boundary conditions: Specifying
u=h

on

∂Ω

(11.3)

ﬁxes the temperature along the edge of the plate.
• Neumann boundary conditions: Let n be the unit outwards normal on the boundary
of the domain. Specifying the normal derivative of the temperature,
∂u
=k
on
∂Ω,
(11.4)
∂n
eﬀectively prescribes the heat ﬂux along the boundary. Setting k = 0 corresponds
to an insulated boundary.
• Mixed boundary conditions: More generally, we can impose Dirichlet conditions on
part of the boundary D  ∂Ω and Neumann conditions on its complement N =
∂Ω \ D. For instance, homogeneous mixed boundary conditions
∂u
(11.5)
=0
on
N,
∂n
correspond to freezing a portion of the boundary and insulating the remainder.
• Robin boundary conditions:
u=0

on

D,

∂u
(11.6)
+βu=τ
on
∂Ω,
∂n
where the edge of the plate sits in a heat bath at temperature τ .
Under reasonable assumptions on the domain, the initial data, and the boundary data,
a general theorem, [34, 38, 99], guarantees the existence of a unique solution u(t, x, y) to
any of these initial-boundary value problems for all subsequent times t > 0. Our practical
goal is to both compute and understand the behavior of the solution in speciﬁc situations.
Derivation of the Diﬀusion and Heat Equations
The physical derivation of the two-dimensional (and three-dimensional) heat equation relies
on the same basic thermodynamic laws that were used, in Section 4.1, to establish the
one-dimensional version. The ﬁrst principle is that heat energy ﬂows from hot to cold as

11.1 Diﬀusion in Planar Media

437

rapidly as possible. According to multivariable calculus, [8, 108], the negative temperature
gradient − ∇u points in the direction of the steepest decrease in the temperature function
u at a point, and so heat energy will ﬂow in that direction. Therefore, the heat ﬂux vector
w, which measures the magnitude and direction of the ﬂow of heat energy, should be
proportional to the temperature gradient:
w(t, x, y) = − κ(x, y) ∇u(t, x, y).

(11.7)

The scalar quantity κ(x, y) > 0 measures the thermal conductivity of the material, so
(11.7) is the multi-dimensional form of Fourier’s Law of Cooling (4.5). We are assuming
that the thermal conductivity depends only on the position (x, y) ∈ Ω, which means that
the material in the plate
(a) is not changing in time;
(b) is isotropic, meaning that its thermal conductivity is the same in all directions;
(c) and, moreover, its thermal conductivity is not aﬀected by any change in temperature.
Dropping either assumption (b) or (c) would result in a considerably more challenging
nonlinear diﬀusion equation.
The second thermodynamic principle is that, in the absence of external heat sources,
heat can enter any subregion R ⊂ Ω only through its boundary ∂R. (Keep in mind that
the plate is insulated from above and below.) Let ε(t, x, y) denote the heat energy density
at each time and point in the domain, so that

HR (t) =
ε(t, x, y) dx dy
R

represents the total heat energy contained within the subregion R at time t. The amount
of additional heat energy entering R at a boundary point x ∈ ∂R is given by the normal
component of the heat ﬂux vector, namely − w · n, where, as always, n denotes the outward
unit normal to the boundary ∂R. Thus, the total heat ﬂux entering the region
& R is obw · n ds.
tained by integration along the boundary of R, resulting in the line integral −
∂R
Equating the rate of change of heat energy to the heat ﬂux yields

&

∂ε
dHR
=
(t, x, y) dx dy = −
w · n ds = −
∇ · w dx dy,
dt
R ∂t
∂R
R
where we applied the divergence form of Green’s Theorem, (6.80), to convert the ﬂux line
integral into a double integral. Thus,

 
∂ε
+ ∇ · w dx dy = 0.
(11.8)
∂t
R
Keep in mind that this result must hold for any subdomain R ⊂ Ω. Now, according to
Exercise 11.1.13, the only way in which an integral of a continuous function can vanish for
all subdomains is if the integrand is identically zero, and so
∂ε
+ ∇ · w = 0.
∂t

(11.9)

In this manner, we arrive at the basic conservation law relating the heat energy density ε
and the heat ﬂux vector w.

438

11 Dynamics of Planar Media

As in our one-dimensional model, cf. (4.3), the heat energy density ε(t, x, y) is proportional to the temperature, so
ε(t, x, y) = σ(x, y) u(t, x, y),

where

σ(x, y) = ρ(x, y) χ(x, y)

(11.10)

is the product of the density ρ and the speciﬁc heat capacity χ of the material at the
point (x, y) ∈ Ω. Combining this with the Fourier Law (11.7) and the energy balance
equation (11.10) leads to the general two-dimensional diﬀusion equation
∂u
1
= ∇ · κ ∇u
∂t
σ

(11.11)

governing the thermodynamics of an isotropic medium in the absence of external heat
sources or sinks. In full detail, this second-order partial diﬀerential equation is





1
∂
∂u
∂
∂u
∂u
=
κ(x, y)
+
κ(x, y)
.
(11.12)
∂t
σ(x, y) ∂x
∂x
∂y
∂y
Such diﬀusion equations are also used to model movements of populations, e.g., bacteria in a petri dish or wolves in the Canadian Rockies, [81, 84]. Here the solution u(t, x, y)
represents the population density at position (x, y) at time t, which diﬀuses over the domain due to random motions of the individuals. Similar diﬀusion processes model the
mixing of solutes in liquids, with the diﬀusion induced by the random Brownian motion
from molecular collisions. More generally, diﬀusion processes in the presence of chemical
reactions and convection due to ﬂuid motion are modeled by the more general class of
reaction-diﬀusion and convection-diﬀusion equations, [107].
In particular, if the body (or the environment or the solvent) is uniform, then both
σ and κ are constant, and so (11.11) reduces to the heat equation (11.1) with thermal
diﬀusivity
κ
κ
γ= =
.
(11.13)
σ
ρχ
Both the heat and more general diﬀusion equations are examples of parabolic partial differential equations, the terminology being adapted from Deﬁnition 4.12 to apply to partial
diﬀerential equations in more than two variables. As we will see, all the basic qualitative
features of solutions to the one-dimensional heat equation carry over to parabolic partial
diﬀerential equations in higher dimensions.
Indeed, the general diﬀusion equation (11.12) can be readily ﬁt into the self-adjoint
dynamical framework of Section 9.5, taking the form
ut = − ∇∗ ◦ ∇u.

(11.14)

The gradient operator ∇ maps scalar ﬁelds u to vector ﬁelds v = ∇u; its adjoint ∇∗ , which
goes in the reverse direction, is taken with respect to the weighted inner products


  =
 (x, y) κ(x, y) dx dy,
u(x, y) u
(x, y) σ(x, y) dx dy,  v , v
v(x, y) · v
u,u
 =
Ω

Ω

(11.15)
between, respectively, scalar and vector ﬁelds. As in (9.33), a straightforward integration
by parts tells us that


 
1
1 ∂(κ v1 ) ∂(κ v2 )
v1
∗
∇ v = − ∇ · (κ v) = −
+
,
when
v=
. (11.16)
v2
σ
σ
∂x
∂y

11.1 Diﬀusion in Planar Media

439

Therefore, the right-hand side of (11.14) equals
− ∇∗ ◦ ∇u =

1
∇ · (κ ∇u),
σ

(11.17)

which thereby recovers the general diﬀusion equation (11.11). As always, the validity of
the adjoint formula (11.16) rests on the imposition of suitable homogeneous boundary
conditions: Dirichlet, Neumann, mixed, or Robin.
In particular, to obtain the heat equation, we take σ and κ to be constant, and so
the inner products (11.15) reduce, up to a constant factor, to the usual L2 inner products
between scalar and vector ﬁelds. In this case, the adjoint of the gradient is, up to a scale
factor, minus the divergence: ∇∗ = − γ ∇· , where γ = κ/σ. In this scenario, (11.14)
reduces to the two-dimensional heat equation (11.1).
Separation of Variables
Let us now discuss analytical solution techniques. According to Section 9.5, the separable
solutions to any linear evolution equation
ut = − S[ u ]

(11.18)

u(t, x, y) = e− λ t v(x, y).

(11.19)

are of exponential form
Since the linear operator S involves diﬀerentiation with respect to only the spatial variables
x, y, we obtain
∂u
= − λ e− λ t v(x, y),
∂t

while

S[ u ] = e− λ t S[ v ].

Substituting back into the diﬀusion equation (11.18) and canceling the exponentials, we
conclude that
S[ v ] = λ v.
(11.20)
Thus, v(x, y) must be an eigenfunction for the linear operator S, subject to the relevant
homogeneous boundary conditions.
In the case of the heat equation (11.1),
S[ u ] = − γ Δu,
and hence, as in Example 9.40, the eigenvalue equation (11.20) is the two-dimensional
Helmholtz equation

 2
∂2v
∂ v
+
+ λ v = 0.
(11.21)
γ Δv + λ v = 0,
or, in detail,
γ
∂x2
∂y 2
According to Theorem 9.34, self-adjointness implies that the eigenvalues are all real and
nonnegative: λ ≥ 0. In the positive deﬁnite cases — Dirichlet and mixed boundary
conditions — they are strictly positive, while the Neumann boundary value problem admits
a zero eigenvalue λ0 = 0 corresponding to the constant eigenfunction v0 (x, y) ≡ 1.
Let us index the eigenvalues in increasing order:
0 < λ1 ≤ λ2 ≤ λ3 ≤ · · · ,

(11.22)

440

11 Dynamics of Planar Media

repeated according to their multiplicities, where λ0 = 0 is an eigenvalue only in the Neumann case, and λk → ∞ as k → ∞. For each eigenvalue λk , let vk (x, y) be an independent
eigenfunction. The corresponding separable solution is
uk (t, x, y) = e− λk t vk (x, y).
Those corresponding to positive eigenvalues are exponentially decaying in time, while a
zero eigenvalue produces a constant solution u0 (t, x, y) ≡ 1. The general solution to the
homogeneous boundary value problem can then be built up as an inﬁnite series in these
basic eigensolutions
u(t, x, y) =

∞


ck uk (t, x, y) =

k=1

∞


ck e− λk t vk (x, y).

(11.23)

k=1

The coeﬃcients ck are prescribed by the initial conditions, which require
∞


ck vk (x, y) = f (x, y).

(11.24)

k=1

Since S is self-adjoint, Theorem 9.33 guarantees orthogonality† of the eigenfunctions under
the L2 inner product on the domain Ω:

 vj , vk  =
vj (x, y) vk (x, y) dx dy = 0,
j = k.
(11.25)
Ω

As a consequence, the coeﬃcients in (11.24) are given by the standard orthogonality formula
(9.104), namely

 f , vk 
ck =
=
 vk 2

f (x, y) vk (x, y) dx dy
Ω



.

(11.26)

2

vk (x, y) dx dy
Ω

(For the more general diﬀusion equation (11.11), one uses the appropriately weighted inner
product.) The exponential decay of the eigenfunction coeﬃcients implies that the resulting
eigensolution series (11.23) converges and thus produces the solution to the initial-boundary
value problem for the diﬀusion equation. See [34; p. 369] for a precise statement and proof
of the general theorem.
Qualitative Properties
Before tackling examples in which we are able to construct explicit formulas for the eigenfunctions and eigenvalues, let us see what the eigenfunction series solution (11.23) can
tell us about general diﬀusion processes. Based on our experience with the case of a onedimensional bar, the ﬁnal conclusions will not be especially surprising. Indeed, they also
apply, word for word, to diﬀusion processes in three-dimensional solid bodies. A reader who
is impatient to see the explicit formulas may wish to skip ahead to the following section,
returning here as needed.
†
As usual, in the case of a repeated eigenvalue, one chooses an orthogonal basis of the
associated eigenspace to ensure orthogonality of all the basis eigenfunctions.

11.1 Diﬀusion in Planar Media

441

Keep in mind that we are still dealing with the solution to the homogeneous boundary
value problem. The ﬁrst observation is that all terms in the series solution (11.23), with the
possible exception of a null eigenfunction term that appears in the semi-deﬁnite Neumann
case, are tending to zero exponentially fast. Since most eigenvalues are large, all the higherorder terms in the series become almost instantaneously negligible, and hence the solution
can be accurately approximated by a ﬁnite sum over the ﬁrst few eigenfunction modes.
As time goes on, more and more of the modes can be neglected, and the solution decays
to thermal equilibrium at an exponentially fast rate. The rate of convergence to thermal
equilibrium is, for most initial data, governed by the smallest positive eigenvalue λ1 > 0
for the Helmholtz boundary value problem on the domain.
In the positive deﬁnite cases of homogeneous Dirichlet or mixed boundary conditions,
thermal equilibrium is u(t, x, y) → u (x, y) ≡ 0. Here, the equilibrium temperature is equal
to the zero boundary temperature — even if this temperature is ﬁxed on only a small part
of the boundary. The initial heat is eventually dissipated away through the uninsulated
part of the boundary. In the semi-deﬁnite Neumann case, corresponding to a completely
insulated plate, the general solution has the form
u(t, x, y) = c0 +

∞


ck e− λk t vk (x, y),

(11.27)

k=1

where the sum is over the positive eigenmodes, λk > 0. Since all the summands are exponentially decaying, the ﬁnal equilibrium temperature u = c0 is the same as the constant
term in the eigenfunction expansion. We evaluate this term using the orthogonality formula
(11.26), and so, as t → ∞,

f (x, y) dx dy

Ω
f ,1
1
=
f (x, y) dx dy. (11.28)
=
u(t, x, y) −→ c0 =

area Ω
 1 2
Ω
dx dy
Ω

We conclude that the equilibrium temperature is equal to the average initial temperature
distribution. Thus, when the plate is fully insulated, the heat energy cannot escape, and
instead redistributes itself in a uniform manner over the domain.
Diﬀusion has a smoothing eﬀect on the initial temperature distribution f (x, y). Assume that the eigenfunction coeﬃcients are uniformly bounded, so | ck | ≤ M for some
constant M . This will certainly be the case if f (x, y) is piecewise continuous or, more generally, belongs to L2 , since Bessel’s inequality, (3.117), which holds for general orthogonal
systems, implies that ck → 0 as k → ∞. Many distributions, including delta functions,
also have bounded Fourier coeﬃcients. Then, at any time t > 0 after the initial instant,
the coeﬃcients ck e− λk t in the eigenfunction series solution (11.23) are exponentially small
as k → ∞, which is enough to ensure smoothness of the solution u(t, x, y) for each t > 0.
Therefore, the diﬀusion process serves to immediately smooth out jumps, corners, and
other discontinuities in the initial data. As time progresses, the local variations in the solution become less and less pronounced, as it asymptotically reaches a constant equilibrium
state.
As a result, diﬀusion processes can be eﬀectively applied to smooth and denoise planar
images. The initial data u(0, x, y) = f (x, y) represents the gray-scale value of the image at
position (x, y), so that 0 ≤ f (x, y) ≤ 1, with 0 representing black and 1 representing white.
As time progresses, the solution u(t, x, y) represents a more and more smoothed version

442

11 Dynamics of Planar Media

Figure 11.1.

Smoothing a gray scale image.

of the image. Although this has the eﬀect of removing unwanted high-frequency noise,
there is also a gradual blurring of the actual features. Thus, the “time” or “multiscale”
parameter t needs to be chosen to optimally balance between the two eﬀects — the larger
t is the more noise is removed, but the more noticeable the blurring. A representative
illustration appears in Figure 11.1. The blurring aﬀects small-scale features ﬁrst, then,
gradually, those at larger and larger scales, until eventually the entire image is blurred to
a uniform gray. To further suppress undesirable blurring eﬀects, modern image-processing
ﬁlters are based on anisotropic (and thus nonlinear ) diﬀusion equations; see [100] for a
survey of recent progress in this active ﬁeld.
Since the forward heat equation eﬀectively blurs the features in an image, we might be
tempted to reverse “time” in order to sharpen the image. However, the argument presented
in Section 4.1 tells us that the backwards heat equation is ill-posed, and hence cannot be
used directly for this purpose. Various “regularization” strategies have been devised to
circumvent this mathematical barrier, and thereby design eﬀective image enhancement
algorithms, [46].
Inhomogeneous Boundary Conditions and Forcing
Let us next brieﬂy discuss how to incorporate inhomogeneous boundary conditions and
external heat sources into the general solution framework. Consider, as a speciﬁc example,
the forced heat equation
ut = γ Δu + F (x, y)

for

(x, y) ∈ Ω,

(11.29)

where F (x, y) represents an unvarying external heat source or sink, subject to inhomogeneous Dirichlet boundary conditions
u(x, y) = h(x, y)

for

(x, y) ∈ ∂Ω,

(11.30)

that ﬁxes the temperature of the plate on its boundary. When the external forcing does
not vary in time, we expect the solution to eventually settle down to an equilibrium conﬁguration: u(t, x, y) → u (x, y) as t → ∞. This will be justiﬁed below.
The time-independent equilibrium temperature u (x, y) satisﬁes the equation obtained
by setting ut = 0 in the evolution equation (11.29), which reduces it to the Poisson equation
− γ Δu = F

for

(x, y) ∈ Ω.

(11.31)

The equilibrium solution is subject to the same inhomogeneous Dirichlet boundary conditions (11.30). Positive deﬁniteness of the Dirichlet boundary value problem implies that

11.1 Diﬀusion in Planar Media

443

there is a unique equilibrium solution, which can be characterized as the sole minimizer of
the associated Dirichlet principle; for details see Section 9.3.
With the equilibrium solution in hand, we let
v(t, x, y) = u(t, x, y) − u (x, y)
measure the deviation of the dynamical solution u from its eventual equilibrium. By
linearity v(t, x, y) satisﬁes the unforced heat equation subject to homogeneous boundary
conditions:
vt = γ Δv,
(x, y) ∈ Ω,
v = 0,
(x, y) ∈ ∂Ω.
(11.32)
Therefore, v can be expanded in an eigenfunction series (11.23), and will decay to zero,
v(t, x, y) → 0, at an exponentially fast rate prescribed by the smallest eigenvalue λ1 of
the associated homogeneous Helmholtz boundary value problem. (Special initial data can
decay at a faster rate, prescribed by a larger eigenvalue.) Consequently, the solution to the
forced inhomogeneous problem (11.29–30) will approach thermal equilibrium,
u(t, x, y) = v(t, x, y) + u (x, y) −→ u (x, y),
at exactly the same exponential rate as its homogeneous counterpart.
The Maximum Principle
Finally, let us state and prove the (Weak) Maximum Principle for the two-dimensional
heat equation. As in the one-dimensional situation described in Section 8.3, it states that
the maximum temperature in a body that is either insulated or having heat removed from
its interior must occur either at the initial time or on its boundary. Observe that there are
no conditions imposed on the boundary temperatures.
Theorem 11.1. Suppose u(t, x, y) is a solution to the forced heat equation
ut = γ Δu + F (t, x, y),

for

(x, y) ∈ Ω,

0 < t < c,

where Ω is a bounded domain, and γ > 0. Suppose F (t, x, y) ≤ 0 for all (x, y) ∈ Ω and
0 ≤ t ≤ c. Then the global maximum of u on the set { (t, x, y) | (x, y) ∈ Ω, 0 ≤ t ≤ c }
occurs either when t = 0 or at a boundary point (x, y) ∈ ∂Ω.
Proof : First, let us prove the result under the assumption that F (t, x, y) < 0
everywhere.
At alocal interior maximum, ut = 0, and, since its Hessian matrix

uxx uxy
2
must be negative semi-deﬁnite, both diagonal entries uxx , uyy ≤ 0
∇ u =
uxy uyy
there. This would imply that ut − γ Δu ≥ 0, resulting in a contradiction. If the maximum
were to occur when t = c, then ut ≥ 0 there, and also uxx , uyy ≤ 0, leading again to a
contradiction.
To generalize to the case F (t, x, y) ≤ 0, which includes the heat equation when
F (t, x, y) ≡ 0, set
v(t, x, y) = u(t, x, y) + ε (x2 + y 2 ),

where

ε > 0.

Then,
∂v
= γ Δv − 4 γ ε + F (t, x, y) = γ Δv + F (t, x, y),
∂t

444

where

11 Dynamics of Planar Media

F (t, x, y) = F (t, x, y) − 4 γ ε < 0.

Thus, by the previous paragraph, the maximum of v occurs either when t = 0 or at a
boundary point (x, y) ∈ ∂Ω. We then let ε → 0 and conclude the same for u. More
precisely, let u(t, x, y) ≤ M on t = 0 or (x, y) ∈ ∂Ω. Then
.
v(t, x, y) ≤ M + C ε,
where
C = max x2 + y 2 (x, y) ∈ ∂Ω < ∞,
since Ω is a bounded domain. Thus,
u(t, x, y) ≤ v(t, x, y) ≤ M + C ε.
Letting ε → 0 proves that u(t, x, y) ≤ M at all (x, y) ∈ Ω, 0 ≤ t ≤ c, which completes the
proof.
Q.E.D.
Remark : The preceding proof can be readily adapted to general diﬀusion equations
(11.12) — assuming that the coeﬃcients σ, κ remain strictly positive throughout the domain.

Exercises
11.1.1. A homogeneous, isotropic circular metal disk of radius 1 meter has its entire boundary
insulated. The initial temperature at a point is equal to the distance of the point from the
center. Formulate an initial-boundary value problem governing the disk’s subsequent
temperature dynamics. What is the eventual equilibrium temperature of the disk?
11.1.2. A homogeneous, isotropic, circular metal disk of radius 2 cm has half its boundary ﬁxed
at 100◦ and the other half insulated. Given a prescribed initial temperature distribution,
set up the initial-boundary value problem governing its subsequent temperature proﬁle.
What is the eventual equilibrium temperature of the disk? Does your answer depend on
the initial temperature?
11.1.3. Given the initial temperature distribution f (x, y) = x y (1 − x)(1 − y) on the unit square
Ω = { 0 ≤ x, y ≤ 1 }, determine the equilibrium temperature when subject to homogeneous
(a) Dirichlet boundary conditions; (b) Neumann boundary conditions.
11.1.4. A square plate with side lengths 1 meter has its right and left edges insulated, its top
edge held at 100◦ , and its bottom edge held at 0◦ . Assuming that the plate is made out of
a homogeneous, isotropic material, formulate an appropriate initial-boundary value
problem describing the temperature dynamics of the plate. Then ﬁnd its eventual equilibrium temperature.
11.1.5. A square plate with side lengths 1 meter has initial temperature 5◦ throughout, and
evolves subject to the Neumann boundary conditions ∂u/∂n = 1 on its entire boundary.
What is the eventual equilibrium temperature?
♥ 11.1.6. Let u(t, x, y) be a solution to the heat equation on a bounded domain Ω subject to
homogeneous Neumann conditions on its boundary ∂Ω. (a) Prove that the total heat
H(t) =
u(t, x, y) dx dy is conserved, i.e., is constant in time. (b) Use part (a) to
Ω
prove that the eventual equilibrium solution is everywhere equal to the average of the initial
temperature u(0, x, y). (c) What can you say about the behavior of the total heat for the
homogeneous Dirichlet boundary value problem? (d) What about an inhomogeneous
Dirichlet boundary value problem?

11.2 Explicit Solutions of the Heat Equation

445

11.1.7. Let u(t, x, y) be a nonconstant solution to the heat equation on a connected, bounded
domain Ω subject to homogeneous Dirichlet boundary conditions on ∂Ω. (a) Prove that its
L2 norm N(t) =

Ω

u(t, x, y)2 dx dy is a strictly decreasing function of t. (b) Is this

also true for mixed boundary conditions? (c) For Neumann boundary conditions?
11.1.8. Are the conclusions in Exercises 11.1.6 and 11.1.7 valid for the general diﬀusion
equation (11.12)?
♦ 11.1.9. Write out the eigenvalue equation governing the separable solutions to the general
diﬀusion equation (11.11), subject to appropriate boundary conditions. Given a complete
system of eigenfunctions, write down the eigenfunction series solution to the initial value
problem u(0, x, y) = f (x, y), including the formulas for the coeﬃcients.
11.1.10. True or false: The equilibrium temperature of a fully insulated nonuniform plate whose
thermodynamics are governed by the general diﬀusion equation (11.12) equals the average
initial temperature.
11.1.11. Let α > 0, and consider the initial-boundary value problem ut = Δu − α u, u(0, x, y) =
f (x, y) on a bounded domain Ω ⊂ R 2 , with boundary conditions ∂u/∂n = 0 on ∂Ω.
(a) Write the equation in self-adjoint form (9.122). Hint: Look at Exercise 9.3.26.
(b) Prove that the problem has a unique equilibrium solution.
11.1.12. Write each of the following linear evolution equations in the self-adjoint form (9.122)
by choosing suitable inner products and a suitable set of homogeneous boundary conditions.
Is the operator you construct positive deﬁnite?
(a) ut = uxx + uyy − u, (b) ut = y uxx + x uyy , (c) ut = Δ2 u.
♦ 11.1.13. Prove that if f (x, y) is continuous and

R

f (x, y) dx dy = 0 for all R ⊂ Ω, then

f (x, y) ≡ 0 for (x, y) ∈ Ω. Hint: Adapt the method in Exercise 6.1.23.

11.2 Explicit Solutions of the Heat Equation
Solving the two-dimensional heat equation in series form requires knowing the eigenfunctions for the associated Helmholtz boundary value problem. Unfortunately, as with
the vast majority of partial diﬀerential equations, explicit solution formulas are few and far
between. In this section, we discuss two speciﬁc cases in which the required eigenfunctions
can be found in closed form. The calculations rely on a further separation of variables,
which, as we know, works in only a very limited class of domains. Nevertheless, interesting
solution features can be gleaned from these particular geometries.
The ﬁrst example is a rectangular domain, and the eigensolutions can be expressed in
terms of elementary functions — trigonometric functions and exponentials. We then study
the heating of a circular disk. In this case, the eigenfunctions are no longer elementary
functions, but, rather, are expressed in terms of Bessel functions. Understanding their
basic properties will require us to take a detour to develop the fundamentals of power
series solutions to ordinary diﬀerential equations.
Heating of a Rectangle
A homogeneous rectangular plate
.
R = 0 < x < a, 0 < y < b

446

11 Dynamics of Planar Media

is heated to a prescribed initial temperature,
u(0, x, y) = f (x, y),

(x, y) ∈ R.

for

(11.33)

Then its top and bottom are insulated, while its sides are held at zero temperature. Our
task is to understand the thermodynamic evolution of the plate’s temperature.
The temperature u(t, x, y) evolves according to the two-dimensional heat equation
ut = γ (uxx + uyy ),

(x, y) ∈ R,

for

t > 0,

(11.34)

where γ > 0 is the plate’s thermal diﬀusivity, while subject to homogeneous Dirichlet
conditions along the boundary of the rectangle at all subsequent times:
u(t, 0, y) = u(t, a, y) = u(t, x, 0) = u(t, x, b) = 0,

0 < x < a,

0 < y < b,

t > 0.
(11.35)
As in (11.19), the eigensolutions to the heat equation are obtained from the usual exponential ansatz u(t, x, y) = e− λ t v(x, y). Substituting this expression into the heat equation,
we conclude that the function v(x, y) solves the Helmholtz eigenvalue problem
(x, y) ∈ R,

γ (vxx + vyy ) + λ v = 0,

(11.36)

subject to the same homogeneous Dirichlet boundary conditions:
v(0, y) = v(a, y) = v(x, 0) = v(x, b) = 0,

0 < x < a,

0 < y < b.

(11.37)

To tackle the rectangular Helmholtz eigenvalue problem (11.36–37), we shall, as in
(4.89), introduce a further separation of variables, writing the solution
v(x, y) = p(x) q(y)
as the product of functions depending on the individual Cartesian coordinates. Substituting
this expression into the Helmholtz equation (11.36), we ﬁnd
γ p (x) q(y) + γ p(x) q  (y) + λ p(x) q(y) = 0.
To eﬀect the variable separation, we collect all terms involving x on one side and all terms
involving y on the other side of the equation, which is accomplished by dividing by v = p q
and rearranging the terms:
γ

q  (y)
p (x)
= −γ
− λ ≡ − μ.
p(x)
q(y)

The left-hand side of this equation depends only on x, whereas the middle term depends
only on y. As before, this requires that the expressions equal a common separation constant,
denoted by − μ. (The minus sign is for later convenience.) In this manner, we reduce our
partial diﬀerential equation to a pair of one-dimensional eigenvalue problems
γ

d2 p
+ μ p = 0,
dx2

γ

d2 q
+ (λ − μ) q = 0,
dy 2

(11.38)

each of which is subject to homogeneous Dirichlet boundary conditions
p(0) = p(a) = 0,

q(0) = q(b) = 0,

(11.39)

11.2 Explicit Solutions of the Heat Equation

447

stemming from the boundary conditions (11.37). To obtain a nontrivial separable solution
to the Helmholtz equation, we seek nonzero solutions to these two supplementary eigenvalue
problems.
We have already solved these particular two boundary value problems (11.38–39) many
times; see, for instance, (4.21). The eigenfunctions are, respectively,
m πx
n πy
,
m = 1, 2, 3, . . . ,
qn (y) = sin
,
n = 1, 2, 3, . . . ,
pm (x) = sin
a
b
with
n2 π 2 γ
m2 π 2 γ
n2 π 2 γ
m2 π 2 γ
,
λ
−
μ
=
,
so
that
λ
=
+
.
a2
b2
a2
b2
Therefore, the separable eigenfunction solutions to the Helmholtz boundary value problem
(11.35–36) have the doubly trigonometric form
n πy
m πx
vm,n (x, y) = sin
sin
,
for
m, n = 1, 2, 3, . . . ,
(11.40)
a
b
with associated eigenvalues
 2

m2 π 2 γ
n2 π 2 γ
m
n2
(11.41)
λm,n =
+
=
+ 2 π2 γ .
a2
b2
a2
b
μ=

Each of these corresponds to an exponentially decaying eigensolution

  2

n πy
m
n2
m πx
− λm,n t
2
sin
vm,n (x, y) = exp −
+ 2 π γ t sin
um,n (t, x, y) = e
2
a
b
a
b
(11.42)
to the original rectangular Dirichlet boundary value problem for the heat equation.
Using the fact that the univariate sine functions form a complete system, it is not
hard to prove, [120], that the separable eigenfunction solutions (11.42) are complete, and
so there are no non-separable eigenfunctions.† As a consequence, the general solution to
the initial-boundary value problem can be expressed as a linear combination
u(t, x, y) =

∞


cm,n um,n (t, x, y) =

m,n = 1

∞


cm,n e− λm,n t vm,n (x, y)

(11.43)

m,n = 1

of the eigenmodes. The coeﬃcients cm,n are prescribed by the initial conditions, which
take the form of a double Fourier sine series
f (x, y) = u(0, x, y) =

∞

m,n = 1

cm,n vm,n (x, y) =

∞

m,n = 1

cm,n sin

n πy
m πx
sin
.
a
b

Self-adjointness of the Laplacian operator coupled with the boundary conditions implies that‡ the eigenfunctions vm,n (x, y) are orthogonal with respect to the L2 inner product
†

This appears to be a general fact, true in all known examples, but I know of no general
proof. Theorem 9.47 can be used to establish completeness of the eigenfunctions, but does not
guarantee that they can all be constructed by separation of variables.
‡

Technically, orthogonality is guaranteed only when the eigenvalues are distinct: λm,n = λk,l .
However, by a direct computation, one ﬁnds that orthogonality continues to hold even when the
indicated eigenfunctions are associated with equal eigenvalues. See the ﬁnal subsection of this
chapter for a discussion of when such “accidental degeneracies” arise.

448

11 Dynamics of Planar Media

Figure 11.2.

Heat diﬀusion in a rectangle.



on the rectangle:
 b a
 vk,l , vm,n  =

vk,l (x, y) vm,n (x, y) dx dy = 0
0

unless

k=m

and

l = n.

0

(The skeptical reader can verify the orthogonality relations directly from the eigenfunction
formulas (11.40).) Thus, we can appeal to our usual orthogonality formula (11.26) to
evaluate the coeﬃcients
 b a
 f , vm,n 
n πy
4
m πx
sin
dx dy,
(11.44)
cm,n =
=
f (x, y) sin
2
 vm,n 
ab 0 0
a
b
where the formula for the norms of the eigenfunctions
 b a
 vm,n  =
2

 b a
sin2

2

vm,n (x, y) dx dy =
0

0

0

0

m πx
n πy
sin2
dx dy = 14 a b (11.45)
a
b

follows from a direct evaluation of the double integral. Unfortunately, while orthogonality
is (mostly) automatic, computation of the norms must inevitably be done “by hand”.
For generic initial temperature distributions, the rectangle approaches thermal equilibrium at a rate equal to the smallest eigenvalue:


1
1
+ 2 π 2 γ,
(11.46)
λ1,1 =
a2
b
i.e., the sum of the reciprocals of the squared lengths of its sides multiplied by the diﬀusion
coeﬃcient. The larger the rectangle, or the smaller the diﬀusion coeﬃcient, the smaller the
value of λ1,1 , and hence the slower the return to thermal equilibrium. The exponentially
fast decay rate of the Fourier series implies that the solution immediately smooths out any
discontinuities in the initial temperature proﬁle. Indeed, the higher modes, with m and
n large, decay to zero almost instantaneously, and so the solution quickly behaves like a
ﬁnite sum over a few low-order modes. Assuming that c1,1 = 0, the slowest-decaying mode

11.2 Explicit Solutions of the Heat Equation

449

in the Fourier series (11.43) is
 


πy
1
1
πx
2
sin
.
+
γ
t
sin
π
c1,1 u1,1 (t, x, y) = c1,1 exp −
2
2
a
b
a
b

(11.47)

Thus, in the long run, the temperature becomes entirely of one sign — either positive
or negative depending on the sign of c1,1 — throughout the rectangle. This observation
is, in fact, indicative of the general phenomenon that an eigenfunction associated with
the smallest positive eigenvalue of a self-adjoint elliptic operator is necessarily of one sign
throughout the domain, [34]. A typical solution is plotted at several times in Figure 11.2.
Non-generic initial conditions, with c1,1 = 0, decay more rapidly, and their asymptotic
temperature proﬁles are not of one sign.

Exercises
11.2.1. A rectangle of size 2 cm by 1 cm has initial temperature f (x, y) = sin πx sin πy for
0 ≤ x ≤ 2, 0 ≤ y ≤ 1. All four sides of the rectangle are held at 0◦ . Assuming that the
thermal diﬀusivity of the plate is γ = 1, write down a formula for its subsequent temperature u(t, x, y). What is the rate of decay to thermal equilibrium?
11.2.2. Solve Exercise 11.2.1 when
the initial temperature f (x, y) is





1, 0 < x < 1,
 1

1
(c) 1 − | 1 − x |
(a) x y,
(b)
2 − 2 −y .
0, 1 < x < 2;
11.2.3. Solve the initial-boundary value problem for the heat equation ut = 2 Δu on the rectangle −1 < x < 1, 0 < y < 1 when the two short sides are kept at0◦ , the two long sides are
−1, x < 0,
0 < y < 1.
insulated, and the initial temperature distribution is u(0, x, y) =
+1, x > 0,
11.2.4. Answer Exercise 11.2.3 when the two long sides are kept at 0◦ and the two short sides
are insulated.
♥ 11.2.5. A rectangular plate of size 1 meter by 3 meters is made out a metal with unit diﬀusivity. The plate is taken from a 0◦ freezer, and, from then on, one of its long sides is heated
to 100◦ , the other is held at 0◦ , while its top, bottom, and both of the short sides are fully
insulated. (a) Set up the initial-boundary value problem governing the time-dependent
temperature of the plate. (b) What is the equilibrium temperature? (c) Use your answer
from part (b) to construct an eigenfunction series for the solution. (d) How long until the
temperature of the plate is everywhere within 1◦ of its eventual equilibrium?
Hint: Once t is no longer small, you can approximate the series solution by its ﬁrst term.
11.2.6. Among all rectangular plates of a prescribed area, which one returns to thermal equilibrium the slowest when subject to Dirichlet boundary conditions? The fastest? Use your
physical intuition to explain your answer, but justify it mathematically.
11.2.7. Answer Exercise 11.2.6 for a fully insulated rectangular plate, i.e., subject to Neumann
boundary conditions.
♥ 11.2.8. A square metal plate is taken from an oven, and then set out to cool, with its top and
bottom insulated. Find the rate of cooling, in terms of the side length and the thermal
diﬀusivity, if (a) all four sides are held at 0◦ ; (b) one side is insulated and the other three
sides are held at 0◦ ; (c) two adjacent sides are insulated and the other two are held at 0◦ ;
(d) two opposite sides are insulated and the other two are held at 0◦ ; (e) three sides are
insulated and the remaining side is held at 0◦ . Order the cooling rates of the plates from
fastest to slowest. Do your results conﬁrm your intuition?

450

11 Dynamics of Planar Media

♥ 11.2.9. Two square plates are made out of the same homogeneous material, and both are initially heated to 100◦ . All four sides of the ﬁrst plate are held at 0◦ , whereas one of the
sides of the second plate is insulated while the other three sides are held at 0◦ . Which plate
cools down the fastest? How much faster? Assuming the thermal diﬀusivity γ = 1, how
long do you have to wait until every point on each plate is within 1◦ of its equilibrium
temperature? Hint: Once t is no longer small, the series solution is well approximated by
its ﬁrst term.
♥ 11.2.10. Multiple choice: On a unit square that is subject to Dirichlet boundary conditions, the
eigenvalues of the Laplace operator are
(a) all simple, (b) at most double, or (c) can have arbitrarily large multiplicity.
♥ 11.2.11. The thermodynamics of a thin circular cylindrical shell of radius a and height h, e.g.,
the side of
⎞ top and bottom are removed, is modeled by the heat equation
⎛ a tin can after its
2
2
1
∂
u
u
∂
∂u
⎠, in which u(t, θ, z) measures the temperature of the point on
+
= γ⎝ 2
∂t
a ∂θ2
∂z 2
the cylinder at time t > 0, angle − π < θ ≤ π, and height 0 < z < h. Keep in mind
that u(t, θ, z) must be a 2 π–periodic function of the angular coordinate θ. Assume that the
cylinder is everywhere insulated, while its two circular ends are held at 0◦ . Given an initial
temperature distribution at time t = 0, write down a series formula for the cylinder’s
temperature at subsequent times. What is the eventual equilibrium temperature?
How fast does the cylinder return to equilibrium?
♥ 11.2.12. Consider the initial-boundary value problem
u(0, x, y) = 0,
ut = uxx + uyy ,

0 < x, y < π,

t > 0,

for the heat equation in a square subject to the Dirichlet conditions
u(t, 0, y) = u(t, π, y) = 0 = u(t, x, 0), u(t, x, π) = f (x), 0 < x, y < π, t > 0.
Write out an eigenfunction series formulas for
(a) the equilibrium solution u (x, y) = lim u(t, x, y); (b) the solution u(t, x, y).
t→∞

11.2.13. Solve Exercise 11.2.1 when one long side of the plate is held at 100◦ .
Hint: See Exercise 11.2.12.

Heating of a Disk — Preliminaries
Let us perform a similar analysis of the thermodynamics of a circular disk. For simplicity
(or by choice of suitable physical units), we will assume that the disk
D = {x2 + y 2 ≤ 1 } ⊂ R 2
has unit radius and unit diﬀusivity γ = 1. We shall solve the heat equation on D subject
to homogeneous Dirichlet boundary values of zero temperature at the circular edge
∂D = C = {x2 + y 2 = 1 }.
Thus, the full initial-boundary value problem is
∂u
∂2u ∂2u
=
+ 2 ,
∂t
∂x2
∂y

x2 + y 2 < 1,
t > 0,
2

2

u(t, x, y) = 0,

x + y = 1,

u(0, x, y) = f (x, y),

x2 + y 2 ≤ 1.

(11.48)

11.2 Explicit Solutions of the Heat Equation

451

We remark that a simple rescaling of space and time, as outlined in Exercise 11.4.7, can
be used to recover the solution for an arbitrary diﬀusion coeﬃcient and a disk of arbitrary
radius from this particular case.
Since we are working in a circular domain, we instinctively pass to polar coordinates
(r, θ). In view of the polar coordinate formula (4.105) for the Laplace operator, the heat
equation and boundary and initial conditions assume the form
∂ 2 u 1 ∂u
1 ∂2u
∂u
= 2 +
+ 2
,
∂t
∂r
r ∂r
r ∂θ2

u(t, 1, θ) = 0,

u(0, r, θ) = f (r, θ),

(11.49)

where the solution u(t, r, θ) is deﬁned for all 0 ≤ r ≤ 1 and t ≥ 0. To ensure that the
solution represents a single-valued function on the entire disk, it is required to be a 2 π–
periodic function of the angular variable:
u(t, r, θ + 2 π) = u(t, r, θ).
To obtain the separable solutions
u(t, r, θ) = e− λ t v(r, θ),

(11.50)

we need to solve the polar coordinate form of the Helmholtz equation
∂2v
1 ∂2v
1 ∂v
+
+
+ λ v = 0,
∂r2
r ∂r r2 ∂θ2

0 ≤ r < 1,
− π < θ ≤ π,

(11.51)

subject to the boundary conditions
v(1, θ) = 0,

v(r, θ + 2 π) = v(r, θ).

(11.52)

To solve the polar Helmholtz boundary value problem (11.51–52), we invoke a further
separation of variables by writing
v(r, θ) = p(r) q(θ).

(11.53)

Substituting this ansatz into (11.51), collecting all terms involving r and all terms involving
θ, and then equating both to a common separation constant, we are led to the pair of
ordinary diﬀerential equations
r2

d2 p
dp
+ (λ r2 − μ) p = 0,
+r
2
dr
dr

d2 q
+ μ q = 0,
dθ2

(11.54)

where λ is the Helmholtz eigenvalue, and μ the separation constant.
Let us start with the equation for q(θ). The second boundary condition in (11.52)
requires that q(θ) be 2 π–periodic. Therefore, the required solutions are the elementary
trigonometric functions
q(θ) = cos m θ

or

sin m θ,

where

μ = m2 ,

(11.55)

with m = 0, 1, 2, . . . a nonnegative integer.
Substituting the formula for the separation constant, μ = m2 , the diﬀerential equation
for p(r) takes the form
r2

d2 p
dp
+ (λ r2 − m2 ) p = 0,
+r
dr2
dr

0 ≤ r ≤ 1.

(11.56)

452

11 Dynamics of Planar Media

Ordinarily, one imposes two boundary conditions in order to pin down a solution to such a
second-order ordinary diﬀerential equation. But our Dirichlet condition, namely p(1) = 0,
speciﬁes its value at only one of the endpoints. The other endpoint is a singular point for
the ordinary diﬀerential equation, because the coeﬃcient of the highest-order derivative,
namely r2 , vanishes at r = 0. This situation might remind you of our solution to the Euler
diﬀerential equation (4.111) in the context of separable solutions to the Laplace equation
on the disk. As there, we require that the solution be bounded at r = 0, and so seek
eigensolutions that satisfy the boundary conditions
| p(0) | < ∞,

p(1) = 0.

(11.57)

While (11.56) appears in a variety of applications, it is more challenging than any
ordinary diﬀerential equation we have encountered so far. Indeed, most solutions cannot
be written in terms of the elementary functions (rational functions, trigonometric functions,
exponentials, logarithms, etc.) you see in ﬁrst-year calculus. Nevertheless, owing to their
ubiquity in physical applications, its solutions have been extensively studied and tabulated,
and so are, in a sense, well known, [85, 86, 119].
To simplify the subsequent analysis, we make a preliminary rescaling of the independent variable, replacing r by
√
z = λ r.
(We know the eigenvalue λ > 0, since we are dealing with a positive deﬁnite boundary
value problem.) Note that, by the chain rule,
dp √ dp
= λ
,
dr
dz

d2 p
d2 p
=
λ
,
dr2
dz 2

and hence
r

dp
dp
=z
,
dr
dz

r2

2
d2 p
2 d p
=
z
.
dr2
dz 2

The net eﬀect is to eliminate the eigenvalue parameter λ (or, rather, hide it in the change
of variables), so that (11.56) assumes the slightly simpler form
z2

d2 p
dp
+ (z 2 − m2 ) p = 0.
+z
2
dz
dz

(11.58)

The resulting ordinary diﬀerential equation (11.58) is known as Bessel’s equation, named
after the early-nineteenth-century German astronomer Wilhelm Bessel, who ﬁrst encountered its solutions, now known as Bessel functions, in his study of planetary orbits. Special
cases had already appeared in the investigations of Daniel Bernoulli on vibrations of a hanging chain, and in those of Fourier on the thermodynamics of a cylindrical body. To make
further progress, we need to take time out to study their basic properties, and this will require us to develop the method of power series solutions of ordinary diﬀerential equations.
With this in hand, we can then return to complete our solution to the heat equation on a
disk.

11.3 Series Solutions of Ordinary Diﬀerential Equations
When confronted with a novel ordinary diﬀerential equation, we have several available
options for deriving and understanding its solutions. For instance, the “look-up” method

11.3 Series Solutions of Ordinary Diﬀerential Equations

453

relies on published handbooks. One of the most useful references that collects many solved
diﬀerential equations is the classic German compendium by Kamke, [62]. Two more recent
English-language handbooks are [93, 127]. In addition, many symbolic computer algebra
programs, including Mathematica and Maple, will produce solutions, when expressible
in terms of both elementary and special functions, to a wide range of diﬀerential equations.
Of course, use of numerical integration to approximate solutions, [24, 60, 80], is always an option. Numerical methods do, however, have their limitations, and are best
accompanied by some understanding of the underlying theory, coupled with qualitative
or quantitative expectations of how the solutions should behave. Furthermore, numerical
methods provide less than adequate insight into the nature of the special functions that
regularly appear as solutions of the particular diﬀerential equations arising in separation
of variables. A numerical approximation cannot, in itself, establish rigorous mathematical
properties of the solutions of the diﬀerential equation.
A more classical means of constructing and approximating the solutions of diﬀerential
equations is based on their power series expansions, a.k.a. Taylor series. The Taylor expansion of a solution at a point x0 is found by substituting a general power series into the
diﬀerential equation and equating coeﬃcients of the various powers of x − x0 . The initial
conditions at x0 serve to uniquely determine the coeﬃcients and hence all the derivatives of
the solution at the initial point. The Taylor expansion of a special function is an eﬀective
tool for deducing some of its key properties, as well as providing a means of computing reasonable numerical approximations to its values within the radius of convergence of
the series. (However, serious numerical computations more often rely on nonconvergent
asymptotic expansions, [85].)
In this section, we provide a brief introduction to the basic series solution techniques for
ordinary diﬀerential equations, concentrating on second-order linear diﬀerential equations,
since these form by far the most important class of examples arising in applications. At a
regular point, the method will produce a standard Taylor expansion for the solution, while
so-called regular singular points require a slightly more general type of series expansion.
Generalizations to irregular singular points, higher-order equations, nonlinear equations,
and even linear and nonlinear systems are deferred to more advanced texts, including
[54, 59].

The Gamma Function
Before delving into the machinery of series solutions and special functions, we need to
introduce the gamma function, which eﬀectively generalizes the factorial operation to nonintegers. Recall that the factorial of a nonnegative integer n ≥ 0 is deﬁned inductively by
the iterative formula
n ! = n · (n − 1) !,

starting with

0 ! = 1.

(11.59)

When n is a positive integer, the iteration terminates, yielding the familiar expression
n ! = n(n − 1)(n − 2) · · · 3 · 2 · 1.

(11.60)

However, for more general values of n, the iteration never stops, and it cannot be used to
compute its factorial. Our goal is to circumvent this diﬃculty, and introduce a function
f (x) that is deﬁned for all values of x and will play the role of such a factorial. First,

454

11 Dynamics of Planar Media

mimicking (11.59), the function should satisfy the functional equation
f (x) = x f (x − 1)

(11.61)

where deﬁned. If, in addition, f (0) = 1, then we know that f (n) = n ! whenever n is a
nonnegative integer, and hence such a function will extend the deﬁnition of the factorial
to more general real and complex numbers.
A moment’s thought should convince the reader that there are many possible ways
to construct such a function; see Exercise 11.3.6 for a nonstandard example. The most
important version is due to Euler. The modern deﬁnition of Euler’s gamma function relies
on an integral formula discovered by the eighteenth-century French mathematician Adrien–
Marie Legendre, who will play a starring role in Chapter 12.
Deﬁnition 11.2. The gamma function is deﬁned by
 ∞
Γ(x) =
e− t tx−1 dt.

(11.62)

0

The ﬁrst fact is that, for real x, the gamma function integral converges only when
x > 0; otherwise the singularity of tx−1 at t = 0 is too severe. The key property that turns
the gamma function into a substitute for the factorial function relies on an elementary
integration by parts:
 ∞
 ∞
∞
Γ(x + 1) =
e− t tx dt = − e− t tx
+ x
e− t tx−1 dt.
t=0

0

0

The boundary terms vanish whenever x > 0, while the ﬁnal integral is merely Γ(x). Therefore, the gamma function satisﬁes the recurrence relation
Γ(x + 1) = x Γ(x).

(11.63)

If we set f (x) = Γ(x + 1), then (11.63) becomes (11.61). Moreover, by direct integration,
 ∞
e− t dt = 1.
Γ(1) =
0

Combining this with the recurrence relation (11.63), we deduce that
Γ(n + 1) = n !

(11.64)

whenever n ≥ 0 is a nonnegative integer. Therefore, we can identify x ! with the value
Γ(x + 1) whenever x > −1 is any real number.
Remark : The reader may legitimately ask why not replace tx−1 by tx in the deﬁnition
of Γ(z), which would avoid the n + 1 in (11.64). There is no good answer; we are merely
following a well-established precedent set by Legendre and enshrined in all subsequent
works.
Thus, at integer values of x, the gamma function agrees with the elementary factorial.
A few other values can be computed exactly. One important case is at x = 12 . Using the
substitution t = s2 , with dt = 2 s ds, we obtain
 ∞
 ∞
√
2
− t −1/2
1
e t
dt =
2 e− s ds = π,
(11.65)
Γ 2 =
0

0

11.3 Series Solutions of Ordinary Diﬀerential Equations

Figure 11.3.

455

The gamma function.

where the ﬁnal integral was evaluated in (2.100). Thus, using the identiﬁcation with the
√
factorial function, we identify this value with − 21 ! = π. The recurrence relation
(11.63) will then produce the value of the gamma function at all half-integers 12 , 32 , 52 , . . . .
For example,
√
(11.66)
Γ 32 = 12 Γ 12 = 12 π,
√
and hence 12 ! = 12 π. The recurrence relation can also be employed to extend the deﬁnition
of Γ(x) to (most) negative values of x. For example, setting x = − 12 in (11.63), we have
√
so
Γ − 12 = −2 Γ 12 = −2 π .
Γ 12 = − 12 Γ − 12 ,
The only points at which this device fails are the negative integers, and indeed, Γ(x) has
a singularity when x = −1, −2, −3, . . . . A graph† of the gamma function is displayed in
Figure 11.3.
Remark : Most special functions of importance for applications arise as solutions to
fairly simple ordinary diﬀerential equations. The gamma function is a signiﬁcant exception.
Indeed, it can be proved, [11], that the gamma function does not satisfy any algebraic
diﬀerential equation!

Regular Points
We are now ready to develop the method of series solutions to ordinary diﬀerential equations. Before we proceed to develop the general computational machinery, a naı̈ve calculation in an elementary example will be enlightening.
†

The axes are at diﬀerent scales; the tick marks are at integer values.

456

11 Dynamics of Planar Media

Example 11.3. Consider the initial value problem
d2 u
(11.67)
+ u = 0,
u(0) = 1,
u (0) = 0.
dx2
Let us investigate whether we can construct an analytic solution in the form of a convergent
power series
∞

2
3
un x n
(11.68)
u(x) = u0 + u1 x + u2 x + u3 x + · · · =
n=0

that is based at the initial point x0 = 0. Term-by-term diﬀerentiation yields the following
series expansions† for its derivatives:
du
= u1 + 2 u2 x + 3 u3 x 2 + 4 u4 x 3 + · · ·
dx
2

=

∞


(n + 1) un+1 xn ,

n=0
∞


(11.69)

d u
= 2 u2 + 6 u3 x + 12 u4 x2 + 20 u5 x3 + · · · =
(n + 1)(n + 2) un+2 xn .
dx2
n=0
The next step is to substitute the series (11.68–69) into the diﬀerential equation and collect
common powers of x:
d2 u
+ u = (2 u2 + u0 ) + (6 u3 + u1 ) x + (12 u4 + u2 ) x2 + (20 u5 + u3 ) x3 + · · · = 0.
dx2
At this point, one focuses attention on the individual coeﬃcients, appealing to the following
basic observation:
Two convergent power series are equal if and only if all their coeﬃcients are equal.
In particular, a power series represents the zero function‡ if and only if all its coeﬃcients
are 0. In this manner we obtain the following inﬁnite sequence of algebraic recurrence
relations among the coeﬃcients:
1

2 u2 + u0 = 0,

x

6 u3 + u1 = 0,

2

x
x3

12 u4 + u2 = 0,
20 u5 + u3 = 0,

x4
..
.
xn

30 u6 + u4 = 0,
..
.
(n + 1)(n + 2) un+2 + un = 0.

(11.70)

Now, the initial conditions serve to prescribe the ﬁrst two coeﬃcients:
u(0) = u0 = 1,

u (0) = u1 = 0.

†
When working with the series in summation form, it helps to re-index in order to display
the term of degree n.
‡

Here it is essential that we work with analytic functions, since this result is not true for
2
C functions! For example, the function e−1/x has identically zero power series at x0 = 0; see
Exercise 11.3.21.
∞

11.3 Series Solutions of Ordinary Diﬀerential Equations

457

We then solve the recurrence relations in order: The ﬁrst determines u2 = − 12 u0 = − 12 ;
1
1
1
the second, u3 = − 16 u1 = 0; next, u4 = − 12
u2 = 24
; then u5 = − 20
u3 = 0; then
1
1
u6 = − 30 u4 = − 720 ; and so on. In general, it is not hard to see that
u2k =

(−1)k
,
(2 k) !

u2k+1 = 0,

k = 0, 1, 2, . . . .

Hence, the required series solution is
1 3
1
x − 720
x6 +
u(x) = 1 − 12 x2 + 24

··· =

∞

(−1)k
k=0

k!

x2k ,

which, by the ratio test, converges for all x. We have thus recovered the well-known Taylor
series for cos x, which is indeed the solution to the initial value problem. Changing the
initial conditions to u(0) = u0 = 0, u (0) = u1 = 1, will similarly produce the usual
Taylor expansion of sin x. Note that the generation of the Taylor series does not rely on
any a priori knowledge of trigonometric functions or the direct solution method for linear
constant-coeﬃcient ordinary diﬀerential equations.
Building on this experience, let us describe the general method. We shall concentrate
on solving a second-order homogeneous linear diﬀerential equation
p(x)

d2 u
du
+ q(x)
+ r(x) u = 0.
dx2
dx

(11.71)

The coeﬃcients p(x), q(x), r(x) are assumed to be analytic functions on some common
domain. This means that, at a point x0 within the domain, they admit convergent power
series expansions
p(x) = p0 + p1 (x − x0 ) + p2 (x − x0 )2 + · · · ,
(11.72)
q(x) = q0 + q1 (x − x0 ) + q2 (x − x0 )2 + · · · ,
r(x) = r0 + r1 (x − x0 ) + r2 (x − x0 )2 + · · · .
We expect that solutions to the diﬀerential equation are also analytic. This expectation is
justiﬁed, provided that the equation is regular at the point x0 , in the following sense.
Deﬁnition 11.4. A point x = x0 is a regular point of a second-order linear ordinary
diﬀerential equation (11.71) if the leading coeﬃcient does not vanish there:
p0 = p(x0 ) = 0.
A point where p(x0 ) = 0 is known as a singular point.
In short, at a regular point, the second-order derivative term does not disappear, and
so the equation is “genuinely” of second order.
Remark : The deﬁnition of a singular point assumes that the other two coeﬃcients do
not also vanish there, so that either q(x0 ) = 0 or r(x0 ) = 0. If all three functions happen
to vanish at x0 , we can cancel any common factor (x − x0 )k , and hence, without loss of
generality, assume that at least one of the coeﬃcient functions is nonzero at x0 .
Proofs of the basic existence theorem for diﬀerential equations at regular points can
be found in [18, 54, 59].

458

11 Dynamics of Planar Media

Theorem 11.5. Let x0 be a regular point for the second-order homogeneous linear
ordinary diﬀerential equation (11.71). Then there exists a unique, analytic solution u(x)
to the initial value problem
u(x0 ) = a,

u (x0 ) = b.

(11.73)

The radius of convergence of the power series for u(x) is at least as large as the distance
from the regular point x0 to the nearest singular point of the diﬀerential equation in the
complex plane.
Thus, every solution to an analytic diﬀerential equation at a regular point x0 can be
expanded in a convergent power series
u(x) = u0 + u1 (x − x0 ) + u2 (x − x0 )2 + · · · =

∞


un (x − x0 )n .

(11.74)

n=0

Since the power series necessarily coincides with the Taylor series for u(x), its coeﬃcients†
u(n) (x0 )
n!
are multiples of the derivatives of the function at the point x0 . In particular, the ﬁrst two
coeﬃcients,
(11.75)
u1 = u (x0 ) = b,
u0 = u(x0 ) = a,
un =

are ﬁxed by the initial conditions. The remaining coeﬃcients will then be uniquely prescribed thanks to the uniqueness of solutions to initial value problems.
Near a regular point, the second-order diﬀerential equation (11.71) admits two linearly
independent analytic solutions, which we denote by u
(x) and u
(x). The general solution
can be written as a linear combination of the two basis solutions:
u(x) = a u
(x) + b u
(x).

(11.76)

A convenient choice is to have the ﬁrst satisfy the initial conditions
u
(x0 ) = 1,

u
  (x0 ) = 0,

(11.77)

u
(x0 ) = 0,

u
  (x0 ) = 1,

(11.78)

and the second satisfy

although other conventions may be used depending on the circumstances. Given (11.77–
78), the linear combination (11.76) automatically satisﬁes the initial conditions (11.73).
The basic computational strategy to construct the power series solution to the initial
value problem is a straightforward adaptation of the method used in Example 11.3. One
substitutes the known power series (11.72) for the coeﬃcient functions and the unknown
power series (11.74) for the solution into the diﬀerential equation (11.71). Multiplying out
the formulas and collecting the common powers of x − x0 will result in a (complicated)
power series whose individual coeﬃcients must be equated to zero. The lowest-order terms
are multiples of (x − x0 )0 = 1, i.e., the constant terms. They produce a linear relation
u2 = R2 (u0 , u1 ) = R2 (a, b)
†
Some authors prefer to include the n !’s in the original power series; this is purely a matter
of personal taste.

11.3 Series Solutions of Ordinary Diﬀerential Equations

459

that prescribes the coeﬃcient u2 in terms of the initial data (11.75). The coeﬃcient of
(x − x0 ) leads to a relation
u3 = R3 (u0 , u1 , u2 ) = R3 (a, b, R2 (a, b))
that prescribes u3 in terms of the initial data and the previously computed coeﬃcient u2 .
And so on. At the nth stage of the procedure, the coeﬃcient of (x − x0 )n produces the
linear recurrence relation
un+2 = Rn (u0 , u1 , . . . , un+1 ),

n = 0, 1, 2, . . . ,

(11.79)

that will prescribe the (n + 2)nd order coeﬃcient in terms of the previously computed
coeﬃcients. In this fashion, we will have constructed a formal power series solution to the
diﬀerential equation at a regular point. The one remaining issue is whether the resulting
power series actually converges. The full analysis can be found in [54, 59], and will serve
to complete the proof of the general Existence Theorem 11.5.
Rather than continue on in general, the best way to learn the method is to work
through another, less trivial, example.

The Airy Equation
We will illustrate the procedure by constructing power series solutions to the Airy equation
d2 u
= x u.
dx2

(11.80)

This second-order linear ordinary diﬀerential equation, which arises in applications to optics, rainbows, and dispersive waves, has solutions that cannot be expressed in terms of
elementary functions.
For the Airy equation (11.80), the leading coeﬃcient is constant, and so every point
is a regular point. For simplicity, we will look only for power series based at the origin
x0 = 0, and therefore of the form (11.68). Equating the two series


u (x) = 2 u2 + 6 u3 x + 12 u4 x + 20 u5 x + · · · =
2

3

x u(x) = u0 x + u1 x2 + u2 x3 + · · ·

=

∞

n=0
∞


(n + 1)(n + 2) un+2 xn ,
un−1 xn ,

n=1

leads to the following recurrence relations relating the coeﬃcients:
1

2 u2 = 0,

x
x2

6 u3 = u0 ,
12 u4 = u1 ,

x3

20 u5 = u2 ,

4

30 u6 = u3 ,
..
.

x
..
.

xn

(n + 1)(n + 2) un+2 = un−1 .

460

11 Dynamics of Planar Media

As before, we solve them in order: The ﬁrst equation determines u2 . The second prescribes
1
1
u3 = 16 u0 in terms of u0 . Next, we ﬁnd u4 = 12
u1 in terms of u1 , followed by u5 = 20
u2 =
1
1
0; then u6 = 30 u3 = 180 u0 is ﬁrst given in terms of u3 , but we already know the latter in
terms of u0 . And so on.
Let us now construct two basis solutions. The ﬁrst has the initial conditions
u1 = u
  (0) = 0.

(0) = 1,
u0 = u

The recurrence relations imply that the only nonzero coeﬃcients cn occur when n = 3 k is
a multiple of 3. Moreover,
u3 k−3
u3 k =
.
3 k (3 k − 1)
A straightforward induction proves that
u3 k =

1
.
3 k (3 k − 1)(3 k − 3)(3 k − 4) · · · 6 · 5 · 3 · 2

The resulting solution is
1
u
(x) = 1+ 16 x3 + 180
x6 +· · · = 1+

∞

k=1

x3 k
. (11.81)
3 k (3 k − 1)(3 k − 3)(3 k − 4) · · · 6 · 5 · 3 · 2

Note that the denominator is similar to a factorial, except every third term is omitted.
A straightforward application of the ratio test conﬁrms that the series converges for all
(complex) x, in conformity with the general Theorem 11.5, which guarantees an inﬁnite
radius of convergence because the Airy equation has no singular points.
Similarly, starting with the initial conditions
u1 = u
  (0) = 1,

(0) = 0,
u0 = u

we ﬁnd that the only nonzero coeﬃcients un occur when n = 3 k + 1. The recurrence
relation
u3 k+1 =

u3 k−2
(3 k + 1)(3 k)

yields

u3 k+1 =

1
.
(3 k + 1)(3 k)(3 k − 2)(3 k − 3) · · · 7 · 6 · 4 · 3

The resulting solution is
∞


x3 k+1
.
(3 k + 1)(3 k)(3 k − 2)(3 k − 3) · · · 7 · 6 · 4 · 3
k=1
(11.82)
Again, the denominator skips every third term in the product. Every solution to the Airy
equation can be written as a linear combination of these two basis power series solutions:
1 4
1
x + 504
x7 + · · · = x +
u
(x) = x + 12

u(x) = a u
(x) + b u
(x),

where

a = u(0),

b = u (0).

Both power series (11.81, 82), converge quite rapidly, and so the ﬁrst few terms will provide
a reasonable approximation to the solutions for moderate values of x.
We have, in fact, already encountered another solution to the Airy equation. According
to formula (8.97), the integral
 ∞
1
cos s x + 13 s3 ds
Ai(x) =
(11.83)
π 0

11.3 Series Solutions of Ordinary Diﬀerential Equations

461

deﬁnes the Airy function of the ﬁrst kind . Let us prove that it satisﬁes the Airy diﬀerential
equation (11.80):
d2
Ai(x) = x Ai(x).
dx2
Before diﬀerentiating, we recall the integration by parts argument in (8.96) to re-express
the Airy integral in absolutely convergent form:

2 ∞ s sin s x + 13 s3
Ai(x) =
ds.
π 0
(x + s2 )2
We are now permitted to diﬀerentiate under the integral sign, producing (after some algebra)
%
$
 ∞
d s (x + s2 ) cos s x + 13 s3 − sin s x + 13 s3
2
d2
ds = 0.
Ai(x) − x Ai(x) =
dx2
π 0 ds
(x + s2 )3
Thus, the Airy function must be a certain linear combination of the two basic series solutions:
Ai(x) = Ai(0) u
(x) + Ai (0) u
(x).
Its values at x = 0 are, in fact, given by
 ∞
Γ 13
1
1
Ai(0) =
cos 13 s3 ds =
= 2/3 2 ≈ .355028 ,
1/6
π 0
2π 3
3 Γ 3
 ∞
1/6
3 Γ 23
1
1
Ai (0) = −
= − 1/3 1 ≈ − .258819.
s sin 13 s3 ds = −
π 0
2π
3 Γ 3

(11.84)

The second and third expressions involve the gamma function (11.62); a proof, based on
complex integration, can be found in [85; p. 54].

Exercises
11.3.1. Find (a) Γ

 
5
2 ,

(b) Γ





11.3.2. Prove that Γ n + 12 =

 
7
2 ,









(c) Γ − 32 , (d) Γ − 52 .

√
π (2 n) !
for every positive integer n.
22 n n !

11.3.3. Let x ∈ C be complex. (a) Prove that the gamma function integral (11.62) converges,
provided Re x > 0. (b) Is formula (11.63) valid when x is complex?
♦ 11.3.4. Prove that Γ(x) =
n! =

1

0

1
0

(− log s)x−1 ds, and hence, for 0 ≤ n ∈ Z, we have

(− log s)n ds. Remark : Euler ﬁrst established the latter identity directly, and used

it to deﬁne the gamma function.
∞√
3
x e− x dx.
11.3.5. Evaluate
0

♦ 11.3.6. Can you construct a function f (x) that satisﬁes the factorial functional equation (11.61)
and has the values f (x) = 1 for 0 ≤ x ≤ 1? If so, is f (x) = Γ(x + 1)?

462

11 Dynamics of Planar Media

11.3.7. Explain how to construct the power series for sin x by solving the diﬀerential equation
(11.67).
11.3.8. Construct two independent power series solutions to the Euler equation x2 u − 2 u = 0
based at the point x0 = 1.
11.3.9. Construct two independent power series solutions to the equation u + x2 u = 0 based at
the point x0 = 0.
11.3.10. Consider the ordinary diﬀerential equation u + 2 x u + 2 u = 0. (a) Find two linearly
independent power series solutions in powers of x. (b) What is the radius of convergence
of your power series? (c) By inspection of your series, ﬁnd one solution to the equation
expressible in terms of elementary functions. (d) Find an explicit (non-series) formula for
the second independent power series solution.
11.3.11. Answer Exercise 11.3.10 for the equation u + 12 x u − 12 u = 0, which is a special case
of equation (8.63).
11.3.12. Consider the ordinary diﬀerential equation u + x u + 2 u = 0. (a) Find two linearly
independent power series solutions based at x0 = 0. (b) Write down the power series for
the solution to the initial value problem u(0) = 1, u (0) = −1. (c) What is the radius of
convergence of your power series solution in part (a)? Can you justify this by direct inspection of your power series?
♦ 11.3.13. The Hermite equation of order n is
d2 u
du
− 2x
+ 2 n u = 0.
(11.85)
dx2
dx
Assuming n ∈ N is a nonnegative integer: (a) Find two linearly independent power series
solutions based at x0 = 0, and then show that one of your solutions is a polynomial of
degree n. (b) Prove that the Hermite polynomial Hn (x) deﬁned in (8.64) solves the
Hermite equation (11.85) and hence is a multiple of the polynomial solution you found in
part (a). What is the multiple? (c) Prove that the Hermite polynomials are orthogonal
with respect to the inner product  u , v  =

∞

−∞

2

u(x) v(x) e− x dx.

11.3.14. Use the ratio test to directly determine the radius of convergence of the series solutions (11.81, 82) to the Airy equation.
11.3.15. Write down the general solution to the following ordinary diﬀerential equations:
(a) u + (x − c) u = 0, where c is a ﬁxed constant;
(b) u = λ x u, where λ = 0 is a ﬁxed nonzero constant.
♦ 11.3.16. The Airy function of the second kind is deﬁned by




1 ∞
exp s x − 13 s3 + sin s x + 13 s3 ds.
(11.86)
Bi(x) =
π 0
(a) Prove that Bi(x) is well deﬁned and a solution to the Airy equation. (b) Given that†
1
31/6

 ,
  ,
Bi(0) =
Bi
(0)
=
(11.87)
31/6 Γ 32
Γ 31
explain why every solution to the Airy equation can be written as a linear combination of
Ai(x) and Bi(x). (c) Write the two series solutions (11.81, 82) in terms of Ai(x) and Bi(x).
11.3.17. Use the Fourier transform to construct an L2 solution to the Airy equation. Can you
identify your solution?
♦ 11.3.18. Apply separation of variables to the Tricomi equation (4.137), and write down all
separable solutions. Hint: See Exercise 11.3.15(b) and Exercise 11.3.16.
†

See [ 85; p. 54] for a proof.

11.3 Series Solutions of Ordinary Diﬀerential Equations
♥ 11.3.19. (a) Show that u(x) =

∞

463

(n − 1) ! xn is a power series solution to the ﬁrst-order linear

n=1

ordinary diﬀerential equation x2 u − u + x = 0. (b) For which x does the series converge?
(c) Find an analytic formula for the general solution to the equation. (d) Find a secondorder homogeneous linear ordinary diﬀerential equation that has this power series as a
(formal) solution. Remark : The lesson of this exercise is that not all power series solutions
to ordinary diﬀerential equations converge. Theorem 11.5 guarantees convergence at a regular point, but in this example the power series is based at the singular point x0 = 0.
11.3.20. True or false: The only function f (x) that has identically zero Taylor series is the zero
function.
⎧
⎨ − 1/x2
, x = 0,
♦ 11.3.21. Deﬁne f (x) = ⎩ e
(a) Prove that f is a C∞ function for all x ∈ R.
0,
x = 0.
(b) Prove that f (x) is not analytic by showing that its Taylor series at x0 = 0 does not
converge to f (x) when x = 0.

Regular Singular Points
As we have just seen, constructing power series solutions at regular points is a reasonably
straightforward computational exercise: one writes down a power series with arbitrary
coeﬃcients, substitutes into the diﬀerential equation along with a pair of initial conditions,
and recursively solves for the coeﬃcients. Finding a general formula for the coeﬃcients
might be challenging, but producing their successive numerical values, degree by degree, is
a mechanical exercise.
However, at a singular point, the solutions cannot be typically written as an ordinary
power series, and one needs to be cleverer. Of course, you may object — why not just solve
the equation away from the singular point and be done with it. But there are multiple
reasons not to do this. First, one may be unable to discover a general formula for the
power series coeﬃcients at regular points. Second, the most informative and interesting
behavior of solutions is typically found at the singular points, and so series solutions based
at singular points are particularly enlightening. And ﬁnally, one of the boundary conditions
required for us to complete our construction of separable solutions to partial diﬀerential
equations often occurs at a singular point.
Singular points appear in two guises. The easier to handle, and, fortunately, the ones
that arise in almost all applications, are known as “regular singular points”. Irregular
singular points are nastier, and we will not make any attempt to understand them in this
text; the curious reader is referred to [54, 59].
Deﬁnition 11.6. A second-order linear homogeneous ordinary diﬀerential equation
that can be written the form
d2 u
du
+ c(x) u = 0,
(11.88)
+ (x − x0 ) b(x)
dx2
dx
where a(x), b(x), and c(x) are analytic at x = x0 and, moreover, a(x0 ) = 0, is said to have
a regular singular point at x0 .
(x − x0 )2 a(x)

The simplest example of a second-order equation with a regular singular point at
x0 = 0 is the Euler equation
a x2 u + b x u + c u = 0,
(11.89)

464

11 Dynamics of Planar Media

with a, b, c all constant and a = 0. Note that all other points are regular points. Euler
equations can be readily solved by substituting the power ansatz u(x) = xr . We ﬁnd
a x2 u + b x u + c u = a r(r − 1)xr + b r xr + c xr = 0,
provided the exponent r satisﬁes the indicial equation
a r (r − 1) + b r + c = 0.
If this quadratic equation has two distinct roots r1 = r2 , we obtain two linearly independent
(x) = xr2 . The general solution u(x) =
(possibly complex) solutions u
(x) = xr1 and u
r1
r2
c1 x + c2 x is a linear combination thereof. Note that unless r1 or r2 is a nonnegative
integer, all nonzero solutions have a singularity at the singular point x = 0. A repeated root,
(x) = xr1 , and requires an additional logarithmic
r1 = r2 , has only one power solution, u
r1
term, u
(x) = x log x, for the second independent solution. In this case, the general
solution has the form u(x) = c1 xr1 + c2 xr1 log x.
The series solution method at more general regular singular points is modeled on the
simple example of the Euler equation. One now seeks a solution that has a series expansion
of the form†
u(x) = (x−x0 )r

∞

n=0

un (x−x0 )n = u0 (x−x0 )r +u1 (x−x0 )r+1 +u2 (x−x0 )r+2 +· · · . (11.90)

The exponent r is known as the index . If r = 0, or, more generally, if r is a positive
integer, then (11.90) is an ordinary power series, but we allow the possibility of a nonintegral, or even complex, index r. We can assume, without any loss of generality, that the
leading coeﬃcient u0 = 0. Indeed, if uk = 0 is the ﬁrst nonzero coeﬃcient, then the series
begins with the term uk (x − x0 )r+k , and we merely replace r by r + k to write it in the
form (11.90). Since any scalar multiple of a solution is a solution, we can further assume
that u0 = 1, in which case we call (11.90) a normalized Frobenius series in honor of the
German mathematician Georg Frobenius, who systematically established the calculus of
series solutions at regular singular points in the late 1800s. The index r, and the higherorder coeﬃcients u1 , u2 , . . ., are then found by substituting the normalized Frobenius series
into the diﬀerential equation (11.88) and equating the coeﬃcients of the powers of x − x0
to zero.
Warning: Unlike those in ordinary power series expansions, the coeﬃcients u0 = 1
and u1 are not prescribed by the initial conditions at the point x0 .
Since
u(x) = (x − x0 )r + u1 (x − x0 )r+1 + · · · ,
(x − x0 ) u (x) = r (x − x0 )r + (r + 1)u1 (x − x0 )r+1 + · · · ,

(x − x0 )2 u (x) = r (r − 1) (x − x0 )r + (r + 1) r u1 (x − x0 )r+1 + · · · ,
the terms of lowest order in equation (11.88) are multiples of (x − x 0) r. Equating their
coeﬃcients to zero produces a quadratic equation of the form
a0 r (r − 1) + b0 r + c0 = 0,

(11.91)

†
If r is real but non-integral, and x < x0 , then one can replace x−x0 by x0 −x or, alternatively,
use absolute values throughout.

11.3 Series Solutions of Ordinary Diﬀerential Equations

465

where, referring back to (11.71),
a0 = a(x0 ),

b0 = b(x0 ),

c0 = c(x0 ),

are the leading coeﬃcients in the power series expansions of the individual coeﬃcient functions. The quadratic equation (11.91) is known as the indicial equation, since it determines
the possible indices r in the Frobenius expansion (11.90) of a solution.
As with the Euler equation, the quadratic indicial equation usually has two roots,
say r1 and r2 , which provide two allowable indices, and one thus expects to ﬁnd two
independent Frobenius expansions. Usually, this expectation is realized, but there is an
important exception. The general result is summarized in the following list:
(x) and
(i ) If r1 − r2 is not an integer, then there are two linearly independent solutions u
u
(x), each having convergent normalized Frobenius expansions of the form (11.90).
(ii ) If r1 = r2 , then there is only one solution u
(x) with a normalized Frobenius expansion
(11.90). One can construct a second independent solution of the form
u
(x) = log(x − x0 ) u
(x) + v(x),

where

v(x) =

∞

n=1

vn (x − x0 )n+r1

(11.92)

is a convergent Frobenius series.
(iii ) Finally, if r1 = r2 + k, where k > 0 is a positive integer, then there is a nonzero
solution u
(x) with a convergent Frobenius expansion corresponding to the larger
index r1 . One can construct a second independent solution of the form
u
(x) = c log(x − x0 ) u
(x) + v(x),

where

v(x) = (x − x0 )r2 +

∞

n=1

vn (x − x0 )n+r2

(11.93)
is a convergent Frobenius series, and c is a constant, which may be 0, in which case
the second solution u
(x) is also of Frobenius form.
Thus, in every case, the diﬀerential equation has at least one nonzero solution with a convergent Frobenius expansion. If the second independent solution does not have a Frobenius
expansion, then it requires an additional logarithmic term of a well-prescribed form. Rather
than try to develop the general theory in any more detail here, we will content ourselves
to work through a couple of particular examples.
Example 11.7. Consider the second-order ordinary diﬀerential equation


d2 u
x du
1
+
+ u = 0.
+
dx2
x
2 dx

(11.94)

We look for series solutions based at x = 0. Note that, upon multiplying by x2 , the
equation takes the form

x2 u + x 1 + 12 x2 u + x2 u = 0,
and hence x0 = 0 is a regular singular point, with a(x) = 1, b(x) = 1 + 12 x2 , c(x) = x2 .
We thus look for a solution that can be represented by a Frobenius expansion:
u(x) = xr + u1 xr+1 + · · · + un xn+r + · · · ,

x u (x) = r xr + (r + 1)u1 xr+1 + · · · + (n + r)un xn+r + · · · ,

(11.95)
r+2
1 3 
1
+ 12 (r + 1)u1 xr+3 + · · · + 12 (n + r − 2) un−2 xn+r + · · · ,
2 x u (x) = 2 r x
x2 u (x) = r(r − 1) xr + (r + 1) r u1 xr+1 + · · · + (n + r)(n + r − 1)un xn+r + · · · .

466

11 Dynamics of Planar Media

Substituting into the diﬀerential equation, we ﬁnd that the coeﬃcient of xr leads to the
indicial equation
r2 = 0.
There is only one root, r = 0, and hence, even though we are at a singular point, the
Frobenius expansion reduces to an ordinary power series. The coeﬃcient of xr+1 = x tells
us that u1 = 0. The general recurrence relation, for n ≥ 2, is
n2 un + 12 n un−2 = 0,
and hence

un−2
.
2n
Therefore, the odd coeﬃcients u2k+1 = 0 are all zero, while the even ones are
un = −

u2k = −

u2k−2
u2k−4
u2k−6
(−1)k
=
=−
= ··· = k
,
4k
4 k (4 k − 4)
4 k( 4 k − 4)(4 k − 8)
4 k!

since

u0 = 1.

The resulting power series assumes a recognizable form:


∞
∞


2
1
x2 k
2k
u2k x =
= e− x /4 ,
u
(x) =
−
k!
4
k=1

k=1

which is an explicit elementary solution to the ordinary diﬀerential equation (11.94).
Since there is only one root to the indicial equation, the second solution u
(x) will
require a logarithmic term. It can be constructed by a second application of the Frobenius
method using the more complicated form (11.92). Alternatively, since the ﬁrst solution
is known, we can use a well-known reduction trick, [23]. Given one solution u
(x) to a
second-order linear ordinary diﬀerential equation, the general solution can be found by
substituting the ansatz
2
(11.96)
u(x) = v(x) u
(x) = v(x) e− x /4
into the equation. In this case,


 






x
1 x
1
1 x






+
u +u=v u
+
u
 +u
+
u
 + v  u
 +
 + v 2u

 +
u +
x 2
x
2
x 2
 


2
1 x
= e−x /4 v  +
−
v .
x
2
If u is to be a solution, v ′ must satisfy a linear first-order ordinary diﬀerential equation:
$
#
( x2 /4
e
1 x
c 2
′′
−
v ′ = 0,
and hence
v ′ = ex /4 ,
v=c
dx + b,
v +
x
x
2
x
where c, b are arbitrary constants. We conclude that the general solution to the original
diﬀerential equation is
) (
*
2
2
ex /4
u
%(x) = v(x) u
!(x) = c
dx + b e− x /4 .
(11.97)
x
Bessel’s Equation
Perhaps the most important “non-elementary” ordinary diﬀerential equation is
x2 u + x u + (x2 − m2 ) u = 0,

(11.98)

11.3 Series Solutions of Ordinary Diﬀerential Equations

467

known as Bessel’s equation of order m. We assume here that the order m ≥ 0 is a
nonnegative real number. (Exercise 11.3.30 investigates Bessel equations of imaginary
order.) The Bessel equation arises from separation of variables in a variety of partial
diﬀerential equations, including the Laplace, heat, and wave equations on a disk, a cylinder,
and a spherical ball.
The Bessel equation cannot (except in a few particular instances) be solved in terms
of elementary functions, and so the use of power series is essential. The leading coeﬃcient,
p(x) = x2 , is nonzero except when x = 0, and so all points except the origin are regular.
Therefore, at any x0 = 0, the standard power series construction can be used to produce
the solutions of the Bessel equation. However, the recurrence relations for the coeﬃcients
are not particularly easy to solve in closed form. Moreover, applications tend to demand
understanding the behavior of solutions at the singular point x0 = 0.
Comparison with (11.88) immediately shows that x0 = 0 is a regular singular point,
and so we seek solutions in Frobenius form. We substitute the ﬁrst, second, and fourth
expressions in (11.95) into the Bessel equation and then equate the coeﬃcients of the
various powers of x to zero. The lowest power, xr , provides the indicial equation
r(r − 1) + r − m2 = r2 − m2 = 0.
It has two solutions, r = ± m, except when m = 0, for which r = 0 is the only index.
The higher powers of x lead to recurrence relations for the coeﬃcients un in the
Frobenius series. Replacing m2 by r2 produces


(r + 1)2 − r2 u1 = (2 r + 1)u1 = 0,
u1 = 0,
xr+1 :


1
xr+2 :
,
(r + 2)2 − r2 u2 + 1 = (4 r + 4)u2 + 1 = 0,
u2 = −
4r + 4


u1
xr+3 :
= 0,
(r + 3)2 − r2 u3 + u1 = (6 r + 9)u3 + u1 = 0,
u3 = −
6r + 9
and, in general,


(r + n)2 − r2 un + un−2 = n(2 r + n)un + un−2 = 0.
xr+n :
Thus, the general recurrence relation is
un = −

1
u
,
n(2 r + n) n−2

n = 2, 3, 4, . . . .

(11.99)

Starting with u0 = 1, u1 = 0, it is easy to deduce that all un = 0 for all odd n = 2 k + 1,
while for even n = 2 k,
u2k = −
=

u2k−2
u2k−4
=
= ···
4 k(k + r)
16 k(k − 1)(r + k)(r + k − 1)

(−1)k
.
22k k(k − 1) · · · 3 · 2 (r + k)(r + k − 1) · · · (r + 2)(r + 1)

We have thus found the series solution
u
(x) =

∞

k=0

u2k x

r+2k

=

∞

k=0

(−1)k xr+2k
.
22k k! (r + k)(r + k − 1) · · · (r + 2)(r + 1)

(11.100)

So far, we have not paid attention to the precise values of the indices r = ± m. In
order to continue the recurrence, we need to ensure that the denominators in (11.99) are

468

11 Dynamics of Planar Media

never 0. Since n > 0, a vanishing denominator will appear whenever 2 r + n = 0, and so
r = − 12 n is either a negative integer −1, −2, −3, . . . or half-integer − 12 , − 32 , − 52 , . . . . This
will occur when the order m = − r = 12 n is either an integer or a half-integer. Indeed,
these are precisely the situations in which the two indices, namely r1 = − m and r2 = m,
diﬀer by an integer, r2 − r1 = n, and so we are in the tricky case (iii ) of the Frobenius
method.
There is, in fact, a major diﬀerence between the integral and the half-integral cases.
Recall that the odd coeﬃcients u2k+1 = 0 in the Frobenius series automatically vanish, and
so we only have to worry about the recurrence relation (11.99) for even values of n. When
n = 2 k, the factor 2 r + n = 2 (r + k) = 0 vanishes only when r = − k is a negative integer;
the half-integral values do not, in fact cause problems. Therefore, if the order m ≥ 0 is not
an integer, then the Bessel equation of order m admits two linearly independent Frobenius
solutions, given by the expansions (11.100) with exponents r = + m and r = − m. On the
other hand, if m is an integer, there is only one Frobenius solution, namely the expansion
(11.100) for the positive index r = + m. The Frobenius recurrence with index r = − m
breaks down, and the second independent solution must include a logarithmic term; details
appear below.
By convention, the standard Bessel function of order m is obtained by multiplying
the Frobenius solution (11.100) with r = m by
1
2m m !

,

1

or, more generally,

2m Γ(m + 1)

,

(11.101)

where the ﬁrst factorial form can be used if m is a nonnegative integer, while the more
general gamma function expression must be employed for non-integral values of m. The
result is
∞


(−1)k xm+2k
(11.102)
22k+m k ! (m + k) !
k=0


xm+4
xm+6
xm+2
1
xm −
+
−
+··· .
= m
2 m!
4 (m+1) 32 (m+1)(m+2) 384 (m+1)(m+2)(m+3)

Jm (x) =

When m is non-integral, the (m + k) ! should be replaced by Γ(m + k + 1), and m ! by
Γ(m + 1). With this convention, the series is well deﬁned for all real m except when
m = −1, −2, −3, . . . is a negative integer. Actually, if m is a negative integer, the ﬁrst
m terms in the series vanish, because, at negative integer values, Γ(− n) = ∞. With this
convention, one can prove that
J− m (x) = (−1)m Jm (x),

m = 1, 2, 3, . . . .

(11.103)

A simple application of the ratio test tells us that the power series converges for all
(complex) values of x, and hence Jm (x) is everywhere analytic. Indeed, the convergence
is quite rapid when x is of moderate size, and so summing the series is a reasonably eﬀective method for computing the Bessel function Jm (x) — although in serious applications
one adopts more sophisticated numerical techniques based on asymptotic expansions and
integral formulas, [85, 86]. In particular, we note that
J0 (0) = 1,

Jm (0) = 0,

m > 0.

(11.104)

Figure 11.4 displays graphs of the ﬁrst four Bessel functions for 0 ≤ x ≤ 20; the vertical
axes range from −.5 to 1.0. Most software packages, both symbolic and numeric, include

11.3 Series Solutions of Ordinary Diﬀerential Equations

469

J0 (x)

J1 (x)

J2 (x)

J3 (x)
Figure 11.4.

Bessel functions.

routines for accurately evaluating and graphing Bessel functions, and their properties can
be regarded as well known.
Example 11.8. Consider the Bessel equation of order m = 12 . There are two indices,
r = ± 12 , and the Frobenius method yields two independent solutions: J1/2 (x) and J− 1/2 (x).
For the ﬁrst, with r = 12 , the recurrence relation (11.99) takes the form
un = −

un−2
.
(n + 1) n

Starting with u0 = 1 and u1 = 0, the general formula is easily found to be
⎧
⎨ (−1)k
,
n = 2 k even,
un =
(n + 1) !
⎩
0
n = 2 k + 1 odd.
Therefore, the resulting solution is
u
(x) =

√

x

∞

k=0

∞

(−1)k
1  (−1)k
sin x
x2k = √
x2k+1 = √ .
(2 k + 1) !
x
(2 k + 1) !
x
k=0

According to (11.101), the Bessel function of order 12 is obtained by dividing this function
by

√
π
3
2Γ 2 =
,
2
where we used (11.66) to evaluate the gamma function at 32 . Therefore,

2
J1/2 (x) =
sin x .
πx

(11.105)

470

11 Dynamics of Planar Media

Similarly, for the other index r = − 12 , the recurrence relation
un = −
leads to the formula

⎧
⎨ (−1)k
,
un =
⎩ n!
0

un−2
n (n − 1)

n = 2 k even,
n = 2 k + 1 odd,

for its coeﬃcients, corresponding to the solution
∞

(−1)k

u
(x) = x− 1/2

k=0

cos x
x2k = √ .
(2 k) !
x

Therefore, in view of (11.101) and (11.65), the Bessel function of order − 12 is
√
J− 1/2 (x) =

2
1
2

Γ

cos x
√ =
x



2
cos x .
πx

(11.106)

As we noted above, if m is not an integer, the two independent solutions to the Bessel
equation of order m are Jm (x) and J− m (x). However, when m is an integer, (11.103)
implies that these two solutions are constant multiples of each other, and so one must look
elsewhere for a second independent solution. One method is to use a generalized Frobenius
expansion involving a logarithmic term, i.e., (11.92) when m = 0 (see Exercise 11.3.33)
or (11.93) when m > 0. A second approach is to employ the reduction procedure used in
Example 11.7. Yet another option relies on the following limiting procedure; see [85, 119]
for full details.
Theorem 11.9. If m > 0 is not an integer, then the Bessel functions Jm (x) and
J− m (x) provide two linearly independent solutions to the Bessel equation of order m. On
the other hand, if m = 0, 1, 2, 3, . . . is an integer, then a second independent solution,
traditionally denoted by Ym (x) and called the Bessel function of the second kind of order
m, can be found as a limiting case
Ym (x) = lim

ν→m

Jν (x) cos ν π − J− ν (x)
sin ν π

(11.107)

of a certain linear combination of Bessel functions of non-integral order ν.
With some further analysis, it can be shown that the Bessel function of the second
kind of order m has the logarithmic Frobenius expansion

x#
2"
Ym (x) =
γ + log
Jm (x) +
bk x2k−m ,
π
2
∞

m = 0, 1, 2, . . . ,

k=0

with coeﬃcients
⎧
(m − k − 1) !
⎪
⎪
⎨ − π 22k−m k ! ,
bk =
(−1)k−m−1 (hk−m + hk )
⎪
⎪
⎩
,
π 22k−m k ! (k − m) !

0 ≤ k ≤ m − 1,
k ≥ m,

(11.108)

11.3 Series Solutions of Ordinary Diﬀerential Equations

471

Y0 (x)

Y1 (x)

Y2 (x)

Y3 (x)
Bessel functions of the second kind.

Figure 11.5.
where
h0 = 0,

hk = 1 +

while
γ = lim

k→∞

1 1
1
+ + ··· + ,
2 3
k

k > 0,


hk − log k ≈ .5772156649 . . .

(11.109)

is known as the Euler or Euler–Mascheroni constant. All Bessel functions of the second
kind have a singularity at the origin x = 0; indeed, by inspection of (11.108), we ﬁnd that
the leading asymptotics as x → 0 are
Y0 (x) ∼

2
log x,
π

Ym (x) ∼ −

2m (m − 1) !
,
π xm

m > 0.

(11.110)

Figure 11.5 contains graphs of the ﬁrst four Bessel function of the second kind on the
interval 0 < x ≤ 20; the vertical axis ranges from −1 to 1.
Finally, we show how Bessel functions of diﬀerent orders are interconnected by two
important recurrence relations.
Proposition 11.10. The Bessel functions are related by the following formulae:
dJm
m
+
J (x) = Jm−1 (x),
dx
x m

−

m
dJm
+
J (x) = Jm+1 (x).
dx
x m

Proof : Diﬀerentiating the power series
xm Jm (x) =

∞

k=0

(−1)k x2m+2k
22k+m k ! (m + k) !

(11.111)

472

11 Dynamics of Planar Media

produces
∞

 (−1)k 2 (m + k)x2m+2k−1
d
[ xm Jm (x) ] =
dx
22k+m k ! (m + k) !
k=0

= xm

∞

k=0

(−1)k xm−1+2k
= xm Jm−1 (x).
22k+m−1 k ! (m − 1 + k) !

(11.112)

Expansion of the left-hand side of this formula leads to
xm

dJm
d
+ m xm−1 Jm (x) =
[ xm Jm (x) ] = xm Jm−1 (x),
dx
dx

which establishes the ﬁrst recurrence formula (11.111). The second is proved by a similar
Q.E.D.
manipulation involving diﬀerentiation of x−m Jm (x).
For example, using the second recurrence formula (11.111) along with (11.105), we
can write the Bessel function of order 32 in elementary terms:
dJ1/2 (x)

1
J (x)
2 x 1/2

 dx

 
2 cos x
2 sin x
2 sin x − x cos x
sin x
−
=
.
+
=−
π x1/2
π 2 x3/2
π
2 x3/2
x3/2

J3/2 (x) = −

+

(11.113)

Iterating, one concludes that Bessel functions of half-integral order, m = ± 12 , ± 32 , ± 52 , . . . ,
are all elementary functions,
in that they can be written in terms of trigonometric func√
tions and powers of x . We will make use of these functions in our treatment of the
three-dimensional heat and wave equations in spherical geometry. On the other hand, all
of the other Bessel functions are non-elementary special functions.
With this, we conclude our brief introduction to the method of Frobenius and the
basics of Bessel functions. The reader interested in delving further into either the general
method or the host of additional properties of Bessel functions is encouraged to consult a
more specialized text, e.g., [59, 85, 119].

Exercises
11.3.22. Consider the ordinary diﬀerential equation 2 x u + u + x u = 0. (a) Prove that x = 0
is a regular singular point. (b) Find two independent series solutions in powers of x.
u
u
= 2 . (a) Classify all x0 ∈ R as either a
2−x
x
(i ) regular point; (ii ) regular singular point; and/or (iii ) irregular singular point. Explain
your answers. (b) Find a series solution to the equation based at the point x0 = 0, or
explain why none exists. What is the radius of convergence of your series?

♥ 11.3.23. Consider the diﬀerential equation



1 
u + u = 0.
x
(a) Classify all x0 ∈ R as either (i ) a regular point; (ii ) a regular singular point;
(iii ) an irregular singular point; (iv ) none of the above. Explain your answers.
(b) Write out the ﬁrst ﬁve nonzero terms in a series solution.

11.3.24. Consider the diﬀerential equation u + 1 −

11.3 Series Solutions of Ordinary Diﬀerential Equations

473

11.3.25. Consider the diﬀerential equation 4 x u + 2 u + u = 0. (a) Classify the values of x for
which the equation has regular points, regular singular points, and irregular singular points.
(b) Find two independent series solutions, in powers of x. For what values of x do your
series converge? (c) By inspection of your series, write the general solution to the equation
in terms of elementary functions.
♥ 11.3.26. The Chebyshev diﬀerential equation is (1 − x2 )u − x u + m2 u = 0. (a) Find all
(i ) regular points; (ii ) regular singular points; (iii ) irregular singular points. (b) Show
that if m is an integer, the equation has a polynomial solution of degree m, known as a
Chebyshev polynomial . Write down the Chebyshev polynomials of degrees 1, 2, and 3.
(c) For m = 1, ﬁnd two linearly independent series solutions based at the point x0 = 1.
11.3.27. Write the following Bessel functions in terms of elementary functions:
(a) J5/2 (x), (b) J7/2 (x), (c) J−3/2 (x).
♦ 11.3.28. Prove the identity (11.103).
11.3.29. Suppose that u(x) solves Bessel’s equation. (a) Find a second order ordinary diﬀeren√
tial equation satisﬁed by the function w(x) = x u(x). (b) Use this result to rederive the
formulas for J1/2 (x) and J−1/2 (x).
♦ 11.3.30. Let m ≥ 0 be real, and consider the modiﬁed Bessel equation of order m:
x2 u + x u − (x2 + m2 ) u = 0.

(11.114)

(a) Explain why x0 = 0 is a regular singular point.
(b) Use the method of Frobenius to construct a series solution based at x0 = 0. Can you
relate your solutions to the Bessel function Jm (x)?
♦ 11.3.31. (a) Let a, b, c be constants with b, c = 0. Show that the function u(x) = xa J0 (b xc )
solves the ordinary diﬀerential equation
d2 u
du
+ (1 − 2 a) x
+ (b2 c2 x2 c + a2 ) u = 0.
dx2
dx
What is the general solution to this equation?
(b) Find the general solution to the ordinary diﬀerential equation
x2

x2

d2 u
du
+ αx
+ (β x2 c + γ) u = 0,
dx2
dx

for constants α, β, γ, c with β, c = 0.
d2 u
+ e−2 t u = 0 describes
dt2
the vibrations of a weakening spring whose stiﬀness k(t) = e−2 t is exponentially decaying
in time. (a) Show that this equation can be solved in terms of Bessel functions of order 0.
Hint: Perform a change of variables. (b) Does the solution tend to 0 as t → ∞?

♥ 11.3.32. Let k > 0 be a constant. The ordinary diﬀerential equation

 (x) = J (x) is a solution to the Bessel equation of order 0, namely
♥ 11.3.33. We know that u
0

x u + u + x u = 0.

(11.115)

In accordance with the general Frobenius method, construct a second solution of the form
u(x) = J0 (x) log x +

∞

n=1

vn xn .

11.3.34. Is it possible to have all solutions to an ordinary diﬀerential equation bounded at a
regular singular point? If not, explain why not. If true, give an example where this
happens.

474

11 Dynamics of Planar Media

11.4 The Heat Equation in a Disk, Continued
Now that we have acquired some familiarity with the solutions to Bessel’s ordinary diﬀerential equation, we are ready to analyze the separable solutions to the heat equation in a
polar geometry. At the end of Section 11.2, we were left with the task of solving the Bessel
equation (11.58) of integer order m. As we now know, there are two independent solutions,
namely the Bessel function of the ﬁrst kind Jm , (11.102), and the more complicated Bessel
function of the second kind Ym , (11.107), and hence the general solution has the form
p(z) = c1 Jm (z) + c2 Ym (z),

√
for constants c1 , c2 . Reverting to our original radial coordinate r = z/ λ , we conclude
that every solution to the radial equation (11.56) has the form
√
√
p(r) = c1 Jm λ r + c2 Ym λ r .
Now, the singular point r = 0 represents the center of the disk, and the solutions must
remain bounded there. While this is true for Jm (z), the second Bessel function Ym (z) has,
according to (11.110), a singularity at z = 0 and so is unsuitable for the present purposes.
(On the other hand, it plays a role in other situations, e.g., the heat equation on an annular
ring.) Thus, every separable solution that is bounded at r = 0 comes from the rescaled
Bessel function of the ﬁrst kind of order m:
√
(11.116)
p(r) = Jm λ r .
The Dirichlet boundary condition at the disk’s rim r = 1 requires
√
p(1) = Jm λ = 0.
√
Therefore, in order that λ be a bona ﬁde eigenvalue, λ must be a root of the mth order
Bessel function Jm .
Remark : We already know, thanks to the positive deﬁniteness of the Dirichlet boundary value problem, that the Helmholtz eigenvalues must all be positive, λ > 0, and so there
will be no diﬃculty in taking its square root.
The graphs of Jm (z) strongly indicate, and, indeed, it can be rigorously proved,
[85, 119], that as z increases above 0, each Bessel function oscillates, with slowly decreasing amplitude, between positive and negative values. In fact, asymptotically,



2
Jm (z) ∼
as
z −→ ∞,
(11.117)
cos z − 12 m + 14 π
πz
and so the oscillations become essentially the same as a (phase-shifted) cosine whose amplitude decreases like z − 1/2 . As a consequence, there exists an inﬁnite sequence of Bessel
roots, which we number in increasing order:
Jm (ζm,n ) = 0,

where

0 < ζm,1 < ζm,2 < ζm,3 < · · ·

with

ζm,n −→ ∞

as n −→ ∞.

(11.118)

It is worth emphasizing that the Bessel functions are not periodic, and so their roots
are not evenly spaced. However, as a consequence of (11.117), the large Bessel roots are
asymptotically close to the evenly spaced roots of the shifted cosine:
ζm,n ∼

n + 12 m − 14 π

as

n −→ ∞.

(11.119)

11.4 The Heat Equation in a Disk, Continued

475

Owing to their physical importance in a wide range of problems, the Bessel roots have
been extensively tabulated. The accompanying table displays all Bessel roots that are < 12
in magnitude. The columns of the table are indexed by m, the order of the Bessel function,
and the rows by n, the root number.
Table of Bessel Roots ζm,n
9
m

0

1

2

3

4

5

1

2.4048

3.8317

5.1356

6.3802

7.5883

2

5.5201

7.0156

8.4172

3

8.6537 10.1735 11.6198
..
..
11.7915
.
.
..
.

9.7610 11.0647
..
..
.
.

8.7715
..
.

n

4
..
.

6

7

...

9.9361 11.0864 . . .
..
..
.
.

Remark : According to (11.102),
Jm (0) = 0

for

m > 0,

while

J0 (0) = 1.

However, we do not count 0 as a bona ﬁde Bessel root, since it does not lead to a valid
eigenfunction for the Helmholtz boundary value problem.
Summarizing our progress so far, the eigenvalues
2
λm,n = ζm,n
,

n = 1, 2, 3, . . . ,

m = 0, 1, 2, . . . ,

(11.120)

of the Bessel boundary value problem (11.56–57) are the squares of the roots of the Bessel
function of order m. The corresponding eigenfunctions are
wm,n (r) = Jm (ζm,n r) ,

n = 1, 2, 3, . . . ,

m = 0, 1, 2, . . . ,

(11.121)

deﬁned for 0 ≤ r ≤ 1. Combining (11.121) with the formula (11.55) for the angular components, we conclude that the separable solutions (11.53) to the polar Helmholtz boundary
value problem (11.51) are
v0,n (r) = J0 (ζ0,n r),
vm,n (r, θ) = Jm (ζm,n r) cos m θ,
vm,n (r, θ) = Jm (ζm,n r) sin m θ,

where

m, n = 1, 2, 3, . . . .

(11.122)

These solutions deﬁne the normal modes for the unit disk; Figure 11.6 plots the ﬁrst few of
them. The eigenvalues λ0,n are simple, and contribute radially symmetric eigenfunctions,
whereas the eigenvalues λm,n for m > 0 are double, and produce two linearly independent
separable eigenfunctions, with trigonometric dependence on the angular variable.
Recalling the original ansatz (11.50), we have at last produced the basic separable
eigensolutions
2

2

u0,n (t, r) = e− ζ0,n t v0,n (r) = e− ζ0,n t J0 (ζ0,n r),
2

2

2
− ζm,n
t

2
− ζm,n
t

um,n (t, r, θ) = e− ζm,n t vm,n (r, θ) = e− ζm,n t Jm (ζm,n r) cos m θ,
u
m,n (t, r, θ) = e

vm,n (r, θ) = e

Jm (ζm,n r) sin m θ,

(11.123)
m, n = 1, 2, 3, . . . ,

476

11 Dynamics of Planar Media

v0,1

v0,2

v0,3

v1,1

v1,2

v1,3

v2,1

v2,2

v2,3

v3,1

v3,2

v3,3

Figure 11.6.

Normal modes for a disk.

to the homogeneous Dirichlet boundary value problem for the heat equation on the unit
disk. The general solution is obtained by linear superposition, in the form of an inﬁnite
series
∞
∞



1 
u(t, r, θ) =
a0,n u0,n (t, r) +
m,n (t, r, θ) , (11.124)
am,n um,n (t, r, θ) + bm,n u
2 n=1
m,n = 1

where the initial factor of 12 is included, as with ordinary Fourier series, for later conve-

11.4 The Heat Equation in a Disk, Continued

477

nience. As usual, the coeﬃcients am,n , bm,n are determined by the initial condition
∞
∞



1 
a0,n v0,n (r) +
am,n vm,n (r, θ) + bm,n vm,n (r, θ) = f (r, θ).
u(0, r, θ) =
2 n=1
m,n = 1

(11.125)
This requires that we expand the initial data into a Fourier–Bessel series in the eigenfunctions. As before, it is possible to prove, [34], that the separable eigenfunctions are
complete — there are no other eigenfunctions — and hence every (reasonable) function
deﬁned on the unit disk can be written as a convergent series in the Bessel eigenfunctions.
Theorem 9.33 gurantees that the eigenfunctions are orthogonal† with respect to the
standard L2 inner product


 1 π

u,v =

u(x, y) v(x, y) dx dy =
D

u(r, θ) v(r, θ) r dθ dr
0

−π

on the unit disk. (Note the extra factor of r coming from the polar coordinate form of
the area element dx dy = r dr dθ.) The L2 norms of the Fourier–Bessel eigenfunctions are
given by the interesting formulae


√
 v0,n  = π J1 (ζ0,n ) ,

 vm,n  =  
vm,n  =

π
2

Jm+1 (ζm,n ) ,

(11.126)

which involve the value of the Bessel function of the next-higher order at the appropriate
Bessel root. A proof of (11.126) can be found in Exercise 11.4.22, while numerical values
are provided in the accompanying table.
v m,n 
Norms of the Fourier–Bessel Eigenfunctions  vm,n  =  
9
n
1
2
3
4
5

m

0

1

2

3

4

5

6

7

.9202
.6031
.4811
.4120
.3661

.5048
.3761
.3130
.2737
.2462

.4257
.3401
.2913
.2589
.2353

.3738
.3126
.2736
.2462
.2257

.3363
.2906
.2586
.2352
.2171

.3076
.2725
.2458
.2255
.2095

.2847
.2572
.2347
.2169
.2025

.2658
.2441
.2249
.2092
.1962

Orthogonality of the eigenfunctions implies that the coeﬃcients in the Fourier–Bessel

†

For the two independent eigenfunctions corresponding to one of the double eigenvalues,
orthogonality must be veriﬁed by hand, but, in this case, it follows easily from the orthogonality
of their trigonometric components.

478

11 Dynamics of Planar Media

t=0

t = .02

t = .06

t = .08
Heat diﬀusion in a disk.

Figure 11.7.

t = .04



t = .1

series (11.125) are given by the inner product formulae
a0,n = 2
am,n =
bm,n =

 f , v0,n 
2
=
2
 v0,n 
π J1 (ζ0,n )2

 1 π
f (r, θ) J0 (ζ0,n r) r dθ dr,

0

 f , vm,n 
2
=
2
 vm,n 
π Jm+1 (ζm,n )2
 f , vm,n 
2
=
2

v m,n 
π Jm+1 (ζm,n )2

−π
 1 π

−π
 1 π

f (r, θ) Jm (ζm,n r) r cos m θ dθ dr,

(11.127)

0

0

−π

f (r, θ) Jm (ζm,n r) r sin m θ dθ dr.

In accordance with the general theory, each individual separable solution (11.123) to
2
the heat equation decays exponentially fast, at a rate λm,n = ζm,n
prescribed by the square
of the corresponding Bessel root. In particular, the dominant mode, meaning the one that
persists the longest, is
2
u0,1 (t, r, θ) = e− ζ0,1 t J0 (ζ0,1 r).
(11.128)
Its decay rate is prescribed by the smallest positive eigenvalue:
2
≈ 5.783,
ζ0,1

(11.129)

which is the square of the smallest root of the Bessel function J0 (z). Since J0 (z) > 0 for
0 ≤ z < ζ0,1 , the dominant eigenfunction v0,1 (r, θ) = J0 (ζ0,1 r) > 0 is radially symmetric and strictly positive within the entire disk. Consequently, for most initial conditions

11.4 The Heat Equation in a Disk, Continued

479

(speciﬁcally those for which a0,1 = 0), the disk’s temperature distribution eventually becomes entirely of one sign and radially symmetric, while decaying exponentially fast to zero
at the rate given by (11.129). See Figure 11.7 for a plot of a typical solution. Note how,
in accordance with the theory, the solution soon acquires a radial symmetry as it decays
to thermal equilibrium.

Exercises
11.4.1. At the initial time t0 = 0, a concentrated unit heat source is instantaneously applied at
position x = 12 , y = 0, to a circular metal disk of unit radius and unit thermal diﬀusivity
whose outside edge is held at 0◦ . Write down an eigenfunction series for the resulting temperature distribution at time t > 0. Hint: Be careful working with the delta function in
polar coordinates; see Exercise 6.3.6.

11.4.2. Solve Exercise 11.4.1 when the concentrated unit heat source is instantaneously applied
at the center of the disk.
♥ 11.4.3. (a) Write down the Fourier–Bessel series for the solution to the heat equation on a unit
disk with γ = 1, whose circular edge is held at 0◦ and subject to the initial conditions
u(0, x, y) ≡ 1 for x2 + y 2 ≤ 1. Hint: Use (11.112) to evaluate the integrals for the
coeﬃcients. (b) Approximate the time t ≥ 0 after which the temperature of the disk is
everywhere ≤ .5◦ .
♣ 11.4.4. (a) Write down the ﬁrst three nonzero terms in the Fourier–Bessel series for the solution
to the heat equation on a unit disk with γ = 1 whose circular edge is held at 0◦ subject to
the initial conditions u(0, r, θ) = 1 − r for r ≤ 1. Use numerical integration to evaluate the
coeﬃcients. (b) Use your approximation to determine at which times t ≥ 0 the temperature of the disk is everywhere ≤ .5◦ .
11.4.5. Prove that every separable eigenfunction of the Dirichlet boundary value problem for
the Helmholtz equation in the unit disk can be written in the form
c Jm (ζm,n r) cos(m θ − α) for ﬁxed c = 0 and − π < α ≤ π.
11.4.6. Suppose the initial data f (r, θ) in (11.49) satisﬁes

1 π
0 −π

f (r, θ) J0 (ζ0,1 r) r dθ dr = 0.

(a) What is the decay rate to equilibrium of the resulting heat equation solution u(t, r, θ)?
(b) Prove that, generically, the asymptotic temperature distribution has half the disk above
the equilibrium temperature and the other half below. Can you predict the diameter that
separates the two halves? (c) If you know that a0,1 = 0, and also that the long-time
temperature distribution is radially symmetric, what is the (generic) decay rate? What is
the asymptotic temperature distribution?
♦ 11.4.7. Show how to use a scaling symmetry to solve the heat equation in a disk of radius R
knowing the solution in a disk of radius 1.
11.4.8. Use rescaling, as in Exercise 11.4.7, to produce the solution to the Dirichlet initialboundary value problem for a disk of radius 2 with diﬀusion coeﬃcient γ = 5.
11.4.9. If it takes a disk of unit radius 3 minutes to reach (approximate) thermal equilibrium,
how long will it take a disk of radius 2 made out of the same material and subject to the
same homogeneous boundary conditions to reach equilibrium?
11.4.10. Assuming Dirichlet boundary conditions, does a square or a circular disk of the same
area reach thermal equilibrium faster? Use your intuition ﬁrst, and then check using the
explicit formulas.

480

11 Dynamics of Planar Media

11.4.11. Answer Exercise 11.4.10 when the square and circle have the same perimeter.
11.4.12. Which reaches thermal equilibrium faster: a disk whose edge is held at 0◦ or a disk of
the same radius that is fully insulated?
11.4.13. A circular metal disk is removed from an oven and then fully insulated.
True or false: (a) The eventual equilibrium temperature is constant.
(b) For large t
0, the temperature u(t, x, y) becomes more and more radially symmetric.
If false, what can you say about the temperature proﬁle at large times?
♥ 11.4.14. (a) Write down an eigenfunction series formula for the temperature dynamics of a disk
of radius 1 that has an insulated boundary. (b) What is the eventual equilibrium temperature? (c) Is the rate of decay to thermal equilibrium (i ) faster, (ii ) slower, or (iii ) the
same as a disk with Dirichlet boundary conditions?
♥ 11.4.15. Write out a series solution for the temperature in a half-disk of radius 1, subject to
(a) homogeneous Dirichlet boundary conditions on its entire boundary; (b) homogeneous
Dirichlet conditions on the circular part of its boundary and homogeneous Neumann
conditions on the straight part. (c) Which of the two boundary conditions results in a
faster return to equilibrium temperature? How much faster?
11.4.16. A large sheet of metal is heated to 100◦ . A circular disk and a semi-circular half-disk
of the same radius are cut out of it. Their edges are then held at 0◦ , while being fully
insulated from above and below.
(a) True or false: The half-disk goes to thermal equilibrium twice as fast as the disk.
(b) If you need to wait 20 minutes for the circular disk to cool down enough to be picked up
in your bare hands, how long do you need to wait to pick up the semi-circular disk?
♣ 11.4.17. Two identical plates have the shape of an annular ring { 1 < r < 2 } with inner radius
1 and outer radius 2. The ﬁrst has an insulated inner boundary and outer boundary held
at 0◦ , while the second has an insulated outer boundary and inner boundary held at 0◦ . If
both start out at the same temperature, which reaches thermal equilibrium faster?
Quantify the rates of decay.
♥ 11.4.18. Let m ≥ 0 be a nonnegative integer. In this exercise, we investigate the completeness
of the eigenfunctions of the Bessel boundary value problem (11.56–57). To this end, deﬁne
the Sturm–Liouville linear diﬀerential operator


1 d
du
m2
S[ u ] = −
x
+ 2 u,
x dx
dx
x

subject to the boundary conditions | u (0) | < ∞, u(1) = 0, and either | u(0) | < ∞ when
m = 0, or u(0) = 0 when m > 0.
(a) Show that S is self-adjoint relative to the inner product  f , g  =

 1
0

f (x) g(x) x dx.

(b) Prove that the eigenfunctions of S are the rescaled Bessel functions Jm (ζm,n x) for
n = 1, 2, 3, . . . . What are the orthogonality relations?

ξ), cf. (9.59),
(c) Find the Green’s function G(x; ξ) and modiﬁed Green’s function G(x;
associated with the boundary value problem S[ u ] = 0.
(d) Use the criterion of Theorem 9.47 to prove that the eigenfunctions are complete.
11.4.19. Determine the Bessel roots ζ1/2,n . Do they satisfy the asymptotic formula (11.119)?
♣ 11.4.20. Use a numerical root ﬁnder to compute the ﬁrst 10 Bessel roots ζ3/2,n , n = 1, . . . , 10.
Compare your values with the asymptotic formula (11.119).
♦ 11.4.21. Prove that Jm−1 (ζm,n ) = − Jm+1 (ζm,n ).
♦ 11.4.22. In this exercise, we prove formula (11.126).
(a) First, use the recurrence formulae (11.111) to prove

d  2
x Jm (x)2 − Jm−1 (x) Jm+1 (x)
= 2 x Jm (x)2 .
dx
(b) Integrate both sides of the previous formula from 0 to the Bessel zero ζm,n and then

11.5 The Fundamental Solution to the Planar Heat Equation

481

use Exercise 11.4.21 to show that
2
2
ζm,n
ζm,n
ζm,n
x Jm (x)2 dx = −
Jm−1 (ζm,n ) Jm+1 (ζm,n ) =
Jm+1 (ζm,n )2 .
0
2
2
(c) Next, use a change of variables to establish the identity
1
0

z Jm (ζm,n z)2 dz = 12 Jm+1 (ζm,n )2 .

(d) Finally, use the formulae for vm,n and vm,n to complete the proof of (11.126).
♦ 11.4.23. Prove directly that the eigenfunctions vm,n (r, θ) and vm,n (r, θ) in (11.122) are orthogonal with respect to the L2 inner product on the unit disk.
11.4.24. Establish the following alternative formulae for the eigenfunction norms:


√ 
π  

 Jm (ζm,n )  .
 v0,n  = π  J0 (ζ0,n )  ,
 vm,n  =  vm,n  =
2

11.5 The Fundamental Solution to the Planar Heat Equation
As we learned in Section 8.1, the fundamental solution to the heat equation measures
the temperature distribution resulting from a concentrated initial heat source, e.g., a hot
soldering iron applied instantaneously at a single point on a metal plate. The physical
problem is modeled mathematically by taking a delta function as the initial data along
with the relevant homogeneous boundary conditions. Once the fundamental solution is
known, one is able to use linear superposition to recover the solution generated by any
other initial data.
As in our one-dimensional analysis, we shall concentrate on the most tractable case,
in which the domain is the entire plane: Ω = R 2 . Thus, our ﬁrst goal is to solve the initial
value problem
u(0, x, y) = δ(x − ξ) δ(y − η),
ut = γ Δu,
(11.130)
for t > 0 and (x, y) ∈ R 2 . The solution u = F (t, x; ξ) = F (t, x, y; ξ, η) to this initial value
problem is known as the fundamental solution for the heat equation on R 2 .
The quickest route to the desired formula relies on the following means of combining
solutions of the one-dimensional heat equation to produce solutions of the two-dimensional
version.
Lemma 11.11. Let v(t, x) and w(t, x) be any two solutions to the one-dimensional
heat equation ut = γ uxx . Then their product
u(t, x, y) = v(t, x) w(t, y)

(11.131)

is a solution to the two-dimensional heat equation ut = γ (uxx + uyy ).
Proof : Our assumptions imply that vt = γ vxx , while wt = γ wyy when we write
w(t, y) as a function of t and y. Therefore, diﬀerentiating (11.131), we ﬁnd
 2

∂u
∂v
∂w
∂2v
∂2w
∂ u ∂2u
=
w+v
=γ
w+γv
=γ
+ 2 ,
∂t
∂t
∂t
∂x2
∂y 2
∂x2
∂y
and hence u(t, x, y) solves the two-dimensional heat equation.

Q.E.D.

482

11 Dynamics of Planar Media

For example, if
2

2

v(t, x) = e− γ α t sin α x,

w(t, y) = e− γ β t sin β y,

are separable solutions of the one-dimensional heat equation, then
2

2

u(t, x, y) = e− γ (α +β ) t sin α x sin β y
are the separable solutions we used to solve the heat equation on a rectangle. A more
interesting case is to choose
v(t, x) =

2
1
√
e− (x−ξ) /(4 γ t) ,
2 πγ t

w(t, y) =

2
1
√
e− (y−η) /(4 γ t) ,
2 πγ t

(11.132)

to be the fundamental solutions (8.14) to the one-dimensional heat equation at respective locations x = ξ and y = η. Multiplying these two solutions together produces the
fundamental solution for the two-dimensional problem.
Theorem 11.12. The fundamental solution to the heat equation ut = γ Δu corresponding to a unit delta function placed at position (ξ, η) ∈ R 2 at the initial time t0 = 0
is
2
2
1
(11.133)
F (t, x, y; ξ, η) =
e− [ (x−ξ) +(y−η) ]/(4 γ t) .
4 πγ t
Proof : Since we already know that both functions (11.132) are solutions to the onedimensional heat equation, Lemma 11.11 guarantees that their product, which equals
(11.133), solves the two-dimensional heat equation for t > 0. Moreover, at the initial
time,
u(0, x, y) = v(0, x) w(0, y) = δ(x − ξ) δ(y − η)
is a product of delta functions, and hence the result follows. Indeed, the total heat

 ∞
 ∞
u(t, x, y) dx dy =
v(t, x) dx
w(t, y) dy = 1,
t ≥ 0,
−∞

−∞

remains constant, while

lim u(t, x, y) =

t→0+

∞,

(x, y) = (ξ, η),

0,

otherwise,

has the standard delta function limit at the initial time instant.

Q.E.D.

Figure 11.8 depicts the evolution of the fundamental solution when γ = 1 at the
indicated times. Observe that the initially concentrated temperature spreads out in a
radially symmetric manner, while the total amount of heat remains constant. At any
individual point (x, y) = (0, 0), the initially zero temperature rises slightly at ﬁrst, but
then decays monotonically back to zero at a rate proportional to 1/t. As in the onedimensional case, since the fundamental solution is > 0 for all t > 0, the heat energy has
an inﬁnite speed of propagation.
Both the one- and two-dimensional fundamental solutions have bell-shaped proﬁles
known as Gaussian ﬁlters. The most important diﬀerence is the initial factor.
In a one√
dimensional medium, the fundamental solution decays in proportion to 1/ t, whereas in
the plane the decay is more rapid, being proportional to 1/t. The physical explanation is
that the heat energy is able to spread out in two independent directions, and hence diﬀuses

11.5 The Fundamental Solution to the Planar Heat Equation

Figure 11.8.

483

t = .01

t = .02

t = .05

t = .1

The fundamental solution to the planar heat equation.



away from its initial source more rapidly. As we shall see, the decay in three-dimensional
space is more rapid still, being proportional to t−3/2 for similar reasons; see (12.120).
The principal use of the fundamental solution is for solving the general initial value
problem. We express the initial temperature distribution as a superposition of delta function impulses,

u(0, x, y) = f (x, y) =
f (ξ, η) δ(x − ξ, y − η) dξ dη,
where, at the point (ξ, η) ∈ R 2 , the impulse has magnitude f (ξ, η). Linearity implies that
the solution is then given by the same superposition of fundamental solutions.
Theorem 11.13. The solution to the initial value problem
ut = γ Δu,

u(0, x, y) = f (x, y),

(x, y) ∈ R 2 ,

for the planar heat equation is given by the linear superposition formula

2
2
1
f (ξ, η) e− [ (x−ξ) +(y−η) ]/(4 γ t) dξ dη.
u(t, x, y) =
4 πγ t

(11.134)

484

11 Dynamics of Planar Media

t=0

t = .01

t = .05

t = .1

t = .2

t = .5


Figure 11.9.

Diﬀusion of a disk.

We can interpret the solution formula (11.134) as a two-dimensional convolution
u(t, x, y) = F (t, x, y) ∗ f (x, y)

(11.135)

of the initial data with a one-parameter family of progressively wider and shorter Gaussian
ﬁlters
2
2
1
(11.136)
e− (x +y )/(4 γ t) .
F (t, x, y) = F (t, x, y; 0, 0) =
4 πγ t
As in (7.54), such a convolution can be interpreted as a Gaussian weighted averaging of
the function f (x, y), which has the eﬀect of smoothing out the initial data.
Example 11.14.
region, say

If our initial temperature distribution is constant on a circular

u(0, x, y) =

1

x2 + y 2 < 1,

0,

otherwise,

then the solution can be evaluated using (11.134), as follows:

2
2
1
e−[ (x−ξ) +(y−η) ]/(4 t) dξ dη,
u(t, x, y) =
4π t
D
where the integral is over the unit disk D = { ξ 2 + η 2 ≤ 1 }. Unfortunately, the integral
cannot be expressed in terms of elementary functions. On the other hand, numerical

11.5 The Fundamental Solution to the Planar Heat Equation

485

evaluation of the integral is straightforward. A plot of the resulting radially symmetric
solution appears in Figure 11.9. One could also interpret this solution as the diﬀusion of
an animal population in a uniform isotropic environment or bacteria in a similarly uniform
large petri dish that are initially conﬁned to a small circular region.

Exercises
11.5.1. Solve the following initial value problem: ut = 5 (uxx + uyy ),

2

2

u(0, x, y) = e− (x +y ) .

11.5.2. Write down an integral formula for the solution to the following initial value problem:
u(0, x, y) = (1 + x2 + y 2 )−2 .
ut = 3(uxx + uyy ),
11.5.3. At the initial time t = 0, a unit heat source is instantaneously applied at the origin
of the (x, y)–plane. For t > 0, what is the maximum temperature experienced at a point
(x, y) = 0? At what time is the maximum temperature achieved? Does the temperature
approach an equilibrium value as t → ∞? If so, how fast?
11.5.4. (a) Find an eigenfunction series representation of the fundamental solution for the heat
equation ut = Δu on the unit square { 0 ≤ x, y ≤ 1 } when subject to homogeneous Dirichlet boundary conditions. (b) Write the solution to the initial value problem u(0, x, y) =
f (x, y) in terms of the fundamental solution. (c) Discuss how your formula is related to the
Fourier series solution (11.43).
11.5.5. Let u(t, x, y) be a solution to the heat equation on all of R 2 such that u and  ∇u  → 0
u(t, x, y) dx dy is constant.
rapidly as  x  → ∞. (a) Prove that the total heat H(t) =
(b) Explain how this can be reconciled with the statement that u(t, x, y) → 0 as t → ∞ at
all points (x, y) ∈ R 2 .
♦ 11.5.6. Consider the initial value problem ut = γ Δu+H(t, x, y), u(0, x, y) = 0, for the inhomogeneous heat equation on the entire (x, y)–plane, where H(t, x, y) represents a time-varying
external heat source. Derive an integral formula for its solution. Hint: Mimic the solution
method in Section 8.1.
11.5.7. A ﬂat plate of inﬁnite extent with unit thermal diﬀusivity starts oﬀ at 0◦ . From then
on, a unit heat source is continually applied at the origin. Find the resulting temperature
distribution. Does the temperature eventually reach a steady state?
Hint: Use Exercise 11.5.6.
♥ 11.5.8. Building on Example 11.14, we model the “diﬀusion” of a set D ⊂ R 2 as the solution
u(t, x, y) to the heat
equation ut = Δu subject to the initial condition u(0, x, y) = χD (x, y),

1, (x, y) ∈ D,
is the characteristic function of the set D.
where χD (x, y) =
0, (x, y) ∈ D,
(a) Write down a formula for the diﬀusion of the set D.
(b) True or false: At each t, the diﬀusion u(t, x, y) is the characteristic function of a set Dt .
(c) Prove that 0 < u(t, x, y) < 1 for all (x, y) and t > 0. (d) What is lim u(t, x, y)?
t→∞

(e) Write down a formula for the diﬀusion of a unit square D = { 0 ≤ x, y ≤ 1 }, and then
plot the result at several times. Discuss what you observe.
11.5.9. (a) Explain why the delta function on R 2 satisﬁes the scaling law δ(x, y) = β 2 δ(βx, β y),
for β = 0. (b) Verify that the fundamental solution to the heat equation on R 2 obeys the
same scaling law: F (t, x, y) = β 2 F (β 2 t, β x, β y). (c) Is the fundamental solution a
similarity solution?

486

11 Dynamics of Planar Media

11.5.10. (a) Find the fundamental solution on R 2 to the cable equation ut = γ Δu − α u, where
α > 0 is constant. (b) Use your solution to write down a formula for the solution to the
general initial value problem u(0, x, y) = f (x, y) for (x, y) ∈ R 2 .
11.5.11. (a) Prove that if v(t, x) and w(t, x) solve the dispersive wave equation (8.90), then
their product u(t, x, y) = v(t, x) w(t, y) solves the two-dimensional dispersive equation
ut + uxxx + uyyy = 0.
(b) What is the fundamental solution on R 2 of the latter equation? (c) Write down an integral formula for the solution to the initial value problem u(0, x, y) = f (x, y) for (x, y) ∈ R 2 .
11.5.12. Deﬁne the two-dimensional convolution f ∗ g of functions f (x, y) and g(x, y) so that
equation (11.135) is valid.

11.6 The Planar Wave Equation
Let us next consider the two-dimensional wave equation
 2

∂ u ∂2u
∂2u
2
2
= c Δu = c
+ 2 ,
∂t2
∂x2
∂y

(11.137)

which models the unforced transverse vibrations of a homogeneous membrane, e.g., a drum.
Here, u(t, x, y) represents the vertical displacement of the membrane at time t and position
(x, y) ∈ Ω, where the domain Ω ⊂ R 2 , assumed bounded, represents the undeformed shape.
The constant c2 > 0 encapsulates the membrane’s physical properties — density, tension,
stiﬀness, etc.; its square root, c, is called, as in the one-dimensional case, the wave speed ,
since it represents the speed of propagation of localized signals.
Remark : In this simpliﬁed model, we are only allowing small, transverse (vertical)
displacements of the membrane. Large elastic vibrations lead to the nonlinear partial
diﬀerential equations of elastodynamics, [7]. In particular, the bending vibrations of a
ﬂexible elastic plate are governed by a more complicated fourth-order partial diﬀerential
equation.
The solution u(t, x, y) to the wave equation will be uniquely speciﬁed once we impose
suitable boundary and initial conditions. The Dirichlet conditions
u(t, x, y) = h(x, y),

(x, y) ∈ ∂Ω,

(11.138)

correspond to gluing our membrane to a ﬁxed boundary — a rim; more generally, we can
also allow h to depend on t, modeling a membrane attached to a moving boundary. On
the other hand, the homogeneous Neumann conditions
∂u
(11.139)
(t, x, y) = 0,
(x, y) ∈ ∂Ω,
∂n
represent a free boundary where the membrane is not attached to any support — although
in this model, its edge is allowed to move only in a vertical direction. Mixed boundary
conditions attach part of the boundary and leave the remaining portion free to vibrate:
u=h

on

D  ∂Ω,

∂u
=0
∂n

on

N = ∂Ω \ D.

(11.140)

11.6 The Planar Wave Equation

487

Since the wave equation is of second order in time, to uniquely specify the solution we need
to impose two initial conditions,
u(0, x, y) = f (x, y),

∂u
(0, x, y) = g(x, y),
∂t

(x, y) ∈ Ω.

(11.141)

The ﬁrst speciﬁes the membrane’s initial displacement, while the second prescribes its
initial velocity.
Separation of Variables
Unfortunately, the d’Alembert solution method does not apply to the two-dimensional
wave equation in any obvious manner. The reason is that, unlike the one-dimensional
version (2.69), one cannot factorize the planar wave operator  = ∂t2 − c2 ∂x2 − c2 ∂y2 , thus
precluding any sort of reduction to a ﬁrst-order partial diﬀerential equation. However, this
is not the end of the story, and we will return to this issue at the end of Section 12.6.
We thus fall back on our universal solution tool for linear partial diﬀerential equations
— separation of variables. According to the general framework established in Section 9.5,
the separable solutions to the wave equation have the trigonometric form
uk (t, x, y) = cos(ωk t) vk (x, y)

u
k (t, x, y) = sin(ωk t) vk (x, y).

and

(11.142)

Substituting back into the wave equation, we ﬁnd that vk (x, y) must be an eigenfunction
of the associated Helmholtz boundary value problem
 2

∂ u ∂2u
c2
+
(11.143)
+ λk v = 0,
∂x2
∂y 2
whose eigenvalue λk = ωk2 equals the square of the vibrational frequency. According to
Theorem 9.47, on a bounded domain, there is an inﬁnite number of such normal modes with
progressively faster vibrational frequencies: ωk → ∞ as k → ∞. In addition, in the positive
semi-deﬁnite case — which occurs under homogeneous Neumann boundary conditions —
there is a single constant null eigenfunction, leading to the additional separable solutions
u0 (t, x, y) = 1

and

u
0 (t, x, y) = t.

(11.144)

The ﬁrst represents a stationary membrane that has been displaced to a ﬁxed height, while
the second represents a membrane that is moving oﬀ in the vertical direction with constant
unit speed. (Think of the membrane moving in outer space unaﬀected by any external
gravitational force.) As in Section 9.5, the general solution can be written as an inﬁnite
series in the eigensolutions (11.142). Unfortunately, as we know, the Helmholtz boundary
value problem can be explicitly solved only on a rather restricted class of domains. Here
we will content ourselves with investigating the two most important cases: rectangular and
circular membranes.
Remark : The vibrational frequencies represent the tones and overtones one hears when
the drum membrane vibrates. An interesting question is whether two drums of diﬀerent
shapes can have identical sounds — the exact same vibrational frequencies. Or, more
descriptively, can one “hear” the shape of a drum? It was not until 1992 that the answer
was shown to be no, but for quite subtle reasons. See [47] for a discussion and some
examples of diﬀerently shaped drums that have the same vibrational frequencies.

488

11 Dynamics of Planar Media

Vibration of a Rectangular Drum
Let us ﬁrst consider the vibrations of a membrane in the shape of a rectangle
R = { 0 < x < a, 0 < y < b },
with side lengths a and b, whose edges are ﬁxed to the (x, y)–plane. Thus, we seek to solve
the wave equation
utt = c2 Δu = c2 (uxx + uyy ),

0 < x < a,

0 < y < b,

(11.145)

subject to the initial and boundary conditions
0 < x < a,

u(t, 0, y) = u(t, a, y) = 0 = u(t, x, 0) = u(t, x, b),
u(0, x, y) = f (x, y),

(11.146)

0 < y < b.

ut (0, x, y) = g(x, y),

As we saw in Section 11.2, the eigenfunctions and eigenvalues for the associated Helmholtz
equation on a rectangle,
c2 (vxx + vyy ) + λ v = 0,

(x, y) ∈ R,

(11.147)

when subject to the homogeneous Dirichlet boundary conditions
v(0, y) = v(a, y) = 0 = v(x, 0) = v(x, b),

0 < x < a,

are
m πx
n πy
sin
,
vm,n (x, y) = sin
a
b

0 < y < b,


where

2 2

λm,n = π c

n2
m2
+
a2
b2

(11.148)


,

(11.149)

with m, n = 1, 2, . . . . The fundamental frequencies of vibration are the square roots of the
eigenvalues, so

/
m2
n2
(11.150)
ωm,n = λm,n = π c
+
,
m, n = 1, 2, . . . .
a2
b2
The frequencies will depend upon the underlying geometry — meaning the side lengths —
of the rectangle, as well as the wave speed c, which, in turn, is a function of the membrane’s
density and stiﬀness. The higher the wave speed, or the smaller the rectangle, the faster
the vibrations. In layman’s terms, (11.150) quantiﬁes the observation that smaller, stiﬀer
drums made of less-dense material vibrate faster.
According to (11.142), the normal modes of vibration of our rectangle are



n πy
m2
n2
m πx
sin
,
um,n (t, x, y) = cos π c
+ 2 t sin
2
a
b
a
b


(11.151)

n πy
m2
n2
m πx
u
m,n (t, x, y) = sin π c
sin
.
+ 2 t sin
a2
b
a
b
The general solution can then be written as a double Fourier series
u(t, x, y) =

∞


m,n = 1

m,n (t, x, y)
am,n um,n (t, x, y) + bm,n u



11.6 The Planar Wave Equation

489

t=0

t = .2

t = .4

t = .6

t = .8

t = 1.0

t = 1.2

t = 1.4

t = 1.6


Figure 11.10.

Vibrations of a square membrane.

in the normal modes. The coeﬃcients am,n , bm,n are ﬁxed by the initial displacement
u(0, x, y) = f (x, y) and the initial velocity ut (0, x, y) = g(x, y). Indeed, the usual orthogonality relations among the eigenfunctions imply
 vm,n , f 
4
am,n =
=
2
 vm,n 
ab

 b a

m πx
n πy
sin
dx dy,
(11.152)
a
b
0 0
 b a
 vm,n , g 
4
m πx
n πy
√
=
g(x, y) sin
sin
dx dy.
bm,n =
2
2
2
2
2
ωm,n  vm,n 
a
b
πc m b +n a 0 0
f (x, y) sin

Since the fundamental frequencies are not rational multiples of each other, the general
solution is a genuinely quasiperiodic superposition of the various normal modes.
In Figure 11.10, we plot the solution resulting from the initially concentrated displacement†
2
2
u(0, x, y) = f (x, y) = e− 100 [ (x−.5) +(y−.5) ]
at the center of a unit square, so a = b = 1, with unit wave speed, c = 1. Note that, unlike
a concentrated displacement of a one-dimensional string, which remains concentrated at
all subsequent times and periodically repeats, the initial displacement here spreads out in
a radially symmetric manner and propagates to the edges of the rectangle, where it reﬂects
†

The alert reader may object that the initial displacement f (x, y) does not exactly satisfy
the Dirichlet boundary conditions on the edges of the rectangle. But this does not prevent the
existence of a well-deﬁned (weak) solution to the initial value problem, whose initial boundary
discontinuities will subsequently propagate into the square. However, here these are so tiny as to
be unnoticeable in the solution graphs.

490

11 Dynamics of Planar Media

and then interacts with itself. Moreover, due to the quasiperiodicity of the solution, the
drum’s motion never exactly repeats, and the initially concentrated displacement never
quite reforms.

Vibration of a Circular Drum
Let us next analyze the vibrations of a circular membrane of unit radius. In polar coordinates, the planar wave equation (11.137) takes the form
∂2u
= c2
∂t2



1 ∂2u
∂ 2 u 1 ∂u
+
+
∂r2
r ∂r
r2 ∂θ2


.

(11.153)

We will again consider the homogeneous Dirichlet boundary value problem
t ≥ 0,

u(t, 1, θ) = 0,

− π ≤ θ ≤ π,

(11.154)

∂u
(0, r, θ) = g(r, θ),
∂t

(11.155)

along with initial conditions
u(0, r, θ) = f (r, θ),

representing the initial displacement and velocity of the membrane. As always, we build up
the general solution as a quasiperiodic linear combination of the normal modes as speciﬁed
by the eigenfunctions for the associated Helmholtz boundary value problem.
As we saw in Section 11.2, the eigenfunctions of the Helmholtz equation on a disk
of radius 1, say, subject to homogeneous Dirichlet boundary conditions, are products of
trigonometric and Bessel functions:
v0,n (r, θ) = J0 (ζ0,n r),
vm,n (r, θ) = Jm (ζm,n r) cos m θ,

m, n = 1, 2, 3, . . . .

(11.156)

v m,n (r, θ) = Jm (ζm,n r) sin m θ,

Here r, θ are the usual polar coordinates, while ζm,n > 0 denotes the nth (positive) root
of the mth order Bessel function Jm (z), cf. (11.118). The corresponding eigenvalue is its
2
, and hence the natural frequencies of vibration are equal to the Bessel
square, λm,n = ζm,n
roots scaled by the wave speed:
ωm,n = c


λm,n = c ζm,n .

(11.157)

A table of their values (for the case c = 1) can be found in the preceding section. The Bessel
roots do not follow any easily discernible pattern, and are not rational multiples of each
other. This result, known as Bourget’s hypothesis, [119; p. 484], was rigorously proved by
the German mathematician Carl Ludwig Siegel in 1929, [106]. Thus, the vibrations of a
circular drum are also truly quasiperiodic, thereby providing a mathematical explanation
of why drums sound dissonant.
The frequencies ω0,n = c ζ0,n correspond to simple eigenvalues, with a single radially
symmetric eigenfunction J0 (ζ0,n r), while the “angular modes” ωm,n , for m > 0, are double,
each possessing two linearly independent eigenfunctions (11.156). According to the general

11.6 The Planar Wave Equation

Figure 11.11.

491

Vibration of a disk.



492

11 Dynamics of Planar Media

formula (11.142), each eigenfunction engenders two independent normal modes of vibration,
having the explicit forms
cos(c ζ0,n t) J0 (ζ0,n r),

sin(c ζ0,n t) J0 (ζ0,n r),

cos(c ζm,n t) Jm (ζm,n r) cos m θ,

sin(c ζm,n t) Jm (ζm,n r) cos m θ,

cos(c ζm,n t) Jm (ζm,n r) sin m θ,

sin(c ζm,n t) Jm (ζm,n r) sin m θ.

(11.158)

The general solution to (11.153–154) is then expressed as a Fourier–Bessel series:
∞

u(t, r, θ) =


1 
a0,n cos(c ζ0,n t) + c0,n sin(c ζ0,n t) J0 (ζ0,n r)
2 n=1
∞


(11.159)
am,n cos(c ζm,n t) + cm,n sin(c ζm,n t) cos m θ
+


m,n = 1
+ bm,n cos(c ζm,n t) + dm,n sin(c ζm,n t) sin m θ Jm (ζm,n r),

whose coeﬃcients am,n , bm,n , cm,n , dm,n are determined, as usual, by the initial displacement and velocity of the membrane (11.155). In Figure 11.11, the vibrations due to an
initially oﬀ-center concentrated displacement are displayed; the wave speed is c = 1, and the
time interval between successive plots is Δt = .3. Again, the motion is only quasiperiodic
and, no matter how long you wait, never quite returns to its original conﬁguration.

Exercises
11.6.1. Use your physical intuition to decide whether the following statements are true or false.
Then justify your answer.
(a) Increasing the stiﬀness of a membrane increases the wave speed.
(b) Increasing the density of a membrane increases the wave speed.
(c) Increasing the size of a membrane increases the wave speed
11.6.2. Two uniform membranes have the same shape, but are made out of diﬀerent materials.
Assuming that they are both subject to the same homogeneous boundary conditions, how
are their vibrational frequencies related?
11.6.3. List the numerical values of the six lowest vibrational frequencies of a unit square with
wave speed c = 1 when subject to homogeneous Dirichlet boundary conditions. How many
linearly independent normal modes are associated with each of these frequencies?
♥ 11.6.4. The rectangular membrane R = { −1 < x < 1, 0 < y < 1 } has its two short sides
attached to the (x, y)–plane, while its long sides are left free. The membrane is initially
displaced so that its right half is one unit above, while its left half is one unit below the
plane, and then released with zero initial velocity. (This discontinuous initial data serves
to model a very sharp transition region.) Assume that the physical units are chosen so the
wave speed c = 1. (a) Write down an initial-boundary value problem that governs the
vibrations of the membrane. (b) What are the fundamental frequencies of vibration of the
membrane? (c) Find the eigenfunction series solution that describes the subsequent motion of the membrane. (d) Is the motion (i ) periodic? (ii ) quasiperiodic? (iii ) unstable?
(iv ) chaotic? Explain your answer.
11.6.5. Determine the solution to the following initial-boundary value problems for the wave
equation on the rectangle R = { 0 < x < 2, 0 < y < 1 }:

utt = uxx + uyy ,
u(t, x, 0) = u(t, x, 1) = u(t, 0, y) = u(t, 2, y) = 0,
(a)
u(0, x, y) = sin πy,
ut (0, x, y) = sin πy;

11.6 The Planar Wave Equation
(b)

⎧
⎪
⎨u =u
tt
xx + uyy ,
⎪
⎩

u(t, x, 0) = u(t, x, 1) =

(c) ⎪

⎪
⎩ u(0, x, y) =

1,
0,

u(t, x, 0) = u(t, x, 1) = u(t, 0, y) = u(t, 2, y) = 0,
0 < x < 1,
1 < x < 2,

⎧
⎪
u = 2 uxx + 2 uyy ,
⎪
⎨ tt

(d) ⎪

⎪
⎩ u(0, x, y) = 0,

∂u
∂u
(t, 0, y) =
(t, 2, y) = 0,
∂x
∂x

ut (0, x, y) = sin πy;

u(0, x, y) = sin πy,

⎧
⎪
u = uxx + uyy ,
⎪
⎨ tt


493

ut (0, x, y) = 0;

u(t, x, 0) = u(t, x, 1) = u(t, 0, y) = u(t, 2, y) = 0,

1, 0 < x < 1,
ut (0, x, y) =
0, 1 < x < 2.

11.6.6. True or false: The more sides of a rectangle that are tied down, the faster it vibrates.
11.6.7. Answer Exercise 11.6.3 when (a) two adjacent sides of the square are tied down and
the other two are left free; (b) two opposite sides of the square are tied down and the other
two are left free; (c) the membrane is freely ﬂoating in outer space.
11.6.8. A square drum has two sides ﬁxed to a support and two sides left free. Does the drum
vibrate faster if the ﬁxed and free sides are adjacent to each other or on opposite sides?
11.6.9. Write down a periodic solution to the wave equation on a unit square, subject to
homogeneous Dirichlet boundary conditions, that is not a normal mode. Does it vibrate at
a fundamental frequency?
11.6.10. A rectangular drum with side lengths 1 cm by 2 cm and unit wave speed c = 1 has its
boundary ﬁxed to the (x, y)–plane while subject to a periodic external forcing of the form
F (t, x, y) = cos(ω t) h(x, y). (a) At which frequencies ω will the forcing incite resonance
in the drum? (b) If ω is a resonant frequency, write down the condition(s) on h(x, y) that
ensure excitation of a resonant mode.
11.6.11. The right half of a rectangle of side lengths 1 by 2 is initially displaced, while the left
half is quiescent. True or false: The ensuing vibrations are restricted to the right half of
the membrane.
♥ 11.6.12. A torus (inner tube) can be obtained by gluing together each of the two pairs of
opposite sides of a rubber rectangle. The (small) vibrations of the torus are described by
the following periodic initial-boundary value problem for the wave equation, in which x, y
represent angular variables:
u(0, x, y) = f (x, y),
ut (0, x, y) = g(x, y),
utt = c2 Δu = c2 (uxx + uyy ),
u(t, − π, y) = u(t, π, y),
ux (t, − π, y) = ux (t, π, y),
− π < x < π,
u(t, x, − π) = u(t, x, π),
ux (t, x, − π) = ux (t, x, π),
− π < y < π.
(a) Find the fundamental frequencies and normal modes of vibration. (b) Write down a
series for the solution. (c) Discuss the stability of a vibrating torus. Is the motion
(i ) periodic; (ii ) quasiperiodic; (iii ) chaotic; (iv ) none of these?
11.6.13. The forced wave equation utt = c2 Δu + F (x, y) on a bounded domain Ω ⊂ R 2
models a membrane subject to a constant external forcing function F (x, y). Write down
an eigenfunction series solution to the forced wave equation when the membrane is subject
to homogeneous Dirichlet boundary conditions and initial conditions u(0, x, y) = f (x, y),
ut (0, x, y) = g(x, y). Hint: Expand the forcing function in an eigenfunction series.
11.6.14. A circular drum of radius ζ0,1 ≈ 2.4048 has initial displacement and velocity


∂u
u(0, x, y) = 0,
(0, x, y) = 2 J0 x2 + y 2 .
∂t
Assuming that the circular edge of the drum is ﬁxed to the (x, y)–plane, describe, both
qualitatively and quantitatively, its subsequent motion.
11.6.15. Write out the integral formulae for the coeﬃcients in the Fourier–Bessel series solution
(11.159) to the wave equation in a circular disk in terms of the initial data
u(0, r, θ) = f (r, θ), ut (0, r, θ) = g(r, θ).

494

11 Dynamics of Planar Media

11.6.16. A circular drum at rest is struck with a concentrated blow at its center. Write down
an eigenfunction series describing the resulting vibration.
♥ 11.6.17. (a) Set up and solve the initial-boundary value problem for the vibrations of a uniform
circular drum of unit radius that is freely ﬂoating in space. (b) Discuss the stability of the
drum’s motion. (c) Are the vibrations slower or faster than when its edges are ﬁxed to a
plane?
11.6.18. A ﬂat quarter-disk of radius 1 has its circular edge and one of its straight edges
attached to the (x, y)–plane, while the other straight edge is left free. At time t = 0 the
disk is struck with a hammer (unit delta function) at its midpoint, i.e., at radius 12 and
halfway between the straight edges. (a) Write down an initial-boundary value problem for
the subsequent vibrations of the quarter-disk. Hint: Be careful with the form of the delta
function in polar coordinates; see Exercise 6.3.6. (b) Assuming that the physical units are
chosen so that the wave speed c = 1, determine the quarter-disk’s vibrational frequencies.
(c) Write down an eigenfunction series solution for the subsequent motion. (d) Is the
motion unstable? periodic? If so, what is the period?
11.6.19. True or false: Assuming homogeneous Dirichlet boundary conditions, the fundamental frequencies of a vibrating half-disk are exactly twice those of the full disk of the same
radius.
♥ 11.6.20. The edge of a circular drum is moved periodically up and down, so u(t, 1, θ) = cos ω t.
Assuming that the drum is initially at rest, discuss its response.
♣ 11.6.21. A drum is in the shape of a circular annulus with outer radius 1 meter and inner
radius .5 meter. Find numerical values for its ﬁrst three fundamental vibrational
frequencies.
♥ 11.6.22. A homogeneous rope of length 1 and weight 1 is suspended from the ceiling. Taking x
as the vertical coordinate, with x = 1 representing the ﬁxed end and x = 0 the free end, the
planar displacement u(t, x) of the rope satisﬁes the initial-boundary value problem


| u(t, 0) | < ∞,
u(t, 1) = 0,
∂2u
∂
∂u
=
x
,
t > 0, 0 < x < 1.
∂u
∂t2
∂x
∂x
u(0, x) = f (x),
(0, x) = g(x),
∂t
√
(a) Find the solution. Hint: Let y = x . (b) Are the vibrations periodic or quasiperiodic?
(c) Describe the behavior of the rope when subject to uniform periodic external forcing
F (t, x) = a cos ω t.

Scaling and Symmetry
Symmetry methods can also be eﬀectively employed in the analysis of the wave equation. Let us consider the simultaneous rescaling
t −→ α t,

x −→ β x,

y −→ β y,

(11.160)

of time and space, whose eﬀect is to change the function u(t, x, y) into a rescaled version
U (t, x, y) = u(α t, β x, β y).

(11.161)

The chain rule is employed to relate their derivatives:
2
2
∂2U
∂2U
2 ∂ u
2 ∂ u
=
α
,
=
β
,
∂t2
∂t2
∂x2
∂x2
Therefore, if u satisﬁes the wave equation

utt = c2 Δu,

2
∂2U
2 ∂ u
=
β
.
∂y 2
∂y 2

11.6 The Planar Wave Equation

495

then U satisﬁes the rescaled wave equation
Utt =

α2 c2
Δ U = C 2 Δ U,
β2

where the rescaled wave speed is

C=

αc
.
β

(11.162)

In particular, rescaling only time by setting α = 1/c, β = 1, results in a unit wave speed
C = 1. In other words, we are free to choose our unit of time measurement so as to ﬁx the
wave speed equal to 1.
If we set α = β, scaling time and space in the same proportion, then the wave speed
does not change, C = c, and so
t −→ β t,

x −→ β x,

y −→ β y,

(11.163)

deﬁnes a symmetry transformation for the wave equation: If u(t, x, y) is any solution to
the wave equation, then so is its rescaled version
U (t, x, y) = u(β t, β x, β y)

(11.164)

for any choice of scale parameter β = 0. Observe that if u(t, x, y) is deﬁned on a domain
Ω, then the rescaled solution U (t, x, y) will be deﬁned on the rescaled domain

 $
%
$
1
x
y
 = Ω=
$ (x, y) ∈ Ω = { (x, y) | (β x, β y) ∈ Ω } .
Ω
,
(11.165)
$
β
β β
For instance, setting the scaling parameter β = 2 halves the size of the domain. The
normal modes for the rescaled domain have the form
Un (t, x, y) = un (β t, β x, β y) = cos(β ωn t) vn (β x, β y),
 (t, x, y) = u
U
 (β t, β x, β y) = sin(β ω t) v (β x, β y),
n

n

n

n

and hence the rescaled vibrational frequencies are Ωn = β ωn . Thus, when β < 1, the
rescaled membrane is larger by a factor 1/β, and its vibrations are slowed down by the
reciprocal factor β. For instance, a drum that is twice as large will vibrate twice as slowly,
and hence have an octave lower overall tone. Musically, this means that all drums of a
similar shape have the same pattern of overtones, diﬀering only in their overall pitch, which
is a function of their size, tautness, and density.
In particular, choosing β = 1/R will rescale the unit disk into a disk of radius R. The
fundamental frequencies of the rescaled disk are
Ωm,n = β ωm,n =

c
ζ ,
R m,n

(11.166)

where c is the wave speed and ζm,n are the Bessel roots, deﬁned in (11.118). Observe
that the ratios ωm,n /ωm ,n between vibrational frequencies remain the same, independent
of the size of the disk R and the wave speed c. In general, we deﬁne the relative vibrational frequencies to be the ratios between the individual frequencies and the dominant,
or smallest, one. Thus, the relative vibrational frequencies of a circular drum are
ρm,n =

ωm,n
ζm,n
=
,
ω0,1
ζ0,1

where

ω0,1 =

c ζ0,1
c
≈ 2.4 .
R
R

(11.167)

The relative frequencies (11.167) are independent of the size, stiﬀness or composition of
the drum membrane. In the following table, we display a list of all relative vibrational
frequencies (11.167) that are < 6. Once the lowest frequency ω0,1 has been determined

496

11 Dynamics of Planar Media

— either theoretically, numerically, or experimentally — all the higher overtones ωm,n =
ρm,n ω0,1 are simply obtained by rescaling.
Relative Vibrational Frequencies of a Circular Disk
&
n
1
2
3
4
..
.

m

0

1

2

3

4

5

6

7

8

9

...

1.000 1.593 2.136 2.653 3.155 3.647 4.132 4.610 5.084 5.553 . . .
..
..
..
2.295 2.917 3.500 4.059 4.601 5.131 5.651
.
.
.
..
..
3.598 4.230 4.832 5.412 5.977
.
.
..
..
..
4.903 5.540
.
.
.
..
..
.
.

Exercises
11.6.23. True or false: Two rectangular membranes, made out of the same material and both
subject to Dirichlet boundary conditions, have the same relative vibrational frequencies if
and only if they are have similar shapes.
11.6.24. True or false: (a) The vibrational frequencies of a square with side lengths a = b = 2
are four times as slow as those of a square with side lengths a = b = 1.
(b) The vibrational frequencies of a rectangle with side lengths a = 2, b = 1, are twice as
slow as those of a square with side lengths a = b = 1.
11.6.25. A vibrating rectangle of unknown size has wave speed c = 1 and is subject to homogeneous Dirichlet boundary conditions. How many of its lowest vibrational frequencies do you
need to know in order to determine the size of the rectangle?
11.6.26. Answer Exercise 11.6.25 when the rectangle is subject to homogeneous Neumann
boundary conditions.
♣ 11.6.27. A circular drum has the A above middle C, which has a frequency of 440 Hertz, as its
lowest tone. What notes are the ﬁrst ﬁve overtones nearest? Try playing these on a piano
or guitar. Or, if you have a synthesizer, try assembling notes of these frequencies to see how
closely it reproduces the dissonant sound of a drum.
11.6.28. In an orchestra, a circular snare drum of radius 1 foot sits near a second circular drum
made out of the same material. Vibrations of the ﬁrst drum are observed to excite an undesired resonant vibration in its partner. What are the possible radii of the second drum?
11.6.29. True or false: The relative vibrational frequencies of a half-disk, subject to Dirichlet
boundary conditions, are a subset of the relative vibrational frequencies of a full disk.
11.6.30. True or false: If u(t, x, y) = cos(ω t) v(x, y) is a normal mode of vibration for a unit
 (t, x, y) =
square subject
to homogeneous Dirichlet boundary conditions, then the function u

1
1
cos(ω t) v 2 x, 3 y is a normal mode of vibration for a 2 × 3 rectangle that is subject to the
same boundary conditions, but with a possibly diﬀerent wave speed. If true, how are the
wave speeds of the two rectangles related?
11.6.31. Prove that if u(t, x, y) is a solution to the two-dimensional wave equation, so is the
translated function U (t, x, y) = u(t − t0 , x − x0 , y − y0 ), for any constants t0 , x0 , y0 .

11.6 The Planar Wave Equation

497

♦ 11.6.32. (a) Prove that if u(t, x, y) solves the wave equation, so does U (t, x, y) = u(− t, x, y).
Thus, unlike the heat equation, the wave equation is time-reversible, and its solutions can
be unambiguously followed backwards in time. (b) Suppose u(t, x, y) solves the initial value
problem (11.141). Write down the initial value problem satisﬁed by U (t, x, y).
11.6.33. (a) Prove that, on R 2 , the solution to the pure displacement initial value problem
utt = c2 Δu, u(0, x, y) = f (x, y), ut (0, x, y) = 0, is an even function of t.
(b) Prove that the solution to the pure velocity initial value problem utt = c2 Δu,
u(0, x, y) = 0, ut (0, x, y) = g(x, y), is an odd function of t.
Hint: Use Exercise 11.6.32 and uniqueness of solutions to the initial value problem.
11.6.34. Suppose v(t, x) is any solution to the one-dimensional wave equation vtt = vxx . Prove
that u(t, x, y) = v(t, a x + b y), for any constants (a, b) = (0, 0), solves the two-dimensional
wave equation utt = c2 (uxx + uyy ) for some choice of wave speed. Describe the behavior of
such solutions.
11.6.35. A traveling-wave solution to the two-dimensional wave equation has the form
u(t, x, y) = v(x − a t, y − a t), where a is a constant. Find the partial diﬀerential equation
satisﬁed by the function v(ξ, η). Is the equation hyperbolic?
11.6.36. Is the counterpart of Lemma 11.11 valid for the wave equation? In other words, if
v(t, x) and w(t, x) are any two solutions to the one-dimensional wave equation, is their
product u(t, x, y) = v(t, x) w(t, y) a solution to the two-dimensional wave equation?
11.6.37. (a) How would you solve an initial-boundary value problem for the wave equation on a
rectangle that is not aligned with the coordinate axes? (b) Apply your method to set up
and solve an initial-boundary value problem on the square R = { | x + y | < 1, | x − y | < 1 }.

Chladni Figures and Nodal Curves
When a membrane vibrates, its individual atoms typically move up and down in a quasiperiodic manner. As such, there is little correlation between their motions at diﬀerent locations.
However, if the membrane is set to vibrate in a pure eigenmode, say
un (t, x, y) = cos(ωn t) vn (x, y),

(11.168)


then all points move up and down at a common frequency ωn = λn , which is the square
root of the eigenvalue corresponding to the eigenfunction vn (x, y). The exceptions are the
points where the eigenfunction vanishes:
vn (x, y) = 0,

(11.169)

which remain stationary. The set of all points (x, y) ∈ Ω that satisfy (11.169) is known as
the nth Chladni ﬁgure of the domain Ω, named in honor of the eighteenth-century German
physicist and musician Ernst Chladni who ﬁrst observed them experimentally by exciting a
metal plate with his violin bow, [43]. The mathematical models governing such vibrating
plates were formulated by the French mathematician Sophie Germain in the early 1800s.
It can be shown that, in general, each Chladni ﬁgure consists of a ﬁnite system of nodal
curves, [34, 43], that partition the membrane into disjoint nodal regions. As the membrane
vibrates, the nodal curves remain stationary, while each nodal region is entirely either
above or below the equilibrium plane, except, momentarily, when the entire membrane
has zero displacement. As Chladni discovered in his original experiments, scattering small

498

11 Dynamics of Planar Media

1.000

1.593

2.136

2.295

2.653

2.917

3.155

3.500

3.598

Figure 11.12.
Nodal curves and relative vibrational
frequencies of a circular membrane.
particles (e.g., ﬁne sand) over a membrane or plate vibrating in an eigenmode will enable
us to visualize the Chladni ﬁgure, because the particles will tend to accumulate along the
stationary nodal curves. Adjacent nodal regions, lying on the opposite sides of a nodal
curve, move in opposing directions — when one is up, its neighbors are down, and then
they switch roles as the membrane becomes momentarily ﬂat. Let us look at a couple of
examples where the Chladni ﬁgures can be readily determined.

11.6 The Planar Wave Equation

499

Example 11.15. Circular Drums. Since the eigenfunctions (11.156) for a disk are
products of trigonometric functions in the angular variable and Bessel functions of the
radius, the nodal curves for the normal modes of vibrations of a circular membrane are
rays emanating from and circles centered at the origin. Consequently, the nodal regions
are annular sectors. Chladni ﬁgures associated with the ﬁrst nine normal modes, indexed
by their relative frequencies, are plotted in Figure 11.12. Representative displacements of
the membrane in each of the ﬁrst twelve modes can be found earlier, in Figure 11.6. The
dominant (lowest frequency) mode is the only one that has no nodal curves; it has the
form of a radially symmetric bump where the entire membrane ﬂexes up and down. The
next lowest modes vibrate proportionally faster at a relative frequency ρ1,1 ≈ 1.593. The
most general solution with this vibrational frequency is a linear combination of the two
eigensolutions: α u1,1 + β u
1,1 . Each such combination has a single diameter as a nodal
curve, whose angle with the horizontal depends on the ratio β/α. The two semicircular
halves of the drum vibrate in opposing directions — when the top half is up, the bottom
half is down and vice versa. The next set of modes have two perpendicular diameters as
nodal curves; the four quadrants of the drum vibrate in tandem, with opposite quadrants
moving in the same direction. Next in increasing order of vibrational frequency is a single
mode, which has a circular nodal curve whose (relative) radius equals the ratio of the
ﬁrst two roots of the order zero Bessel function, ζ0,1 /ζ0,2 ≈ .43565; see Exercise 11.6.39
for a justiﬁcation. In this case, the inner disk and the outer annulus vibrate in opposing
directions. And so on . . . .
Example 11.16. Rectangular Drums. For most rectangular drums, the Chladni ﬁgures are relatively uninteresting. Since the normal modes (11.151) are separable products
of trigonometric functions in the coordinate variables x, y, the nodal curves are equally
spaced straight lines parallel to the sides of the rectangle. The internodal regions are
smaller rectangles, of identical size and shape, with adjacent rectangles vibrating in opposite directions.
More interesting ﬁgures appear when the rectangle admits multiple eigenvalues — socalled accidental degeneracies. Note that two of the eigenvalues (11.149) coincide, λm,n =
λk,l , if and only if
m2
n2
k2
l2
+
=
+
,
(11.170)
a2
b2
a2
b2
where (m, n) = (k, l) are distinct pairs of positive integers. In such situations, the two
eigenmodes happen to vibrate with a common frequency ω = ωm,n = ωk,l . Consequently,
any linear combination of the eigenmodes, e.g.,


m πx
n πy
k πx
l πy
cos(ω t) α sin
sin
+ β sin
sin
,
α, β ∈ R,
a
b
a
b
is also a pure vibration, and hence qualiﬁes as a normal mode. The associated nodal curves,
α sin

m πx
n πy
k πx
l πy
sin
+ β sin
sin
= 0,
a
b
a
b

0 ≤ x ≤ a,
0 ≤ y ≤ b,

(11.171)

have a more intriguing geometry, which can change dramatically as the coeﬃcients α, β
vary.
'
(
For example, on the unit square R = 0 < x, y < 1 , an accidental degeneracy occurs
whenever
m2 + n2 = k 2 + l2
(11.172)

500

11 Dynamics of Planar Media

α=β=1

α = 2, β = 1

Figure 11.13.

α = 5, β = 1

Some Chladni ﬁgures for a square membrane.

for distinct pairs of positive integers (m, n) = (k, l). The simplest possibility arises whenever m = n, in which case we can merely reverse the order, setting k = n, l = m. In
Figure 11.13 we plot three sample nodal curves
α sin 4 πx sin πy + sin πx sin 4 πy = 0,
corresponding to three diﬀerent linear combinations of the eigenfunctions√with m = l = 4,
n = k = 1. The associated vibrational frequency is, in all cases, ω4,1 = c 17 π, where c is
the wave speed.
Classifying accidental degeneracies of rectangles takes us into the realm of number
theory, [9, 29]. In the case of a square, equation (11.172) is asking us to locate all integer
points (m, n) ∈ Z2 that lie on a common circle.
Remark : Bourget’s hypothesis, mentioned after (11.157), implies that ζm,n = ζk,l
whenever (m, n) = (k, l). This implies that a disk has no accidental degeneracies, and
hence all its nodal curves are concentric circles and diameters.

Exercises
♦ 11.6.38. Suppose that a membrane is vibrating in a normal mode. Prove that the membrane
lies instantaneously completely ﬂat at regular time intervals.
♦ 11.6.39. For a vibrating disk of unit radius, determine the radius of the circular nodal curve for
the next-to-lowest circular mode.
11.6.40. Order the ﬁve nodal circles displayed in Figure 11.12 according to their size.
11.6.41. Sketch the Chladni ﬁgures in a unit disk corresponding to the following vibrational
frequencies. Determine numerical values for the radii of any circular nodal curves.
(a) ω0,4 , (b) ω2,4 , (c) ω4,2 , (d) ω3,3 , (e) ω5,1 .
11.6.42. True or false: Any diameter of a circular disk is a nodal curve for some normal mode.
11.6.43. True or false: The nodal curves on a semicircular disk are all semicircles and rays
emanating from the center.

11.6 The Planar Wave Equation

501

11.6.44. (a) Find the smallest distinct pair of positive integers (k, l) = (m, n) satisfying (11.172)
that are not obtained by simply reversing the order, i.e., (k, l) = (n, m). (b) Find the
next-smallest example. (c) Plot two or three Chladni ﬁgures arising from such degenerate
eigenfunctions.
♥ 11.6.45. Let R be a rectangle all of whose sides are ﬁxed to the (x, y)–plane. Suppose that all
its nodal curves are straight lines. What can you say about its side lengths a, b?
11.6.46. True or false: The nodal regions of a vibrating rectangle are similarly shaped
rectangles.
♦ 11.6.47. Prove that any point of intersection (x0 , y0 ) of two nodal curves associated with the
same normal mode is a critical point of the associated eigenfunction: ∇v(x0 , y0 ) = 0.
11.6.48. True or false: The nodal curves on a domain do not depend on the choice of boundary
conditions.

Chapter 12

Partial Diﬀerential Equations in Space

At last we have ascended to the ultimate rung of the dimensional ladder (at least for those
of us living in a three-dimensional universe): partial diﬀerential equations in physical space.
As in the one- and two-dimensional settings developed in the preceding chapters, the main
protagonists are the Laplace and Poisson equations, modeling equilibrium conﬁgurations of
solid bodies; the three-dimensional wave equation, governing vibrations of solids, liquids,
and electromagnetic waves; and the three-dimensional heat equation, modeling spatial
diﬀusion processes. To conclude this chapter — and the book — we will also analyze the
particular three-dimensional Schrödinger equation that governs the hydrogen atom, and
thereby characterizes atomic orbitals.
Fortunately, almost everything of importance has already appeared in the previous
chapters, and appending a third dimension is, for the most part, simply a matter of appropriately adapting the constructions. We have already developed the principal solution techniques: separation of variables, Green’s functions, and fundamental solutions. In
three-dimensional problems, separation of variables is applicable in a variety of coordinate
systems, including the usual rectangular, cylindrical, and spherical coordinates. The ﬁrst
two do not lead to anything fundamentally new, and are therefore relegated to the exercises. Separation in spherical coordinates requires spherical Bessel functions and spherical
harmonics, which play essential roles in a wide variety of physical systems, both classical
and quantum.
The Green’s function for the three-dimensional Poisson equation in space can be identiﬁed as the classic Newton (Coulomb) 1/r gravitational (electrostatic) potential. The
fundamental solution for the three-dimensional heat equation can be easily guessed from
its one- and two-dimensional forms. The three-dimensional wave equation, surprisingly,
has an explicit solution formula, named after Kirchhoﬀ, of electrical fame, but originally
due to Poisson. Counterintuitively, the best way to handle the two-dimensional wave equation is by “descending” from the simpler(!) three-dimensional Kirchhoﬀ formula. Descent
reveals a remarkable diﬀerence between waves in planar and spatial media. Huygens’ Principle states that three-dimensional waves emanating from a localized initial disturbance
remain localized as they propagate through space. In contrast, initially concentrated twodimensional disturbances leave a slowly decaying remnant that never entirely disappears.
The ﬁnal section is concerned with the Schrödinger equation for a hydrogen atom,
that is, the quantum-dynamical system governing the spatial motion of a single electron
around a positively charged nucleus. As we will see, the spherical harmonic eigensolutions
account for the observed quantum energy levels of atoms that underly the periodic table
and hence the foundations of molecular chemistry.
P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0_12, © Springer International Publishing Switzerland 2014

503

504

12 Partial Diﬀerential Equations in Space

12.1 The Three–Dimensional Laplace and Poisson Equations
We begin our investigations, as usual, with systems in equilibrium, deferring dynamics
until later. The prototypical equilibrium system is the three-dimensional Laplace equation
Δu =

∂2u ∂2u ∂2u
+ 2 + 2 = 0,
∂x2
∂y
∂z

(12.1)

T

in which x = ( x, y, z ) represents rectangular coordinates on R 3 . The solutions u(x, y, z)
continue to be known as harmonic functions. The Laplace equation models unforced
equilibria; Poisson’s equation is the inhomogeneous version
− Δu = f (x, y, z),

(12.2)

whose right-hand side represents some form of external forcing.
The basic boundary value problem for the Laplace and Poisson equations seeks a
solution inside a bounded domain Ω ⊂ R 3 subject to either Dirichlet boundary conditions,
prescribing the function values on the domain’s boundary:
u=h

on

∂Ω,

(12.3)

or Neumann boundary conditions, prescribing its normal derivative or ﬂux through the
boundary:
∂u
=k
on
∂Ω,
(12.4)
∂n
or mixed boundary conditions, in which one imposes Dirichlet conditions on part of the
boundary and Neumann conditions on the remainder. Keep in mind that the boundary of
the solid domain Ω consists of one or more piecewise smooth closed surfaces, which will be
oriented by use of the outward — meaning exterior to the domain — unit normal n.
The boundary value problems for the three-dimensional Laplace and Poisson equations
govern a wide variety of physical systems, including:
• Heat conduction: The solution u represents the equilibrium temperature in a solid
body. The inhomogeneity f represents some form of internal heat source or sink.
Dirichlet conditions correspond to ﬁxing the temperature on the bounding surface(s), whereas homogeneous Neumann conditions correspond to an insulated
boundary, i.e., one that does not allow any heat ﬂux.
• Ideal ﬂuid ﬂow : Here the solution u to the Laplace equation represents the velocity potential for an incompressible, irrotational steady-state ﬂuid ﬂow inside a container
governed by the velocity vector ﬁeld v = ∇u. Homogeneous Neumann boundary
conditions correspond to a solid boundary that the ﬂuid cannot penetrate.
• Elasticity: In certain restricted contexts, u represents an equilibrium deformation of
a solid body, e.g., the radial deformation of an elastic ball.
• Electrostatics: In applications to electromagnetism, u is the electric potential in a
conducting medium; its gradient ∇u prescribes the electromotive force on a charged
particle. The inhomogeneity f represents an external electrostatic force ﬁeld.
• Gravitation: The Newtonian gravitational potential in ﬂat empty space is also prescribed by the Laplace equation. (In contrast, Einstein’s theory of general relativity requires a vastly more complicated nonlinear system of partial diﬀerential
equations, [75].)

12.1 The Three–Dimensional Laplace and Poisson Equations

505

Self–Adjoint Formulation and Minimum Principle
The Laplace and Poisson equations naturally ﬁt into the general self-adjoint equilibrium
framework summarized in Chapter 9. We introduce the L2 inner products

u(x, y, z) u
(x, y, z) dx dy dz,
u,u
 =
Ω

(12.5)
 =
 (x, y, z) dx dy dz,
v,v
v(x, y, z) · v
Ω

 , which are deﬁned on the
between, respectively, scalar ﬁelds u, u
, and vector ﬁelds v, v
domain Ω ⊂ R 3 . We assume that the functions in question are suﬃciently nice in order
that these inner products be well deﬁned; if Ω is unbounded, this, in essence, requires that
they decay reasonably rapidly to zero at large distances.
When subject to suitable homogeneous boundary conditions, the three-dimensional
Laplace equation can be placed in our standard self-adjoint form
− Δu = − ∇ · ∇u = ∇∗ ◦ ∇u.
(12.6)
This relies on the fact that the adjoint of the gradient operator with respect to the L2 inner
products (12.5) is minus the divergence operator:
∇∗ v = − ∇ · v.
(12.7)
As usual, the determination of the adjoint rests on an integration by parts formula, which,
in three-dimensional space, is a consequence of the Divergence Theorem from multivariable
calculus, [8, 108]:
Theorem 12.1. Let Ω ⊂ R 3 be a bounded domain whose boundary ∂Ω consists
of one or more piecewise smooth simple closed surfaces. Let n denote the unit outward
normal to the boundary of Ω. Let v be a C1 vector ﬁeld deﬁned on Ω and continuous
up to its boundary. Then the surface integral, with respect to surface area, of the normal
component of v over the boundary of the domain equals the triple integral of its divergence
over the domain:


v · n dS =
∇ · v dx dy dz.
(12.8)
∂Ω

Ω

Replacing v by the product u v of a scalar ﬁeld u and a vector ﬁeld v yields



(u ∇ · v + ∇u · v) dx dy dz =
∇ · (u v) dx dy dz =
u (v · n) dS. (12.9)
Ω

∂Ω

Ω

Rearranging the terms produces the desired integration by parts formula for triple integrals:



(∇u · v) dx dy dz =
u (v · n) dS −
u (∇ · v) dx dy dz.
(12.10)
Ω

∂Ω

Ω

The boundary surface integral will vanish, provided either u = 0 or v · n = 0 at each point
on ∂Ω. When u = 0 on all of ∂Ω, we have homogeneous Dirichlet conditions. Setting
v · n = 0 everywhere on ∂Ω results in the homogeneous Neumann boundary value problem
owing to the identiﬁcation of v = ∇u. Finally, the mixed boundary value problem takes
u = 0 on part of ∂Ω and v · n = 0 on the rest. Thus, subject to one of these choices, the
integration by parts formula (12.10) reduces to
 ∇u , v  =  u , − ∇ · v ,
which suﬃces to establish the adjoint formula (12.7).

(12.11)

506

12 Partial Diﬀerential Equations in Space

Remark : Adopting more general weighted inner products results in a more general
elliptic boundary value problem. See Exercise 12.1.9 for details.
According to Theorem 9.20, the self-adjoint formulation (12.6) automatically implies
positive semi-deﬁniteness of the boundary value problem, with positive deﬁniteness if
ker ∇ = {0}. Since, on a connected domain, only constant functions are annihilated by
the gradient operator — see Lemma 6.16, which also applies to three-dimensional domains
— both the Dirichlet and mixed boundary value problems are positive deﬁnite, while the
Neumann boundary value problem is only positive semi-deﬁnite.
Finally, in the positive deﬁnite cases, Theorem 9.26 implies that the solution can
be characterized by the three-dimensional version of the Dirichlet minimization principle
(9.82).
Theorem 12.2. The solution u(x, y, z) to the Poisson equation (12.2) subject to
homogeneous Dirichlet or mixed boundary conditions (12.3) is the unique function that
minimizes the Dirichlet integral


1 2
2
2
2
1
|
∇u
|
−

u
,
f

=
(12.12)
2
2 (ux + uy + uz ) − f u dx dy dz
Ω

2

among all C functions that satisfy the prescribed boundary conditions.
As in the two-dimensional version discussed in Chapter 9, the Dirichlet minimization
principle continues to hold in the case of the inhomogeneous Dirichlet boundary value
problem. Modiﬁcations for the inhomogeneous mixed boundary value problem appear in
Exercise 12.1.13.

Exercises
12.1.1. Find bases for the following: (a) the space of harmonic polynomials u(x, y, z) of degree
≤ 2; (b) the space of homogeneous cubic harmonic polynomials u(x, y, z).
12.1.2. True or false: (a) Every harmonic polynomial is homogeneous.
(b) Every homogeneous polynomial is harmonic.
12.1.3. Solve the Poisson boundary value problem − Δu = 1 on the unit ball x2 + y 2 + z 2 < 1
with homogeneous Dirichlet boundary conditions. Hint: Look for a polynomial solution.
♦ 12.1.4. Prove that if u(x, y, z) solves the Laplace equation, then so does the translated function
U (x, y, z) = u(x − a, y − b, z − c) for constants a, b, c.
♦ 12.1.5. (a) Prove that if u(x, y, z) solves Laplace’s equation, so does the rescaled function
U (x, y, z) = u(λ x, λ y, λ z) for any constant λ. (b) More generally, show that
U (x, y, z) = μ u(λ x, λ y, λ z) + c solves Laplace’s equation for any constants λ, μ, c.
♦ 12.1.6. Let A be a constant nonsingular 3 × 3 matrix, u(x) a C1 scalar ﬁeld, and v(x) a C1
vector ﬁeld. Set U (x) = u(A x) and V(x) = v(A x). Prove that
(a) ∇U (x) = AT ∇u(A x), (b) ∇ · V(x) = w(A x), where w(x) = ∇ · (A v)(x).
♦ 12.1.7. Prove that every rotation and reﬂection is a symmetry of the Laplace equation. In
other words, if Q is any 3 × 3 orthogonal matrix, so QT Q = I , and u(x) is a harmonic
function, then so is U (x) = u(Q x). Hint: Use Exercise 12.1.6.

12.2 Separation of Variables for the Laplace Equation

507

♦ 12.1.8. The Weak Maximum Principle: Let Ω ⊂ R 2 be a bounded domain. Let u(x, y, z) solve
the Poisson equation − Δu = f (x, y, z), where f (x, y, z) < 0 for all (x, y, z) ∈ Ω.
(a) Prove that the maximum value of u occurs on the boundary ∂Ω.
Hint: Explain why u cannot have a local maximum at any interior point in Ω.
(b) Generalize your result to the case f (x, y, z) ≤ 0.
Hint: Look at vε (x, y, z) = u(x, y, z) + ε (x2 + y 2 + z 2 ) and let ε → 0+ .
♦ 12.1.9. Find the equilibrium equations corresponding to minimizing | ∇u |2 subject to homogeneous Dirichlet boundary conditions, where the indicated norm is based on the weighted
inner product
 v , w  =
v(x, y, z) · w(x, y, z) σ(x, y, z) dx dy dz,
Ω

with σ(x, y, z) > 0 a positive scalar function.
♦ 12.1.10. Prove the following vector calculus identities:
(a) ∇ · (u v) = ∇u · v + u ∇ · v, (b) ∇ × (u v) = ∇u × v + u ∇ × v,
(c) ∇ · (v × w) = (∇ × v) · w − v · (∇ × w), (d) ∇ × (∇ × v) = ∇(∇ · v) − Δv.
(In the ﬁnal term, the Laplacian Δ acts component-wise on the vector ﬁeld v.)
♦ 12.1.11. Let Ω be a bounded domain with piecewise smooth boundary ∂Ω. Prove the following
∂u
identities:
(a)
Δu dx dy dz =
dS,
Ω
∂Ω ∂n
∂u
u Δu dx dy dz =
u
 ∇u 2 dx dy dz.
dS −
(b)
Ω
∂Ω
Ω
∂n
12.1.12. Suppose the inhomogeneous Neumann boundary value problem (12.1, 4) has a solution.
k dS = 0. (b) Is the solution unique? If not, what is the most general
(a) Prove that
∂Ω

solution? (c) State and prove an analogous result for the inhomogeneous Poisson equation
− Δu = f (x, y, z). (d) Provide a physical explanation for your answers.
♦ 12.1.13. Find a minimization principle that characterizes the solution to the inhomogeneous
mixed boundary value problem − Δu = f on Ω, with u = g on D  ∂Ω, and ∂u/∂n = h
on N = ∂Ω \ D.
♥ 12.1.14. (a) Prove that, subject to suitable boundary conditions, the curl ∇× deﬁnes a selfadjoint operator with respect to the L2 inner product between vector ﬁelds. What kinds
of boundary conditions do you need to impose for your integration by parts argument to be
valid? Hint: Use the identity in Exercise 12.1.10(c). (b) What operator on vector ﬁelds is
given by the self-adjoint composition S = (∇×)∗ ◦ (∇×)? (c) Choose a set of homogeneous boundary conditions that make S self-adjoint. Is the resulting boundary value
problem S[ v ] = f positive deﬁnite? If not, what does the Fredholm Alternative say about
its solvability?

12.2 Separation of Variables for the Laplace Equation
In this section, we revisit the method of separation of variables in the context of the threedimensional Laplace equation. As always, its applicability is unfortunately restricted to
rather special, but important, geometric conﬁgurations, the simplest being rectangular,
cylindrical, and spherical domains. Since the ﬁrst two are straightforward extensions of
their two-dimensional counterparts, we will discuss only spherically separable solutions in
any detail.
The simplest domain to which the separation of variables method applies is a rectan-

508

12 Partial Diﬀerential Equations in Space

gular box:
B = { 0 < x < a, 0 < y < b, 0 < z < c }.
For functions of three variables, one begins the separation process by splitting oﬀ one of
them, by setting u(x, y, z) = v(x) w(y, z), say. The function v(x) satisﬁes a simple secondorder ordinary diﬀerential equation, while w(y, z) solves the two-dimensional Helmholtz
equation (11.21), which is further separated by writing w(y, z) = p(y) q(z). The resulting
fully separated solutions u(x, y, z) = v(x) p(y) q(z) are (mostly) products of trigonometric
and hyperbolic functions. Implementation of the technique and analysis of the resulting
series solutions are relegated to Exercise 12.2.34.
In the case that the domain is a cylinder, one passes to cylindrical coordinates r, θ, z,
where
x = r cos θ,
y = r sin θ,
z = z,
(12.13)
to eﬀect the separation. Writing u(r, θ, z) = v(r, θ) w(z), one ﬁnds that w(z) satisﬁes a
simple second-order ordinary diﬀerential equation, while v(r, θ) solves the two-dimensional
polar Helmholtz equation (11.51) on a disk. Applying a further separation to v(r, θ), as
in Chapter 11, produces fully separable solutions u(r, θ, z) = p(r) q(θ) w(z) as products of
Bessel functions of the cylindrical radius r, trigonometric functions of the polar angle θ,
and hyperbolic functions of z; see Exercise 12.2.40.
The most interesting case is that of spherical coordinates, which we proceed to analyze
in detail in the following subsection.
Remark : These are just three of the many coordinate systems in which the threedimensional Laplace equation separates. See [78, 79] for 37 additional exotic types, including ellipsoidal, toroidal, and parabolic spheroidal coordinates. The resulting separable
solutions are written in terms of new classes of special functions that solve interesting
second-order ordinary diﬀerential equations, all of Sturm–Liouville form (9.71).

Laplace’s Equation in a Ball
Suppose a solid ball (e.g., the Earth) is subject to a speciﬁed steady temperature distribution on its spherical boundary. Our task is to determine the equilibrium temperature
within the ball. We assume that the body is composed of an isotropic, uniform medium
and, to slightly simplify the analysis, choose units in which its radius equals 1.
To ﬁnd the equilibrium temperature within the ball, we must solve the Dirichlet boundary value problem
∂2u ∂2u ∂2u
+ 2 + 2 = 0,
∂x2
∂y
∂z
u(x, y, z) = h(x, y, z),

x2 + y 2 + z 2 < 1,
2

2

(12.14)

2

x + y + z = 1,

where h is prescribed on the bounding unit sphere. Problems in spherical geometries are
most naturally analyzed in spherical coordinates r, ϕ, θ. Our convention is to set
x = r sin ϕ cos θ,

y = r sin ϕ sin θ,

z = r cos ϕ,

(12.15)

where − π < θ ≤ π is the azimuthal angle or longitude, while 0 ≤ ϕ ≤ π is the zenith

angle or latitude on the sphere of radius r = x2 + y 2 + z 2 . In other words, ϕ measures

12.2 Separation of Variables for the Laplace Equation

509

z
(x, y, z)

r

y

ϕ
(x, y, 0)
θ
x
Spherical coordinates.

Figure 12.1.
T

the angle between the vector ( x, y, z ) and the positive z–axis, while θ measures the
angle between its projection ( x, y, 0 )T on the (x, y)–plane and the positive x–axis; see
Figure 12.1. On Earth, longitude θ is measured from the Greenwich prime meridian, while
latitude is measured from the equator, and so equals 12 π − ϕ (although the everyday units
are degrees, not radians), whereby ϕ is sometimes referred to as the “co-latitude”.
Warning: In many books, particularly those in physics, the roles of θ and ϕ are reversed , leading to much confusion when one is perusing the literature. We prefer the
mathematical convention, since the azimuthal angle θ coincides with the cylindrical angle
coordinate — as well as the polar coordinate on the (x, y)–plane — thus avoiding unnecessary confusion when going from one coordinate system to the other. You must be attentive
to the convention being used when consulting any reference!
In spherical coordinates, the Laplace equation for u(r, ϕ, θ) takes the form
Δu =

∂ 2 u 2 ∂u
∂2u
cos ϕ ∂u
1 ∂2u
1
+
+ 2
= 0.
+ 2
+ 2 2
2
2
∂r
r ∂r
r ∂ϕ
r sin ϕ ∂ϕ r sin ϕ ∂θ2

(12.16)

This important formula is the ﬁnal result of a fairly nasty chain rule computation, whose
details are left to the motivated reader. (Set aside lots of paper and keep an eraser handy!)
To construct separable solutions to the spherical coordinate form (12.16) of the Laplace
equation, we begin by separating oﬀ the radial part of the solution, setting
u(r, ϕ, θ) = v(r) w(ϕ, θ).

(12.17)

r2
Substituting this ansatz into (12.16), multiplying the resulting equation through by v w ,
and then placing all the terms involving r on one side yields


2
1
dv
1
2 d v
(12.18)
+ 2r
r
= − ΔS [ w ],
2
v
dr
dr
w
where
ΔS [ w ] =

1 ∂2w
∂ 2 w cos ϕ ∂w
+
+
.
∂ϕ2
sin ϕ ∂ϕ
sin2 ϕ ∂θ2

(12.19)

510

12 Partial Diﬀerential Equations in Space

The second-order diﬀerential operator ΔS , which involves only the angular components
of the full Laplacian operator Δ, is of particular signiﬁcance. It is known as the spherical Laplacian, and governs the equilibrium and dynamics of thin spherical shells — see
Example 12.15 below.
Returning to equation (12.18), our usual separation argument applies. The left-hand
side depends only on r, while the right-hand side depends only on the angles ϕ, θ. This can
occur only when both sides are equal to a common separation constant, which we denote by
μ. As a consequence, the radial component v(r) satisﬁes the ordinary diﬀerential equation
r2 v  + 2 r v  − μ v = 0,

(12.20)

which is of Euler type (11.89), and hence can be readily solved. However, let us put this
equation aside for the time being, and concentrate our eﬀorts on the more complicated
angular components.
The second equation in (12.18) assumes the form
ΔS [ w ] + μ w =

1 ∂2w
∂ 2 w cos ϕ ∂w
+
+
+ μ w = 0.
∂ϕ2
sin ϕ ∂ϕ
sin2 ϕ ∂θ2

(12.21)

This second-order partial diﬀerential equation can be regarded as the eigenvalue equation
for the spherical Laplacian operator ΔS and is known as the spherical Helmholtz equation.
To ﬁnd explicit solutions, we adopt a further separation of angular variables,
w(ϕ, θ) = p(ϕ) q(θ),

(12.22)

which we substitute into (12.21). Dividing the result by the product w = p q, multiplying
by sin2 ϕ, and then rearranging terms, we are led to the separated system


1
d2 p
dp
1 d2 q
sin2 ϕ 2 + cos ϕ sin ϕ
+ μ sin2 ϕ = −
= ν,
p
dϕ
dϕ
q dθ2
where, by our usual argument, ν is another separation constant. The spherical Helmholtz
equation thereby splits into a pair of ordinary diﬀerential equations
sin2 ϕ

d2 p
dp
+ cos ϕ sin ϕ
+ (μ sin2 ϕ − ν) p = 0,
2
dϕ
dϕ

d2 q
+ ν q = 0.
dθ2

The equation for q(θ) is easy to solve. As one circumnavigates the sphere, the azimuthal
angle θ increases from − π to π, so q(θ) must be a 2 π–periodic function. Thus, q(θ) solves
the well-studied periodic boundary value problem treated, for instance, in (4.109). Up to
a constant multiple, nonzero periodic solutions occur only when the separation constant
assumes one of the values ν = m2 , where m = 0, 1, 2, . . . is an integer, with
q(θ) = cos m θ

or

sin m θ,

m = 0, 1, 2, . . . .

(12.23)

2

Each positive ν = m > 0 admits two linearly independent 2 π–periodic solutions, while
when ν = 0, only the constant solutions are periodic.
The Legendre Equation and Ferrers Functions
With this information, we endeavor to solve the zenith diﬀerential equation
sin2 ϕ

d2 p
dp
+ (μ sin2 ϕ − m2 ) p = 0.
+ cos ϕ sin ϕ
2
dϕ
dϕ

(12.24)

12.2 Separation of Variables for the Laplace Equation

511

This is not so easy, and constructing analytic formulas for its solutions requires some
ingenuity. The motivation behind the following steps may not be so apparent; indeed,
they are the culmination of a long, detailed study of this important diﬀerential equation
by mathematicians over the last 200 years.
As an initial simpliﬁcation, let us get rid of the trigonometric functions, by invoking
the change of variables
t = cos ϕ,

with

p(ϕ) = P (cos ϕ) = P (t).

Since
0 ≤ ϕ ≤ π,
According to the chain rule,

we have

0≤

(12.25)


1 − t2 = sin ϕ ≤ 1.


dp
dP dt
dP
dP
=
= − sin ϕ
= − 1 − t2
,
dϕ
dt dϕ
dt
dt
 

2
d2 p
dP
d
dP
2 d P
2
−
=
(1
−
t
.
=
−
sin
ϕ
1
−
t
)
−t
dϕ2
dt
dt
dt2
dt
Substituting these expressions into (12.24), we conclude that P (t) must satisfy

d2 P
dP 
+ μ (1 − t2 ) − m2 P = 0.
− 2 t (1 − t2 )
(12.26)
2
dt
dt
Unfortunately, the resulting diﬀerential equation is still not elementary, but at least its
coeﬃcients are polynomials. It is known as the Legendre diﬀerential equation of order m,
having ﬁrst been employed by Adrien–Marie Legendre to study the gravitational attraction
of ellipsoid al bodies. In the cases of interest to us, the order parameter m is an integer,
while the separation constant μ plays the role of an eigenvalue.
Power series solutions to the Legendre equation can be constructed by the standard
techniques presented in Section 11.3. The most general solution is a new type of special
function, called a Legendre function, [86]. However, it turns out that the solutions we are
actually interested in can all be written in terms of elementary algebraic functions. First
of all, since t = cos ϕ, the solution only needs to be deﬁned on the interval −1 ≤ t ≤ 1,
the so-called cut locus. The endpoints of the cut locus, t = 1 and t = −1, correspond to
the sphere’s north pole, ϕ = 0, and south pole, ϕ = π, respectively. Both endpoints are
singular points for the Legendre equation, since the coeﬃcient (1 − t2 )2 of the leading-order
derivative vanishes when t = ± 1. In fact, both are regular singular points, as you are asked
to show in Exercise 12.2.11. Since ultimately we need the separable solution (12.17) to be a
well-deﬁned function of x, y, z (even at points where the spherical coordinates degenerate,
i.e., on the z–axis), we need p(ϕ) to be well deﬁned at ϕ = 0 and π, and this requires P (t)
to be bounded at the singular points:
(1 − t2 )2

| P (− 1) | < ∞,

| P (+ 1) | < ∞.

(12.27)

Let us begin our analysis with the Legendre equation of order m = 0
d2 P
dP
+ μ P = 0.
(12.28)
− 2t
dt2
dt
In this case, the eigenfunctions, i.e., solutions to the Legendre boundary value problem
(12.27–28), are the Legendre polynomials
(1 − t2 )

Pn (t) =

(−1)n dn
(1 − t2 )n ,
2n n ! dtn

(12.29)

512

12 Partial Diﬀerential Equations in Space

P0 (t)

P1 (t)

P2 (t)

P3 (t)

P4 (t)

P5 (t)

Legendre polynomials.

Figure 12.2.

with corresponding eigenvalue parameter μ = n (n + 1). (The initial factor is by common
convention, [86]; see (12.64) for the explicit formula.) The ﬁrst few are
P0 (t) = 1,

P2 (t) = 32 t2 − 12 ,

P1 (t) = t,

4
15 2
3
P4 (t) = 35
8 t − 4 t + 8,

P3 (t) = 52 t3 − 32 t,

5
35 3
15
P5 (t) = 63
8 t − 4 t + 8 t,

and are graphed in Figure 12.2.
Each Legendre polynomial clearly satisﬁes the boundary conditions (12.27). To verify
that they are indeed solutions to the diﬀerential equation (12.28), we set
Qn (t) = (1 − t2 )n .
By the chain rule, the derivative of Qn (t) is
Qn = −2 n t (1 − t2 )n−1 ,

and hence

(1 − t2 )Qn = −2 n t (1 − t2 )n = −2 n t Qn .

Diﬀerentiating the latter formula yields
(1 − t2 )Qn − 2 t Qn = −2 n t Qn − 2 n Qn ,

or

(1 − t2 )Qn = −2(n − 1)t Qn − 2 n Qn .

A simple induction proves that the k th order derivative Q(k)
n (t) =

dk Qn
satisﬁes
dtk

(1 − t2 ) Q(k+2)
= −2(n − k − 1) t Q(k+1)
− 2[ n + (n − 1) + · · · + (n − k) ] Q(k)
n
n
n
= −2(n − k − 1) t Q(k+1)
− (k + 1)(2n − k) Q(k)
n
n .

(12.30)

12.2 Separation of Variables for the Laplace Equation

513

In particular, when k = n, this reduces to
= 2 t Q(n+1)
− n (n + 1) Q(n)
(1 − t2 )Q(n+2)
n
n
n = 0,
and so Pn (t) = Q(n)
n (t) satisﬁes
(1 − t2 )Pn − 2 t Pn + n (n + 1) Pn = 0,
which is precisely the order 0 Legendre equation (12.28) with μ = n (n + 1). The Legendre
polynomial Pn is a constant multiple of Pn , and hence it too satisﬁes the order 0 Legendre
equation. According to Theorem 12.3 below, the Legendre polynomials form a complete
system of eigenfunctions for the order 0 Legendre boundary value problem.
When the order m > 0, the eigenfunctions of the Legendre boundary value problem
(12.26–27) are not always polynomials. They are known as the Ferrers functions, named
after the nineteenth-century British mathematician Norman Ferrers, or, more generally, as
associated Legendre functions. They have the explicit formula†
Pnm (t) = (1 − t2 )m/2

dm
P (t)
dtm n

(1 − t2 )m/2
= (−1)n
2n n !

dn+m
(1 − t2 )n ,
dtn+m

n = m, m + 1, . . . ,

(12.31)

which generalizes the formula (12.29) for the Legendre polynomials. The eigenvalue parameter for Pnm (t) is also μ = n (n + 1). In particular Pn0 (t) = Pn (t). Here is a list of the
ﬁrst few Ferrers functions, which, for completeness, includes Legendre polynomials:

P10 (t) = t,
P11 (t) = 1 − t2 ,
P00 (t) = 1,

P21 (t) = 3 t 1 − t2 ,
P22 (t) = 3 (1 − t2 ),
P20 (t) = − 12 + 32 t2 ,

2
P31 (t) = − 32 + 15
1 − t2 ,
P30 (t) = − 32 t + 52 t3 ,
2 t
P32 (t) = 15 t (1 − t2 ),

P33 (t) = 15 (1 − t2 )3/2 ,

2
35 4
P40 (t) = 38 − 15
4 t + 8 t ,
2
105 2
P42 (t) = − 15
2 + 2 t (1 − t ),

35 3
P41 (t) = − 15
2 t+ 2 t
P43 (t) = 105 t (1 − t2 )3/2 ,



(12.32)
1 − t2 ,
P44 (t) = 105 (1 − t2 )2 .

When m = 2 k ≤ n is an even integer, Pnm (t) is a polynomial function, while when m =
√
2 k + 1 ≤ n is odd, there is an extra factor of 1 − t2 . Keep in mind that the square root
is real and positive, since we are restricting our attention to the interval −1 ≤ t ≤ 1. If
m > n, formula (12.31) reduces to the zero function and so is not included in the ﬁnal
tally.
Warning: Even though half of the Ferrers functions are polynomials, only those with
m = 0, i.e., Pn (t) = Pn0 (t), are called Legendre polynomials.
†
Warning: Some authors include a (−1)m factor in the formula, resulting in the opposite sign
when m is odd. Another source of confusion is that many tables deﬁne the associated Legendre
functions using the alternative initial factor (t2 − 1)m/2 . But this is unsuitable, since we are solely
interested in values of t lying in the interval −1 ≤ t ≤ 1, and this convention would result in a
complex-valued function when m is odd. Following [ 86 ], we use the term “Ferrers function” to
refer to the restriction of the associated Legendre function to the cut locus −1 ≤ t ≤ 1.

514

12 Partial Diﬀerential Equations in Space

0 ≤ P11 (t) ≤ 1

−1.5 ≤ P21 (t) ≤ 1.5

0 ≤ P22 (t) ≤ 3

−1.5 ≤ P31 (t) ≤ 2.07

−5.77 ≤ P32 (t) ≤ 5.77

0 ≤ P33 (t) ≤ 15

−2.64 ≤ P41 (t) ≤ 2.64 −7.5 ≤ P42 (t) ≤ 9.64 −34.1 ≤ P43 (t) ≤ 34.1
Figure 12.3.

0 ≤ P44 (t) ≤ 105

Ferrers functions.

Figure 12.3 displays graphs of the Ferrers functions Pnm (t) for 1 ≤ m ≤ n ≤ 4.
Pay particular attention to the fact that, owing to the choice of normalization factor, the
graphs have very diﬀerent vertical scales, as indicated by their minimum and maximum
values (rounded to two decimal places) written below each — although one always has the
freedom to rescale the eigenfunctions as desired, e.g., so as to be orthonormal.
To show that the Ferrers functions Pnm (t) satisfy the Legendre diﬀerential equation
(12.26) of order m, we substitute k = m + n in (12.30):
(1 − t2 )

d2 Rnm
dt2

− 2 (m + 1) t

dRnm
dt

+ (m + n + 1) (n − m) Rnm = 0,

(12.33)

where
(t).
Rnm (t) = Q(m+n)
n
This is not the order m Legendre equation, but it can be converted into it by setting
Rnm (t) = (1 − t2 )− m/2 Snm (t).

12.2 Separation of Variables for the Laplace Equation

515

Diﬀerentiating, we obtain
dRnm

= (1 − t2 )− m/2

dSnm

− m t (1 − t2 )− m/2−1 Snm ,
dt
dt
d2 Snm
dSnm
d2 Rnm
2 − m/2
2 − m/2−1
=
(1
−
t
)
−
2
m
t
(1
−
t
)
dt2
dt2
dt


+ m + m(m + 1)t2 (1 − t2 )− m/2−2 Snm .
Therefore, after a little algebra, equation (12.33) takes the alternative form
(1 − t2 )− m/2+1

d2 Snm
dt2

dSnm
− 2 t (1 − t2 )− m/2
dt


+ n (n + 1) (1 − t2 ) − m2 (1 − t2 )− m/2−1 Snm = 0,

which, when multiplied by (1−t2 )m/2+1 , is precisely the order m Legendre equation (12.26)
with μ = n (n + 1). Thus,
Snm (t) = (1 − t2 )m/2 Rnm (t) = (1 − t2 )m/2

dn+m
(1 − t2 )n ,
dtn+m

which is a constant multiple of the Ferrers function Pnm (t), is a solution to the order m
Legendre equation. Moreover, we note that
Pnm (1) = Pnm (−1) = 0,

when

m > 0,

(12.34)

and we conclude that Pnm (t) is an eigenfunction for the order m Legendre boundary value
problem.
The following result states that the Ferrers functions provide a complete list of solutions to the Legendre boundary value problem (12.26–27).
Theorem 12.3. Let m ≥ 0 be a nonnegative integer. Then the order m Legendre
boundary value problem prescribed by (12.26–27) has eigenvalues μn = n (n + 1) for n =
0, 1, 2, . . ., and associated eigenfunctions Pnm (t), where m = 0, . . . , n. Moreover, the Ferrers
eigenfunctions form a complete orthogonal system relative to the L2 inner product on the
cut locus [ − 1, 1 ].
Returning to the zenith variable ϕ via (12.25), Theorem 12.3 implies that our original
boundary value problem
sin2 ϕ

d2 p
dp
+ (μ sin2 ϕ − m2 ) p = 0,
+ cos ϕ sin ϕ
dϕ2
dϕ

| p(0) |, | p(π) | < ∞, (12.35)

has its eigenvalues and eigenfunctions expressed in terms of the Ferrers functions:
μn = n (n + 1),

m
pm
n (ϕ) = Pn (cos ϕ),

for

0 ≤ m ≤ n.

(12.36)

√
Since Pnm (t) is either a polynomial or a polynomial multiplied by a power of 1 − t2 ,
the eigenfunction pm
n (ϕ) is a trigonometric polynomial of degree n, which we call a trigono-

516

12 Partial Diﬀerential Equations in Space

p00 (ϕ) ≡ 1

−1 ≤ p01 (ϕ) ≤ 1

0 ≤ p11 (ϕ) ≤ 1

−.5 ≤ p02 (ϕ) ≤ 1

−1.5 ≤ p12 (ϕ) ≤ 1.5

0 ≤ p22 (ϕ) ≤ 3

−1 ≤ p03 (ϕ) ≤ 1

−1.5 ≤ p13 (ϕ) ≤ 2.07

−5.77 ≤ p23 (ϕ) ≤ 5.77

0 ≤ p33 (ϕ) ≤ 15

−.43 ≤ p04 (ϕ) ≤ 1

−2.64 ≤ p14 (ϕ) ≤ 2.64

−7.5 ≤ p24 (ϕ) ≤ 9.64

−34.1 ≤ p34 (ϕ) ≤ 34.1

0 ≤ p44 (ϕ) ≤ 105

Figure 12.4.

Trigonometric Ferrers functions.

metric Ferrers function. Here are the ﬁrst few, written in Fourier form, as in (3.38):
p00 (ϕ) = 1,

p01 (ϕ) = cos ϕ,

p11 (ϕ) = sin ϕ,

p02 (ϕ) = 14 + 34 cos 2 ϕ,

p12 (ϕ) = 32 sin 2 ϕ,

p22 (ϕ) = 32 − 32 cos 2 ϕ,

p03 (ϕ) = 38 cos ϕ + 58 cos 3 ϕ,

p13 (ϕ) = 38 sin ϕ + 15
8 sin 3 ϕ,

15
p23 (ϕ) = 15
4 cos ϕ − 4 cos 3 ϕ,

15
p33 (ϕ) = 45
4 sin ϕ − 4 sin 3 ϕ,

9
5
p04 (ϕ) = 64
+ 16
cos 2 ϕ + 35
64 cos 4 ϕ,
2
45
15
p4 (ϕ) = 16 + 4 cos 2 ϕ − 105
16 cos 4 ϕ,
4
315
105
p4 (ϕ) = 8 − 2 cos 2 ϕ + 105
8 cos 4 ϕ.

p14 (ϕ) = 58 sin 2 ϕ + 35
16 sin 4 ϕ,
3
105
p4 (ϕ) = 4 sin 2 ϕ − 105
8 sin 4 ϕ,

(12.37)

It is also instructive to plot the eigenfunctions in terms of the zenith angle ϕ; see Figure 12.4.
As in Figure 12.3, the vertical scales are not the same, as indicated by the listed minimum
and maximum values.

12.2 Separation of Variables for the Laplace Equation

517

Spherical Harmonics
At this stage, we have determined both angular components of our separable solutions
(12.22). Multiplying the two parts together results in the spherical angle functions
Ynm (ϕ, θ) = pm
n (ϕ) cos m θ,
m

Y (ϕ, θ) = pm (ϕ) sin m θ,
n

n = 0, 1, 2, . . . ,

(12.38)

m = 0, 1, . . . , n,

n

known as spherical harmonics. They satisfy the spherical Helmholtz equation
ΔS Ynm + n (n + 1) Ynm = 0 = ΔS Ynm + n (n + 1) Ynm ,

(12.39)

and so are eigenfunctions for the spherical Laplacian operator, (12.19), with associated
eigenvalues μn = n (n + 1) for n = 0, 1, 2, . . . . The nth eigenvalue μn admits a (2 n + 1)–
dimensional eigenspace, spanned by the spherical harmonics
Yn0 (ϕ, θ), Yn1 (ϕ, θ), . . . , Ynn (ϕ, θ), Yn1 (ϕ, θ), . . . , Ynn (ϕ, θ).
(The omitted function Yn0 (ϕ, θ) ≡ 0 is trivial, and so does not contribute.) In Figure 12.5
we plot the ﬁrst few spherical harmonic surfaces r = Ynm (ϕ, θ). In these graphs, in view of
the spherical coordinate formulae (12.15), points with a negative r coordinate appear on
the opposite side of the origin from their positive r counterparts. Incidentally, the graphs of
the other spherical harmonic surfaces r = Ynm (ϕ, θ), when m > 0, are obtained by rotation
around the z–axis by 90◦ ; see Exercise 12.2.20. On the other hand, the graphs of Yn0 are
cylindrically symmetric (why?), and hence unaﬀected by such a rotation.
Self-adjointness of the spherical Laplacian, as per Exercise 12.2.21, implies that the
spherical harmonics are orthogonal with respect to the L2 inner product
 π  π

f g dS =
f (ϕ, θ) g(ϕ, θ) sin ϕ dϕ dθ
(12.40)
f ,g =
S1

−π

0

given by integrating the product of the functions with respect to the surface area element
dS = sin ϕ dϕ dθ on the unit sphere S1 = {  x  = 1 }. More correctly, self-adjointness only
guarantees orthogonality of the harmonics corresponding to distinct eigenvalues: μn = μl .
However, the orthogonality relations

m
k
 Yn , Yl  =
Ynm Ylk dS = 0,
for
(m, n) = (k, l),
S1

 Ynm , Ylk  =
Ynm Ylk dS = 0,
for all
(m, n), (k, l),
(12.41)
S1

Ynm Ylk dS = 0,
 Ynm , Ylk  =
for
(m, n) = (k, l),
S1

do, in fact, hold in full generality; Exercise 12.2.22 asks you to supply the details. Moreover,
their norms can be explicitly computed:
 Yn0 2 =

4π
,
2n + 1

 Ynm 2 =  Ynm 2 =

2 π(n + m) !
,
(2 n + 1)(n − m) !

m = 1, . . . , n.
(12.42)

Proofs of the latter formulae are outlined in Exercise 12.2.24.
With some further work, it can be shown that the spherical harmonics form a complete
orthogonal system of functions on the unit sphere. This means that any reasonable (e.g.,

518

12 Partial Diﬀerential Equations in Space

Y00 (ϕ, θ)

Y10 (ϕ, θ)

Y11 (ϕ, θ)

Y20 (ϕ, θ)

Y21 (ϕ, θ)

Y22 (ϕ, θ)

Y30 (ϕ, θ)

Y31 (ϕ, θ)

Y32 (ϕ, θ)

Y33 (ϕ, θ)

Y40 (ϕ, θ)

Y41 (ϕ, θ)

Y42 (ϕ, θ)

Y43 (ϕ, θ)

Y44 (ϕ, θ)

Figure 12.5.

Spherical harmonics.

12.2 Separation of Variables for the Laplace Equation

519

piecewise C1 or even L2 ) function h: S1 → R, can be expanded into a convergent spherical
harmonic series


∞
n
:
;


c0,0
c0,n 0
m
m
h(ϕ, θ) =
cm,n Yn (ϕ, θ) + 
+
Y (ϕ) +
cm,n Yn (ϕ, θ)
. (12.43)
2
2 n
n=1
m=1
Applying the orthogonality relations (12.41), we ﬁnd that the spherical harmonic coeﬃcients are given by the inner products
c0,n =

2  h , Yn0 
,
 Yn0 2

cm,n =

 h , Ynm 
,
 Ynm 2


cm,n =

 h , Ynm 
,
 Y m 2
n

0 ≤ n,
1 ≤ m ≤ n,

or, explicitly, using (12.40) and the formulae (12.42) for the norms,
 π  π
(2 n + 1)(n − m) !
cm,n =
h(ϕ, θ) pm
n (ϕ) cos m θ sin ϕ dϕ dθ,
2 π (n + m) !
−π 0
 π  π
(2 n + 1)(n − m) !
h(ϕ, θ) pm

cm,n =
n (ϕ) sin m θ sin ϕ dϕ dθ.
2 π (n + m) !
−π 0

(12.44)

As with an ordinary Fourier series, the extra 12 was appended to the c0,n terms in (12.43)
so that equations (12.44) remain valid for all values of m, n. In particular, the constant
term in the spherical harmonic series is the mean of the function h over the unit sphere:

 π  π
c0,0
1
1
h dS =
h(ϕ, θ) sin ϕ dϕ dθ.
(12.45)
=
2
4π
4 π −π 0
S1
Remark : Establishing uniform convergence of a spherical harmonic series (12.43) is
more challenging than in the Fourier series case, because, unlike the trigonometric functions, the orthonormal spherical harmonics are not uniformly bounded. A recent survey of
what is known in this regard can be found in [10].
Remark : An alternative approach is to replace the real trigonometric functions by
complex exponentials, and work with the complex spherical harmonics †
i mθ
Ynm (ϕ, θ) = Ynm (ϕ, θ) + i Ynm (ϕ, θ) = pm
,
n (ϕ) e

n = 0, 1, 2, . . . ,
m = − n, − n + 1, . . . , n.

(12.46)

The associated orthogonality and expansion formulas are relegated to the exercises.

Harmonic Polynomials
To complete our solution to the Laplace equation on the solid ball, we still need to solve the
ordinary diﬀerential equation (12.20) for the radial component v(r). In view of our analysis
of the spherical Helmholtz equation, the original separation constant is μ = n (n + 1) for
some nonnegative integer n ≥ 0, and so the radial equation takes the form
r2 v  + 2 r v  − n (n + 1) v = 0.

(12.47)

Here we use the convention that Ynm = Yn− m , Ynm = − Yn− m , and Yn0 ≡ 0, which is
compatible with their deﬁning formulas (12.38).
†

520

12 Partial Diﬀerential Equations in Space

To solve this Euler equation, we substitute the power ansatz v(r) = rα , and ﬁnd that the
exponent α must satisfy the quadratic indicial equation
α2 + α − n (n + 1) = 0,

and hence

α=n

or

α = − (n + 1).

Therefore, the two linearly independent solutions are
v1 (r) = rn

and

v2 (r) = r− n−1 .

(12.48)

Since we are currently interested only in solutions that remain bounded at r = 0 — the
center of the ball — we will retain just the ﬁrst solution v(r) = rn for our subsequent
analysis.
At this stage, we have solved all three ordinary diﬀerential equations for the separable solutions. We combine (12.23, 38, 48) to produce the following spherically separable
solutions to the Laplace equation:
Hnm = rn Ynm (ϕ, θ) = rn pm
n (ϕ) cos m θ,
m
n m
n m

H = r Y (ϕ, θ) = r p (ϕ) sin m θ,
n

n

n

n = 0, 1, 2, . . . ,
m = 0, 1, . . . , n.

(12.49)

Although apparently complicated, these solutions are, perhaps surprisingly, elementary
polynomial functions of the rectangular coordinates x, y, z, and hence are harmonic polynomials. The ﬁrst few are
H00 = 1,

H10 = z,

H20 = z 2 − 12 x2 − 12 y 2 ,

H30 = z 3 − 32 x2 z − 32 y 2 z,

H11 = x,
 1 = y,
H

H21 = 3 x z,
 1 = 3 y z,
H

H31 = 6 x z 2 − 32 x3 − 32 x y 2 ,
 1 = 6 y z 2 − 3 x2 y − 3 y 3 ,
H

1

The polynomials

2
H22 = 3 x2 − 3 y 2 ,
 2 = 6 x y,
H
2

3
2
2
H32 = 15 x2 z − 15 y 2 z,
 2 = 30 x y z,
H
3
H33 = 15 x3 − 45 x y 2,
 3 = 45 x2 y − 15 y 3.
H
3

(12.50)

 1, . . . , H
n
Hn0 , Hn1 , . . . , Hnn , H
n
n

are homogeneous of degree n. Orthogonality of the spherical harmonics implies that they
form a basis for the vector space comprised of all homogeneous harmonic polynomials of
degree n, which hence has dimension 2 n + 1.
The harmonic polynomials (12.49) form a complete system, and therefore the general solution to the Laplace equation inside the unit ball can be written as a harmonic
polynomial series:


∞
n




c0,n 0
c0,0
m
m

,
u(x, y, z) =
+
Hn (x, y, z) +
cm,n Hn (x, y, z)
cm,n Hn (x, y, z) + 
2
2
n=1
m=1
(12.51)
or equivalently, in spherical coordinates,


∞
n




c0,0
c0,n n 0
u(r, ϕ, θ) =
+
r Yn (ϕ) +
cm,n rn Ynm (ϕ, θ) + 
cm,n rn Ynm (ϕ, θ)
.
2
2
n=1
m=1
(12.52)

12.2 Separation of Variables for the Laplace Equation

521

The coeﬃcients cm,n , 
cm,n are uniquely prescribed by the boundary conditions. Indeed,
substituting (12.52) into the Dirichlet boundary conditions on the unit sphere r = 1 yields
∞

c0,0
u(1, ϕ, θ) =
+
2
n=1



n



c0,n 0
Yn (ϕ) +
cm,n Ynm (ϕ, θ)
cm,n Ynm (ϕ, θ) + 
2
m=1


= h(ϕ, θ).

(12.53)
Thus, the coeﬃcients cm,n , 
cm,n are given by the inner product formulae (12.44). If the
terms in the resulting series are uniformly bounded — which occurs for all piecewise continuous functions h, as well as all L2 functions and many generalized functions such as the
delta function — then the harmonic polynomial series (12.52) converges everywhere, and,
in fact, uniformly on any smaller ball  x  = r ≤ r0 < 1.
Averaging, the Maximum Principle, and Analyticity
In rectangular coordinates, the nth summand of the series (12.51) is a homogeneous polynomial of degree n. Therefore, repeating the argument used in the two-dimensional situation
(4.115), we conclude that the harmonic polynomial series is, in fact, a power series, and
hence provides the Taylor expansion for the harmonic function u(x, y, z) at the origin! In
particular, its convergence for all r < 1 implies that the harmonic function u(x, y, z) is
analytic at x = y = z = 0.
The constant term in such a Taylor series can be identiﬁed with the value of the
function at the origin: u(0, 0, 0) = 12 c0,0 . On the other hand, since u = h on S1 = ∂Ω, the
coeﬃcient formula (12.45) tells us that
u(0, 0, 0) =



c0,0
1
=
2
4π

u dS.

(12.54)

S1

Therefore, we have established the three-dimensional counterpart of Theorem 4.8: the value
of a harmonic function u at the center of the sphere is equal to the average of its values
∂ i+j+k u
(0, 0, 0) appears, up
on the sphere’s surface. Moreover, each partial derivative
∂xi ∂y j ∂z k
to a factor, as the coeﬃcient of the terms xi y j z k in the Taylor series, and hence can be
expressed as a certain linear combination of the coeﬃcients cm,n , 
cm,n , which are in turn
given by the integral formulae (12.44).
More generally, the value of a harmonic function at the center of any ball contained
within its domain equals the average of its values over the bounding sphere. As with the
planar version in Theorem 4.8, it is preferable to give a direct proof that doesn’t rely on
the series expansion (12.51).
Theorem 12.4. If u(x) is a harmonic function deﬁned on a domain Ω ⊂ R 3 , then u
is analytic inside Ω. Moreover, its value at any x0 ∈ Ω is obtained by averaging its values
on any sphere Sa = {  x − x0  = a } centered at x0 :
1
u(x0 ) =
4 πa2


u dS,

(12.55)

Sa

provided the enclosed ball lies within its domain of analyticity: Ba = {  x − x0  ≤ a } ⊂ Ω.

522

12 Partial Diﬀerential Equations in Space

Proof : Let us denote the average of u over the sphere of radius a by

1
u dS
g(a) =
4 πa2
Sa
 π  π
1
u(x0 + a sin ϕ cos θ, y0 + a sin ϕ sin θ, z0 + a cos ϕ) sin ϕ dϕ dθ.
=
4 π −π 0
By continuity, as the radius a → 0, the average of u on the sphere Sa tends to its value at
the center: g(a) → u(x0 ).
On the other hand, since u ∈ C2 and harmonic in Ba ⊂ Ω, the derivative

 π  π
∂u
∂u
1
∂u

+ sin ϕ sin θ
+ cos ϕ
sin ϕ dϕ dθ
g (a) =
sin ϕ cos θ
4 π −π 0
∂x
∂y
∂z


1
1
∂u
=
dS
=
Δu dx dy dz = 0,
4 πa2
4 πa2
Sa ∂n
Ba
where n denotes the unit outwards normal to Sa = ∂Ba , and we used the divergence
identity in Exercise 12.1.11(a). We conclude that g(a) is constant, and hence g(a) = u(x0 )
for any a > 0 provided Ba ⊂ Ω.
Q.E.D.
Arguing as in the planar case of Theorem 4.9, we readily establish the corresponding
Strong Maximum Principle for harmonic functions of three variables.
Theorem 12.5. A nonconstant harmonic function cannot have a local maximum or
minimum at any interior point of its domain of deﬁnition. Moreover, its global maximum
or minimum (if any) is located on the boundary of the domain.
For instance, the Maximum Principle implies that the maximum and minimum temperatures in a solid body in thermal equilibrium are to be found only on its boundary. In
physical terms, since heat energy must ﬂow away from an internal maximum and towards
an internal minimum, any local temperature extremum inside the body would preclude it
from being in thermal equilibrium. The Maximum Principle immediately implies a Uniqueness Theorem for both the Laplace and Poisson equations, cf. Theorem 4.10, which in turn
establishes the solution formula (12.51) and hence analyticity of every harmonic function.
Example 12.6. In this example, we shall determine the electrostatic potential inside
a hollow sphere when the upper and lower hemispheres are held at diﬀerent constant
potentials. This device is called a spherical capacitor and is realized experimentally by
separating the two charged conducting hemispherical shells by a thin insulating ring at
the equator. A straightforward scaling argument allows us to choose our units so that the
sphere has unit radius, while the potential is set equal to 1 on the upper hemisphere and
equal to 0, i.e., grounded, on the lower hemisphere. The resulting electrostatic potential
satisﬁes the Laplace equation
Δu = 0

inside a solid ball

 x  < 1,

and is subject to Dirichlet boundary conditions

1,
z > 0,
u(x, y, z) = h(x, y, z) ≡
on the unit sphere
0,
z < 0,

(12.56)

 x  = 1.

(12.57)

The solution will be prescribed by a harmonic polynomial series (12.51) whose coeﬃcients are ﬁxed by the boundary values (12.57). Before tackling the required computation,

12.2 Separation of Variables for the Laplace Equation

523

let us ﬁrst note that since the boundary data does not depend upon the azimuthal angle
θ, the solution u = u(r, ϕ) will also be independent of θ. Therefore, we need only consider
the θ-independent spherical harmonic polynomials (12.38), which are those with m = 0.
Thus,
∞
∞
1 
1 
0
c H (x, y, z) =
c rn Pn (cos ϕ),
(12.58)
u(x, y, z) =
2 n=0 n n
2 n=0 n
where we abbreviate cn = c0,n . The boundary conditions (12.57) require

∞
1,
0 ≤ ϕ < 12 π,
1 
cn Pn (cos ϕ) = h(ϕ) =
u|r=1 =
1
2 n=0
0,
2 π < ϕ ≤ π.
The coeﬃcients are given by (12.44), which, in the case m = 0, reduce to
cn =

2n + 1
2π



 π/2
h Yn0 dS = (2 n + 1)

S1

 1
Pn (cos ϕ) sin ϕ dϕ = (2 n + 1)

0

Pn (t) dt,
0

(12.59)
since h = 0 when 12 π < ϕ ≤ π. The ﬁrst few are c0 = 1, c1 = 32 , c2 = 0, c3 = − 78 , c4 = 0.
Therefore, the solution has the explicit Taylor expansion
21 3
35 3
u(x, y, z) = 12 + 34 r cos ϕ − 128
r cos ϕ − 128
r cos 3 ϕ + · · ·
2
2
7 3
= 12 + 34 z + 21
32 (x + y ) z − 16 z + · · · .

(12.60)

Note in particular that the value u(0, 0, 0) = 12 at the center of the sphere is the average
of its boundary values, in accordance with Theorem 12.4. The solution depends only on
the cylindrical coordinates r, z, which is a consequence of the invariance of the Laplace
equation under general rotations, coupled with the invariance of the boundary data under
rotations around the z–axis.
Remark : The same solution u(x, y, z) describes the thermal equilibrium in a solid
sphere whose upper hemisphere is held at temperature 1◦ and lower hemisphere at 0◦ .
Example 12.7. A closely related problem is to determine the electrostatic potential
outside a spherical capacitor. As in the preceding example, we take our capacitor of radius
1, with electrostatic charge of 1 on the upper hemisphere and 0 on the lower hemisphere.
Here, we need to solve the Laplace equation Δu = 0 in the unbounded domain Ω =
{  x  > 1 } — the exterior of the unit sphere — subject to the same Dirichlet boundary
conditions (12.57). We anticipate that the potential will be vanishingly small at large
distances away from the capacitor: r =  x   1. Therefore, the harmonic polynomial
solutions (12.49) will not help us solve this problem, since (except for the constant case)
they become unboundedly large far away from the origin.
However, revisiting our original separation of variables argument will produce a different class of solutions having the desired decay properties. When we solved the radial
equation (12.47), we discarded the solution v2 (r) = r− n−1 because it had a singularity at
the origin. In the present situation, the behavior of the function at r = 0 is irrelevant; our
requirement is that the solution decay as r → ∞, and v2 (r) has this property. Therefore,
we will utilize the complementary harmonic functions
Knm (x, y, z) = r− 2 n−1 Hnm (x, y, z) = r− n−1 Ynm (ϕ, θ) = r− n−1 pm
n (ϕ) cos m θ,
m
−
2
n−1
m
−
n−1
m
−
n−1
 (x, y, z) = r
 (x, y, z) = r
K
H
Y (ϕ, θ) = r
pm (ϕ) sin m θ,
n

n

n

n

(12.61)

524

12 Partial Diﬀerential Equations in Space

for solving such exterior problems. For the capacitor problem, we need only those that are
independent of θ, whereby m = 0. We write the resulting solution as a series
∞
∞
1 
1 
cn Kn0 (x, y, z) =
c r− n−1 Pn (cos ϕ).
(12.62)
u(x, y, z) =
2 n=0
2 n=0 n
The boundary conditions
∞

1 
c P (cos ϕ) = h(ϕ) ≡
u|r=1 =
2 n=0 n n



1,

0 ≤ ϕ < 12 π,

0,

1
2 π < ϕ ≤ π,

are identical to those in the previous example. Therefore, the coeﬃcients are given by
(12.59), leading to the series expansion
21 cos ϕ + 35 cos 3 ϕ
3 cos ϕ
1
−
+ ···
(12.63)
+
2
2r
4r
128 r4
3z
21 (x2 + y 2 ) z − 14 z 3
1
+
+
+ ··· .
= 
4 (x2 + y 2 + z 2 )3/2
32 (x2 + y 2 + z 2 )7/2
2 x2 + y 2 + z 2

u(x, y, z) =

Observe that the higher-order terms become negligible at large distances, and hence the
potential is asymptotic to that associated with a point charge concentrated at the origin
of magnitude 12 , which is the average of the boundary potential over the sphere. This is
indicative of a general fact, to be explored in Exercise 12.2.32.

Exercises
12.2.1. A solid ball of radius R has its upper hemispherical surface held at temperature T1 and
its lower hemispherical surface held at temperature T0 . Find the resulting equilibrium
temperature.
12.2.2. A solid ball has its top hemispherical surface insulated and its bottom hemispherical
surface held at a ﬁxed temperature of 10◦ . Find its equilibrium temperature.
12.2.3. Find the potential inside a spherical capacitor of radius R when the upper hemisphere
is at potential α and the lower is at β.
12.2.4. Find the potential u(x, y, z) inside a unit spherical capacitor that has the indicated
boundary values on the unit sphere x2 + y 2 + z 2 = 1: (a) x, (b) x2 + y 2 , (c) x3 .
Hint: The potential is a polynomial.
12.2.5. Each point on the spherical boundary of a solid ball of radius 1 has temperature equal
to its zenith angle ϕ. (i ) Find the value of the equilibrium temperature at the center of the
ball. (ii ) Find the Taylor polynomial of degree 3, based at the origin, for the equilibrium
temperature distribution.
12.2.6. Solve Exercise 12.2.5 when the boundary temperature equals (a) cos ϕ, (b) cos θ, (c) θ.
12.2.7. A solid spherical container of radius 3 cm contains a hollow spherical cavity of radius
1 cm in its center. The inner cavity is ﬁlled with boiling water at 100◦ , while the entire
container is immersed in an ice water bath at 0◦ . Assume that the container is in thermal
equilibrium. True or false: The temperature at a point half-way between the container’s
inner and outer boundaries is 50◦ . If true, explain. If false, what is the temperature at such
a point?
12.2.8. Find the electrostatic potential between two concentric spherical metal shells of
respective radii 1 and 1.2, given that the inner shell is grounded, while the outer shell has
potential equal to 1.

12.2 Separation of Variables for the Laplace Equation

525

♦ 12.2.9. Use the chain rule to establish the formula (12.16) for the Laplacian in spherical
coordinates.
♦ 12.2.10. (a) Prove that t = ±1 are both regular singular points for the order 0 Legendre differential equation (12.28). (b) Prove that the Legendre eigenvalue problem (12.27–28) is
deﬁned by a self-adjoint operator with respect to the L2 inner product on the cut locus
[ − 1, 1 ]. (c) Discuss the orthogonality of the Legendre polynomials.
♦ 12.2.11. Solve Exercise 12.2.10 for the Legendre eigenvalue problem (12.26–27) of order m
along with the relevant Ferrers eigenfunctions.
♦ 12.2.12. Suppose m > 0. (a) Find the Green’s function for the boundary value problem
(1 − t2 )

d2 P
dP
m2
− 2t
P = f (t),
−
2
dt
dt
1 − t2

| P (−1) |, | P (1) | < ∞.


m



m

1+t 2
1−t 2
and
.
1−t
1+t
(b) Use part (a) to prove completeness of the Ferrers functions of order m > 0 on [ − 1, 1 ].
(c) Explain why there is no Green’s function in the order m = 0 case.
Remark : When m = 0, one can use the trick of Example 9.49 to prove completeness.
Although the Green’s function for the modiﬁed operator does not have an explicit elementary formula, one can prove that it has logarithmic singularities at the endpoints, and hence
ﬁnite double L2 norm. See [ 120; §43] for details.

Hint: The homogeneous diﬀerential equation has solutions

12.2.13. What happens when n < m in formula (12.31)?
♦ 12.2.14. Prove that the Legendre polynomial (12.29) has the explicit formula
(−1)m

Pn (t) =
0≤2 m≤n

(2 n − 2 m)!
tn−2 m .
2n (n − m) ! m ! (n − 2 m) !

(12.64)

♦ 12.2.15. Prove the following recurrence relation for the Ferrers functions:
Pnm+1 (t) =



1 − t2

dPnm
mt
Pnm (t).
+√
dt
1 − t2

(12.65)

♥ 12.2.16. In this exercise, we determine the L2 norms of the Ferrers functions. (a) First, prove
1
22 n+1 (n ! )2
(1 − t2 )n dt =
that
. Hint: Set t = cos θ and then integrate by parts
−1
(2 n + 1) !
2
. Hint: Integrate by parts repeatedly and then
repeatedly. (b) Prove that  Pn 2 =
2n + 1
use part (a). (c) Prove that  Pnm+1 2 = (n − m)(n + m + 1)  Pnm 2 . Hint: Use (12.65)
(n + m) !
2
and an integration by parts. (d) Finally, prove that  Pnm 2 =
.
2 n + 1 (n − m) !
12.2.17. (a) Prove that Pnm (t) is an even or odd function according to whether m + n is an even
or odd integer. (b) Prove that its Fourier form, pm
n (ϕ), depends only on cos n ϕ, cos(n − 2)ϕ,
cos(n − 4)ϕ, . . . if m is even, and only on sin n ϕ, sin(n − 2)ϕ, sin(n − 4)ϕ, . . . if m is odd.
12.2.18. Let m be ﬁxed. Are the functions pm
n (ϕ) for n = 0, 1, 2, . . . mutually orthogonal with
2
respect to the standard L inner product on [ 0, π ]? If not, is there an inner product that
makes them orthogonal functions?
12.2.19. Prove that the surfaces deﬁned by the ﬁrst three spherical harmonics Y00 , Y10 , and Y11 ,
as in Figure 12.5, are all spheres. Find their centers and radii.
♦ 12.2.20. Explain why the surface deﬁned by r = Ynm (ϕ, θ) is obtained by rotating that deﬁned
by r = Ynm (ϕ, θ) around the z–axis by 90◦ .

526

12 Partial Diﬀerential Equations in Space

♦ 12.2.21. Prove directly that the spherical Laplacian ΔS is a self-adjoint linear operator with
respect to the inner product (12.40).
♦ 12.2.22. (a) In view of Exercise 12.2.21, which orthogonality relations in (12.41) follow from
their status as eigenfunctions of the spherical Laplacian?
(b) Prove the general orthogonality formulae by direct computation.
♦ 12.2.23. State and prove the orthogonality of the complex spherical harmonics (12.46). Then
establish the following formula for their norms:
n = 0, 1, 2, . . . ,
4 π(n + m) !
| Ynm |2 dS =
 Ynm 2 =
(12.66)
m = − n, − n + 1, . . . , n.
S1
(2 n + 1)(n − m) !
♦ 12.2.24. Prove the formulae (12.42) for the norms of the spherical harmonics.
Hint: Use Exercise 12.2.16.
&1 .
♦ 12.2.25. Justify the formulas in (12.50) for (a) H10 , (b) H20 , (c) H
2

12.2.26. Find formulas for the following harmonic polynomials (i ) in spherical coordinates;
&4 .
(ii ) in rectangular coordinates: (a) H40 , (b) H44 , (c) H
4
12.2.27. Explain why every polynomial solution of the Laplace equation is a linear combination
of the harmonic polynomials (12.49). Hint: Look at its Taylor series.
12.2.28. (a) Prove that if u(x, y, z) is any harmonic polynomial, then so are u(y, x, z), u(z, x, y),
and all other functions obtained by permuting the variables x, y, z. (b) Discuss the eﬀect of
such permutations on the basis harmonic polynomials Hnm (x, y, z) appearing in (12.50).
12.2.29. Find the formulas in rectangular coordinates for the following complementary har&1 .
monic functions: (a) K00 , (b) K11 , (c) K20 , (d) K
2
♦ 12.2.30. Let u(x, y, z) be a harmonic function deﬁned on the unit ball r ≤ 1. Prove that its
gradient at the center, ∇u(0), equals the average of the vector ﬁeld v(x) = x u(x) over the
unit sphere r = 1.
♦ 12.2.31. (a) Suppose u(x, y, z) is a solution to the Laplace equation. Prove that the function
U (x, y, z) = r−1 u(x/r2 , y/r2 , z/r2 ) obtained by inversion is also a solution. (b) Explain
how inversion can be used to solve boundary value problems on the exterior of a sphere.
(c) Use inversion to relate the solutions to Examples 12.6 and 12.7.
♦ 12.2.32. Suppose u(r, ϕ, θ) is the potential exterior to a spherical capacitor of unit radius.
(a) Prove that lim r u(r, ϕ, θ) equals the average value of u on the sphere.
r→∞

(b) Use Exercise 12.2.31 to deduce this result as a consequence of Theorem 12.4.
12.2.33. (a) Write out, using spherical coordinates, formulas for the L2 inner product and norm
for scalar ﬁelds f (r, ϕ, θ) and g(r, ϕ, θ) on a solid ball of unit radius centered at the origin.
(b) Let f (x, y, z) = z and g(x, y, z) = x2 + y 2 . Find  f ,  g  and  f , g .
(c) Verify the Cauchy–Schwarz and triangle inequalities for these two functions.
♦ 12.2.34. Use separation of variables to construct a Fourier series solution to the Laplace
equation on a rectangular box, B = { 0 < x < a, 0 < y < b, 0 < z < c }, subject to the

h(x, y), z = 0, 0 < x < a, 0 < y < b,
Dirichlet boundary conditions u(x, y, z) =
0,
at all other points in ∂B.
12.2.35. Find the equilibrium temperature distribution inside a unit cube that has 100◦ temperature on its top face, 0◦ on its bottom face, while all four side faces are insulated.
12.2.36. Solve Exercise 12.2.35 when the top face of the cube has temperature
u(x, y, 1) = cos π x cos π y.
♣ 12.2.37. A solid unit cube is in thermal equilibrium when subject to 100◦ temperature on its
top face and 0◦ on all other faces. True or false: The temperature at the center equals the
average temperature over the surface of the cube.

12.3 Green’s Functions for the Poisson Equation

527

12.2.38. Solve the boundary value problem
∂2u
∂2u
∂2u
−
−
−
+ u = cos x cos y,
0 < x, y, z < π,
∂x2
∂y 2
∂z 2
∂u
∂u
∂u
∂u
∂u
u(x, y, 0) = 1,
(x, y, π) =
(x, 0, z) =
(x, π, z) =
(0, y, z) =
(π, y, z) = 0.
∂z
∂y
∂y
∂z
∂x
12.2.39. Let C be the cylinder of height 1 and diameter 1 that sits on the (x, y)–plane centered
on the z–axis. (a) Write out, in cylindrical coordinates, the explicit formula for the L2
inner product and norm on C.
(b) Let f (x, y, z) = z and g(x, y, z) = x2 + y 2 . Find  f ,  g  and  f , g .
(c) Verify the Cauchy–Schwarz and triangle inequalities for these two functions.
♦ 12.2.40. (a) Write out the Laplace equation in cylindrical coordinates.
(b) Use separation of variables to construct a series solution to the Laplace equation on the
cylinder C = { x2 + y 2 < 1, 0 < z < 1 }, subject to the Dirichlet boundary conditions

h(x, y), z = 0, x2 + y 2 < 1,
u(x, y, z) =
0,
at all other points in ∂C.
12.2.41. A cylinder of radius 1 and height 2 has 100◦ temperature on its top face, 0◦ on its
bottom face, while its curved side is fully insulated. Find its equilibrium temperature
distribution.
12.2.42. Solve Exercise 12.2.41 if the curved sides are kept at 0◦ instead.

12.3 Green’s Functions for the Poisson Equation
We now turn to the inhomogeneous form of the three-dimensional Laplace equation:
the Poisson equation
− Δu = f,
(12.67)
on a solid domain Ω ⊂ R 3 . In order to uniquely specify the solution, we must impose
appropriate boundary conditions: Dirichlet or mixed. (As in the planar version, Neumann
boundary value problems have either inﬁnitely many solutions or no solutions, depending
upon whether the Fredholm conditions are satisﬁed or not.) We only need to discuss the
case of homogeneous boundary conditions, since, by linear superposition, an inhomogeneous
boundary value problem can be split into a homogeneous boundary value problem for the
inhomogeneous Poisson equation along with an inhomogeneous boundary value problem
for the homogeneous Laplace equation.
As in Chapter 6, we begin by analyzing the case of a delta function inhomogeneity
that is concentrated at a single point in the domain. Thus, for each ξ = (ξ, η, ζ) ∈ Ω, the
Green’s function G(x; ξ) = G(x, y, z; ξ, η, ζ) is the unique solution to the Poisson equation
− Δu = δ(x − ξ) = δ(x − ξ) δ(y − η) δ(z − ζ)

for all

x ∈ Ω,

(12.68)

subject to the chosen homogeneous boundary conditions. The solution to the general
Poisson equation (12.67) is then obtained by superposition: We write the forcing function

f (ξ, η, ζ) δ(x − ξ) δ(y − η) δ(z − ζ) dξ dη dζ
(12.69)
f (x, y, z) =
Ω

as a linear superposition of delta functions. By linearity, the solution

u(x, y, z) =
f (ξ, η, ζ) G(x, y, z; ξ, η, ζ) dξ dη dζ
Ω

(12.70)

528

12 Partial Diﬀerential Equations in Space

to the homogeneous boundary value problem for the Poisson equation (12.67) is then given
as the corresponding superposition of the Green’s function solutions.
The Green’s function can also be used to solve the inhomogeneous Dirichlet boundary
value problem
− Δu = 0,
x ∈ Ω,
u = h,
x ∈ ∂Ω.
(12.71)
The same argument that was used in the two-dimensional situation produces the solution

∂G
u(x) = −
(x; ξ) h(ξ) dS,
(12.72)
∂Ω ∂n
where the normal derivative is taken with respect to the variable ξ ∈ ∂Ω. In the case that
Ω is a solid ball, this integral formula eﬀectively sums the spherical harmonic series (12.51);
see Theorem 12.12 below.
The Free–Space Green’s Function
Only in a few speciﬁc instances is an explicit formula for the Green’s function known.
Nevertheless, certain general guiding features can be readily established. The starting
point is to investigate the Poisson equation (12.68) when the domain Ω = R 3 is all of
three-dimensional space. We impose boundary constraints by seeking a solution that goes
to zero, u(x) → 0, at large distances,  x  → ∞. Since the Laplacian operator is invariant
under translations, we can, without loss of generality, place our delta impulse at the origin,
and concentrate on solving the particular case
− Δu = δ(x) ,

x ∈ R3.

Since δ(x) = 0 for all x = 0, the desired solution will, in fact, be a solution to the
homogeneous Laplace equation
Δu = 0,

x = 0,

save, possibly, for a singularity at the origin.
The Laplace equation models the equilibria of a uniform isotropic medium, and so, as
noted in Exercise 12.1.7, is also invariant under three-dimensional rotations. This suggests
that, in any radially symmetric conﬁguration, the solution should depend only on the
distance r =  x  from the origin. Referring to the spherical coordinate form (12.16) of
the Laplacian operator, if u is a function of r only, then its derivatives with respect to the
angular coordinates ϕ, θ are zero, and so u(r) solves the ordinary diﬀerential equation
d2 u 2 du
(12.73)
= 0.
+
dr2
r dr
This equation is, in eﬀect, a ﬁrst-order linear ordinary diﬀerential equation for v = du/dr
and hence is particularly easy to solve:
b
b
du
= v(r) = − 2 ,
and hence
u(r) = a + ,
dr
r
r
where a, b are arbitrary constants. The constant solution u(r) = a does not die away at
large distances, nor does it have a singularity at the origin. Therefore, if our intuition is
valid, the desired solution should be of the form
u=

b
b
b
.
=
= 
r
x
x2 + y 2 + z 2

(12.74)

12.3 Green’s Functions for the Poisson Equation

529

Indeed, this function is harmonic — solves Laplace’s equation — everywhere away from
the origin and has a singularity at x = 0.
The solution (12.74) is, up to a constant multiple, the three-dimensional Newtonian
gravitational potential due to a point mass at the origin. Its gradient,


b
bx
f (x) = ∇
,
(12.75)
= −
x
 x 3
deﬁnes the gravitational force vector at the point x. When b > 0, the force f (x) points
toward the mass at the origin. Its magnitude
f  =

b
b
= 2
 x 2
r

is proportional to the reciprocal of the squared distance, which is the well-known inverse
square law of three-dimensional Newtonian gravity. Formula (12.75) can also be interpreted
as the electrostatic force due to a concentrated electric charge at the origin, with (12.74)
giving the corresponding Coulomb potential. The constant b is positive when the charges
are of opposite signs, leading to an attractive force, and negative in the repulsive case of
like charges.
Returning to our problem, the remaining task is to ﬁx the multiple b such that the
Laplacian of our candidate solution (12.74) has a delta function singularity at the origin;
equivalently, we must determine a = 1/b such that
− Δ(r−1 ) = a δ(x).

(12.76)

This equation is certainly valid away from the origin, since δ(x) = 0 when x = 0. To
investigate near the singularity, we integrate both sides of (12.76) over a small solid ball
Bε = {  x  ≤ ε } of radius ε :


−1
Δ(r ) dx dy dz =
a δ(x) dx dy dz = a,
−
(12.77)
Bε

Bε

where we used the deﬁnition of the delta function to evaluate the right-hand side. On the
other hand, since Δ r−1 = ∇ · ∇ r−1 , we can use the divergence theorem (12.8) to evaluate
the left-hand integral, whence
 



∂ 1
−1
−1
dS,
Δ(r ) dx dy dz =
∇ · ∇(r ) dx dy dz =
Bε
Bε
Sε ∂n r
where the surface integral is over the bounding sphere Sε = ∂Bε = {  x  = ε }. The
sphere’s unit normal n points in the radial direction, and hence the normal derivative
coincides with diﬀerentiation with respect to r; in particular,
 
 
∂ 1
∂ 1
1
=
=− 2.
∂n r
∂r r
r
The surface integral can now be explicitly evaluated:
 



∂
1
1
1
dS
=
−
dS = − 4 π,
dS = −
2
2
∂n
r
r
ε
Sε
Sε
Sε
since Sε has surface area 4 πε2 . Substituting this result back into (12.77), we conclude that
a = 4 π,

and hence

− Δ r−1 = 4 π δ(x).

(12.78)

530

12 Partial Diﬀerential Equations in Space

This is our desired formula! We conclude that a solution to the Poisson equation with a
delta function impulse at the origin is
G(x, y, z) =

1
1
1

=
=
,
2
4π r
4π  x 
4 π x + y2 + z 2

(12.79)

which is the three-dimensional Newtonian potential due to a unit point mass situated at
the origin.
If the singularity is concentrated at some other point ξ = (ξ, η, ζ), then we merely
translate the preceding solution. This leads immediately to the free-space Green’s function
G(x; ξ) = G(x − ξ) =

1
1

=
.
2
4π  x − ξ 
4 π (x − ξ) + (y − η)2 + (z − ζ)2

(12.80)

The superposition principle (12.70) implies the following integral formula for the solutions
to the Poisson equation on all of three-dimensional space.
Theorem 12.8. Assuming that f (x) → 0 suﬃciently rapidly as  x  → ∞, a particular solution to the Poisson equation
− Δu = f,
is given by
1
u (x) =
4π



1
f (ξ)
dξ =
4π
R3  x − ξ 

for


x ∈ R3,

(12.81)

f (ξ, η, ζ) dξ dη dζ

. (12.82)
(x − ξ)2 + (y − η)2 + (z − ζ)2
R3

The general solution is u(x, y, z) = u (x, y, z) + w(x, y, z), where w(x, y, z) is an arbitrary
harmonic function.
Example 12.9. In this example, we compute the gravitational (or electrostatic)
potential in three-dimensional space due to a uniform solid ball, e.g., a spherical planet
such as the Earth. By rescaling, it suﬃces to consider the case in which the forcing function
is equal to 1 inside a ball of radius 1 and zero outside:

1,
 x  < 1,
f (x) =
0,
 x  > 1.
The particular solution to the resulting Poisson equation (12.81) is given by the integral

1
1
u(x) =
dξ dη dζ.
(12.83)
4π
ξ <1  x − ξ 
Clearly, since the forcing function is radially symmetric, the solution u = u(r) is also
radially symmetric. To evaluate the integral, then, we can take x = (0, 0, z) to lie on the
z–axis, so that r =  x  = | z |. We use cylindrical coordinates ξ = (ρ cos θ, ρ sin θ, ζ), so
that

 x − ξ  = ρ2 + (z − ζ)2 .
The integral in (12.83) can then be explicitly computed:
 1  √1−ζ 2 2 π
ρ dθ dρ dζ
1

⎧ 1
4 π −1 0
ρ2 + (z − ζ)2
0
⎪
| z | ≥ 1,
⎪

⎨ 3|z| ,
#
1 1 "
2
=
1 + z − 2 z ζ − | z − ζ | dζ =
2
⎪
2 −1
⎪
⎩ 1 − z , | z | ≤ 1.
2
6

12.3 Green’s Functions for the Poisson Equation

531

u(r)
.5

.25

Figure 12.6.

r
4
1
2
3
Solution to Poisson’s equation in a solid ball.

Therefore, by radial symmetry, the solution is
⎧ 1
⎪
,
r =  x  ≥ 1,
⎨
3r
(12.84)
u(x) =
2
⎪
⎩ 1−r ,
r =  x  ≤ 1,
2
6
plotted, as a function of r =  x , in Figure 12.6. Note that, outside the solid ball, the
solution is a Newtonian potential corresponding to a concentrated point mass of magnitude
4
3 π — the total mass of the planet. We have thus demonstrated a well-known result in
gravitation and electrostatics: the exterior potential due to a spherically symmetric mass
(or electrically charged body) is the same as if all the mass (charge) were concentrated at
its center. In the darkness of outer space, if you cannot see a spherical planet, you can
determine only its mass, not its size, by measuring its external gravitational force.

Bounded Domains and the Method of Images
Suppose we now wish to solve the inhomogeneous Poisson equation (12.67) on a bounded
domain Ω ⊂ R 3 . To construct the desired Green’s function, we proceed as follows. The
Newtonian potential (12.80) is a particular solution to the underlying inhomogeneous equation
− Δu = δ(x − ξ),
x ∈ Ω,
(12.85)
but it almost surely does not have the proper boundary values on ∂Ω. By linearity, the
general solution to such an inhomogeneous linear equation must take the form
u(x) =

1
− v(x),
4π  x − ξ 

(12.86)

where the ﬁrst term is a particular solution, while v(x) is an arbitrary solution to the homogeneous equation Δv = 0, i.e., an arbitrary harmonic function. The solution (12.86) satisﬁes the homogeneous boundary conditions, provided the boundary values of v(x) match
those of the Green’s function. Let us explicitly state the result in the Dirichlet case.
Theorem 12.10.
value problem
− Δu = f

The Green’s function for the homogeneous Dirichlet boundary
for

x ∈ Ω,

u=0

for

 x  ∈ ∂Ω,

532

12 Partial Diﬀerential Equations in Space

x
η
0

ξ

Method of Images for the unit ball.

Figure 12.7.

for the Poisson equation in a domain Ω ⊂ R 3 has the form
G(x; ξ) =

1
− v(x; ξ),
4π  x − ξ 

x, ξ ∈ Ω,

(12.87)

where v(x; ξ) is the harmonic function of x ∈ Ω that satisﬁes
v(x; ξ) =

1
4π  x − ξ 

x ∈ ∂Ω.

for all

(12.88)

In this manner, we have reduced the determination of the Green’s function to the
solution to a particular family of Laplace boundary value problems, which are parametrized
by the point ξ ∈ Ω. In certain domains with simple geometry, the Method of Images can
be used to produce an explicit formula for the Green’s function. As in Section 6.3, the idea
is to match the boundary values of the free-space Green’s function due to a delta impulse
at a point inside the domain with one or more additional Green’s functions corresponding
to impulses at points outside the domain — the “image points”.
The case of a solid ball of radius 1 with Dirichlet boundary conditions is the easiest to
handle. Indeed, the same geometric construction that we used for a planar disk, redrawn
in Figure 12.7, applies here. Although identical to Figure 6.13, we are re-interpreting it as
a three-dimensional diagram, with the circle representing the unit sphere, while the lines
remain lines. The required image point is given by inversion:
η=

ξ
,
 ξ 2

whereby

ξ =

1
.
η

By the similar triangles argument used before, we have
x
x−ξ
ξ
=
=
,
x
η
x−η

 x  = 1.

and therefore

As a result, the function
v(x; ξ) =

1
η
1
ξ
=
4π  x − η 
4 π  ξ −  ξ 2 x 

has the same boundary values on the unit sphere as the Newtonian potential:
η
1
1
=
4π  x − η 
4 π x − ξ 

whenever

 x  = 1.

12.3 Green’s Functions for the Poisson Equation

533

We conclude that their diﬀerence
1
G(x; ξ) =
4π



1
ξ
−
x−ξ
 ξ −  ξ 2 x 


(12.89)

has the required properties of the Green’s function: it satisﬁes the Laplace equation inside
the unit ball except at the delta function singularity x = ξ, and, moreover, G(x; ξ) = 0
has homogeneous Dirichlet conditions on the spherical boundary  x  = 1.
With the Green’s function in hand, we can apply the general superposition formula (12.70) to arrive at a solution to the Dirichlet boundary value problem for the Poisson
equation in the unit ball.
Theorem 12.11. The solution to the Dirichlet boundary value problem
− Δu = f

for

 x  < 1,

u=0

 x  = 1,

for

on the unit ball is given by the integral



1
ξ
1
u(x) =
−
f (ξ) dξ dη dζ.
4π
x−ξ
 ξ −  ξ 2 x 
ξ ≤1

(12.90)

By the same token, formula (12.72) provides a solution to the inhomogeneous Dirichlet
boundary value problem for the Laplace equation on a ball.
Theorem 12.12. The solution to the Dirichlet boundary value problem
− Δu = 0

for

 x  < 1,

u=h

 x  = 1,

for

on the unit ball is given by the following surface integral:

1
1 −  x 2
u(x) =
h(ξ) dS.
3
4π
ξ =1  ξ − x 

(12.91)

Proof : We start with the explicit formula (12.89) for the Green’s function on the
unit ball. Since the normal derivative on the unit sphere  ξ  = 1 can be written as
∂/∂n = ξ · ∇ξ , a short computation demonstrates that
∂G
1
(x; ξ) =
∂n
4π



 ξ 3 x · ξ −  ξ 2  x 2
x · ξ −  ξ 2
−
 x − ξ 3
 ξ −  ξ 2 x 3



The solution formula (12.91) thus immediately follows from (12.72).

=

1  x 2 − 1
.
4 π  ξ − x 3
Q.E.D.

For example, the series solution (12.60) to the spherical capacitor problem of Example 12.6 can thus be re-expressed as a surface integral:

1
(1 − x2 − y 2 − z 2 ) dS
u(x, y, z) =

3/2
4π
{ ξ 2 +η 2 +ζ 2 =1, ζ>0 }
(ξ − x)2 + (η − y)2 + (ζ − z)2
 π  π/2
(1 − x2 − y 2 − z 2 ) sin ϕ dϕ dθ
=

3/2 .
−π 0
(cos θ sin ϕ − x)2 + (sin θ sin ϕ − y)2 + (cos ϕ − z)2

534

12 Partial Diﬀerential Equations in Space

Exercises
12.3.1. Find the equilibrium temperature of a sphere of radius 1 whose boundary is held at 0◦
while a concentrated unit heat source is applied at (a) the center; (b) a point half-way between the center and the boundary.
12.3.2. A hot soldering iron is continually applied to the north pole of a solid spherical ball of
radius 1. Find the equilibrium temperature.
12.3.3. Write down the gravitional potential — both external and internal — due to a spherical
planet of radius R composed out of a uniform material with density ρ.
12.3.4. (a) Find the gravitational potential due to a spherical shell of unit density obtained by
carving out a spherical cavity of radius a from a solid ball of radius b > a. Hint: Use the
solution to Exercise 12.3.3. (b) What is the gravitational force inside the cavity?
(c) Show that outside the shell, the gravitational potential is as if the entire mass were
concentrated at the origin.
♣ 12.3.5. (a) Write down an integral formula for the gravitational potential and gravitational
force ﬁeld due to a mass of unit density in the shape of a solid unit cube that is centered
at the origin. (b) Use numerical
integration to determine the gravitational force vector at
√ √ √ 
the points (3, 0, 0) and
3 , 3 , 3 . Before doing the calculation, see whether you can
predict which experiences a stronger force, and then check your prediction numerically.
(c) Suppose the mass is re-formed into a sphere. How does this aﬀect the gravitational
force at the two points? First predict whether it will increase, decrease, or stay the same.
Then test your prediction by computing the values and comparing with those you computed
in part (b).
12.3.6. A thin hollow metal sphere of unit radius is grounded. Find the electrostatic potential
inside the sphere due to a small solid metal ball of radius ρ < 1 placed at its center, assuming unit charge density throughout the ball.
12.3.7. A thin straight rod of unit density and length 2 is ﬁxed on the z–axis centered at the
origin. Find the induced (a) gravitational potential and (b) gravitational force experienced
by a point (x, y, z) not on the rod.
♥ 12.3.8. (a) Find the gravitational force due to a thin, uniform straight rod of unit density and
inﬁnite length by letting → ∞ in your solution to Exercise 12.3.7(b). (b) Show that the
force ﬁeld of part (a) has a potential function that can be identiﬁed with the two-dimensional
logarithmic gravitational potential due to a point mass at the origin. Thus, two-dimensional
gravitation can be regarded as a cross-section of three-dimensional gravitation due to
inﬁnitely long vertical line masses. (c) Is your potential function the limit, as → ∞, of
the potential function you found in Exercise 12.3.7(a)? Discuss.
12.3.9. Which well-known solutions to the Laplace equation comes from setting m = n = 0 in
(12.61)?
12.3.10. Use the Fredholm Alternative to analyze the existence and uniqueness of solutions to
the homogeneous Neumann boundary value problem for the Poisson equation on a bounded
domain Ω ⊂ R 3 .
♦ 12.3.11. Mimic the proof of Theorem 6.19 to establish the solution formula (12.72).
12.3.12. Use the Method of Images to ﬁnd the Green’s function for a solid hemisphere of unit
radius subject to homogeneous Dirichlet boundary conditions.

12.4 The Heat Equation for Three–Dimensional Media

535

12.4 The Heat Equation for Three–Dimensional Media
Thermal diﬀusion in a uniform isotropic solid body Ω ⊂ R 3 is modeled by the threedimensional heat equation
 2

∂u
∂ u ∂2u ∂2u
(12.92)
+ 2 + 2 ,
= γ Δu = γ
(x, y, z) ∈ Ω.
∂t
∂x2
∂y
∂z
The positivity of the body’s thermal diﬀusivity, γ > 0, is required on both physical and
mathematical grounds. The physical derivation is exactly the same as that for the twodimensional version (11.1), and does not need to be repeated in detail. Brieﬂy, Fourier’s
law expresses the heat ﬂux vector as a multiple of the temperature gradient, w = − κ ∇u,
while energy conservation implies that its divergence is proportional to the rate of change of
temperature: ∇ · w = − σ ut . Combining these two physical laws and assuming uniformity,
whereby κ and σ are constant, produces (12.92) with γ = κ/σ.
As always, we must impose suitable boundary conditions: either Dirichlet conditions u = h that specify the boundary temperature; (homogeneous) Neumann conditions
∂u/∂n = 0 corresponding to an insulated boundary; or a mixture of the two. Given the
body’s temperature
u(t0 , x, y, z) = f (x, y, z)
(12.93)
at an initial time t0 , it can be proved, [38, 61, 99], that the resulting initial-boundary value
problem is well-posed, which means that there is a unique classical solution u(t, x, y, z),
deﬁned at all subsequent times t > t0 , that depends continuously on the initial data.
As in the one- and two-dimensional versions, we begin by restricting our attention to
homogeneous boundary conditions. Separation of variables works as usual, and we quickly
review the basic ideas. One begins by imposing an exponential solution ansatz
u(t, x) = e− λ t v(x).
Substituting into the diﬀerential equation and canceling the exponentials, it follows that v
satisﬁes the Helmholtz eigenvalue problem
γ Δv + λ v = 0,
subject to the relevant boundary conditions. For Dirichlet and mixed boundary conditions,
the Laplacian is a positive deﬁnite operator, and hence the eigenvalues are all strictly
positive,
0 < λ1 ≤ λ2 ≤ · · · ,
with
λn −→ ∞,
as
n → ∞.
Moreover, on a bounded domain, the Helmholtz eigenfunctions are complete, and so linear
superposition implies that the solution can be written as an eigenfunction series
∞


u(t, x) =

cn e−λn t vn (x).

(12.94)

n=1

The coeﬃcients cn are uniquely prescribed by the initial condition (12.93):
u(t0 , x) =

∞

n=1

cn e−λn t0 vn (x) = f (x).

(12.95)

536

12 Partial Diﬀerential Equations in Space

Self-adjointness of the boundary value problem implies orthogonality of the eigenfunctions,
and hence the coeﬃcients are obtained via the usual inner product formulae:

f (x) vn (x) dx dy dz
Ω

f
,
v

n
cn = eλn t0
.
(12.96)
= eλn t0   
 vn 2
2
vn (x) dx dy dz
Ω

The resulting solution decays exponentially fast to thermal equilibrium, u(t, x) → 0
as t → ∞, typically at a rate equal to the smallest positive eigenvalue λ1 > 0, although
special solutions, whose initial series coeﬃcients vanish, will decay at a faster rate governed
by a higher eigenvalue. Since the higher modes — the terms with n  0 — go to zero
extremely rapidly with increasing t, the solution can be well approximated by the ﬁrst few
terms in its eigenfunction expansion. As a consequence, the heat equation rapidly smooths
out discontinuities and eliminates high-frequency noise in the initial data.
Unfortunately, explicit formulas for the eigenfunctions and eigenvalues are rare. Most
explicit eigensolutions of the Helmholtz boundary value problem require a further separation of variables. In a rectangular box, one separates the solution into a product of functions
depending upon the individual Cartesian coordinates, and the eigenfunctions are written
as products of trigonometric functions; see Exercise 12.4.1 for details. In a cylindrical
domain, the separation is eﬀected in cylindrical coordinates, which leads to eigensolutions
involving trigonometric and Bessel functions, as outlined in Exercise 12.4.5. The most interesting and enlightening case is a spherical domain, and we treat this particular problem
in complete detail in the ensuing subsection.

Exercises
♦ 12.4.1. Let B = { 0 < x < a, 0 < y < b, 0 < z < c } be a solid box of size a × b × c.
(a) Write down an initial-boundary value problem for the thermodynamics of the box when
all its sides are all held at 0◦ and its initial temperature is f (x, y, z). (b) Use separation
of variables to construct the normal mode solutions. (c) Write down a series representing
the general solution to the initial-boundary value problem. What are the formulas for the
coeﬃcients in your series? (d) What is the equilibrium temperature? How fast does the
temperature in the box decay to equilibrium?
12.4.2. True or false: In the context of Exercise 12.4.1, among all boxes of a given volume V , a
cube decays slowest to thermal equilibrium. What is the cube’s decay rate?
12.4.3. Answer Exercises 12.4.1 and 12.4.2 when the top of the box, where z = c, is insulated.
12.4.4. A rectangular brick of size 1 cm × 2 cm × 3 cm made out of material with diﬀusion
coeﬃcient γ = 6 is insulated on ﬁve sides, while one of its small ends is held at temperature
u(x, y, 0) = cos π x cos 2 π y. (a) Find the eventual equilibrium temperature distribution.
(b) If the brick is initially heated in an oven, how fast does it return to equilibrium?
#



$

♦ 12.4.5. Let C = 0 ≤ x2 + y 2 < a, 0 < z < h be a solid cylinder of radius a and height h.
(a) Write down an initial-boundary value problem in cylindrical coordinates for the thermodynamics of the cylinder when its sides, top, and bottom are all held at 0◦ .
(b) Use separation of variables to write down a series representing the general solution to
the initial-boundary value problem. What are the formulas for the coeﬃcients in your
series?

12.4 The Heat Equation for Three–Dimensional Media

537

(c) What is the eventual equilibrium temperature?
(d) How fast does the temperature in the cylinder go to equilibrium?
12.4.6. Find the solution to the initial-boundary value problem in Exercise 12.4.5 when the
initial temperature of the cylinder is uniformly 30◦ . Hint: Use (11.112) to evaluate the
coeﬃcients.
♥ 12.4.7. A cylindrical can that contains 355 ml of soda is removed from the refrigerator. Find
the optimal cylindrical shape for such a can in order to keep the soda cold the longest.
Is this the manufactured shape of a standard soda can in your country?
♥ 12.4.8. True or false: Among all solid cylinders of a given volume, the one that reaches thermal
equilibrium the slowest, when subject to homogeneous Dirichlet boundary conditions, is the
one that has the least surface area. Justify your answer.
♥ 12.4.9. Among all fully insulated solid cylinders of unit volume, which cools down
(i ) the slowest?
(ii ) the fastest?
♦ 12.4.10. Write down a series for the solution to the homogeneous Neumann boundary value
problem for the heat equation on a bounded domain Ω ⊂ R 3 , corresponding to the thermodynamics of a completely insulated solid body. What is the equilibrium temperature of the
body? Does the solution decay to equilibrium? If so, how fast?
♦ 12.4.11. Suppose u(t, x, y, z) is a solution to the heat equation on a fully insulated bounded
domain Ω ⊂ R 3 . Use the identities in Exercise 12.1.11 to prove the following:
(a) The total heat H(t) =

Ω

u(t, x, y, z) dx dy dz is conserved, i.e., is constant. Explain

how this can be used to determine the equilibrium temperature of the body.
(b) If u is a non-equilibrium solution, its squared L2 norm E(t) =
u(t, x, y, z)2 dx dy dz
Ω
is a strictly decreasing function of t.
(c) Use part (b) to prove uniqueness of solutions to the initial value problem.
♦ 12.4.12. State and prove a Maximum Principle for the three-dimensional heat equation.

Heating of a Ball
Our goal is to study heat propagation in a solid spherical body, e.g., the Earth.† For
simplicity, we take the diﬀusivity γ = 1, and consider the heat equation on a solid spherical
ball of unit radius, B1 = {  x  < 1 }, that is subject to homogeneous Dirichlet boundary
conditions. Once we know how to solve this particular case, an easy scaling argument, as
outlined in Exercise 12.4.16, will allow us to ﬁnd the solution for a ball of arbitrary radius
and general diﬀusivity.
As usual, when dealing with a spherical geometry, we adopt spherical coordinates
r, ϕ, θ, as in (12.15), in terms of which the heat equation takes the form
∂u
∂2u
∂ 2 u 2 ∂u
1 ∂2u
1
cos ϕ ∂u
= Δu = 2 +
+ 2
+
+
,
∂t
∂r
r ∂r
r ∂ϕ2
r2 sin ϕ ∂ϕ r2 sin2 ϕ ∂θ2

(12.97)

where we have used our handy spherical coordinate formula (12.16) for the Laplacian. The
†
In this admittedly simplistic model, we are assuming that the Earth is composed of a
completely uniform and isotropic solid material.

538

12 Partial Diﬀerential Equations in Space

standard diﬀusive separation of variables ansatz
u(t, r, ϕ, θ) = e− λ t v(r, ϕ, θ)
requires us to analyze the spherical coordinate form of the Helmholtz equation
Δv + λ v =

cos ϕ ∂v
1 ∂2v
1
∂2v
∂ 2 v 2 ∂v
+
+ 2
+ λv = 0
+ 2
+ 2 2
2
2
∂r
r ∂r
r ∂ϕ
r sin ϕ ∂ϕ r sin ϕ ∂θ2

(12.98)

on the unit ball Ω = { r < 1 } under homogeneous Dirichlet boundary conditions. To make
further progress, we invoke a second variable separation, splitting oﬀ the radial coordinate
by setting
v(r, ϕ, θ) = p(r) w(ϕ, θ).
The function w must be 2 π–periodic in θ and well deﬁned on the z–axis, i.e., when ϕ = 0, π.
Substituting this ansatz into (12.98), and separating all the r-dependent terms from those
terms depending on the angular variables ϕ, θ leads to a pair of diﬀerential equations
involving a separation constant, denoted by μ. The ﬁrst is an ordinary diﬀerential equation
d2 p
dp
(12.99)
+ (λ r2 − μ)p = 0,
+ 2r
dr2
dr
for the radial component p(r), while the second is a familiar partial diﬀerential equation
r2

ΔS w + μ w =

1 ∂2w
∂ 2 w cos ϕ ∂w
+
+
+ μ w = 0,
∂ϕ2
sin ϕ ∂ϕ
sin2 ϕ ∂θ2

(12.100)

for its angular counterpart w(ϕ, θ). The operator ΔS is the spherical Laplacian from
(12.19). In Section 12.2, we showed that its eigenvalues are
μm = m(m + 1)

for

m = 0, 1, 2, 3, . . . .

The mth eigenvalue admits 2 m + 1 linearly independent eigenfunctions: the spherical harmonics Ym0 , . . . , Ymm , Ym1 , . . . , Ymm deﬁned in (12.38).
Spherical Bessel Functions
The radial ordinary diﬀerential equation (12.99) can be solved by setting
√
q(r) = r p(r).

(12.101)

We use the product rule to relate the derivatives of q and p, whereby
1 dq
q
3q
dp
d2 p
1 d2 q
1 dq
= 1/2
− 3/2 ,
+
= 1/2
− 3/2
.
2
dr
dr 2 r
dr
dr2
dr 4 r5/2
r
r
r
Substituting these expressions
√ back into (12.99) with μ = μm = m(m + 1) and multiplying
the resulting equation by r, we discover that q(r) must solve the diﬀerential equation
p=

q

r1/2

,

d2 q
dq  2
2
+ λ r − m + 12
+r
q = 0,
(12.102)
dr2
dr
which we recognize as the rescaled Bessel equation (11.56) of half-integer order m + 12 .
Consequently, the solution to (12.102) that remains bounded at r = 0 is (up to a scalar
multiple) the rescaled Bessel function
√
q(r) = Jm+1/2 λ r .
r2

12.4 The Heat Equation for Three–Dimensional Media

539

The corresponding solution
p(r) = r−1/2 Jm+1/2

√
λr

(12.103)

to (12.99) is important enough to warrant a special name.
Deﬁnition 12.13. The spherical Bessel function of order m ≥ 0 is deﬁned by the
formula

π
Sm (x) =
(x).
(12.104)
J
2 x m+1/2

π/2 is included in the deﬁnition so as to avoid
Remark : The multiplicative
factor
√
√
annoying factors of π and 2 in the subsequent formulas.
Surprisingly, unlike the Bessel functions of integer order, the spherical Bessel functions
are all elementary functions! Comparing (12.104) with (11.105), we see that the spherical
Bessel function of order 0 is
sin x
S0 (x) =
.
(12.105)
x
The corresponding explicit formulas for the higher-order spherical Bessel functions can be
obtained through the general recurrence relation
Sm+1 (x) = −

dSm
m
+
S (x),
dx
x m

(12.106)

which is a consequence of the Bessel function recurrence formula (11.111). Indeed,


π dJm+1/2
1 π 1
dSm
J
(x)
=
−
dx
2x
dx
2 2 x3/2 m+1/2




m + 12
1 π 1
π
=−
J
(x)
Jm+3/2 (x) +
Jm+1/2 (x) −
2x
x
2 2 x3/2 m+1/2


m
m
π
π
=−
Jm+3/2 (x) +
Jm+1/2 (x) = − Sm+1 (x) +
S (x).
2x
x
2x
x m
The next few spherical Bessel functions are, therefore,
cos x sin x
dS0
=−
+ 2 ,
dx
x
x
dS1
S1
sin x 3 cos x 3 sin x
+
,
S2 (x) = −
+
=−
−
dx
x
x
x2
x3
dS2
15 cos x 15 sin x
2 S2
cos x 6 sin x
−
+
,
S3 (x) = −
+
=
−
2
dx
x
x
x
x3
x4
S1 (x) = −

(12.107)

and so on. Figure 11.4 provides graphs of the ﬁrst four spherical Bessel functions on the
interval 0 ≤ x ≤ 20; the vertical axes range from −.5 to 1.0. We note that
S0 (0) = 1,

whereas

Sm (0) = 0

for

m > 0,

(12.108)

whose proof is the task of Exercise 12.4.26. Thus, our radial solution (12.103) is, apart
from an inessential constant multiple, a rescaled spherical Bessel function of order m :
√
p(r) = Sm λ r .

540

12 Partial Diﬀerential Equations in Space

S0 (x)

S1 (x)

S2 (x)

S3 (x)

Figure 12.8.

Spherical Bessel functions.

So far, we have not taken into account the (homogeneous) Dirichlet boundary condition
at r = 1. This requires
√
p(1) = 0,
and hence
Sm λ = 0.
√
Therefore, λ must be a root of the mth order spherical Bessel function. We introduce the
notation
0 < σm,1 < σm,2 < σm,3 < · · ·
to denote the successive (positive) spherical Bessel roots, satisfying
Sm (σm,n ) = 0

for

n = 1, 2, . . . .

(12.109)

In particular the roots of the zeroth order spherical Bessel function S0 (x) = x−1 sin x are
just the integer multiples of π:
σ0,n = n π

for

n = 1, 2, . . . .

The higher-order roots are not expressible in terms of known constants. A table of all
spherical Bessel roots that are < 13 appears below. The columns of the table are indexed
by m, the order, while the rows are indexed by n, the root number.
Re-assembling the individual constituents, we have now demonstrated that the separable eigenfunctions of the Helmholtz equation on a solid ball of radius 1, when subject
to homogeneous Dirichlet boundary conditions, are products of spherical Bessel functions
and spherical harmonics,
vk,m,n (r, ϕ, θ) = Sm (σm,n r) Ymk (ϕ, θ),
vk,m,n (r, ϕ, θ) = Sm (σm,n r) Ymk (ϕ, θ),

m = 0, 1, 2, . . . ,
k = 0, . . . , m,
n = 1, 2, 3, . . . .

(12.110)

12.4 The Heat Equation for Three–Dimensional Media

541

Spherical Bessel Roots σm,n

9
n
1
2
3
4
..
.

m

0

1

2

3

4

5

3.1416

4.4934

5.7635

6.9879

8.1826

6

7

8

9.3558 10.5128 11.6570 12.7908 . . .
..
..
..
6.2832 7.7253 9.0950 10.4171 11.7049 12.9665
.
.
.
..
..
..
9.4248 10.9041 12.3229
.
.
.
..
..
12.5664
.
.
..
.

The corresponding eigenvalues
2
λm,n = σm,n
,

m = 0, 1, 2, . . . ,

n = 1, 2, 3, . . . ,

(12.111)

are the squared spherical Bessel roots. Since there are 2 m + 1 independent spherical
harmonics of order m, the eigenvalue λm,n admits 2 m + 1 linearly independent eigenfunctions, namely v0,m,n , . . . , vm,m,n , v1,m,n , . . . , 
v m,m,n . In particular, the radially symmetric
solutions are the eigenfunctions with k = m = 0 :
vn (r) = v0,0,n (r) = S0 (σ0,n r) =

sin n π r
,
nπ r

(12.112)

n = 1, 2, . . . .

Further analysis, cf. [34], demonstrates that the separable solutions (12.110) form a complete system of eigenfunctions for the Helmholtz equation on the unit ball with homogeneous Dirichlet boundary conditions.
We have thus completely determined the basic separable solutions to the heat equation
on a solid unit ball subject to homogeneous Dirichlet boundary conditions. They are
products of exponential functions of time, spherical Bessel functions of the radius, and
spherical harmonics:
2

uk,m,n (t, r, ϕ, θ) = e− σm,n t Sm (σm,n r) Ymk (ϕ, θ),

(12.113)

u
k,m,n (t, r, ϕ, θ) = e− σm,n t Sm (σm,n r) Ymk (ϕ, θ).
2

The general solution can be written as an inﬁnite “Fourier–Bessel–spherical harmonic”
series in these fundamental modes:
u(t, r, ϕ, θ) =

∞
∞




e

2
− σm,n
t

Sm (σm,n r)

m=0 n=1

+

m :


c0,m,n 0
Ym (ϕ, θ)
2

ck,m,n Ymk (ϕ, θ) + 
ck,m,n Ymk (ϕ, θ)


;

(12.114)
.

k=1

The series’ coeﬃcients are uniquely prescribed by the initial data u(0, r, ϕ, θ) = f (r, ϕ, θ),

542

12 Partial Diﬀerential Equations in Space

and their explicit formulae†
ck,m,n =

(2 m + 1)(m − k) !
π (m + k) ! Sm+1 (σm,n )2

(2 m + 1)(m − k) !

ck,m,n =
π (m + k) ! Sm+1 (σm,n )2

 π  π 1
−π

f (r, ϕ, θ) vk,m,n (r, ϕ, θ) r2 sin ϕ dr dϕ dθ,
0

0

 π  π 1
−π

f (r, ϕ, θ) 
vk,m,n (r, ϕ, θ) r2 sin ϕ dr dϕ dθ,
0

0

(12.115)
follow from the usual orthogonality relations among the eigenfunctions, combined with the
formulas

2π
 v0,m,n  =
S
(σ ),
2 m + 1 m+1 m,n

(12.116)
π(m + k) !
S
vk,m,n  =
(σ ),
k > 0,
 vk,m,n  =  
(2 m + 1)(m − k) ! m+1 m,n
for their norms, to be established in Exercise 12.4.29. In particular, the slowest-decaying
mode is the spherically symmetric function
2

e− π t sin πr ,
u0,0,1 (t, r) =
πr

(12.117)

2
= π 2 . Therefore, typically, the decay
corresponding to the smallest eigenvalue λ0,1 = σ0,1
to thermal equilibrium of a unit sphere is at an exponential rate of π 2 ≈ 9.8696, or, to a
very rough approximation, 10.

Exercises
12.4.13. It takes a solid ball of radius 1 cm ten minutes to return to (approximate) thermal
equilibrium. How long does it take a similar ball of radius 2?
12.4.14. If a 200-gram potato served hot from the oven takes 15 minutes until its maximum
temperature is less than 40◦ C, how long does it take a 300-gram potato of the same shape
to cool oﬀ?
♥ 12.4.15. A uniform solid metal ball of radius 1 meter, with diﬀusion coeﬃcient γ = 2, is taken
from a 300◦ oven and immersed in a bucket of ice water. (a) Write down an initial-boundary
value problem that describes the temperature of the ball. (b) Find a series solution for
the temperature. (c) At what time is the temperature ≤ 50◦ throughout the ball?
♦ 12.4.16. Find the decay rate to thermal equilibrium of a solid spherical ball of radius R and
diﬀusion coeﬃcient γ when subject to homogeneous Dirichlet boundary conditions.
12.4.17. True or false: A heated solid hemisphere placed in a 0◦ environment cools down twice
as fast as a solid sphere of the same radius made out of the same material.
12.4.18. A fully insulated solid spherical ball of radius 1 has initial temperature distribution
f (r, ϕ, θ). (a) Write down a formula for the equilibrium temperature of the ball.
(b) What is the rate of decay of the ball to thermal equilibrium?

†

We use the spherical coordinate form of the L2 inner product on the ball.

12.4 The Heat Equation for Three–Dimensional Media

543

12.4.19. Which cools down to equilibrium faster: a fully insulated solid ball or one whose boundary is held ﬁxed at 0◦ ? How much faster?
12.4.20. A solid sphere and solid cube are made out of the same material and have the same
volume. Both are heated in an oven and then submerged in a large vat of water. Which
will cool down faster? Explain and justify your answer.
12.4.21. Answer Exercise 12.4.20 when the two solids have the same surface area.
12.4.22. Suppose the solid spherical shell in Exercise 12.2.7 starts oﬀ at room temperature.
Assuming that the water in the center remains at 100◦ , ﬁnd the rate at which the shell
tends to thermal equilibrium.
♥ 12.4.23. The thermodynamics of a thin, uniform, spherical shell of unit radius is governed by
the spherical heat equation ut = γ ΔS u, u(0, ϕ, θ) = f (ϕ, θ), in which ΔS is the spherical
Laplacian (12.19). The solution u(t, ϕ, θ) represents the temperature of the point on the
unit sphere with angular coordinates ϕ, θ, while f (ϕ, θ) is the initial temperature distribution. (a) Find the eigensolutions. (b) Write down the solution to the initial value problem
as a series in eigensolutions. (c) What is the ﬁnal equilibrium temperature of the spherical
shell? (d) What is its rate of decay to equilibrium? (e) Find the solution and the ﬁnal
equilibrium temperature when f (ϕ, θ) = (i ) sin ϕ cos θ; (ii ) cos 2 ϕ.
12.4.24. A spherical potato, of radius R = 7.5 cm and thermal diﬀusivity γ = .3 cm2 /sec, is
initially at room temperature, 25◦ C, and is placed in a pot of boiling water at 100◦ C.
The potato is cooked when it has reached the temperature of at least 90◦ C throughout.
How long do you have to wait until the potato is done?
12.4.25. (a) Explain why the spherical Bessel function S1 (x) is bounded at x = 0.
What is S1 (0)? (b) Answer the same question for S2 (x).
♦ 12.4.26. Prove the formulae (12.108).
♦ 12.4.27. (a) Find a recurrence relation expressing the spherical Bessel function Sm−1 (x) in
terms of Sm (x). (b) Prove that

d  3
x Sm (x)2 − Sm−1 (x) Sm+1 (x)
= 2 x2 Sm (x)2 .
dx
♦ 12.4.28. Let m ≥ 0 be a ﬁxed integer. (a) Prove that the rescaled spherical Bessel functions
vn (r) = Sm (σm,n r), n = 1, 2, . . . , are mutually orthogonal under the inner product
f ,g =

1
0

f (r) g(r) r2 dr. (b) Prove that  vn  = √1 | Sm+1 (σm,n ) |. Hint: Mimic the
2

method outlined in Exercise 11.4.22, using the identity in Exercise 12.4.27(b).
♦ 12.4.29. (a) Use the result of Exercise 12.4.28 to prove the formulae (12.116) for the L2 norms
of the eigenfunctions (12.110). (b) Justify the formulae (12.115).

The Fundamental Solution to the Heat Equation in Space
For the heat equation (as well as more general diﬀusion equations), the fundamental
solution measures the response of the body to an instantaneously applied concentrated
unit heat source. Thus, given a point ξ = (ξ, η, ζ) ∈ Ω within the body, the fundamental
solution
u(t, x) = F (t, x; ξ) = F (t, x, y, z; ξ, η, ζ)
solves the initial-boundary value problem
ut = Δu,

u(0, x) = δ(x − ξ),

for

x ∈ Ω,

t > 0,

(12.118)

544

12 Partial Diﬀerential Equations in Space

subject to the selected homogeneous boundary conditions — Dirichlet, Neumann, or mixed.
Explicit formulas for the fundamental solution are rare, although in bounded domains
it is possible to construct it as an eigenfunction series, as described in Section 9.5. The
one case amenable to a complete analysis is that in which the heat is distributed over
all of three-dimensional space, so Ω = R 3 . We recall that Lemma 11.11 showed how to
construct solutions of the two-dimensional heat equation as products of one-dimensional
solutions. In a similar manner, if p(t, x), q(t, x), and r(t, x) are any three solutions to the
one-dimensional heat equation ut = γ uxx , then their product
u(t, x, y, z) = p(t, x) q(t, y) r(t, z)

(12.119)

is a solution to the three-dimensional heat equation
ut = γ (uxx + uyy + uzz ).
In particular, choosing
2

p(t, x) =

e− (x−ξ) /4 γ t
√
,
2 πγ t

2

q(t, y) =

2

e− (y−η) /4 γ t
√
,
2 πγ t

r(t, z) =

e− (z−ζ) /4 γ t
√
,
2 πγ t

to all be one-dimensional fundamental solutions, we are immediately led to the fundamental
solution in the form of a three-dimensional Gaussian ﬁlter .
Theorem 12.14. The fundamental solution
2

e− x−ξ /(4 γ t)
F (t, x; ξ) = F (t, x − ξ) =
8 (πγ t)3/2

(12.120)

solves the three-dimensional heat equation ut = γ Δu on R 3 for t > 0, with an initial
temperature equal to a delta function concentrated at the point x = ξ.
Thus, the initially concentrated heat energy immediately begins to spread out in a
spherically symmetric manner, with a minuscule, but nonzero eﬀect that is felt immediately
arbitrarily far away from the initial concentration. At each individual point x ∈ R 3 , after
an initial warm-up, the temperature decays back to zero at a rate proportional to t−3/2
— more rapidly than in two dimensions, because, intuitively, there are more directions in
which the heat energy can disperse.
To solve a more general initial value problem with the initial temperature distributed
over all of space, we ﬁrst write

u(0, x) = f (x) =
f (ξ) δ(x − ξ) dξ dη dζ
as a linear superposition of delta functions. By linearity, the solution to the initial value
problem is given by the corresponding superposition

2
1
f (ξ) e− x−ξ /(4 γ t) dξ dη dζ
(12.121)
u(t, x) =
8 (πγ t)3/2
of the fundamental solutions. Since the fundamental solution has exponential decay as
 x  → ∞, the superposition formula is valid even for initial temperature distributions
that are moderately increasing at large distances. We remark that the integral (12.121)
has the form of a three-dimensional convolution

u(t, x) = F (t, x) ∗ f (x) =
f (ξ) F (t, x − ξ) dξ dη dζ
(12.122)

12.5 The Wave Equation for Three–Dimensional Media

545

of the initial data with a one-parameter family of increasingly spread-out Gaussian ﬁlters.
Thus, as before, convolution with a Gaussian ﬁlter has a smoothing eﬀect on the initial
temperature distribution.

Exercises
12.4.30. True or false: In a three-dimensional medium, heat energy propagates at inﬁnite speed.
12.4.31. A solid spherical ball of radius 1 is heated to 100◦ and inserted into a three-dimensional medium ﬁlling the rest of R 3 with uniform temperature 0◦ .
(a) Write down an integral formula for the subsequent temperature distribution over R 3 at
time t > 0, assuming a common diﬀusion coeﬃcient γ = 1.
(b) Evaluate the resulting integral using spherical coordinates.
12.4.32. (a) Prove that u(t, r) is a spherically symmetric solution to the three-dimensional heat
equation if and only if w(t, r) = r u(t, r) solves the one-dimensional heat equation: wt = wrr .
(b) True or false: If w(t, r) is the fundamental solution for the one-dimensional heat
equation based at r = 0, then u(t, r) = w(t, r)/r is the fundamental solution for the
three-dimensional heat equation based at the origin.
12.4.33. Construct the solution to the initial value problem in Exercise 12.4.31 using radial
symmetry and Exercise 12.4.32.
♥ 12.4.34. Suppose that, as Earth orbits the sun, its surface is subject to yearly periodic
temperature variations a cos ω t, where the frequency ω is given by (4.56). (a) Assuming,
for simplicity, that the Earth is a homogeneous solid ball, of radius R, formulate an initialboundary value problem that governs the temperature ﬂuctuations within the Earth due to
its orbiting the sun. (b) At what depth does the temperature vary out of phase with the
surface, i.e., is the warmest in winter and coldest in summer? Compare your answer with
the root cellar computation at the end of Section 4.1. Hint: Use Exercise 12.4.32.
12.4.35. (a) Prove that if u(t, x) is any (suﬃciently smooth) solution to the heat equation, so is
its time derivative v = ∂u/∂t. (b) Write out the time derivative of the fundamental
solution, and the initial value problem it satisﬁes.
12.4.36. Write down an explicit eigenfunction series for the fundamental solution F (t, x; ξ ) to
the heat equation in a unit cube with thermal diﬀusivity γ = 1 that is subject to homogeneous Dirichlet boundary conditions.
12.4.37. Write down an explicit eigenfunction series for the fundamental solution F (t, x; ξ ) to
the heat equation in a ball of radius 1 that has thermal diﬀusivity γ = 1 and is subject to
homogeneous Dirichlet boundary conditions.
♦ 12.4.38. Justify the statement that formula (12.119) provides a solution to the three-dimensional heat equation.
12.4.39. Fill in the details of the proof of Theorem 12.14.

12.5 The Wave Equation for Three–Dimensional Media
The three-dimensional wave equation
utt = c2 Δu = c2 (uxx + uyy + uzz ),

(12.123)

546

12 Partial Diﬀerential Equations in Space

in which c > 0 denotes the speed of light, governs the propagation of waves in a homogeneous isotropic three-dimensional medium, e.g., electromagnetic waves (light, X-rays, radio
waves, etc.) in empty space. In this context, while the electric and magnetic vector ﬁelds
E, B are intrinsically coupled by the more complicated system of Maxwell’s equations, each
individual component satisﬁes the wave equation; see Exercise 12.5.14 for details.
The wave equation also models certain restricted classes of vibrations of a uniform
solid body. The solution u(t, x) = u(t, x, y, z) represents a scalar-valued displacement of
the body at time t and position x = (x, y, z) ∈ Ω ⊂ R 3 . For example, u(t, x) might
represent the radial displacement of the body. One imposes suitable boundary conditions,
e.g., Dirichlet, Neumann, or mixed, on ∂Ω, along with a pair of initial conditions
∂u
(0, x) = g(x),
∂t

u(0, x) = f (x),

x ∈ Ω,

(12.124)

that specify the body’s initial displacement and initial velocity. As long as the initial and
boundary data are reasonably nice, there exists a unique classical solution to the initialboundary value problem for all −∞ < t < ∞, cf. [38, 61, 99]. Thus, in contrast to the
heat equation, one can follow solutions to the wave equation both forwards and backwards
in time.
Let us focus our attention on the homogeneous boundary value problem. The fundamental vibrational modes are found by imposing our usual trigonometric ansatz
u(t, x, y, z) = cos(ω t) v(x, y, z)

or

sin(ω t) v(x, y, z).

Substituting into the wave equation (12.123), we discover (yet again) that v(x, y, z) must
be an eigenfunction for the associated Helmholtz eigenvalue problem
Δv + λ v = 0,

where

λ=

ω2
,
c2

(12.125)

coupled to the relevant boundary conditions. In the positive deﬁnite cases, i.e., Dirichlet
and mixed boundary conditions, the eigenvalues λk = ωk2 /c2 > 0 are all positive. Each
eigenfunction vk (x, y, z) yields two normal vibrational modes
u
k (t, x, y, z) = sin(ωk t) vk (x, y, z),
uk (t, x, y, z) = cos(ωk t) vk (x, y, z),
√
of frequency ωk = c λk equal to the square root of the corresponding eigenvalue multiplied
by the wave speed. The general solution is a quasiperiodic linear combination,
u(t, x, y, z) =

∞




ak cos(ωk t) + bk sin(ωk t) vk (x, y, z),

(12.126)

k=1

of the eigenmodes. The coeﬃcients ak , bk are uniquely prescribed by the initial conditions
(12.124). Thus,
∞

ak vk (x, y, z) = f (x, y, z),
u(0, x, y, z) =
∂u
(0, x, y, z) =
∂t

k=1
∞

k=1

ωk bk vk (x, y, z) = g(x, y, z).

12.5 The Wave Equation for Three–Dimensional Media

547

The explicit formulas follow immediately from the orthogonality of the eigenfunctions:


f vk dx dy dz
g vk dx dy dz
 f , vk 
1  g , vk 
Ω
Ω






ak =
,
bk =
.
=
=
 vk 2
ωk  vk 2
vk2 dx dy dz
ωk
vk2 dx dy dz
Ω

Ω

(12.127)
In the positive semi-deﬁnite Neumann case, there is an additional zero eigenvalue
λ0 = 0 corresponding to the constant null eigenfunction v0 (x, y, z) ≡ 1. This results in two
additional terms in the eigenfunction expansion — a constant term

1
a0 =
f (x, y, z) dx dy dz
vol Ω
Ω
that equals the average initial displacement, and an unstable mode b0 t that grows linearly
in time, whose speed

1
g(x, y, z) dx dy dz
b0 =
vol Ω
Ω
is the average initial velocity over the entire body. Thus, the unstable mode will be excited
if and only if there is a nonzero net initial velocity: b0 = 0.
Most of the basic solution techniques we learned in the two-dimensional case apply
here, and we will not dwell on the details. The case of a rectangular box is a particularly
straightforward application of the method of separation of variables, and is outlined in the
exercises. A similar analysis, now in cylindrical coordinates, can be applied to the case of
a vibrating cylinder. The most interesting case is that of a solid spherical ball, which is
the subject of the next subsection.
Vibration of Balls and Spheres
Let us focus on the radial vibrations of a solid ball, as modeled by the three-dimensional
wave equation (12.123). The solution u(t, x, y, z) represents the radial displacement of the
“atom” that is situated at position (x, y, z) when the ball is at rest.
For simplicity, we look at the Dirichlet boundary value problem on the unit ball
B1 = {  x  < 1 }. The normal modes of vibration are governed by the Helmholtz equation
(12.125) subject to homogeneous Dirichlet boundary conditions. According to (12.110),
the eigenfunctions are
v0,m,n (r, ϕ, θ) = Sm (σm,n r) Ym0 (ϕ, θ),
vk,m,n (r, ϕ, θ) = Sn (σn,m r) Ymk (ϕ, θ),

n = 1, 2, 3, . . . ,
m = 0, 1, 2, . . . ,

for

vk,m,n (r, ϕ, θ) = Sm (σm,n r) Ymk (ϕ, θ),

(12.128)

k = 1, 2, . . . , m.

Here Sm denotes the mth order spherical Bessel function (12.104), σm,n is its nth root, as
in (12.109), while Ynm , Ynm are the spherical harmonics (12.38). Each eigenvalue
2
λm,n = σm,n
,

m = 0, 1, 2, . . . ,

n = 1, 2, 3, . . . ,

corresponds to 2 m + 1 independent eigenfunctions, namely
vk,m,0 (r, ϕ, θ), vk,m,1 (r, ϕ, θ), . . . , vk,m,m (r, ϕ, θ), vk,m,1 (r, ϕ, θ), . . . , vk,m,m (r, ϕ, θ).

548

12 Partial Diﬀerential Equations in Space

Consequently, the fundamental vibrational frequencies of a solid ball
)
m = 0, 1, 2, . . . ,
n = 1, 2, 3, . . . ,
ωm,n = c λm,n = c σm,n ,

(12.129)

are equal to the spherical Bessel roots σm,n multiplied by the wave speed. There is a
total of 2 (2 m + 1) independent vibrational modes associated with each distinct frequency
(12.129), namely
u0,m,n (t, r, ϕ, θ) = cos(c σm,n t) Sm (σm,n r) Ym0 (ϕ, θ),
u
0,m,n (t, r, ϕ, θ) = sin(c σm,n t) Sm (σm,n r) Ym0 (ϕ, θ),
n = 1, 2, 3, . . . ,

uk,m,n (t, r, ϕ, θ) = cos(c σm,n t) Sm (σm,n r) Ymk (ϕ, θ),

m = 0, 1, 2, . . . ,

u
k,m,n (t, r, ϕ, θ) = sin(c σm,n t) Sm (σm,n r) Ymk (ϕ, θ),

(12.130)

k = 1, 2, . . . , m.

u
k,m,n (t, r, ϕ, θ) = cos(c σm,n t) Sm (σm,n r) Ymk (ϕ, θ),

u
k,m,n (t, r, ϕ, θ) = sin(c σm,n t) Sm (σm,n r) Ymk (ϕ, θ),

In particular, the radially symmetric modes of vibration have, according to (12.105), the
elementary form
cos c n π t sin n π r
,
u0,0,n (t, r, ϕ, θ) = cos(c n π t) S0 (n π r) =
nπ r
n = 1, 2, 3, . . . . (12.131)
sin c n π t sin n π r
,
u
0,0,n (t, r, ϕ, θ) = sin(c n π t) S0 (n π r) =
nπ r
Their vibrational frequencies, ω0,n = c n π, are integral multiples of the lowest frequency
ω0,1 = c π. Thus, intriguingly, if you excite only the radially symmetric modes, the resulting
motion of the ball is periodic. However, more general vibrations are only quasiperiodic.
Adopting the same scaling argument as in (11.166), we conclude that the fundamental
frequencies for a solid ball of radius R and wave speed c are given by ωm,n = c σm,n /R.
The relative vibrational frequencies
ωm,n
σm,n
σm,n
(12.132)
=
=
ω0,1
σ0,1
π
are independent of the size of the ball R or the wave speed c. In the accompanying table,
we display all relative vibrational frequencies that are ≤ 4 in magnitude.
Relative Spherical Bessel Roots σm,n /σ0,1
&
n
1
2
3
4
..
.

m

0

1

2

3

4

6

7

8

...

1.0000 1.4303 1.8346 2.2243 2.6046 2.9780 3.3463 3.7105 . . .
..
..
..
2.0000 2.4590 2.8950 3.3159 3.7258
.
.
.
..
..
3.0000 3.4709 3.9225
.
.
..
..
4.0000
.
.
..
.

12.5 The Wave Equation for Three–Dimensional Media

549

The purely radial modes of vibration (12.131) have individual frequencies
ω0,n =

nπ c
,
R

so

ω0,n
= n,
ω0,1

which appear in the ﬁrst column of the table. The lowest frequency is ω0,1 = π c/R, corresponding to a vibration with period 2 π/ω0,1 = 2 R/c. In particular, for the Earth, the
radius R ≈ 6000 km, and the wave speed in rock is, on average, c ≈ 5 km/sec, so that the
fundamental mode of vibration has period 2 R/c ≈ 2400 seconds, or 40 minutes. Of course,
we have suppressed almost all interesting terrestrial geology in this very crude approximation, which has been based on the assumption that the Earth is a uniform spherical body,
globally vibrating only in its radial direction. A more realistic modeling of the vibrations of
the Earth requires an understanding of the basic partial diﬀerential equations of linear and
nonlinear elastodynamics, [7, 49]. Nonuniformities in the Earth lead to scattering of the
vibrational waves, which are then used to locate subterranean geological structures, e.g., oil
and gas deposits. Localized vibrations of the Earth are also known as seismic waves, and,
of course, earthquakes are their most severe manifestation. We refer the interested reader
to [5] for an introduction to mathematical seismology. Understanding terrestrial vibrations
is an issue of critical importance in geophysics and civil engineering, including the design
of structures, buildings, and bridges, requiring the avoidance of potentially catastrophic
resonant frequencies.
Example 12.15. The radial vibrations of a hollow thin spherical shell (e.g., an elastic
balloon) are governed by the diﬀerential equation
 2

∂2u
cos ϕ ∂u
∂ u
1 ∂2u
2
2
= c ΔS [ u ] = c
+
,
(12.133)
+
∂t2
∂ϕ2
sin ϕ ∂ϕ sin2 ϕ ∂θ2
where ΔS denotes the spherical Laplacian (12.19). The radial displacement u(t, ϕ, θ) of a
point on the sphere depends only on time t and the angular coordinates ϕ, θ. The solution
u(t, ϕ, θ) is required to be 2 π–periodic in the azimuthal angle θ and bounded at the poles,
where ϕ = 0 and π.
According to (12.38), the nth eigenvalue of the spherical Laplacian, λn = n (n + 1),
possesses 2 n + 1 linearly independent eigenfunctions, namely, the spherical harmonics
Yn0 (ϕ, θ), Yn1 (ϕ, θ), . . . , Ynn (ϕ, θ), Yn1 (ϕ, θ), . . . , Ynn (ϕ, θ).
As a consequence, the fundamental frequencies of vibration for a spherical shell are


(12.134)
n = 1, 2, . . . .
ωn = c λn = c n (n + 1) ,
The vibrational solutions are quasiperiodic combinations of the fundamental spherical harmonic modes


cos n (n + 1) t Ynm (ϕ, θ),
sin n (n + 1) t Ynm (ϕ, θ),
(12.135)


sin n (n + 1) t Y m (ϕ, θ).
cos n (n + 1) t Y m (ϕ, θ),
n

n

Representative graphs can be seen in Figure 12.5.√ The smallest positive eigenvalue is
λ1 = 2, yielding a lowest tone of frequency ω1 = c 2. The higher-order frequencies are
irrational multiples of the fundamental frequency, implying that a vibrating spherical bell
sounds dissonant to our ears.
One further remark is in order. The spherical Laplacian operator is only positive semideﬁnite, since the lowest mode has eigenvalue λ0 = 0, which corresponds to the constant

550

12 Partial Diﬀerential Equations in Space

null eigenfunction v0 (ϕ, θ) = Y00 (ϕ, θ) ≡ 1. Therefore, the wave equation (12.133) admits
an unstable mode b0,0 t, corresponding to a uniform radial inﬂation; its coeﬃcient

∂u
3
(0, ϕ, θ) dS
b0,0 =
4π
S1 ∂t
represents the shell’s average initial velocity. The existence of such an unstable mode is an
artifact of the simpliﬁed linear model we are using, which fails to account for nonlinearly
elastic eﬀects that serve to constrain the inﬂation of a spherical balloon.

Exercises
12.5.1. Find the eigenfunction series solution to the initial-boundary value problem for the
wave equation utt = Δu on a unit cube C = { 0 < x, y, z < 1 }, subject to homogeneous
Dirichlet boundary conditions and one of the following sets of initial conditions:
(a) u(0, x, y, z) = 1, ut (0, x, y, z) = 0; (b) u(0, x, y, z) = 0, ut (0, x, y, z) = 1;
(c) u(0, x, y, z) = sin πx sin πy sin πz, ut (0, x, y, z) = 0; (d) u(0, x, y, z) = sin 3 πx,
ut (0, x, y, z) = sin 2 πy; (e) u(0, x, y, z) = 0, ut (0, x, y, z) = x y z (1 − x)(1 − y)(1 − z).
12.5.2. Suppose the cube in Exercise 12.5.1 is subject to homogeneous Neumann boundary
conditions. Which of the preceding initial value problems leads to an unstable motion of
the cube?
12.5.3. (a) Find the separable periodic vibrations of a unit cube subject to homogeneous
Dirichlet boundary conditions. (b) Can you ﬁnd a periodic mode that is not separable?
12.5.4. Answer Exercise 12.5.3 when one face of the cube is left free, while the other ﬁve faces
are ﬁxed.
12.5.5. Given a material with wave speed c = 1.5 cm/sec, ﬁnd the natural vibrational frequencies of a solid rectangular box of size 1 cm × 2 cm × 3 cm whose sides are held ﬁxed. List
the lowest ﬁve such frequencies in order. Does the box vibrate periodically?
12.5.6. Find the natural vibrational frequencies of a solid cylinder of height 2, radius 1, and
wave speed c = 1, when (a) all sides are ﬁxed; (b) the top and bottom of the cylinder are
free, while the curved side is ﬁxed; (c) the curved side of the cylinder is free, while the top
and bottom are ﬁxed.
12.5.7. Among all solid cylinders of unit volume with ﬁxed boundary, ﬁnd the one that vibrates
the slowest.
12.5.8. Does a solid spherical ball that is subject to homogeneous Neumann boundary conditions vibrate (i ) faster, (ii ) slower, or (iii ) at the same rate as the same ball subject to
homogeneous Dirichlet conditions. If your answer is (i ) or (ii ), estimate how much faster
or slower.
12.5.9. A solid cube and solid sphere are made of the same material and have the same volume.
Which vibrates faster when subject to homogeneous Dirichlet boundary conditions?
12.5.10. Assuming that they both have the same wave speed and ﬁxed boundaries, which
vibrates faster: a solid sphere or a circular membrane of the same radius?
12.5.11. A uniform, solid spherical planet is ﬂoating freely in outer space. Find its three
slowest resonant frequencies.
12.5.12. True or false: Suppose we have two uniform solid bodies composed of the same
material. If the ﬁrst body cools down to thermal equilibrium the fastest, then it also
vibrates the fastest. Explain your answer.

12.6 Spherical Waves and Huygens’ Principle

551

12.5.13. (a) Deﬁne what is meant by a nodal curve and a nodal region on a vibrating thin
spherical shell. (b) True or false: All the nodal curves are arcs of circles.
♥ 12.5.14. The propagation of electromagnetic waves (including light) is governed by the electric
ﬁeld E(t, x) and magnetic ﬁeld B(t, x), which are both time-dependent vector ﬁelds deﬁned
for x = (x, y, z) in a domain Ω ⊂ R 3 . In empty space, Maxwell’s equations (as formulated
by Heaviside) are
∂B
∂E
1
∇ × B,
∇ · E = 0,
∇ · B = 0,
= − ∇ × E,
=
(12.136)
∂t
∂t
μ0 0
where μ0 , 0 are, respectively, the permeability and permittivity constants. Prove that all individual components of E and B satisfy the scalar wave equation. What is the wave speed,
i.e., the speed of light in empty space?

12.6 Spherical Waves and Huygens’ Principle
For any dynamical partial diﬀerential equation, the fundamental solution measures the
eﬀect of applying an instantaneous concentrated unit impulse at a single point. Two
representative physical eﬀects to keep in mind are the light waves emanating from a sudden
concentrated blast, e.g., a lightning bolt or a stellar supernova, and the sound waves
due to an explosion or thunderclap, propagating in air at a much slower speed. Linear
superposition utilizes the fundamental solution to build up more general solutions to initial
value problems. For the wave and other second-order vibrational equations, the impulse
can be applied either to the initial displacement or to the initial velocity, resulting in two
distinct types of fundamental solution. The general solution to the initial value problem will
be obtained by a double superposition. In this section, we derive explicit formulas for the
two fundamental solutions for the three-dimensional wave equation on all of space, leading
to Kirchhoﬀ’s formula for the solution to the general initial value problem. An important
consequence is Huygens’ Principle, which states that, in three-dimensional space, localized
initial disturbances remain localized as they propagate. In the ﬁnal subsection, we apply
the method of descent to our three-dimensional solution formulas in order to solve the
two-dimensional wave equation, for which, surprisingly, Huygens’ Principle is no longer
valid.

Spherical Waves
In a uniform isotropic medium, an initial concentrated blast results in a spherically expanding wave, moving away at the speed of light (or sound) in all directions. Invoking
translation invariance, we will assume, without loss of generality, that the source of the
disturbance is at the origin, and so the solution u(t, x) should depend only on the distance
r =  x  from the source. We adopt spherical coordinates and look for a solution u = u(t, r)
to the three-dimensional wave equation (12.123) with no angular dependence. Substituting
the formula (12.16) for the spherical Laplacian and setting both angular derivatives to 0,
we are led to the partial diﬀerential equation

 2
∂2u
∂ u 2 ∂u
2
,
(12.137)
=c
+
∂t2
∂r2
r ∂r

552

12 Partial Diﬀerential Equations in Space

which governs the propagation of spherically symmetric waves in three-dimensional space.
Surprisingly, we can explicitly solve (12.137). The secret is to multiply both sides of the
equation by r:


2
∂ 2 (r u)
∂2u
∂u
∂2u
2
2 ∂
=
c
=
r
=
c
+
2
(r u).
r
∂t2
∂t2
∂r2
∂r
∂r2
Thus, the function
w(t, r) = r u(t, r)
satisﬁes the one-dimensional wave equation
2
∂2w
2 ∂ w
=
c
.
∂t2
∂r2

(12.138)

According to Theorem 2.14, the general solution to the one-dimensional wave equation (12.138) can be written in d’Alembert form
w(t, r) = p(r − c t) + q(r + c t),
where p(ξ) and q(η) are arbitrary functions of a single characteristic variable. Therefore,
spherically symmetric solutions to the three-dimensional wave equation assume the form
u(t, r) =

p(r − c t)
q(r + c t)
+
.
r
r

(12.139)

p(r − c t)
,
r

(12.140)

The ﬁrst summand,
u(t, r) =

represents a wave moving at speed c in the direction of increasing r, and so describes the
illumination from a variable light source that is concentrated at the origin, e.g., a pulsating
quasar in interstellar space. To highlight this interpretation, let us concentrate on the case
that p(ξ) = δ(ξ − a) is a delta function, keeping in mind that more general solutions can
then be assembled by linear superposition. The induced solution
u(t, r) =

δ r − c (t − t0 )
δ(r − c t − a)
=
,
r
r

where

t0 = −

a
,
c

(12.141)

represents a spherical wave propagating through space. At the instant t = t0 , the light is
entirely concentrated at the origin, r = 0. The signal then moves away from the origin
in all directions at speed c. At each later time t > t0 , the wave remains concentrated on
the surface of a sphere of radius r = c (t − t0 ). Its intensity at each point on the sphere,
however, has decreased by a factor 1/r, and so, the farther the light travels away from the
source, the dimmer it becomes. A stationary observer sitting at a ﬁxed point in space will
see only an instantaneous ﬂash of light of intensity 1/r as the spherical wave passes by
at time t = t0 + r/c, where r is the observer’s distance from the light source. A similar
statement holds for sound waves — to an observer, the sound of a distant explosion will
last momentarily. Thunder and lightning are the most familiar examples of this everyday
phenomenon.
On the other hand, for t < t0 , the impulse is concentrated at a negative radius r =
c (t − t0 ) < 0. To interpret this, note that, for spherical coordinates (12.15), replacing r
by − r has the same eﬀect as changing x to the antipodal point − x. Thus, the solution
(12.141) represents a concentrated spherically symmetric light wave arriving from the edges

12.6 Spherical Waves and Huygens’ Principle

553

of the universe at speed c that strengthens in intensity as it collapses into the origin at
t = t0 . After collapse, it immediately reappears and expands back out into the universe.
The second solution in the d’Alembert formula (12.139) has, in fact, exactly the same
physical form under the antipodal identiﬁcation. Indeed, if we set
r = − r,

p(ξ) = − q(− ξ),

then

p(
r − c t)
q(r + c t)
=
.
r
r

Thus, the second d’Alembert solution is redundant, and we only need to consider solutions
of the form (12.140) from now on.
To eﬀectively utilize such spherical wave solutions, we need to understand the nature
of their originating singularity. For simplicity, we set t0 = 0 in (12.141) and concentrate
on the particular solution
δ(r − c t)
,
(12.142)
u(t, r) =
r
which apparently has a bad singularity at the origin, r = 0, at the initial time t = 0. We
need to pin down precisely which sort of distribution (generalized function) it represents.
Invoking the limiting deﬁnition is tricky, and it will be easier to work with the dual characterization of a distribution as a linear functional. Thus, at a ﬁxed time t ≥ 0, we must
evaluate the inner product†

 u(t, ·) , f  =
u(t, x, y, z) f (x, y, z) dx dy dz
of the solution with a smooth test function f (x) = f (x, y, z). We rewrite the triple integral
in spherical coordinates, whereby
 π  π ∞
δ(r − c t)
f (r, ϕ, θ) r2 sin ϕ dr dϕ dθ.
 u(t, ·) , f  =
r
−π 0 0
When t = 0, the r integration can be immediately evaluated, and so
 π  π
 u(t, ·) , f  = c t
f (c t, ϕ, θ) sin ϕ dϕ dθ = 4 πc t Mc t [ f ] ,
−π

where
Mc t [ f ] =

1
4π

 π  π
f (c t, ϕ, θ) sin ϕ dϕ dθ =
−π

(12.143)

0

0

1
4 πc2 t2


f dS

(12.144)

Sc t

'
(
is the mean or average value of the function f on the sphere Sc t =  x  = c t of radius
r = c t and, hence, surface area 4 πc2 t2 . In particular, in the limit as the sphere’s radius
c t → 0, by continuity, the mean reduces to just the value of the function at the origin:
lim Mc t [ f ] = M0 [ f ] = f (0).

(12.145)

t→0

Thus, (12.143) implies that
lim  u(t, · ) , f  =  u(0, · ) , f  = 0

t→0

†

for all functions

f,

For ﬁxed t, we use u(t, ·) to indicate the real-valued function (x, y, z) → u(t, x, y, z) on R 3 .

554

12 Partial Diﬀerential Equations in Space

and hence u(0, x, y, z) ≡ 0 represents a zero initial displacement. In other words, there is,
in fact, no singularity in the solution at t = 0 !
In the absence of any initial displacement, how, then, can the solution (12.142) be
nonzero? Clearly, this must be the result of a nonzero initial velocity. To evaluate ∂u/∂t,
we diﬀerentiate (12.143), whereby
4

∂u
,f
∂t

5


  π  π
∂
∂
f (c t, ϕ, θ) sin ϕ dϕ dθ
 u(t, ·) , f  =
ct
∂t
∂t
−π 0
 π  π
 π  π
∂f
2
f (c t, ϕ, θ) sin ϕ dϕ dθ + c t
(c t, ϕ, θ) sin ϕ dϕ dθ
=c
−π 0
−π 0 ∂r


∂f
= 4 πc Mc t [ f ] + 4 πc2 t Mc t
.
(12.146)
∂r
=

The result is a linear combination of the means of f and its radial derivative fr over the
sphere of radius c t. In the limit, the second term goes to 0, and so, by (12.145),
lim  ut , f  = 4 πc M0 [ f ] = 4 πc f (0).

t→0

Since this holds for all test functions f , we conclude that the initial velocity of our solution
is a multiple of a delta function at the origin:
ut (0, r) = 4 πc δ(x).
Dividing through by 4 πc, we ﬁnd that the spherical expanding wave
u(t, r) =

δ(r − c t)
4 πc r

(12.147)

solves the initial value problem
u(0, x) ≡ 0,

∂u
(0, x) = δ(x),
∂t

corresponding to an initial unit-velocity impulse concentrated at the origin. This solution
can be viewed as the three-dimensional version of the hammer-blow solution to the onedimensional wave equation discussed in Exercise 6.3.28.
More generally, we use the translational symmetry of the wave equation to conclude
that the function
δ x−ξ−ct
(12.148)
,
t ≥ 0,
G(t, x; ξ) =
4 πc  x − ξ 
is the fundamental solution to the wave equation resulting from a unit-velocity impulse
concentrated at the point ξ at the initial time t = 0 :
G(0, x; ξ) = 0,

∂G
(0, x; ξ) = δ(x − ξ).
∂t

(12.149)

With this in hand, we can apply linear superposition to solve the zero initial displacement
initial value problem
u(0, x, y, z) = 0,

∂u
(0, x, y, z) = g(x, y, z).
∂t

(12.150)

12.6 Spherical Waves and Huygens’ Principle

555

Stx
t

1

α
r

x

B1

Figure 12.9.

Cross-section of a sphere intersecting a ball.

Namely, we write the initial velocity

g(x) =
g(ξ) δ(x − ξ) dξ dη dζ
as a superposition of impulses, and immediately conclude that the relevant solution is the
selfsame superposition of spherical waves:


δ x− ξ−ct
1
g(ξ)
dξ dη dζ
u(t, x) =
4 πc
x−ξ

(12.151)
1
x
g(ξ)
dS
=
t
M
[
g
]
,
=
ct
4 πc2 t
 ξ−x =c t
where Mcxt [ g ] denotes the average of the initial velocity function g over the sphere Scxt =
{  ξ − x  = c t } of radius c t centered at the point x. Thus, the value of our solution at
position x and time t > 0 only depends upon the initial data a distance c t away from the
point x.
Example 12.16. Let us set the wave speed c = 1. Suppose that the initial velocity

1,
 x  < 1,
g(x) =
0,
 x  > 1,
is 1 inside the unit ball B1 centered at the origin and 0 outside. To solve the corresponding
initial velocity problem, we must compute the average value of g over a sphere
Stx = { ξ |  ξ − x  = t }
of radius t > 0 centered at a point x ∈ R 3 . Since g = 0 outside the unit ball, its average
will be equal to the surface area of that part of the sphere that is contained inside the unit
ball, namely Stx ∩ B1 , divided by the total surface area of Stx , namely 4 π t2 .
To compute this quantity, let r =  x . If t > r + 1 or 0 < t < r − 1, then the sphere
of radius t lies entirely outside the unit ball, and so the average is 0; if 0 < t < 1 − r, which
requires r < 1 and so x ∈ B1 , then the sphere lies entirely within the unit ball, and so the
average is 1. Otherwise, referring to Figure 12.9 and Exercise 12.6.7, we see that the area

556

12 Partial Diﬀerential Equations in Space

of the spherical cap Stx ∩ B1 is given by


r2 + t2 − 1
πt
2 π t2 (1 − cos α) = 2 π t2 1 −
=
[ 1 − (t − r)2 ] ,
2rt
r

(12.152)

where α denotes the angle between the line joining the centers of the two spheres and
the circle formed by their intersection, whose value is prescribed by the Law of Cosines.
Assembling the diﬀerent subcases, we conclude that
⎧ 1,
0 ≤ t ≤ 1 − r,
⎪
⎨
2
1
−
(t
−
r)
Mcxt [ g ] =
(12.153)
,
| r − 1 | ≤ t ≤ r + 1,
⎪
4rt
⎩
0,
0 ≤ t ≤ r − 1 or t ≥ r + 1.
The solution (12.151) is obtained by multiplying by t, and hence for t ≥ 0,
⎧
t,
0 ≤ t ≤ 1 −  x ,
⎪
⎪
⎪

2
⎨
$
$
1− t−x
$  x  − 1 $ ≤ t ≤  x  + 1,
(12.154)
u(t, x) =
,
⎪
4x
⎪
⎪
⎩
0,
0 ≤ t ≤  x  − 1 or t ≥  x  + 1.
$
$
The resulting function is not smooth at the interfaces t = $  x  − 1 $ and  x  + 1, and
hence does not qualify as a classical solution. Nevertheless, it can be shown that (12.154)
is a bona ﬁde weak solution to the initial value problem.
The ﬁrst two rows of Figure 12.10 plot the solution as a function of time for several
ﬁxed values of r =  x . An observer sitting at the origin will see a linearly increasing
light intensity followed by a sudden blackout. At other points inside the sphere, there
is a similar linear increase, while the subsequent decrease follows a parabolic arc; if the
observer is closer to the edge of the ball than its center, the parabolic portion will continue
to increase for a while before eventually tapering oﬀ. On the other hand, an observer sitting
outside the sphere will experience, after an initially dark period, a symmetric parabolic
increase to a maximal intensity and then a decrease back to dark after a total time lapse
of 2. The second two rows plot the solution as a function of r for various ﬁxed times.
Note that, up until time t = 1, the light spreads out while increasing in intensity near the
origin, after which the solution is of gradually decreasing magnitude, supported within the
domain lying between two concentric spheres of respective radii t − 1 and t + 1.
Returning to the general situation, we note that the solution formula (12.151) handles only nonzero initial velocities. What about solutions resulting from a nonzero initial
displacement? Surprisingly, the answer comes from diﬀerentiation! The key observation is
that if u(t, x) is any (suﬃciently smooth) solution to the wave equation, then so is its time
derivative
∂u
v(t, x) =
(t, x).
(12.155)
∂t
This follows at once from diﬀerentiating both sides of the wave equation with respect to t
and using the equality of mixed partial derivatives. Physically, this implies that the velocity
of a wave obeys the same evolutionary principle as the wave itself, which is a manifestation
of the linearity and time-independence (autonomy) of the equation.
Now, suppose u has initial conditions
u(0, x) = f (x),

ut (0, x) = g(x).

(12.156)

12.6 Spherical Waves and Huygens’ Principle

557

r=0

r = .3

r = .7

r = 1.3

t=0

t = .5

t = .9

t = 1.0

t = 1.1

t = 1.5

Figure 12.10.

Wave equation solution u(t, r) due to
an initial velocity of the unit ball.



What are the initial conditions for its derivative v = ut ? Clearly, its initial displacement
v(0, x) = ut (0, x) = g(x)

(12.157)

equals the initial velocity of u. As for its initial velocity, we have
∂2u
∂v
= 2 = c2 Δu,
∂t
∂t
because we are assuming that u solves the wave equation. Thus, at the initial time, the
velocity,
∂v
(0, x) = c2 Δu(0, x) = c2 Δf (x),
(12.158)
∂t
equals c2 times the Laplacian of the initial displacement f . In particular, if u satisﬁes the
initial conditions
u(0, x) = 0,
ut (0, x) = g(x),
(12.159)
then v = ut satisﬁes the initial conditions
v(0, x) = g(x),

vt (0, x) = 0.

(12.160)

Thus, paradoxically, to solve the initial displacement problem we diﬀerentiate the initial
velocity solution (12.151) with respect to t, and hence


∂ 
∂g
∂u
x
x
x
(t, x) =
t Mc t [ g ] = Mc t [ g ] + c t Mc t
,
(12.161)
v(t, x) =
∂t
∂t
∂n

558

12 Partial Diﬀerential Equations in Space

where we have made use of our computation in (12.146). Therefore, v(t, x) is a linear
combination of the mean of the function g and the mean of its normal or radial derivative
∂g/∂n = ∂g/∂r, taken over a sphere of radius c t centered at the point x. In particular, to
obtain the solution corresponding to a concentrated initial displacement,
∂F
(0, x; ξ) = 0,
∂t

(12.162)

δ  x − ξ  − ct
∂G
(t, x; ξ) = −
,
∂t
4π  ξ − x 

(12.163)

F (0, x; ξ) = δ(x − ξ),
we diﬀerentiate the solution (12.148), resulting in
F (t, x; ξ) =

which is the fundamental solution for the initial displacement problem. Thus, interestingly,
a concentrated initial displacement spawns a spherically expanding doublet, cf. Figure 6.6,
whereas a concentrated initial velocity spawns an expanding spherical singlet or delta wave.
Example 12.17. Let c = 1. Consider the initial conditions

1,
 x  < 1,
∂u
u(0, x) = f (x) =
(0, x) = 0,
∂t
0,
 x  > 1,

(12.164)

modeling the eﬀect of an instantaneously illuminated solid ball. To obtain the resulting
solution, we diﬀerentiate (12.154) with respect to t, leading to
⎧
1,
0 ≤ t < 1 −  x ,
⎪
⎪
⎨ x−t
(12.165)
 x  − 1 ≤ t ≤  x  + 1,
u(t, x) =
,
⎪
2x
⎪
⎩
0,
0 ≤ t <  x  − 1 or t > 1 +  x .
As illustrated in the ﬁrst two rows of Figure 12.11, an observer sitting at the center of
the ball will see a constant light intensity until t = 1, at which time the solution suddenly
goes dark. At other points inside the ball, 0 < r < 1, the downwards jump in intensity
arrives sooner, and even goes below 0, followed by a further linear decrease, and ﬁnally
a jump back to quiescence. An observer placed outside the ball, at radius r =  x  > 1,
will experience, after an initially dark period, a sudden increase in the light intensity at
time t = r − 1, followed by a linear decrease to negative, followed by a jump back up to
darkness at time t = r + 1. The farther away from the source, the fainter the light. In
the second two rows, we plot the same solution as a function of r for diﬀerent values of
t. Note the sudden appearance of a 1/r singularity at the origin at time t = 1, which is
the result of a focusing of the initial discontinuities of u(0, x) = f (x) on the surface of the
unit sphere. Afterwards, the residual radially symmetric disturbance moves oﬀ to ∞ while
gradually decreasing in intensity. Again, the discontinuities imply that (12.165) is not a
classical solution, but it does qualify as a weak solution to the initial value problem.

Kirchhoﬀ ’s Formula and Huygens’ Principle
Linearly combining the two solution formulas (12.151) and (12.161) establishes Kirchhoﬀ’s formula (ﬁrst discovered by Poisson), which is the three-dimensional counterpart to
d’Alembert’s solution formula for the wave equation.

12.6 Spherical Waves and Huygens’ Principle

559

r=0

r = .3

r = .7

r = 1.3

t=0

t = .5

t = .8

t = 1.0

t = 1.25

t = 1.8

Figure 12.11.

Wave equation solution u(t, r) due to

an initial displacement of the unit ball .

560

12 Partial Diﬀerential Equations in Space

Theorem 12.18. The solution to the initial value problem
∂u
(12.166)
(0, x) = g(x),
x ∈ R3,
∂t
for the wave equation in three-dimensional space is given by


∂
∂f
x
x
x
x
u(t, x) =
t Mc t [ f ] + t Mc t [ g ] = Mc t [ f ] + c t Mc t
+ t Mcxt [ g ] , (12.167)
∂t
∂n
utt = c2 Δu,

u(0, x) = f (x),

where Mcxt [ f ] denotes the average of the function f over the sphere Scxt = {  ξ − x  = c t}
of radius c t centered at the point x.
A crucially important consequence of the Kirchhoﬀ solution formula is a celebrated
physical principle ﬁrst set out by the pioneering seventeenth century Dutch scientist Christiaan Huygens.† Roughly, Huygens’ Principle states that, in three-dimensional space,
localized solutions to the wave equation remain localized. More concretely, (12.167) implies that the value of the solution at a point x and time t depends only on the values of
the initial displacements and velocities at a distance c t away. Thus, all signals propagate
along the relativistic light cone
c2 t2 = x2 + y 2 + z 2
in four-dimensional Minkowski space-time. Physically, Huygens’ Principle assures us that
any light that we witness at time t arrived from points that lie a distance exactly d =
c (t − t0 ) away at an earlier time t0 < t. In particular, a localized initial signal, whether
initial displacement or initial velocity, that is concentrated near a point produces a response
that remains concentrated on an ever expanding sphere surrounding the point. In our threedimensional universe, we witness the light from a sudden explosion or lightning bolt for only
a brief moment, after which the view returns to darkness. Similarly, a sharp sound, e.g.,
a thunderclap, remains sharply concentrated with diminishing magnitude as it propagates
through space. Huygens’ Principle is responsible for the important astronomical fact that
the light we now observe from a distant star was generated at a single past time that is
directly proportional to the star’s distance from the Earth. Remarkably, as we will show in
the following subsection, Huygens’ Principle does not hold in a two-dimensional universe!
There, initially concentrated light and sound impulses will spread out as time progresses,
and their eﬀect will be experienced over an extended time range; see below for details.

Exercises
12.6.1. Solve the wave equation in three-dimensional space for the following initial conditions:
(a) u(0, x, y, z) = x + z, ut (0, x, y, z) = 0; (b) u(0, x, y, z) = 0, ut (0, x, y, z) = y;
(c) u(0, x, y, z) = 1/(1 + x2 + y 2 + z 2 ), ut (0, x, y, z) = 0,
(d) u(0, x, y, z) = 0, ut (0, x, y, z) = 1/(1 + x2 + y 2 + z 2 ).
12.6.2. At what points in space-time does a three-dimensional wave vanish if it vanishes
outside a sphere of radius R at the initial time t = 0?
†

Don’t even bother trying to pronounce his name correctly unless you are Dutch!

12.6 Spherical Waves and Huygens’ Principle

561

12.6.3. Consider the initial value problem



1, 0 < x, y, z < 1,
∂2u
∂2u
∂2u
∂2u
∂u
=
+
+
,
u(0, x, y, z) = 0,
(0, x, y, z) =
2
2
2
0, otherwise,
∂t
∂x
∂y
∂z 2
∂t
i.e., the initial velocity is 1 inside a unit cube and 0 outside the cube. We interpret the
solution u(t, x, y, z) as the intensity of light at a given point in space-time, measured in
units that make the speed of light c = 1. (a) Write down an integral formula for
u(t, x, y, z). (b) Suppose a light sensor is placed at the point (2, 2, 1). For which values of
t > 0 will the sensor register a nonzero signal? Sketch a rough graph of what the sensor
measures. (You do not need to ﬁnd the precise formula, but explain how you obtained your
graph.) (c) True or false: The solution u(t, x, y, z) ≥ 0 at all points in space-time.
12.6.4. Is (12.151) a solution to the wave equation for t < 0? If not, write down a solution
formula that is valid for negative t.
12.6.5. True or false: The function u(t, x, y, z) deﬁned by (12.154) is everywhere continuous.
12.6.6. A thermonuclear explosion occurs at the center of the Earth. Would you feel the eﬀect
ﬁrst through a motion at the surface or a change in temperature at the surface? Discuss.
♦ 12.6.7. Prove that the area of the spherical cap Stx ∩ B1 is given by formula (12.152).

Descent to Two Dimensions
So far, we have found explicit formulas for the solution to the wave equation on the onedimensional line, and in three-dimensional space. The two-dimensional case
utt = c2 Δu = c2 (uxx + uyy )

(12.168)

is, counterintuitively, more complicated! For instance, seeking a radially symmetric solution
u(t, r) requires solving the partial diﬀerential equation
∂2u
= c2
∂t2



∂ 2 u 1 ∂u
+
∂r2
r ∂r


,

(12.169)

which, unlike its three-dimensional counterpart (12.137), is not so easily integrated.
However, our solution to the three-dimensional problem can be adapted to construct
a solution to the two-dimensional problem using the so-called Method of Descent. Observe
that any solution u(t, x, y) to the two-dimensional wave equation (12.168) can be viewed
as a solution to the three-dimensional wave equation (12.123) that does not depend upon
the vertical z coordinate, whence ∂u/∂z = 0. Clearly, if the three-dimensional initial data
does not depend on z, then the resulting solution u(t, x, y) will also be independent of z.
Consider ﬁrst the zero initial displacement condition
u(0, x, y) = 0,

∂u
(0, x, y) = g(x, y).
∂t

(12.170)

In the three-dimensional solution formula (12.151), if g(x, y) does not depend on the z–
coordinate, then the integrals over the upper and lower hemispheres
Sc+t =

-

.
 ξ − x  = c t, ζ ≥ z ,

Sc−t =

-

.
 ξ − x  = c t, ζ ≤ z ,

562

12 Partial Diﬀerential Equations in Space

are identical. To evaluate these integrals, we parametrize the upper hemisphere as the
graph of

.
ζ = z + c2 t2 − (ξ − x)2 − (η − y)2 over the disk Dcxt = (ξ − x)2 + (η − y)2 ≤ c2 t2 ,
concluding that


1
1
u(t, x, y) =
g(ξ, η) dS =
g(ξ, η) dS
4 πc2 t
2 πc2 t
Sc t
Sc+t

g(ξ, η)
1

dξ dη
=
2
2
x
2 πc
c t − (ξ − x)2 − (η − y)2
Dc t

(12.171)

solves the initial value problem (12.170). In particular, if we take the initial velocity
∂u
(0, x, y) = g(x, y) = δ(x) δ(y)
∂t
to be a unit impulse concentrated at the origin, then the resulting solution is
⎧
1
⎨

,
x2 + y 2 < c2 t2 ,
2
2
2 πc c t − x2 − y 2
(12.172)
u(t, x, y) =
⎩
2
2
2 2
0,
x +y >c t .

An observer sitting at distance r =  x  = x2 + y 2 from the origin will ﬁrst witness
a concentrated displacement singularity at time t = r/c. However, in contrast to the
three-dimensional solution, even after the impulse passes by, there will continue to be a
decreasing, but nonzero, signal of magnitude roughly proportional to 1/t. In Figure 12.12,
we plot the solution (12.172) for unit wave speed c = 1. The ﬁrst row plots intensity as a
function of t at three diﬀerent radii; note that the initial singularity, indicated by a spike in
the graph, is followed by a progressively smaller residual displacement, which never entirely
disappears. The second row shows the displacement at three diﬀerent times as a function
of r =  x .
As in the three-dimensional case, the solution to the initial displacement conditions
u(0, x, y) = f (x, y),

∂u
(0, x, y) = 0,
∂t

can then be obtained by diﬀerentiation of (12.171) with respect to t, and so

1 ∂
f (ξ, η)

dξ dη .
u(t, x, y) =
2
2
x
2 πc ∂t
c t − (ξ − x)2 − (η − y)2
Dc t

(12.173)

(12.174)

As before, starting with a concentrated impulse, an observer will witness, after a time
lapse t = r/c, an abrupt impulse passing by, followed by a progressively decaying residual
wave. The general solution to the two-dimensional wave equation on all of R 2 is a linear
combination of these two types of solutions (12.171, 174).
As a consequence of these considerations, we discover that Huygens’ Principle is not
valid in a two-dimensional universe. The solution to the two-dimensional wave equation
at a point x at time t depends on the initial displacement and velocity on the entire disk
of radius c t centered at the point, and not just on the points lying a distance c t away.
So a two-dimensional creature would experience not just an initial eﬀect of a concentrated
sound or light wave, but also an “afterglow” of slowly diminishing magnitude. It would be

12.6 Spherical Waves and Huygens’ Principle

563

r = .5

r=1

r = 1.5

t = .5

t=1

t=2

Figure 12.12.

Solution to the two-dimensional wave equation
for a concentrated impulse.



like living in a permanent echo chamber, and so understanding and acting upon sensory
phenomena would be considerably more challenging. In general, it can be proved that
Huygens’ Principle for the wave equation is valid only in spaces of odd dimension n =
2 k + 1 ≥ 3; see also [15] for recent advances in the classiﬁcation of partial diﬀerential
equations that admit a Huygens’ Principle.
Remark : Since the solutions to the two-dimensional wave equation can be interpreted
as three-dimensional solutions with no z dependence, a concentrated delta impulse for the
two-dimensional wave equation would correspond to a three-dimensional initial impulse
that is concentrated along an entire vertical line, e.g., an instantaneous lightning bolt in
the form of an inﬁnite straight line. An observer ﬁxed in space will ﬁrst encounter the
light ﬂash arriving from the closest point on the line, but will subsequently experience the
gradually decreasing eﬀect of the light emitted by points that lie progressively farther away
along the line. This accounts for the two-dimensional afterglow in formula (12.172).

Exercises
12.6.8. Solve initial value problem for the two-dimensional wave equation with the following
initial data (a) u(0, x, y) = x − y, ut (0, x, y) = 0; (b) u(0, x, y) = 0, ut (0, x, y) = y.

564

12 Partial Diﬀerential Equations in Space


12.6.9. (a) Prove that u(t, x, y) = 1/ x2 + y 2 − c2 t2 is a solution to the two-dimensional wave
equation on the domain Ω = { x2 + y 2 > c2 t2 } exterior to the light cone passing through
the origin. What is the corresponding initial data at
t = 0? (b) Use part (a) to solve the

initial value problem u(0, x, y) = 0, ut (0, x, y) = 1/ x2 + y 2 , on Ω.
12.6.10. Consider the two-dimensional wave equation on R 2 with wave speed c = 1. Write
down an integral formula for the solution to the following initial value problems. You need
not evaluate the integrals. (a) u(0, x, y) = x3 − y 3 , ut (0, x, y) = 0;
(b) u(0, x, y) = 0, ut (0, x, y) = y 2 ; (c) u(0, x, y) = x2 + y 2 , ut (0, x, y) = − x2 − y 2 .
12.6.11. (a) Find the solution to the two-dimensional wave equation whose initial displacement
is a concentrated delta impulse at the origin and whose initial velocity is zero.
(b) Is your expression a classical solution when t > 0?
(c) True or false: The solution tends to 0 uniformly as t → ∞.
12.6.12. Use separation of variables to write down an eigenfunction series solution to the
partial diﬀerential equation (12.169) when subject to homogeneous Dirichlet boundary
conditions at r = 1 and bounded at r = 0.
♦ 12.6.13. Write down the fundamental solution for the one-dimensional wave equation with
(a) a concentrated initial displacement at the origin; (b) a concentrated initial velocity
at the origin. (c) Discuss the validity of Huygens’ Principle in a one-dimensional universe.
12.6.14. Discuss how you can construct solutions to the one-dimensional wave equation by
descent from the three-dimensional wave equation.

12.7 The Hydrogen Atom
A hydrogen atom consists of a single electron circling an atomic nucleus that contains a
single proton, which, owing to its relatively tiny size, is assumed to be entirely concentrated
at the origin. As a result of quantization of the corresponding classical Coulomb problem,
the Schrödinger equation† governing the dynamical behavior of the electron moving around
the nucleus takes the explicit form
 2

∂ψ
2
α2
2
∂ ψ ∂2ψ ∂2ψ
α2 ψ

i
=−
Δψ −
ψ=−
+
+
.
−
∂t
2M
r
2M
∂x2
∂y 2
∂y 2
x2 + y 2 + z 2
(12.175)
Here ψ(t, x, y, z) denotes the electron’s time-dependent wave function, which, at each time
t, prescribes its quantum probability density as it circles the nucleus. In the quantized
Hamiltonian operator K = − 12 (2 /M ) Δ − α2 /r, the coeﬃcient of the Laplacian depends
on Planck’s constant  and the electron’s mass M . The ﬁnal term represents the threedimensional electromagnetic (Coulomb) potential function V (x) = α2 /r attracting the
electron to the nucleus, with α representing the electron’s (and proton’s) charge, while r =
 x  is its distance from the nucleus. Incidentally, the quantum-mechanical Schrödinger
equation for multi-electron atoms or even molecules is not so diﬃcult to write down, but
its solution, even for, say, the helium atom, is much more diﬃcult, and, in general, is still
†
The reader is referred to (9.151) and the subsequent discussion for generalities regarding the
Schrödinger equation and quantum mechanics.

12.7 The Hydrogen Atom

565

a major challenge for numerical analysts, even on today’s supercomputers, [116]. Thus,
to keep matters as simple as possible, we will consider only the case of a single electron
hydrogen atom here.
Bound States
According to the analysis in Section 9.5, the normal mode solutions to the Schrödinger
equation are of the form
ψ(t, x, y, z) = e i λ t/ v(x, y, z),
where v is an eigenfunction of the Hamiltonian operator with eigenvalue λ, and hence
satisﬁes


2
α2
Δv + λ +
v = 0.
(12.176)
2M
r
The bound states of the atom, in which the electron remains trapped by the nucleus, are
represented by the nonzero solutions to the eigenvalue problem (12.176) with unit L2 norm:

2
| v(x, y, z) |2 dx dy dz = 1.
v =
The eigenvalue λ speciﬁes the bound state’s energy, and is necessarily negative: λ < 0.
Since we are working on an unbounded domain, the bound states do not form a complete
system of eigenfunctions, and so not every wave function ϕ ∈ L2 (R 3 ) can be approximated
by an eigenfunction series. The missing data are the so-called scattering states arising
from the continuous spectrum of the Schrödinger operator; these represent electrons that
scatter oﬀ the nucleus, and so do not remain trapped in an orbit. (For the classical Kepler
problem of a planet circling a sun, the bound states would correspond to planets following
bounded elliptic orbits, while the scattering states correspond to interstellar comets and
the like moving along unbounded hyperbolic or parabolic trajectories.) We will leave
the discussion of the quantum-mechanical scattering states and the associated continuous
spectrum to a more advanced treatment of the subject, [72, 95].
To understand the bound states, we will apply the method of separation of variables.
We begin by rewriting the eigenvalue problem (12.176) in spherical coordinates:
 2

 
∂ v
1 ∂2v
1
∂2v
α2
2
2 ∂v
cos ϕ ∂v
+ 2
+
v = 0.
+ λ+
+
+ 2
2M
∂r2
r ∂r
r ∂ϕ2
r sin ϕ ∂ϕ r2 sin2 ϕ ∂θ2
r
(12.177)
We then separate oﬀ the radial coordinate, setting
v(r, ϕ, θ) = p(r) w(ϕ, θ).
The angular component satisﬁes the spherical Helmholtz equation
ΔS w + μ w =

1 ∂2w
∂ 2 w cos ϕ ∂w
+
+
+ μ w = 0,
∂ϕ2
sin ϕ ∂ϕ
sin2 ϕ ∂θ2

which we have already solved; see (12.21) and the ensuing discussion. The eigensolutions
are spherical harmonics, which, because the quantum-mechanical solutions are intrinsically
complex-valued, we take in their complex form (12.46). The associated eigenvalue
μ = l (l + 1),

where the integer

l = 0, 1, 2, . . .

(12.178)

566

12 Partial Diﬀerential Equations in Space

is known as the angular quantum number , admits a total of 2 l + 1 linearly independent
eigenfunctions
m = −l, −l + 1, . . . , l − 1, l.

Ylm (ϕ, θ) = Plm (cos ϕ) e i m θ ,

(12.179)

The radial equation with the separation constant (12.178) is
2
2M



d2 p 2 dp
+
dr2
r dr




+

α2
l (l + 1)
λ+
−
r
r2


p = 0.

(12.180)

To eliminate the physical parameters, let us rescale the radial coordinate by setting
√
2 −2 M λ
(12.181)
s = σ r,
where
σ=
,

given that λ < 0. The resulting ordinary diﬀerential equation for the rescaled function
" #
s
P (s) = p
σ
is
d2 P
2 dP
−
+
ds2
s ds



1 n l (l + 1)
− +
4
s
s2

where
α2
2 M α2
=
n=
2
σ



−


P = 0,

M
.
2λ

(12.182)

(12.183)

Equation (12.182) is a version of the generalized Laguerre diﬀerential equation — see Exercise 12.7.4 below — named after the nineteenth-century French mathematician Edmond
Laguerre, who studied its solutions well before the appearance of quantum mechanics. Since
we are searching for bound states, the relevant solutions should be deﬁned on 0 ≤ s < ∞,
remain bounded at s = 0, and go to zero as s → ∞:
lim P (s) < ∞,

s → 0+

lim P (s) = 0.

s→∞

(12.184)

The proof of the following key result is outlined in Exercises 12.7.4–5.
Theorem 12.19. For each pair of nonnegative integers 0 ≤ l < n, the boundary
value problem (12.182, 184) has the eigensolution
l+1
(s),
Pln (s) = sl e− s/2 L2n−l−1

(12.185)

where
s
Ljk (s) =



k
e dk  j+k − s   (−1)i j + k i
s,
s e
=
k ! dsk
i!
j+i
i=0

−j s

are known as generalized† Laguerre polynomials.

†

The ordinary Laguerre polynomials are Lk (s) = L0k (s).

j, k = 0, 1, 2, . . . ,

(12.186)

12.7 The Hydrogen Atom

567

L01 (s)

L02 (s)

L03 (s)

L11 (s)

L12 (s)

L13 (s)

Figure 12.13.

Generalized Laguerre polynomials.

The ﬁrst few generalized Laguerre polynomials are
L00 (s) = 1,

L01 (s) = 1 − s,

L02 (s) = 1 − 2 s + 12 s2 ,

L03 (s) = 1 − 3 s + 32 s2 − 16 s3 ,

L10 (s) = 1,

L11 (s) = 2 − s,

L12 (s) = 3 − 3 s + 12 s2 ,

L13 (s) = 4 − 6 s + 2 s2 − 16 s3 ,

L20 (s) = 1,

L21 (s) = 3 − s,

L22 (s) = 6 − 4 s + 12 s2 ,

L23 (s) = 10 − 10 s + 52 s2 − 16 s3 .

Note that Ljk (s) has degree k. A few graphs, on the interval 0 ≤ t ≤ 6, appear in
Figure 12.13. See [86] for details on their properties.
Atomic Eigenstates and Quantum Numbers
The integer n, whose physical value was noted in (12.183), is known as the principal
quantum number . We further note that the scaling factor in (12.181) can be written as
σ=

2 M α2
2
=
,
n 2
na

where

a=

2
≈ .529 × 10− 10 meter,
M α2

which approximates the radius of the electron’s lowest energy level, is known as the Bohr
radius, in honor of the pioneering Danish quantum physicist Niels Bohr. Reverting to phys-

568

12 Partial Diﬀerential Equations in Space

ical coordinates, the bound state solutions (12.185) become, up to an inessential constant
multiple, the radial wave functions

 l

2r
2r
n
− r/(n a) 2 l+1
.
(12.187)
e
Ln−l−1
βl (r) =
na
na
Combining them with the spherical harmonics (12.179) yields the atomic eigenfunctions
or eigenstates

(2 l + 1) (l − m) ! (n − l − 1) ! n
vl m n (r, ϕ, θ) =
βl (r) Ylm (ϕ, θ),
(12.188)
π a3 n4 (l + m) ! (l + n) !
where the initial factor is selected so as to make  vl m n  = 1, and hence a bona ﬁde wave
function. (A proof of this fact is outlined in Exercise 12.7.8.) The eigenstates depend on
three integers, which have the following physical designations:
• n = 1, 2, 3, . . . :
the principal quantum number ;
• l = 0, 1, . . . , n − 1:
the angular quantum number ;
• m = − l, − l + 1, . . . , l − 1, l :
the magnetic quantum number .
The energy is the associated eigenvalue:
λn = −

α4 M 1
α2 1
=−
,
2
2
2 n
2 a n2

n = 1, 2, 3, . . . .

(12.189)

The fact that the ratios λn /λ1 = 1/n2 between the energy levels of an atom are inverse
squares of integers was one of the key experimental discoveries that precipitated the discovery of quantum mechanics. Observe that the nth energy level has a total of
n−1


(2 l + 1) = n2

(12.190)

l=0

linearly independent bound states (12.188). The dimension of the eigenspace corresponds to
the number of orbital subshells in the atom for the corresponding energy level. The shells
indexed by the angular quantum number, i.e., the order l = 0, 1, 2, . . . of the spherical
harmonic, are traditionally labeled by a letter in the sequence s, p, d, f, g, . . . , where each
successive shell contains 2 l + 1 individual subshells, indexed by the magnetic quantum
number m.
The one missing ingredient in this simple model is the electron’s spin. Since electrons
can have one of two possible spins, the Pauli Exclusion Principle, ﬁrst formulated by the
Austrian physicist Wolfgang Pauli, tells us that each atomic energy shell can be occupied
by at most two electrons. Consequently, the atomic shell with angular quantum number l
may contain up to 2 (2 l + 1) electrons. Keep in mind that, since 0 ≤ l < n, the lth shell
appears only when n is suﬃciently large, so that, according to (12.190), the nth energy
level contains up to 2 n2 electrons.
The resulting atomic conﬁguration of electronic energy shells is the explanation for
Mendeleev’s periodic table. Its rows are indexed by the principal quantum number n,
while the columns are labeled by the angular and magnetic quantum numbers l, m, and
the spin. As one moves up the periodic table, the electrons in each successive element’s
atom progressively ﬁll up the lower energy levels, each new shell containing ﬁrst a single
electron, then two electrons with opposite spins. Thus, hydrogen (in its ground state) has
a single electron in the 1s shell. Helium has two electrons in the 1s shell. Lithium has

12.7 The Hydrogen Atom

569

three electrons, with two of them ﬁlling the 1s shell and the third in the 2s shell. Neon
has ten electrons ﬁlling the ﬁrst two energy levels, with two electrons in the 1s shell, two
in the 2s shell, and six in the 2p shell. And so on. The one complication is that, owing
to the orbital’s geometry, as prescribed by the associated spherical harmonic, the angular
and, to a lesser extent, magnetic quantum numbers also aﬀect the physically observed
energy, and this can cause shells to ﬁll later than might initially be expected. For example,
in potassium and calcium, the 4s shell is successively ﬁlled, followed by scandium, which
begins the process of ﬁlling the 3d subshells. The chemical properties of the elements are,
to a very large extent, determined by the placement of their atom’s electrons within the
outermost energy level. The interested reader can consult, for example, [67, 79] for further
details.

Exercises
12.7.1. If the nucleus contains Z protons circled by a single electron, then its atomic potential
V (x) is rescaled accordingly, replacing α2 /r by Z α2 /r. Discuss the induced eﬀect on the
energy levels of such an atomic ion.
♥ 12.7.2. (a) Write down the time-dependent wave function for a single electron atom when the
electron is in its ground state, i.e., the lowest energy level. (b) What is the probability
density of the electron? (c) What is the probability of ﬁnding the electron within 1 Bohr
radius of the atom? (d) Find the distance d (measured in Bohr radii) so that there is a
95% probability of ﬁnding the electron within a distance d of the nucleus.
♦ 12.7.3. Prove that the two expressions for the Laguerre polynomials in (12.186) agree.
♦ 12.7.4. (a) Let k = 0, 1, 2, . . . be a nonnegative integer. The Laguerre diﬀerential equation of
order k is
(12.191)
x u + (1 − x) u + k u = 0.
Show that x = 0 is a regular singular point. Then prove that the Frobenius solution based
at x = 0 is a polynomial of degree j that coincides with the Laguerre polynomial L0k (x).
(b) Given nonnegative integers j, k ≥ 0, use the Frobenius method to prove that the
generalized Laguerre diﬀerential equation
(12.192)
x u + (j + 1 − x) u + k u = 0
has a polynomial solution that can be identiﬁed with the generalized Laguerre
polynomial Ljk (x) in (12.186).
♦ 12.7.5. Suppose that P (s) solves the ordinary diﬀerential equation (12.182). Prove that
Q(s) = s− l es/2 P (s) solves the diﬀerential equation
dQ
d2 Q
(12.193)
+ [ 2(l + 1) − s ]
+ (n − l − 1)Q = 0.
s
ds2
ds
Then apply the result of Exercise 12.7.4 to complete the proof of Theorem 12.19.
♥ 12.7.6. Suppose f (x) is a polynomial, and let Ljk (s) denote the generalized Laguerre
polynomials (12.186). (a) Prove that, for j, k ≥ 0,
∞
∞ (k)
(−1)k
f (s) Ljk (s) sj e− s ds =
f (s) sj+k e− s ds.
0
0
k!
(b) For ﬁxed j, prove that the generalized Laguerre polynomials Ljk (s), k = 0, 1, 2, . . . ,
are orthogonal with respect to the weighted inner product  f , g  =
(c) Prove the formula for their corresponding norms:  Ljk  =

∞

0

f (s) g(s) sj e− s ds.

(j + k) !
.
k!

570

12 Partial Diﬀerential Equations in Space

♦ 12.7.7. (a) Prove that the generalized Laguerre polynomials satisfy the following recurrence relation:
(k + 1) Ljk+1 (s) − (j + 2 k + 1 − s) Ljk (s) + (j + k) Ljk−1 (s) = 0.
(12.194)
(b) Prove that

∞ j+1 − s  j
2
(j + 2 k + 1) (j + k) !
s
e
Lk (s) ds =
.
0
k!

(12.195)

Hint: Use part (a) and Exercise 12.7.6.
♥ 12.7.8. Prove that the atomic eigenfunctions (12.188) form an orthonormal system of wave
functions with respect to the L2 inner product on R 3 .
Hint: Use Theorem 9.33 and equation (12.195).

Correction to: Introduction to Partial Differential Equations
Peter J. Olver
Correction to:
P. J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in
Mathematics, https://doi.org/10.1007/978-3-319-02099-0

After the second printing of the book in 2016, several errors were identified that needed
correction. The following corrections have been updated within the current version,
along with all known typographical errors.

Page xix: Change Corrected Printing to First Corrected Printing (2016)
Then add

Second Corrected Printing (2020)
Further corrections and improvements to the exposition have been incorporated
into this new printing. I would particularly like to thank Lawrence Baker for his
detailed reading of both the full text and the Solutions Manual, and thereby spotting
many of the required corrections in both. I would also like to thank Henry Boateng,
Joseph Feneuil, Adam Kay, Manuel Mañas, Svitlana Mayboroda, Bruno Poggi, Ma
Shi-Zhuang, Radu Slobodeanu, James Stowe, and John Zweck for their suggestions and
corrections. Finally, I thank Loretta Bartolini at Springer for all her help navigating
the production process.
Also change May 2016 to July, 2020
Page 5: Lines 8–10: change sentence beginning “In general, the domain . . . ” to
In general, the domain D will be an open subset, usually connected, and hence pathwise connected, meaning any two points can be connected by a curve C ⊂ D, and,
particularly in equilibrium equations, often bounded, with a reasonably nice boundary,
denoted by ∂D.
Page 8: In exercise 1.10 (a) change 4 t2 − x2 to 4 t2 + x2 .
The updated online version of the book can be found at
https://doi.org/10.1007/978-3-319-02099-0
© Springer International Publishing Switzerland 2020
P. J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
https://doi.org/10.1007/978-3-319-02099-0_13

C1

C2

Correction to: Introduction to Partial Differential Equations

Page 21: On line 4 below figure, change “. . . counting principle of Chapter 1, . . . ”
to “. . . counting principle formulated on page 6, . . . ”
Page 30: Revise Exercise 2.2.19 :
2.2.19.(a) Find and graph the characteristic curves for the equation ut +(sin x) ux = 0.
Suppose you are given initial data (i) u(0, x) = cos 12 x , (ii) u(0, x) = cos 12 πx.
(b) Write down a formula for the solution. (c) Graph your solution at times
t = 0, 1, 2, 3, 5, and 10. (d) What is the limiting solution profile as t → ∞?
Page 57: In the last two displayed formulas, the first term on the right hand side of
the equals sign is missing a minus sign:




∂v
1 ∂u η − ξ η + ξ
1 ∂u η − ξ η + ξ
(ξ, η) = −
,
+
,
,
∂ξ
2 c ∂t
2c
2
2 ∂x
2c
2
and so, in particular,
∂v
1 ∂u
1 ∂u
(ξ, ξ) = −
(0, ξ) +
(0, ξ) = 0,
∂ξ
2 c ∂t
2 ∂x
Page 61: Revise Exercise 2.4.12 :
2.4.12. Given a classical solution u(t, x) of the wave equation, let E = 12 (u2t +c2 u2x )
be the associated energy density and P = ut ux the momentum density. (a) Prove that
∂P/∂t = ∂E/∂x and ∂E/∂t = c2 ∂P/∂x. Explain why both E and P are conserved
densities for the wave equation. (b) Show that E(t, x) and P (t, x) both satisfy the wave
equation. (c) Suppose that both ut (t, x) → 0 and ux (t, x) → 0 as | x | → ∞ sufficiently
R∞
rapidly in order that the integrals defining the total momentum P(t) = −∞ P (t, x) dx
R∞
and the total energy E(t) = −∞ E(t, x) dx are defined and finite for each t ∈ R. Show
that P(t) and E(t) are conserved quantities, i.e., they are constants, independent of
the time t.
Page 88: Revise Exercise 3.2.47 :
3.2.47. (a) Graph the partial sums s3 (x), s5 (x), s10 (x) of the Fourier series (3.55).
Do you notice a Gibbs phenomenon? If so, what is the amount of overshoot? If
not, explain why. (b) Answer the same question for the Fourier series (3.81) for the
function sign x.
Page 91: Change page header to 3.2 Fourier Series
Page 131: In (4.37), change final + to −:
u(t, x) ≈ 12 a0 + e− t (a1 cos x + b1 sin x) = 12 a0 + r1 e− t cos(x − δ1 ),
Page 152: On line -6, change “. . . appear the context of boundary . . . ” to
“. . . appear in the context of boundary . . . ”

(4.37)

C3

Correction to: Introduction to Partial Differential Equations

Page 163: On the last line of table, change x4 − 4 x2 y 2 + y 4 to x4 − 6 x2 y 2 + y 4
Page 168: On line 11, change “. . . easy consequence of Green’s Theorem, . . . ” to
“. . . easy consequence of Green’s Theorem 6.13, . . . ”
Page 188: In example 5.4, change the sentence after the displayed formula to
used earlier in Example 4.1, along with homogeneous Dirichlet boundary conditions,
so u(t, 0) = u(t, 1) = 0.

Page 190: In equation (5.28), change O (∆t)2 to O(∆t):
∂u
u(tj , xm ) − u(tj−1 , xm )
+ O(∆t).
(tj , xm ) ≈
∂t
∆t
Page 195:: Change sentence after equation (5.40):

(5.28)

We work on the interval 0 ≤ x ≤ 1, and use step sizes ∆t = ∆x = .005. Let us try
four different values of the wave speed.
Page 198: In equation (5.45), change the denominator to 2 ∆x:

∂u
uj,m+1 − uj,m−1
(tj , xm ) ≈
+ O (∆x)2 .
∂x
2 ∆x

(5.45)

Page 210: Correct last displayed equation by switching indices on the ui,j :
u1,1 = .1831,

u2,1 = .2589,

u3,1 = .1831,

u1,2 = .3643,

u2,2 = .5152,

u3,2 = .3643,

u1,3 = .5409,

u2,3 = .7649,

u3,3 = .5409,

Page 211: In equation (5.78), the sub- and super-diagonal matrix elements should be
−1, not − ρ2 :
 2 (1 + ρ2 )

−1





Bρ = 





−1

2 (1 + ρ2 )
−1

−1
2 (1 + ρ2 )
−1

−1
2 (1 + ρ2 )

..

..

.
−1

−1

..
.
.
2
2 (1 + ρ )
−1
−1
2 (1 + ρ2 )





 (5.78)





Page 212: In equation (5.82), replace w(n−1) by Un−1 w(n−1) and Uj by Uk :
z(1) = b
f (1) ,

Un−1 w(n−1) = z(n−1) ,

z(j+1) = b
f (j+1) − Lj z(j) ,

Uk w(k) = z(k) − ρ2 w(k+1) ,

j = 1, 2, . . . , n − 2,

k = n − 2, n − 3, . . . , 1,

(5.82)

In equation (5.83), replace Lj by Lk :

w(k) = Lk w(k+1) − ρ−2 z(k) ,

k = n − 2, n − 3, . . . , 1.

(5.83)

C4

Correction to: Introduction to Partial Differential Equations

Page 228: Revise Exercise 6.1.25 :
6.1.25. Prove from first principles that the sequence (6.24) converges nonuniformly
to the step function.
Page 229: In equation (6.40), delete initial fraction:
Z π
Z π sin n + 1  x
Z π X
n
1
1
2
sn (x) dx =
dx
=
e i k x dx = 1,
1
2
π
2
π
sin
x
−π
−π
−π
2

(6.40)

k = −n

Page 238: In the first integral in the displayed equation after (6.65) change sinh ω y
to sinh ω ξ:
Z x
Z 1
sinh ω (1 − x) sinh ω ξ
sinh ω x sinh ω (1 − ξ)
u(x) =
dξ +
dξ
ω sinh ω
ω sinh ω
0
x
Page 239: At end of line 10 add the following footnote after “case”:
†
However, one can suitably extend the notion of Green’s function in such situations; see, for instance, a preprint by J. Franklin, Green’s functions for Neumann
boundary conditions; arXiv 1201.6059, 2012.
Page 241: Change page header to
6.2 Green’s Functions for One-Dimensional Boundary Value Problems
Page 250: Change line before and equation (6.105) to the following:
Then, by (6.89),
ZZ
ZZ
I
∂(log r)
1=
δ(x, y) dx dy = − b
∆(log r) dx dy = − b
ds
∂n
Dε
Dε
Cε
I
I
Z π
∂(log r)
1
= −b
ds = − b
ds = − b
dθ = −2 π b,
∂r
Cε
Cε r
−π

(6.105)

Page 250: In the displayed formula after Theorem 6.17, change R 2 to Ω:
ZZ
u(x, y) = −
G0 (x, y; ξ, η) ∆u(ξ, η) dξ dη.
Ω

2

In equation (6.108), change R to Ω twice and add brackets to second integrand :
ZZ
ZZ


δ(x − ξ) δ(y − η) u(ξ, η) dξ dη =
− ∆G0 (x, y; ξ, η) u(ξ, η) dξ dη. (6.108)
Ω

Ω

Page 254: Change line 1 to
“The Green’s function for the homogeneous Dirichlet boundary value . . . ”
Page 258: Correct left hand side of equation (6.135):
∂G
1
1 − r2
(r, θ; 1, φ) = −
,
∂ρ
2 π 1 + r2 − 2 r cos(θ − φ)

(6.135)

C5

Correction to: Introduction to Partial Differential Equations

Pages 276–7: Correct proof of Proposition 7.10 :
Proof : Note that
g(x) =

Z x

f (ξ) dξ =

−∞

Z ∞

−∞

σ(x − ξ) f (ξ) dξ,

where σ(x) is the step function (6.23). The latter expression is just the convolution
integral (7.52) between the two functions:
g(x) = σ ∗ f (x).
Thus, according to the convolution formula (7.55), the Fourier transform of the integral
is given (up to multiple) by the product of the individual Fourier transforms:
√
gb(k) = 2 π σ
b(k) fb(k).

Consulting our table of Fourier transforms, we find
r

√
π
i
i
i
gb(k) = 2 π
δ(k) − √
fb(k) = − fb(k)+ π fb(k) δ(k) = − fb(k)+ π fb(0) δ(k),
2
k
k
2π k
which establishes the desired formula (7.45).

Q.E.D.

Page 277: In the displayed equation immediately above the Exercises, delete one
factor of 1/k in the first term after the equals sign:
r
r


i π −|k|
π 3/2
π 3/2
π e− | k |
b
f (k) = −
e
+ √ δ(k) − √ δ(k) = − i
.
k 2
2
k
2
2
Page 278: In Exercise 7.2.12, insert a factor of
fb(k) =

√

2π

∞
X

n = −∞

√

2 π in the formula

cn δ(k − n).

Page 281: In Exercise 7.3.6.(a) add
Hint: You may wish to solve Exercise 7.3.12 first.
Page 289: In Exercise 7.4.10, change (a) and (b) to (i) and (ii)
Page 291: In second paragraph, delete sentence “We next present . . . equations.”
and then change “Finally, we discuss . . . ” to “We next discuss . . . ”
Add the following sentence at the end of the third paragraph:
The next section presents the Maximum Principle that rigorously justifies the entropic decay of temperature in a heated body and underlies much of the advanced
mathematical analysis of parabolic partial differential equations.
Page 310: In the second displayed formula after equation (8.63), insert factor of 12 :


x
√ .
u(t, x) = c1 + c2 erf
2 t

C6

Correction to: Introduction to Partial Differential Equations

Page 361: Change page header to
9.2 Self-Adjoint and Positive Definite Linear Functions
Page 363: Insert parenthetical comment at the end of the page:
(The case q(x) ≡ 0 can also be positive definite, when subject to suitable boundary
conditions, but is treated differently, in accordance with the weighted inner product
construction appearing in Example 9.23.)
Page 389: Change (9.131–132) to the following:
u(t, x) =
=

∞
X


k=1
∞
X
k=1



ck uk (t, x) + dk u
ek (t, x)





ck cos(ωk t) + dk sin(ωk t) vk (x) =

∞
X

k=1

(9.131)
rk cos(ωk t − δk ) vk ,

where (rk , δk ) are the polar coordinates of (ck , dk ):
ck = rk cos δk ,

dk = rk sin δk .

(9.132)

Page 391: Change (9.145) to the following:
0 = h h − 2 a ωk vk , vk i = h h , vk i − 2 a ωk k vk k2 , and hence a =

h h , vk i
, (9.145)
2 ωk k vk k2

Page 392: Correct sign errors in (9.149):
v? (x) =

sin k πx
k 2 π 2 c2 − ω 2

,

so that

u? (t, x) =

and in the last displayed equation:
z(0, x) = f (x) −

sin k πx
,
k 2 π 2 c2 − ω 2

cos ω t sin k πx
,
k 2 π 2 c2 − ω 2

(9.149)

∂z
(0, x) = g(x),
∂t

Page 393: In Exercise 9.5.14 change h(x) ≡ sin k πx to h(x) = sin k πx
Page 406: On line 4, change “. . . special case of (9.75).” to
“. . . special case of (9.75); see also Exercise 9.3.22.”
Page 411: On line 9 in paragraph beginning “The first step . . . ”, change
“vertexvertices” to “vertices”
Page 413: In the last equation in (10.32), change yk to yl :
ωlν (xi , yi ) = αlν + βlν xi + γlν yi = 0,
ωlν (xj , yj ) = αlν + βlν xj + γlν yj = 0,
ωlν (xl , yl ) = αlν + βlν xl + γlν yl = 1.

(10.32)

C7

Correction to: Introduction to Partial Differential Equations

Page 416: In equation (10.38), change

n
X

i=1

ci ∇ϕi

!2

to

n
X

i=1

Page 417: Correct last line in equation (10.45):

2

ci ∇ϕi

1 (yj − yl )(yl − yi ) + (xl − xj )(xi − xl )
(xi − xl ) · (xj − xl )
| ∆ν | = −
, i 6= j,
2
2
(∆ν )
2 | ∆ν |
1 (yj − yl )2 + (xl − xj )2
k x j − x l k2
ν
kii
=
|
∆
|
=
(10.45)
ν
2
(∆ν )2
2 | ∆ν |
(xi − xl ) · (xj − xl ) + (xl − xj ) · (xi − xj )
ν
ν
=
= − kij
− kil
.
2 ∆ν

ν
kij
=

Pages 418–9: Change first sentence in Example 10.7 to:
A metal plate has the shape of an oval running track, consisting of a square, with
side lengths 2 m, and two semi-circular disks glued onto opposite sides, as sketched in
Figure 10.9.
Page 426: In Exercise 10.3.16, change n = 2 in part (b) to n = 3 and change
n = 3 in part (c) to n = 4.
Page 434: In Exercise 10.4.3(c), change “. . . wave equation.” to
“. . . transport equation.”
Page 450: In Exercise 11.2.12, correct the boundary conditions:
u(t, 0, y) = u(t, π, y) = 0 = u(t, x, 0), u(t, x, π) = f (x),

0 < x, y < π,

t > 0.

Page 464: At end of line right before equation (11.90) add the following footnote:
†
If r is real but non-integral, and x < x0 , then one can replace x − x0 by x0 − x
or, alternatively, use absolute values throughout.
Pages 464–5: In line before equation (11.91) change “. . . order in the equation are
multiples . . . ” to “. . . order in equation (11.88) are multiples . . . ”.
In equation (11.91) and the subsequent formula, change all occurrences of s, t, r to
a, b, c, and add to line separating them:
a0 r (r − 1) + b0 r + c0 = 0,

(11.91)

where, referring back to (11.71),
a0 = a(x0 ),

b0 = b(x0 ),

c0 = c(x0 ),

Page 465: In case (iii), change r2 = r1 + k to r1 = r2 + k; change “smaller” to
“larger”, and change xr2 to (x − x0 )r2 in equation (11.93):

C8

Correction to: Introduction to Partial Differential Equations

(iii) Finally, if r1 = r2 +k, where k > 0 is a positive integer, then there is a nonzero
solution u
b(x) with a convergent Frobenius expansion corresponding to the larger index
r1 . One can construct a second independent solution of the form
u
e(x) = c log(x − x0 ) u
b(x) + v(x),

where

v(x) = (x − x0 )r2 +

∞
X

n=1

vn (x − x0 )n+r2
(11.93)

Page 466: Correct formulas after equation (11.96) as follows:








 
1 x
1 x
1 x
00
0
00
0
0
0
u +
+
u +u=v u
b +
+
u
b +u
b + v 2u
b +
+
u
b + v 00 u
b
x 2
x 2
x 2


 
1 x
−x2 /4
00
=e
v +
−
v0 .
x 2
If u is to be a solution, v 0 must satisfy a linear first-order ordinary differential equation:
v ′′ +

#

1 x
−
x
2

$

v ′ = 0,

and hence

v′ =

c x2 /4
e
,
x

(
v=c

2

ex /4
dx + b,
x

where c, b are arbitrary constants. We conclude that the general solution to the
original differential equation is
) (
*
2
2
ex /4
u
%(x) = v(x) u
!(x) = c
dx + b e− x /4 .
(11.97)
x
Page 471: In Figure 11.5, redraw the graphs of Y1 (x), Y2 (x), Y3 (x):

Y0 (x)

Y1 (x)

Y2 (x)

Y3 (x)

Figure 11.5.

Bessel functions of the second kind.

C9

Correction to: Introduction to Partial Differential Equations

Page 473: Change page header to
11.3 Series Solutions of Ordinary Differential Equations
]
Page 489: Add movie symbol
to Figure 11.10.

Page 495: Change sentence including (11.67) to the following:
In general, we define the relative vibrational frequencies to be the ratios between
the individual frequencies and the dominant, or smallest, one. Thus, the relative
vibrational frequencies of a circular drum are
ρm,n =

ωm,n
ζm,n
=
,
ω0,1
ζ0,1

where

ω0,1 =

c ζ0,1
c
≈ 2.4 .
R
R

(11.67)

Page 499: In Example 11.15, correct the equation two lines from the end :
ζ0,1 /ζ0,2 ≈ .43565
Page 500: In Exercise 11.6.41, switch the indices on ωi,j :
(a) ω0,4 , (b) ω2,4 , (c) ω4,2 , (d) ω3,3 , (e) ω5,1 .
Page 532: In displayed equation on line -3, change comma to semicolon in v(x; ξ)
Page 548: In equation (12.131), add t dependence to u0,0,n and u
b0,0,n , and correct
denominators in final expressions:
cos c n π t sin n π r
,
nπ r
sin c n π t sin n π r
u
b0,0,n (t, r, ϕ, θ) = sin(c n π t) S0 (n π r) =
,
nπ r
u0,0,n (t, r, ϕ, θ) = cos(c n π t) S0 (n π r) =

n = 1, 2, 3, . . . (. 12.131)

Page 553: In equation (12.145) and the next displayed equation, the limit is as t → 0:
lim Mc t [ f ] = M0 [ f ] = f (ø).

(12.145)

t→0

lim h u(t, ·) , f i = h u(0, ·) , f i = 0

t→0

for all functions

f,

Page 555: Replace the period in equation (12.151) by a comma, and replace the
following sentence by
where Mcxt [ g ] denotes the average of the initial velocity function g over the sphere
Scxt = { k ξ − x k = c t } of radius c t centered at the point x. Thus, the value of our
solution at position x and time t > 0 only depends upon the initial data a distance c t
away from the point x.
Page 577: Change page header to

B.2 Bases and Dimension

Page 592: In reference [89] change 2005 to 2006

C10

Correction to: Introduction to Partial Differential Equations

Page 603: Add entry: Franklin, J. 239
Page 605: Change page header to

Author Index

Page 607–636: Changes and additions to the index :
boundary value problem, Neumann 148, 238, 239, 245, 352, 387, 534
cap 556, 561
co-latitude 509
conformal mapping 256
connected 5, 17, 141, 245, 345, 359
connected, pathwise 5, 245
conservation law 15, 38, 46, 131, 201, 255, 256, 295, 304, 332, 337, 431, 437
conservation law, energy 61, 62, 151, 535
conservation law, momentum 61
continuous viii, 7, 63, 80, 82, 94, 102, 231
convergence, nonuniform 267
density, energy 61, 122
energy, conservation 61, 62, 151, 535
energy, total 61, 62, 151
Fredholm Alternative 240, 339, 350, 356, 390, 507, 527, 534
momentum 39, 61, 62, 286, 287
Neumann boundary value problem 148, 238, 239, 245, 352, 387, 534
pathwise connected 5, 245
rectangle 121, 156, 214, 248, 255, 312, 361, 374, 415, 445, 482, 488, 493, 496, 497, 499
spherical cap 556, 561
square 369, 385, 415, 418, 426, 444, 449, 479, 489, 492, 496, 499
total energy 61, 62, 151
total momentum 62

Appendix A

Complex Numbers

The purpose of this short appendix is to review the basics of complex numbers and complex
arithmetic, which are used throughout much of the text.
A complex
number is an expression of the form z = x + i y, where x, y ∈ R are real
√
and i = −1 is the imaginary unit. The set of all complex numbers is denoted by C. We
call x = Re z the real part of z and y = Im z the imaginary part of z = x + i y. (Note: The
imaginary part is the real number y, not i y.) A real number x is merely a complex number
with zero imaginary part, Im z = 0, and so we may regard R ⊂ C. Complex addition and
multiplication are based on simple adaptations of the rules of real arithmetic to include
the identity i 2 = −1, and so
(x + i y) + (u + i v) = (x + u) + i (y + v),
(x + i y) (u + i v) = (x u − y v) + i (x v + y u).

(A.1)

Complex numbers enjoy all the usual laws of real addition and multiplication, including
commutativity: z w = w z.
We can identify a complex number x + i y with a vector (x, y) ∈ R 2 in the real,
two-dimensional plane. For this reason, C is sometimes referred to as the complex plane.
(Although keep in mind that, as a complex vector space, C is only one-dimensional.) Based
on this identiﬁcation, we shall employ the standard terminology of planar vector calculus
— domain, curve, etc. — without alteration. Complex addition (A.1) corresponds to vector
addition, but the vector interpretation of complex multiplication is more obscure.
The complex conjugate of z = x + i y is z = x − i y. Note that Re z = Re z, while
Im z = − Im z. Geometrically, the complex conjugate of z is obtained by reﬂecting the
corresponding vector through the real axis, as illustrated in Figure A.1. In particular,
z = z if and only if z is real. In general,
z−z
z+z
,
Im z =
.
2
2i
Complex conjugation is compatible with complex arithmetic:
Re z =

z + w = z + w,

(A.2)

z w = z w.

In particular, the product of a complex number and its conjugate,
z z = (x + i y) (x − i y) = x2 + y 2 ,

(A.3)

is real and nonnegative. Its square root is known as the modulus or norm of the complex
number z = x + i y, and written

| z | = x2 + y 2 .
(A.4)
P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0, © Springer International Publishing Switzerland 2014

571

572

A Complex Numbers

z

y
r
θ
x

z
Complex numbers.

Figure A.1.

Note that | z | ≥ 0, with | z | = 0 if and only if z = 0. The modulus | z | generalizes the
absolute value of a real number and coincides with the standard Euclidean norm in the
(x, y)–plane. This implies the validity of the triangle inequality
| z + w | ≤ | z | + | w |.

(A.5)

Equation (A.3) can be rewritten in terms of the modulus as
z z = | z |2 .

(A.6)

Rearranging the factors, we deduce the formula for the reciprocal of a nonzero complex
number:
z
1
=
,
z
| z |2

z = 0,

or, equivalently,

x − iy
1
= 2
.
x+ iy
x + y2

(A.7)

The general formula for complex division,
wz
w
=
z
| z |2

or

(x u + y v) + i (x v − y u)
u+ iv
=
,
x + iy
x2 + y 2

(A.8)

is an immediate consequence.
The modulus of a complex number,
r = |z| =


x2 + y 2 ,

is one component of its polar coordinate representation
x = r cos θ,

y = r sin θ

or

z = r(cos θ + i sin θ).

(A.9)

The polar angle θ, which measures the angle that the line connecting z to the origin makes
with the horizontal axis, is known as the phase, and written
θ = ph z.

(A.10)

As such, the phase is deﬁned only up to an integer multiple of 2 π. The unique principal
value of the phase is restricted to − π < ph z ≤ π. A more common term for the polar

A Complex Numbers

573

angle is the argument of z, written arg z = ph z. However, in conformity with [85, 86], we
prefer to use “phase” here, in part to avoid confusion with the argument z of a function
f (z).
Euler’s celebrated formula for the complex exponential,
e i θ = cos θ + i sin θ,

(A.11)

can be used to compactly rewrite the polar form (A.9) of a complex number as
z = r e i θ,

where

r = | z |,

θ = ph z.

(A.12)

Consequently, the complex logarithm has the form
log z = log(r e i θ ) = log r + log e i θ = log r + i θ = log | z | + i ph z.

(A.13)

More generally, the complex exponential is given by
ez = ex cos y + i ex sin y,

for

z = x + i y.

(A.14)

We note that the modulus and phase of a product of complex numbers can be readily
computed:
| z w | = | z | | w |,
ph(z w) = ph z + ph w,
(A.15)
the latter formula requiring that we allow multiply valued phases; the formula does not
hold as stated for all z, w when the principal value of the phase is used. Similarly, the
modulus and phase of the reciprocal of a nonzero complex number are
 
1
1
1
=
,
ph
= − ph z.
(A.16)
z
|z|
z
On the other hand, complex conjugation preserves the modulus, but negates the phase:
| z | = | z |,

ph z = − ph z.

(A.17)

The latter formula is not valid for the principal value of the phase when z lies on the
negative real axis.

Appendix B

Linear Algebra

In this appendix, we collect basic results and deﬁnitions from linear algebra that are used
in our study of partial diﬀerential equations. The reader is referred to [89] for the proofs
and further details.

B.1 Vector Spaces and Subspaces
Vector spaces and their ancillary structures provide the common language of linear algebra. The basic deﬁnition is modeled on the prototypical ﬁnite-dimensional example: the
Euclidean space R n , which is the set of all real (column) vectors with n entries, equipped
with the operations of vector addition and scalar multiplication. More generally:
Deﬁnition B.1. A (real) vector space is a set V equipped with two operations:
(i ) Addition: adding any pair of elements v, w ∈ V produces another vector v + w ∈ V .
(ii ) Scalar Multiplication: multiplying an element v ∈ V by a scalar c ∈ R produces a
vector c v ∈ V .
These are subject to the following axioms: for all u, v, w ∈ V and all scalars c, d ∈ R,
(a) Commutativity of Addition: v + w = w + v.
(b) Associativity of Addition: u + (v + w) = (u + v) + w.
(c) Additive Identity: There is a zero element 0 ∈ V satisfying v + 0 = v = 0 + v.
(d) Additive Inverse: For each v ∈ V there is an element − v ∈ V such that
v + (− v) = 0 = (− v) + v.
(e) Distributivity: (c + d) v = (c v) + (d v), and c (v + w) = (c v) + (c w).
(f ) Associativity of Scalar Multiplication: c (d v) = (c d) v.
(g) Unit for Scalar Multiplication: the scalar 1 ∈ R satisﬁes 1 v = v.
Complex vector spaces are deﬁned in an identical manner, the only diﬀerence being
that the scalars are allowed to be complex numbers. In this case, the prototype is the space
C n consisting of column vectors with n complex entries.
While ﬁnite-dimensional vector spaces play a signiﬁcant role in the study of partial
diﬀerential equations, particularly in the design of numerical solution schemes, for us the
more important examples are inﬁnite-dimensional vector spaces whose elements (“vectors”)
are functions. The main example is the following:
P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0, © Springer International Publishing Switzerland 2014

575

576

B Linear Algebra

Example B.2. Let I ⊂ R be an interval. The function space F = F (I), whose
elements are all real-valued functions f (x) deﬁned for x ∈ I, has the structure of a vector
space. Addition of functions in F is deﬁned in the usual manner: (f + g)(x) = f (x) + g(x)
for all x ∈ I. Multiplication by scalars c ∈ R is the same as multiplication by constants,
(c f )(x) = c f (x). The zero element is the constant function that is identically 0 for all
x ∈ I. With these operations, all the vector space axioms listed in Deﬁnition B.1 are valid,
and hence F (I) is a real vector space.
More generally, if Ω ⊂ R n is any subset of n-dimensional Euclidean space, the function
space F (Ω) is deﬁned as the set of all real-valued functions f (x1 , . . . , xn ) deﬁned for all
x = (x1 , . . . , xn ) ∈ Ω. Addition and scalar (constant) multiplication of functions are
deﬁned in the same manner.
A subspace of a vector space V is a subset W ⊂ V that is a vector space in its own
right. In particular, a subspace W must contain the zero element of V .
Proposition B.3. A nonempty subset W ⊂ V of a vector space is a subspace if and
only if
(a) for every v, w ∈ W , the sum v + w ∈ W , and
(b) for every v ∈ W and every c ∈ R, the scalar product c v ∈ W .
For example, a complete list of subspaces of V = R 3 is (i ) the origin { 0 }; (ii ) every
line through the origin; (iii ) every plane through the origin; (iv ) all of R 3 .
Example B.4. Here are some examples of subspaces of the function space F (I).
(a) The space P (n) of polynomials of degree ≤ n.
(b) The space C0 (I) of all continuous functions on the interval I.
(c) The space Cn (I) consisting of all functions f (x) that have n continuous derivatives
f  (x), f  (x), . . . , f (n) (x) on† I.

(d) The space C∞ (I) = n≥0 Cn (I) of inﬁnitely diﬀerentiable, or smooth, functions is
also a subspace.
(e) The space A(I) of analytic functions. Recall that a function f (x) is called analytic
at a point a if it is smooth, and, moreover, its Taylor series


f (a) + f (a) (x − a) + 12 f  (a) (x − a)2 +

··· =

∞

f (n) (a)
n=0

n!

(x − a)n

(B.1)

converges to f (x) for all x suﬃciently close to a. (The series is not required to
converge on the entire interval I.) Not every smooth function is analytic, and so
A(I)  C∞ (I); see Exercise 11.3.21 for an explicit example.

B.2 Bases and Dimension
Deﬁnition B.5. Let v1 , . . . , vk belong to a vector space V . A sum of the form
k

c 1 v1 + c 2 v2 + · · · + c k vk =
c i vi ,
(B.2)
i=1

†

We use one-sided derivatives at any endpoint that belongs to the interval.

B.2 Bases and Dimension

577

where the coeﬃcients c1 , c2 , . . . , ck are any scalars, is known as a linear combination of the
elements v1 , . . . , vk . Their span is the subspace W = span {v1 , . . . , vk } ⊂ V consisting of
all possible linear combinations.
Deﬁnition B.6. The elements v1 , . . . , vk ∈ V are called linearly dependent if there
exist scalars c1 , . . . , ck , not all zero, such that
c1 v1 + · · · + ck vk = 0.

(B.3)

Elements that are not linearly dependent are called linearly independent.
In particular, a collection of functions f1 (x), . . . , fn (x) is linearly dependent if and
only if there exist constants c1 , . . . , cn , not all zero, such that the linear combination
c1 f1 (x) + · · · + cn fn (x) ≡ 0

(B.4)

is identically zero. Conversely, if the only choice of constants for which (B.4) holds is
c1 = · · · = cn = 0, then the functions are linearly independent.
Deﬁnition B.7. A basis of a vector space V is a ﬁnite collection of elements
v1 , . . . , vn ∈ V that (a) spans V , and (b) is linearly independent.
The simplest example is the standard basis of R n , consisting of the n vectors
⎛ ⎞
⎛ ⎞
⎛ ⎞
1
0
0
⎜0⎟
⎜1⎟
⎜0⎟
⎜ ⎟
⎜ ⎟
⎜ ⎟
⎜0⎟
⎜0⎟
⎜0⎟
⎜
⎟
⎜
⎟
⎟
e1 = ⎜ .. ⎟,
e2 = ⎜ .. ⎟,
... ,
en = ⎜
⎜ ... ⎟,
⎜.⎟
⎜.⎟
⎜ ⎟
⎝0⎠
⎝0⎠
⎝0⎠
0

0

(B.5)

1

so that ei is the vector with 1 in the ith slot and 0’s elsewhere. However, there are many
other bases of R n ; indeed, any n linearly independent vectors v1 , . . . , vn ∈ R n form a basis.
Lemma B.8. The elements v1 , . . . , vn form a basis of V if and only if every v ∈ V
can be written uniquely as a linear combination of the basis elements:
v = c 1 v1 + · · · + c n vn =

n

i=1

c i vi .

(B.6)

The coeﬃcients (c1 , . . . , cn ) are called the coordinates of the vector v with respect to the
given basis.
Theorem B.9. Suppose the vector space V has a basis v1 , . . . , vn . Then every other
basis of V has the same number of elements in it. This number is called the dimension of
V , and written dim V = n.
On the other hand, if the vector space contains inﬁnitely many linearly independent
elements, then it does not have a basis in the sense of Deﬁnition B.7, and is thus inﬁnitedimensional . All of the function spaces and subspaces listed above are inﬁnite-dimensional
vector spaces. An example of a ﬁnite-dimensional function space is the space P (n) ⊂ F (R)
consisting of all polynomials p(x) = a0 + a1 x + · · · + an xn of degree ≤ n. The monomials
1, x, x2 , . . . , xn form a basis, and hence P (n) has dimension n + 1. (On the other hand, the
vector space containing all polynomials is inﬁnite-dimensional.)

578

B Linear Algebra

B.3 Inner Products and Norms
The dot product on Euclidean space R n plays an essential role in geometry, analysis, and
mechanics. Its basic properties inspire the general deﬁnition of an inner product on a
vector space.
Deﬁnition B.10. An inner product on the real vector space V is a pairing that takes
two elements v, w ∈ V and produces a real number  v , w  ∈ R, subject to the following
three axioms for all u, v, w ∈ V , and scalars c, d ∈ R.
(i ) Bilinearity:
 c u + d v , w  = c  u , w  + d  v , w ,
(B.7)
 u , c v + d w  = c  u , v  + d  u , w .
(ii ) Symmetry:
 v , w  =  w , v .
(iii ) Positivity:
v,v > 0

whenever

v = 0,

(B.8)
while

 0 , 0  = 0.

(B.9)

Given an inner product, the associated norm of an element v ∈ V is deﬁned as the
positive square root of its inner product with itself:

v = v,v.
(B.10)
Bilinearity of the inner product implies that
cv = |c|v
for any scalar c. The positivity axiom implies that  v  ≥ 0 is real and nonnegative,
and equals 0 if and only if v = 0 is the zero element. A vector space norm induces a
notion of distance between elements v, w ∈ V , with dist(v, w) =  v − w . In particular,
dist(v, w) = 0 if and only if v = w.
Example B.11. The most familiar example of an inner product is the dot product †
 v , w  = v · w = vT w = v1 w1 + v2 w2 + · · · + vn wn
on the Euclidean space R n . The associated Euclidean norm

√
v =
v·v =
v12 + v22 + · · · + vn2

(B.11)

(B.12)

conforms to our usual notion of distance between points in Euclidean space.
To ﬁnd the most general inner product on R n , we need to introduce the important
class of positive deﬁnite matrices.
Deﬁnition B.12.
positivity condition

An n × n matrix C is called positive deﬁnite if it satisﬁes the
vT C v > 0

for all

0 = v ∈ Rn.

(B.13)

We will sometimes write C > 0 to mean that C is a positive deﬁnite matrix.
†

The elements v ∈ R n are to be regarded as column vectors, while the transpose, written
v , is the corresponding row vector.
T

B.3 Inner Products and Norms

579

Warning:The condition
C > 0 does not mean that

 all the
 entries of C are positive.
3 −1
1 2
For example,
is positive deﬁnite, whereas
is not.
−1 1
2 1
Many authors, including [89], require that a positive deﬁnite matrix also be symmetric.
We will not impose this condition here a priori. However, most of the positive deﬁnite
matrices we will encounter in applications will be symmetric (or, more generally, selfadjoint — as in Example 9.15). For a symmetric matrix, the most useful test for positive
deﬁniteness is to perform Gaussian Elimination on the matrix C, which is positive deﬁnite
if and only if no row interchanges are needed, and all the pivots are positive, [89].
Proposition B.13. Every inner product on R n is given by
 v , w  = vT C w

for

v, w ∈ R n ,

(B.14)

where C > 0 is a symmetric positive deﬁnite matrix.
The next example is of particular signiﬁcance in Fourier analysis and partial diﬀerential
equations.
Example B.14. Let [ a, b ] ⊂ R be a bounded closed interval. The integral
 b
f ,g =
f (x) g(x) dx

(B.15)

a

deﬁnes an inner product on the space C0 [ a, b ] of continuous functions. The associated
norm

 b
f  =
f (x)2 dx
(B.16)
a

is known as the L2 norm of the function f over the interval [ a, b ]. The positivity of the
norm:  f  > 0 for f = 0, follows from the fact that the only continuous nonnegative
 b
function g(x) ≥ 0 that satisﬁes
g(x) dx = 0 is the zero function g(x) ≡ 0. Extending
a

this construction to spaces containing discontinuous functions is trickier, since there are
discontinuous functions that are not identically zero, but nevertheless have zero norm
integral. An example is a function that is zero except at a single point. Further discussion
can be found in Section 3.5.
The two most important inequalities in mathematical analysis apply to any inner
product space.
Theorem B.15.
inequalities

Every inner product satisﬁes the Cauchy–Schwarz and triangle

|  v , w  | ≤  v   w ,

 v + w  ≤  v  +  w ,

for all

v, w ∈ V. (B.17)

Equality holds if and only if v and w are parallel, i.e., scalar multiples of each other.
Proof : We begin with the Cauchy–Schwarz inequality: |  v , w  | ≤  v   w . The
case w = 0 is trivial, and so we assume w = 0. Let t ∈ R be an arbitrary scalar. Using
the three inner product axioms, we have
0 ≤  v + t w 2 =  v + t w , v + t w  =  v 2 + 2 t  v , w  + t2  w 2 ,

(B.18)

580

B Linear Algebra

with equality holding if and only if v = − t w, which requires v and w to be parallel vectors.
We ﬁx v and w, and consider the right-hand side of (B.18) as a quadratic function of t.
Its minimum value occurs when t =  w −2  v , w . Substituting this value into (B.18),
we obtain
 v , w 2
 v , w 2
 v , w 2
2
+
=

v

−
,
0 ≤  v 2 − 2
 w 2
 w 2
 w 2
and hence  v , w 2 ≤  v 2  w 2 , which, upon taking the square root, establishes the
Cauchy–Schwarz inequality. Again, as noted above, equality holds if and only if v and w
are parallel.
To establish the triangle inequality, we compute
 v + w 2 =  v + w , v + w  =  v 2 + 2  v , w  +  w 2
≤  v 2 + 2  v   w  +  w 2 =  v  +  w 

2

,

where the middle inequality follows from the Cauchy–Schwarz inequality (which clearly
also holds if the absolute value is removed.) Taking square roots of both sides completes
the proof.
Q.E.D.
We will also have occasion to use inner products on complex vector spaces. To ensure
that the associated norm remains positive, the real deﬁnition must be modiﬁed. The
complex conjugate of a complex scalar c = a + i b, with a, b ∈ R, will be indicated by an
overbar: c = a − i b. When dealing with a complex inner product space, one must pay
careful attention to complex conjugation.
Deﬁnition B.16. An inner product on the complex vector space V is a pairing that
takes two vectors v, w ∈ V and produces a complex number  v , w  ∈ C, subject to the
following requirements, for u, v, w ∈ V , and c, d ∈ C:
(i ) Sesquilinearity:
 c u + d v , w  = c  u , w  + d  v , w ,
(B.19)
 u , c v + d w  = c  u , v  + d  u , w .
(ii ) Conjugate Symmetry:
 v , w  =  w , v .

(B.20)

(iii ) Positivity:
 v 2 =  v , v  ≥ 0,

and

v,v = 0

if and only if

v = 0.

(B.21)

Example B.17. The simplest example is the Hermitian
⎛ dot
⎞ product⎛
⎞
w1
z1
⎜ z2 ⎟
⎜ w2 ⎟
⎜ ⎟
⎜
⎟
z · w = zT w = z1 w 1 + z2 w 2 + · · · + zn wn , for z = ⎜ .. ⎟, w = ⎜ .. ⎟, (B.22)
⎝ . ⎠
⎝ . ⎠
zn

between complex vectors v, w ∈ C n .

wn

Example B.18. Let C0 [ − π, π ] denote the complex vector space consisting of all
complex-valued continuous functions f (x) = u(x) + i v(x) depending on the real variable
−π ≤ x ≤ π. The L2 Hermitian inner product on C0 [ − π, π ] is deﬁned as
 π
f ,g =
f (x) g(x) dx ,
(B.23)
−π

B.4 Orthogonality

581

i.e., the integral of f times the complex conjugate of g, with corresponding norm


π
π 

2
f  =
| f (x) | dx =
u(x)2 + v(x)2 dx .
−π

(B.24)

−π

Inner products on complex vector spaces also satisfy the Cauchy–Schwarz and triangle
inequalities (B.17). The proof is left as an exercise for the reader; see [89; Exercise 3.6.46].

B.4 Orthogonality
Deﬁnition B.19. Two elements v, w ∈ V of an inner product space V are called
orthogonal if their inner product vanishes:  v , w  = 0.
For ordinary Euclidean space equipped with the dot product, two vectors are orthogonal if and only if they are perpendicular, i.e., meet at a right angle.
Deﬁnition B.20. A basis u1 , . . . , un of an inner product space V is called orthogonal
if  ui , uj  = 0 for all i = j. The basis is called orthonormal if, in addition, each vector
has unit length:  ui  = 1, for all i = 1, . . . , n.
For example, the standard basis vectors (B.5) form an orthonormal basis of R n with
respect to the dot product, but they are not orthonormal for any other inner product
thereon.
Theorem B.21. If v1 , . . . , vn form an orthogonal basis, then the corresponding
coordinates of a vector
v = a1 v1 + · · · + an vn

are given by

ai =

 v , vi 
.
 vi  2

(B.25)

.

(B.26)

Moreover, the vector’s norm can be computed using the formula
n


v =
2

a2i  vi 2 =

i=1

2
n 

v,v 
i

i=1

 vi 

Proof : We compute the inner product of (B.25) with one of the basis vectors. By
orthogonality,

 n
n


aj vj , vi =
aj  uj , ui  = ai  vi 2 .
 v , vi  =
j =1

j =1

To prove formula (B.26), we similarly expand
 v 2 =  v , v  =

n


ai aj  vi , vj  =

i,j = 1

n


a2i  vi 2 .

Q.E .D.

i=1

In the case of an orthonormal basis, the formulas (B.25–26) simplify to
v = c1 u1 + · · · + cn un ,

where

ci =  v , ui ,

 v  = c21 + · · · + c2n .

(B.27)

582

B Linear Algebra

Example B.22. A particularly important orthogonal basis is provided by the following vectors lying in C n :
ωk = 1, ζ k , ζ 2 k , ζ 3 k , . . . , ζ (n−1) k

T

= 1, e2 k π i /n , e4 k π i /n , . . . , e2(n−1) k π i /n

T

k = 0, . . . , n − 1,

(B.28)

,

where
ζ = e2 π i /n .

(B.29)

Orthogonality relies on the fact that its powers, ζ k = e2 k π i /n , k = 0, . . . , n − 1, are the
complex roots of the elementary polynomial
z n − 1 = (z − 1)(1 + z + z 2 + · · · + z n−1 ),
while

(B.30)

ζ = e−2 π i /n = ζ −1 .

Since when 0 < k ≤ n − 1, the complex number ζ k = 1 is a root of the polynomial (B.30),
it must also be a root of the second factor. This implies that

n,
k ≡ 0 mod n,
k
2k
3k
(n−1) k
1 +ζ +ζ +ζ + ··· +ζ
=
0,
k ≡ 0 mod n,
where the former case k ≡ 0 mod n follows by direct substitution of ζ k = 1. Thus, the
Hermitian inner products of the vectors (B.28) equal
 ωk , ωl  =

n−1

j =0

ζ

jk

ζ

jl

=

n−1



ζ

j (k−l)

j =0

=

n,

k = l,

0,

k = l,

(B.31)

provided 0 ≤ k, l ≤ n − 1, thereby establishing orthogonality. These vectors are the discrete analogues of the orthogonal complex exponential functions that are used to construct
complex Fourier series. They are the basis of the discrete Fourier transform, [89; §5.7],
and their orthogonality is the key to modern signal processing.

B.5 Eigenvalues and Eigenvectors
The eigenvalues and eigenvectors of a matrix ﬁrst appear when solving linear systems
of ordinary diﬀerential equations. But their essential importance extends across all of
mathematics and its manifold applications. Extensions of the eigenvalue method to linear
operators on function spaces are critical to the analysis of partial diﬀerential equations.
Deﬁnition B.23. Let A be an n × n matrix. A scalar λ is called an eigenvalue of A
if there is a nonzero vector v = 0, called an associated eigenvector , such that
A v = λ v.

(B.32)

In particular, a matrix has λ = 0 as an eigenvalue if and only if it has a null eigenvector
v = 0, satisfying A v = 0, and hence is a singular (non-invertible) matrix, with vanishing
determinant: det A = 0. An eigenvalue is called simple if it admits only one linearly
independent eigenvalue; more generally, the multiplicity of an eigenvalue is deﬁned as the

B.6 Linear Iteration

583

dimension of the eigenspace consisting of all solutions to the eigenequation (B.32), including
0. Thus, a simple eigenvalue has multiplicity 1.
Even if A is a real matrix, we must allow the possibility of complex eigenvectors.
Matrices with a “complete” set of eigenvectors are the most common, and also the easiest
to deal with.
Deﬁnition B.24. An n × n real or complex matrix A is called complete if there
exists a basis of C n consisting of its (complex) eigenvectors.
It is not hard to show that eigenvectors corresponding to diﬀerent eigenvalues are
necessarily linearly independent. This means that matrices with all distinct (and hence
simple) eigenvalues are necessarily complete:
Proposition B.25. Any n × n matrix with n distinct eigenvalues is complete.
Unfortunately,
not all matrices with repeated

  eigenvalues
  are complete. For instance,
1 0
1
0
is complete, since, for instance,
and
form an eigenvector basis of
0 1
0
1


 
1 1
1
2
C , whereas
is not, since it has only one independent eigenvector, namely
.
0 1
0
Incomplete matrices are much more challenging to deal with, both theoretically and numerically. Fortunately, we can safely ignore the incomplete cases in this text.
The most common way for orthogonal bases to arise is as eigenvector bases of symmetric matrices. (Orthogonality is with respect to the standard dot product on R n .) The
extension of this result to “self-adjoint” operators on function space forms the foundation
of Fourier analysis and its generalizations.


Theorem B.26. Let A = AT be a real symmetric n × n matrix. Then
(a) All the eigenvalues of A are real.
(b) Eigenvectors corresponding to distinct eigenvalues are orthogonal.
(c) There is an orthonormal basis of R n consisting of n eigenvectors of A.
Let us demonstrate orthogonality, leaving the remaining steps in the proof to [89;
Theorem 8.20]. If
A v = λ v,
A w = μ w,
where λ = μ are distinct real eigenvalues, then, by symmetry of A,
λ v · w = (A v) · w = (A v)T w = vT A w = v · (A w) = v · (μ w) = μ v · w,
and hence
(λ − μ) v · w = 0.
Since λ = μ, this implies that the eigenvectors v, w are necessarily orthogonal.

B.6 Linear Iteration
For numerical applications, we will require some basic results on iteration of linear systems.
Consider ﬁrst a homogeneous linear iterative system of the form
u(k+1) = A u(k) ,

u(0) = u0 ,

(B.33)

584

B Linear Algebra

in which A is an n × n matrix and u0 ∈ R n or C n . The solution to such a system is
evidently obtained by repeatedly multiplying the initial vector u0 by the matrix A, and so
u(k) = Ak u0 .

(B.34)

Deﬁnition B.27. A matrix A is called convergent if every solution to the homogeneous linear iterative system (B.33) tends to zero in the limit: u(k) → 0 as k → ∞.
Equivalently, A is convergent if and only if its powers converge to the zero matrix: Ak → O
as k → ∞.
The solution formula (B.34), while elementary, is not particularly enlightening. An
alternative approach is to recognize that if λj is an eigenvalue of A and vj a corresponding
eigenvector, then
(k)
(B.35)
uj = λkj vj
is a solution, since
(k)

A uj

(k+1)

= λkj A vj = λk+1
vj = u j
j

.

Moreover, linear combinations of such eigensolutions are also solutions. In particular, if A
is complete, then we can write down the general solution to (B.33) as a linear combination
of the independent eigensolutions:
u(k) = c1 λk1 v1 + c2 λk2 v2 + · · · + cn λkn vn ,

(B.36)

where {v1 , . . . , vn } is the eigenvector basis. The coeﬃcients c1 , . . . , cn are uniquely determined by the initial conditions,
u(0) = c1 v1 + c2 v2 + · · · + cn vn = u0 ,
which relies on the fact that the eigenvectors v1 , . . . , vn form a basis. Now, A is convergent
if and only if all solutions u(k) → 0. The individual eigensolution (B.35) goes to zero if and
only if its associated eigenvalue is strictly less than 1 in modulus: | λj | < 1. This proves
the following result for complete matrices. The proof in the incomplete case relies on the
Jordan canonical form, [89; Chapter 10].
Theorem B.28. The matrix A is convergent if and only if all its eigenvalues satisfy
| λ | < 1.
Deﬁnition B.29. The spectral radius of a matrix A is deﬁned as the maximal modulus of all of its real and complex eigenvalues: ρ(A) = max { | λ1 |, . . . , | λk | }.
Corollary B.30. The matrix A is convergent if and only if ρ(A) < 1.
Indeed, the spectral radius essentially governs the rate of convergence of the iterative
system — the closer it is to 0, the faster the convergence rate.
Next, consider the inhomogeneous linear iterative system
v(k+1) = A v(k) + b,

v(0) = v0 ,

(B.37)

where b a ﬁxed vector. A ﬁxed point is a vector v that satisﬁes
v = A v + b,

or, equivalently,

( I − A)v = b,

(B.38)

where I is the identity matrix of the same size as A. Thus, if 1 is not an eigenvalue of
A (which cannot happen when A is convergent), then I − A is nonsingular, and so the
iterative system has a unique ﬁxed point.

B.7 Linear Functions and Systems

585

Theorem B.31. Assume that 1 is not an eigenvalue of A. Then all solutions to
(B.37) converge to the ﬁxed point, v(k) → v as k → ∞ if and only if A is a convergent
matrix.
Proof : Let u(k) = v(k) − v , so that v(k) → v if and only if u(k) → 0. Now,
u(k+1) = v(k+1) − v = (A v(k) + b) − (A v + b) = A(v(k) − v ) = A u(k) ,
and hence u(k) solves the homogeneous version (B.33). Thus, the result is an immediate
consequence of Deﬁnition B.27.
Q.E.D.

B.7 Linear Functions and Systems
The most basic structural features of linear diﬀerential equations, both ordinary and
partial, linear boundary value problems, etc., are founded on the concept of a linear function
between vector spaces.
Deﬁnition B.32. Let U and V be real vector spaces. A function L: U → V is called
linear if it obeys two basic rules:
L[ u + v ] = L[ u ] + L[ v ],

L[ c u ] = c L[ u ],

(B.39)

for all u, v ∈ U and all scalars c.
We will refer to U as the domain space of the function L, and V as the target space.
The latter is to emphasize the fact that the range of L, namely
rng L = { v ∈ V | v = L[ u ] for some u ∈ U } ,

(B.40)

may very well be a proper subspace of the target space V .
Theorem B.33. Every linear function L: R n → R m is given by matrix multiplication, L[ v ] = A v, where A is an m × n matrix.
Proving that matrix multiplication satisﬁes the linearity conditions (B.39) is easy. The
converse is established by seeing what the linear function does to the basis vectors of R n ;
see [89; Theorem 7.5].
Corollary B.34. Every linear function L: R n → R is given by taking the dot product
with a ﬁxed vector a ∈ R n :
L[ v ] = a · v.
(B.41)
When U is a function space, a linear function is also referred to as a linear operator
in order to avoid confusion with the elements of U . If the target space V = R, then the
term linear functional is also often used for L: U → R.
Here are some representative examples that arise in applications.
Example B.35. (a) Evaluation of a function at a point, namely L[ f ] = f (x0 ),
deﬁnes a linear operator L: C0 [ a, b ] → R.
(b) Integration,
 b
f (x) dx,
(B.42)
I[ f ] =
a

also deﬁnes a linear functional I: C [ a, b ] → R.
0

586

B Linear Algebra

(c) The operation Ma [ f (x) ] = a(x) f (x) of multiplication by a continuous function
a deﬁnes a linear operator Ma : C0 [ a, b ] → C0 [ a, b ].
(d) Diﬀerentiation of functions, D[ f ] = f  , serves to deﬁne a linear operator
1
D: C [ a, b ] → C0 [ a, b ].
(e) A general linear ordinary diﬀerential operator of order n,
L = an (x) Dn + an−1 (x) Dn−1 + · · · + a1 (x) D + a0 (x),

(B.43)

is obtained by summing such operators. If the coeﬃcient functions a0 (x), . . . , an (x) are
continuous, then
L[ u ] = an (x)

dn u
dn−1 u
du
+ a0 (x)u
+
a
(x)
+ · · · + a1 (x)
n−1
n
n−1
dx
dx
dx

(B.44)

deﬁnes a linear operator from Cn [ a, b ] to C0 [ a, b ].
Linear partial diﬀerential equations are based on linear partial diﬀerential operators,
which are discussed in Chapter 1. They are particular examples of the general concept of
a linear system.
Deﬁnition B.36. A linear system is an equation of the form
L[ u ] = f ,

(B.45)

in which L: U → V is a linear function, f ∈ V , while the desired solution u ∈ U . The
system is homogeneous if f = 0; otherwise, it is called inhomogeneous.
Note that, by the deﬁnition (B.40) of the range of L, the linear system (B.45) will
have a solution if and only if f ∈ rng L. In particular, a homogeneous linear system always
has a solution, namely u = 0. However, it may possibly admit other, nonzero, solutions.
Theorem B.37. If z1 , . . . , zk are all solutions to the same homogeneous linear system
L[ z ] = 0,

(B.46)

then any linear combination c1 z1 + · · · + ck zk , for any scalars c1 , . . . , ck , is also a solution.
In other words, the set of solutions to a homogeneous linear system (B.46) forms a
subspace of the domain space U , known as the kernel of the linear function L:
ker L = { z ∈ U | L[ z ] = 0 } .

(B.47)

Theorem B.38. If the inhomogeneous linear system L[ u ] = f has a particular
solution u , which requires f ∈ rng L, then the general solution is u = u + z, where
z ∈ ker L is any solution to the corresponding homogeneous system L[ z ] = 0.
The Superposition Principle for inhomogeneous linear systems allows us to combine
solutions corresponding to diﬀerent right-hand sides.
Theorem B.39. Suppose that for each i = 1, . . . , k, we know a particular solution
ui to the inhomogeneous linear system L[ u ] = f i for some f i ∈ rng L. Then, given scalars
c1 , . . . , ck , a particular solution to the combined inhomogeneous system
L[ u ] = c1 f 1 + · · · + ck f k

(B.48)

B.7 Linear Functions and Systems

587

is the corresponding linear combination
u = c1 u1 + · · · + ck uk

(B.49)

of particular solutions. The general solution to the inhomogeneous system (B.48) is
u = u + z = c1 u1 + · · · + ck uk + z,

(B.50)

where z ∈ ker L is an arbitrary solution to the associated homogeneous system L[ z ] = 0.

References

[ 1 ] Abdulloev, K. O., Bogolubsky, I. L., and Makhankov, V. G., One more example of
inelastic soliton interaction, Phys. Lett. A 56 (1976), 427–428.
[ 2 ] Ablowitz, M. J., and Clarkson, P. A., Solitons, Nonlinear Evolution Equations and
the Inverse Scattering Transform, L.M.S. Lecture Notes in Math., vol. 149,
Cambridge University Press, Cambridge, 1991.
[ 3 ] Abraham, R., Marsden, J. E., and Ratiu, T., Manifolds, Tensor Analysis, and
Applications, Springer–Verlag, New York, 1988.
[ 4 ] Airy, G. B., On the intensity of light in the neighborhood of a caustic, Trans.
Cambridge Phil. Soc. 6 (1838), 379–402.
[ 5 ] Aki, K., and Richards, P. G., Quantitative Seismology, W. H. Freeman, San
Francisco, 1980.
[ 6 ] Ames, W. F., Numerical Methods for Partial Diﬀerential Equations, 3rd ed.,
Academic Press, New York, 1992.
[ 7 ] Antman, S. S., Nonlinear Problems of Elasticity, Appl. Math. Sci., vol. 107,
Springer–Verlag, New York, 1995.
[ 8 ] Apostol, T. M., Calculus, Blaisdell Publishing Co., Waltham, Mass., 1967–1969.
[ 9 ] Apostol, T. M., Introduction to Analytic Number Theory, Springer–Verlag, New
York, 1976.
[ 10 ] Atkinson, K., and Han, W., Spherical Harmonics and Approximations on the Unit
Sphere: An Introduction, Lecture Notes in Math., vol. 2044, Springer, Berlin,
2012.
[ 11 ] Bank, S. B., and Kaufman, R. P., A note on Hölder’s theorem concerning the
gamma function, Math. Ann. 232 (1978), 115–120.
[ 12 ] Batchelor, G. K., An Introduction to Fluid Dynamics, Cambridge University Press,
Cambridge, 1967.
[ 13 ] Bateman, H., Some recent researches on the motion of ﬂuids, Monthly Weather
Rev. 43 (1915), 63–170.
[ 14 ] Benjamin, T. B., Bona, J. L., and Mahony, J. J., Model equations for long waves
in nonlinear dispersive systems, Phil. Trans. Roy. Soc. London A 272 (1972),
47–78.
[ 15 ] Berest, Y., and Winternitz, P., Huygens’ principle and separation of variables,
Rev. Math. Phys. 12 (2000), 159–180.
[ 16 ] Berry, M. V., Marzoli, I., and Schleich, W., Quantum carpets, carpets of light,
Physics World 14(6) (2001), 39–44.
[ 17 ] Birkhoﬀ, G., Hydrodynamics — A Study in Logic, Fact and Similitude, 2nd ed.,
Princeton University Press, Princeton, 1960.
[ 18 ] Birkhoﬀ, G., and Rota, G.–C., Ordinary Diﬀerential Equations, Blaisdell Publ. Co.,
Waltham, Mass., 1962.

P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0, © Springer International Publishing Switzerland 2014

589

590

References

[ 19 ] Black, F., and Scholes, M., The pricing of options and corporate liabilities, J.
Political Economy 81 (1973), 637–654.
[ 20 ] Blanchard, P., Devaney, R. L., and Hall, G. R., Diﬀerential Equations, Brooks–Cole
Publ. Co., Paciﬁc Grove, Calif., 1998.
[ 21 ] Boussinesq, J., Théorie des ondes et des remous qui se propagent le long d’un
canal rectangulaire horizontal, en communiquant au liquide contenu dans ce
canal des vitesses sensiblement pareilles de la surface au fond, J. Math. Pures
Appl. 17 (2) (1872), 55–108.
[ 22 ] Boussinesq, J., Essai sur la théorie des eaux courants, Mém. Acad. Sci. Inst. Nat.
France 23 (1) (1877), 1–680.
[ 23 ] Boyce, W. E., and DiPrima, R. C., Elementary Diﬀerential Equations and Boundary
Value Problems, 7th ed., John Wiley & Sons, Inc., New York, 2001.
[ 24 ] Bradie, B., A Friendly Introduction to Numerical Analysis, Prentice–Hall, Inc.,
Upper Saddle River, N.J., 2006.
[ 25 ] Bronstein, M., Symbolic integration I : Transcendental Functions, Springer–Verlag,
New York, 1997.
[ 26 ] Burgers, J. M., A mathematical model illustrating the theory of turbulence, Adv.
Appl. Mech. 1 (1948), 171–199.
[ 27 ] Cantwell, B. J., Introduction to Symmetry Analysis, Cambridge University Press,
Cambridge, 2003.
[ 28 ] Carleson, L., On the convergence and growth of partial sums of Fourier series,
Acta Math. 116 (1966), 135–157.
[ 29 ] Carmichael, R., The Theory of Numbers, Dover Publ., New York, 1959.
[ 30 ] Chen, G., and Olver, P. J., Dispersion of discontinuous periodic waves, Proc. Roy.
Soc. London 469 (2012), 20120407.
[ 31 ] Coddington, E. A., and Levinson, N., Theory of Ordinary Diﬀerential Equations,
McGraw–Hill, New York, 1955.
[ 32 ] Cole, J. D., On a quasilinear parabolic equation occurring in aerodynamics, Q.
Appl. Math. 9 (1951), 225–236.
[ 33 ] Courant, R., Friedrichs, K. O., and Lewy, H., Über die partiellen Diﬀerenzengleichungen der mathematischen Physik, Math. Ann. 100 (1928), 32–74.
[ 34 ] Courant, R., and Hilbert, D., Methods of Mathematical Physics, vol. I,
Interscience Publ., New York, 1953.
[ 35 ] Courant, R., and Hilbert, D., Methods of Mathematical Physics, vol. II,
Interscience Publ., New York, 1953.
[ 36 ] Drazin, P. G., and Johnson, R. S., Solitons: An Introduction, Cambridge University
Press, Cambridge, 1989.
[ 37 ] Dym, H., and McKean, H. P., Fourier Series and Integrals, Academic Press, New
York, 1972.
[ 38 ] Evans, L. C., Partial Diﬀerential Equations, Grad. Studies Math. vol. 19, Amer.
Math. Soc., Providence, R.I., 1998.
[ 39 ] Feller, W., An Introduction to Probability Theory and Its Applications, 3rd ed., J.
Wiley & Sons, New York, 1968.
[ 40 ] Fermi, E., Pasta, J., and Ulam, S., Studies of nonlinear problems. I., preprint,
Los Alamos Report LA 1940, 1955; in: Nonlinear Wave Motion, A. C. Newell,
ed., Lectures in Applied Math., vol. 15, American Math. Soc., Providence,
R.I., 1974, pp. 143–156.
[ 41 ] Forsyth, A. R., The Theory of Diﬀerential Equations, Cambridge University Press,
Cambridge, 1890, 1900, 1902, 1906.
[ 42 ] Fourier, J., The Analytical Theory of Heat, Dover Publ., New York, 1955.

References

591

[ 43 ] Gander, M. J., and Kwok, F., Chladni ﬁgures and the Tacoma bridge: motivating
PDE eigenvalue problems via vibrating plates, SIAM Review 54 (2012),
573–596.
[ 44 ] Garabedian, P., Partial Diﬀerential Equations, 2nd ed., Chelsea Publ. Co., New
York, 1986.
[ 45 ] Gardner, C. S., Greene, J. M., Kruskal, M. D., and Miura, R. M., Method for
solving the Korteweg–deVries equation, Phys. Rev. Lett. 19 (1967), 1095–1097.
[ 46 ] Gonzalez, R. C., and Woods, R. E., Digital Image Processing, 2nd ed.,
Prentice–Hall, Inc., Upper Saddle River, N.J., 2002.
[ 47 ] Gordon, C., Webb, D. L., and Wolpert, S., One cannot hear the shape of a drum,
Bull. Amer. Math. Soc. 27 (1992), 134–138.
[ 48 ] Gradshteyn, I. S., and Ryzhik, I. W., Table of Integrals, Series and Products,
Academic Press, New York, 1965.
[ 49 ] Gurtin, M. E., An Introduction to Continuum Mechanics, Academic Press, New
York, 1981.
[ 50 ] Haberman, R., Elementary Applied Partial Diﬀerential Equations, 3rd ed.,
Prentice–Hall, Inc., Upper Saddle River, NJ, 1998.
[ 51 ] Hairer, E., Lubich, C., and Wanner, G., Geometric Numerical Integration,
Springer–Verlag, New York, 2002.
[ 52 ] Hale, J. K., Ordinary Diﬀerential Equations, 2nd ed., R.E. Krieger Pub. Co.,
Huntington, N.Y., 1980.
[ 53 ] Henrici, P., Applied and Computational Complex Analysis, vol. 1, J. Wiley &
Sons, New York, 1974.
[ 54 ] Hille, E., Ordinary Diﬀerential Equations in the Complex Domain, John Wiley &
Sons, New York, 1976.
[ 55 ] Hobson, E. W., The Theory of Functions of a Real Variable and the Theory of
Fourier’s Series, Dover Publ., New York, 1957.
[ 56 ] Hopf, E., The partial diﬀerential equation ut + u ux = μ u, Commun. Pure Appl.
Math. 3 (1950), 201–230.
[ 57 ] Howison, S., Practical Applied Mathematics: Modelling, Analysis, Approximation,
Cambridge University Press, Cambridge, 2005.
[ 58 ] Hydon, P. E., Symmetry Methods for Diﬀerential Equations, Cambridge Texts in
Appl. Math., Cambridge University Press, Cambridge, 2000.
[ 59 ] Ince, E. L., Ordinary Diﬀerential Equations, Dover Publ., New York, 1956.
[ 60 ] Iserles, A., A First Course in the Numerical Analysis of Diﬀerential Equations,
Cambridge University Press, Cambridge, 1996.
[ 61 ] Jost, J., Partial Diﬀerential Equations, Graduate Texts in Mathematics, vol. 214,
Springer–Verlag, New York, 2007.
[ 62 ] Kamke, E., Diﬀerentialgleichungen Lösungsmethoden und Lösungen, vol. 1, Chelsea,
New York, 1971.
[ 63 ] Keller, H. B., Numerical Methods for Two-Point Boundary-Value Problems, Blaisdell,
Waltham, MA, 1968.
[ 64 ] Knobel, R., An Introduction to the Mathematical Theory of Waves, American
Mathematical Society, Providence, RI, 2000.
[ 65 ] Korteweg, D. J., and de Vries, G., On the change of form of long waves
advancing in a rectangular channel, and on a new type of long stationary
waves, Phil. Mag. (5) 39 (1895), 422–443.
[ 66 ] Landau, L. D., and Lifshitz, E. M., Quantum Mechanics (Non-relativistic Theory),
Course of Theoretical Physics, vol. 3, Pergamon Press, New York, 1977.
[ 67 ] Levine, I. N., Quantum Chemistry, 5th ed., Prentice–Hall, Inc., Upper Saddle
River, N.J., 2000.

592

References

[ 68 ] Lighthill, M. J., Introduction to Fourier Analysis and Generalised Functions,
Cambridge University Press, Cambridge, 1970.
[ 69 ] Lin, C. C., and Segel, L. A., Mathematics Applied to Deterministic Problems in the
Natural Sciences, SIAM, Philadelphia, 1988.
[ 70 ] McOwen, R. C., Partial Diﬀerential Equations: Methods and Applications,
Prentice–Hall, Inc., Upper Saddle River, N.J., 2002.
[ 71 ] Merton, R. C., Theory of rational option pricing, Bell J. Econ. Management Sci. 4
(1973), 141–183.
[ 72 ] Messiah, A., Quantum Mechanics, John Wiley & Sons, New York, 1976.
[ 73 ] Miller, W., Jr., Symmetry and Separation of Variables, Encyclopedia of
Mathematics and Its Applications, vol. 4, Addison–Wesley Publ. Co., Reading,
Mass., 1977.
[ 74 ] Milne–Thompson, L. M., The Calculus of Finite Diﬀerences, Macmillan and Co.,
Ltd., London, 1951.
[ 75 ] Misner, C. W., Thorne, K. S., and Wheeler, J. A., Gravitation, W. H. Freeman, San
Francisco, 1973.
[ 76 ] Miura, R. M., Gardner, C. S., and Kruskal, M. D., Korteweg–deVries equation
and generalizations. II. Existence of conservation laws and constants of the
motion, J. Math. Phys. 9 (1968), 1204–1209.
[ 77 ] Moon, F. C., Chaotic Vibrations, John Wiley & Sons, New York, 1987.
[ 78 ] Moon, P., and Spencer, D. E., Field Theory Handbook, Springer-Verlag, New York,
1971.
[ 79 ] Morse, P. M., and Feshbach, H., Methods of Theoretical Physics, McGraw–Hill, New
York, 1953.
[ 80 ] Morton, K. W., and Mayers, D. F., Numerical Solution of Partial Diﬀerential
Equations, 2nd ed., Cambridge University Press, Cambridge, 2005.
[ 81 ] Murray, J. D., Mathematical Biology, 3rd ed., Springer-Verlag, New York,
2002–2003.
[ 82 ] Oberhettinger, F., Tables of Fourier Transforms and Fourier Transforms of
Distributions, Springer-Verlag, New York, 1990.
[ 83 ] Øksendal, B., Stochastic Diﬀerential Equations: An Introduction with Applications,
Springer–Verlag, New York, 1985.
[ 84 ] Okubo, A., Diﬀusion and Ecological Problems: Mathematical Models,
Springer-Verlag, New York, 1980.
[ 85 ] Olver, F. W. J., Asymptotics and Special Functions, Academic Press, New York,
1974.
[ 86 ] Olver, F. W. J., Lozier, D. W., Boisvert, R. F., and Clark, C. W., eds., NIST
Handbook of Mathematical Functions, Cambridge University Press, Cambridge,
2010.
[ 87 ] Olver, P. J., Applications of Lie Groups to Diﬀerential Equations, 2nd ed.,
Graduate Texts in Mathematics, vol. 107, Springer–Verlag, New York, 1993.
[ 88 ] Olver, P. J., Dispersive quantization, Amer. Math. Monthly 117 (2010), 599–610.
[ 89 ] Olver, P. J., and Shakiban, C., Applied Linear Algebra, Prentice–Hall, Inc., Upper
Saddle River, N.J., 2006.
[ 90 ] Oskolkov, K. I., A class of I. M. Vinogradov’s series and its applications in
harmonic analysis, in: Progress in Approximation Theory , Springer Ser.
Comput. Math., 19, Springer, New York, 1992, pp. 353–402.
[ 91 ] Pinchover, Y., and Rubinstein, J., An Introduction to Partial Diﬀerential
Equations, Cambridge University Press, Cambridge, 2005.
[ 92 ] Pinsky, M. A., Partial Diﬀerential Equations and Boundary–Value Problems with
Applications, 3rd ed., McGraw–Hill, New York, 1998.

References

593

[ 93 ] Polyanin, A. D., and Zaitsev, V. F., Handbook of Exact Solutions for Ordinary
Diﬀerential Equations, 2nd ed., Chapman & Hall/CRC, Boca Raton, Fl.,
2003.
[ 94 ] Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P., Numerical
Recipes: The Art of Scientiﬁc Computing, 3rd ed., Cambridge University
Press, Cambridge, 2007.
[ 95 ] Reed, M., and Simon, B., Methods of Modern Mathematical Physics, Academic
Press, New York, 1972.
[ 96 ] Royden, H. L., and Fitzpatrick, P. M., Real Analysis, 4th ed., Pearson Education
Inc., Boston, MA, 2010.
[ 97 ] Rudin, W., Principles of Mathematical Analysis, 3rd ed., McGraw–Hill, New York,
1976.
[ 98 ] Rudin, W., Real and Complex Analysis, 3rd ed., McGraw–Hill, New York, 1987.
[ 99 ] Salsa, S., Partial Diﬀerential Equations in Action: From Modelling to Theory,
Springer–Verlag, New York, 2008.
[ 100 ] Sapiro, G., Geometric Partial Diﬀerential Equations and Image Analysis,
Cambridge University Press, Cambridge, 2001.
[ 101 ] Schrödinger, E., Collected Papers on Wave Mechanics, Chelsea Publ. Co., New
York, 1982.
[ 102 ] Schumaker, L. L., Spline Functions: Basic Theory, John Wiley & Sons, New York,
1981.
[ 103 ] Schwartz, L., Théorie des distributions, Hermann, Paris, 1957.
[ 104 ] Scott Russell, J., On waves, in: Report of the 14th Meeting, British Assoc. Adv.
Sci., 1845, pp. 311–390.
[ 105 ] Sethares, W. A., Tuning, Timbre, Spectrum, Scale, Springer–Verlag, New York,
1999.
[ 106 ] Siegel, C. L., Über einige Anwendungen diophantischer Approximationen, in:
Gesammelte Abhandlungen, vol. 1, Springer–Verlag, New York, 1966, pp.
209–266.
[ 107 ] Smoller, J., Shock Waves and Reaction–Diﬀusion Equations, 2nd ed.,
Springer-Verlag, New York, 1994.
[ 108 ] Stewart, J., Calculus: Early Transcendentals, vols. 1 & 2, 7th ed., Cengage
Learning, Mason, OH, 2012.
[ 109 ] Stokes, G. G., On a diﬃculty in the theory of sound, Phil. Mag. 33(3) (1848),
349–356.
[ 110 ] Stokes, G. G., Mathematical and Physical Papers, Cambridge University Press,
Cambridge, 1880–1905.
[ 111 ] Stokes, G. G., Mathematical and Physical Papers, 2nd ed., Johnson Reprint Corp.,
New York, 1966.
[ 112 ] Strang, G., Introduction to Applied Mathematics, Wellesley Cambridge Press,
Wellesley, Mass., 1986.
[ 113 ] Strang, G., and Fix, G. J., An Analysis of the Finite Element Method,
Prentice–Hall, Inc., Englewood Cliﬀs, N.J., 1973.
[ 114 ] Strauss, W. A., Partial Diﬀerential Equations: An Introduction, John Wiley &
Sons, New York, 1992.
[ 115 ] Thaller, B., Visual Quantum Mechanics, Springer–Verlag, New York, 2000.
[ 116 ] Thijssen, J., Computational Physics, Cambridge University Press, Cambridge, 1999.
[ 117 ] Titchmarsh, E. C., Theory of Functions, Oxford University Press, London, 1968.
[ 118 ] Varga, R. S., Matrix Iterative Analysis, 2nd ed., Springer–Verlag, New York, 2000.

594

References

[ 119 ] Watson, G. N., A Treatise on the Theory of Bessel Functions, Cambridge
University Press, Cambridge, 1952.
[ 120 ] Weinberger, H. F., A First Course in Partial Diﬀerential Equations, Dover Publ.,
New York, 1995.
[ 121 ] Wiener, N., I Am a Mathematician, Doubleday, Garden City, N.Y., 1956.
[ 122 ] Whitham, G. B., Linear and Nonlinear Waves, John Wiley & Sons, New York,
1974.
[ 123 ] Wilmott, P., Howison, S., and Dewynne, J., The Mathematics of Financial
Derivatives, Cambridge University Press, Cambridge, 1995.
[ 124 ] Yong, D., Strings, chains, and ropes, SIAM Review 48 (2006), 771–781.
[ 125 ] Zabusky, N. J., and Kruskal, M. D., Interaction of “solitons” in a collisionless
plasma and the recurrence of initial states, Phys. Rev. Lett. 15 (1965),
240–243.
[ 126 ] Zienkiewicz, O. C., and Taylor, R. L., The Finite Element Method, 4th ed.,
McGraw–Hill, New York, 1989.
[ 127 ] Zwillinger, D., Handbook of Diﬀerential Equations, Academic Press, Boston, 1992.
[ 128 ] Zygmund, A., Trigonometric Series, 3rd ed., Cambridge University Press,
Cambridge, 2002.

Symbol Index

Symbol

Meaning

Page(s)

c+d
z+w
A+B
v+w
f +g
zw
z/w
c v, c A, c f
z
Ω
0
>0
≥0
f −1
A−1
f (x+ ), f (x− )
n!
 
n
k
|·|
·

addition of scalars
complex addition
addition of matrices
addition of vectors
addition of functions
complex multiplication
complex division
scalar multiplication
complex conjugate
closure of subset or domain
zero vector
positive deﬁnite
positive semi-deﬁnite
inverse function
inverse matrix
one-sided limits
factorial

575
571
575
575
575
571
572
575
571
243
xvi, 575
355, 578
355
xvi
xvi
xvi
163, 453

binomial coeﬃcient

163

absolute value, modulus
norm

94, 225, 571

·
| · |
v·w
z·w
·
·,·

double norm
norm
dot product
Hermitian dot product
expected value
inner product

 · , · 
[ 0, 1 ]
{f | C }
∈

inner product
closed interval
set
element of

73, 89, 106, 284, 356,
578, 579, 581
380
356
578
580
287
73, 89, 107, 285, 341,
578, 579, 581
341
xvi
xvi
xvi

P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0, © Springer International Publishing Switzerland 2014

595

596

Symbol Index

∈
⊂, 
∪
∩
\
:=
≡
≡
◦

not element of
subset
union
intersection
set theoretic diﬀerence
deﬁnition of symbol
identical equality of functions
equivalence in modular arithmetic
composition
convolution
adjoint operator
Fourier series representation
asymptotic equality
function
convergent sequence
weak convergence
one-sided limits
space derivatives
time derivatives
partial derivatives

∗
L∗
∼
∼
f: X → Y
xn → x
fn  f
f (x+ ), f (x− )
u , u , . . .
 
u, u, . . .
ux , uxx , utx , . . .
du d2 u
,
,...
ordinary derivatives
dx dx2
∂
partial derivative
∂
boundary of domain
∂u ∂ 2 u ∂ 2 u
,
,...
partial derivatives
,
∂x ∂x2 ∂t ∂x
∂
partial derivative operator
∂x ,
∂x
∂
normal derivative
∂n
∇
gradient
∇·
divergence
∇×
curl
2
∇
Laplacian
wave operator

n

summation

xvi
xvi
xvi
xvi
xvi
xvi
xvi
xvi
xvi
95, 281
341
74
300
xvi
xvi
230
41, 79
xvii
xvii
xvii, 1
xvii, 1
xvii, 1
5, 152, 504
xvii, 1
2
153, 244, 504
150, 242, 345, 505
242, 347, 505
242
243
50
xvi



i=1

f (x) dx
 b
f (x) dx
a

indeﬁnite integral

xvii

deﬁnite integral

xvii

Symbol Index

597

 ∞
− f (x) dx
principal value integral
−∞

f (x, y) dx dy
double integral
 Ω
f (x, y, z) dx dy dz
triple integral
Ω

f (s) ds
line integral with respect to arc length
C
v dx
line integral
C
&
v dx
line integral around closed curve
C
f dS
surface integral

283
243
505
244
243
243
505

∂Ω

a
A
ak
Ai
arg
b
B
bk
Bi
c
c
C
cg
ck
ck
cp

Bohr radius
space of analytic functions
Fourier coeﬃcient
Airy function
argument (see phase)
ﬁnite element vector
magnetic ﬁeld
Fourier coeﬃcient
Airy function of the second kind
wave speed
ﬁnite element coeﬃcient vector
complex numbers
group velocity
complex Fourier coeﬃcient
eigenfunction series coeﬃcient
phase velocity

567
576
74, 89
327, 460
xvi, 573
401
551
74, 89
462
19, 24, 50, 486, 546
401
xv, 571
331
89
378
330

C0
Cn
C∞
Cn
coker
cos
cosh
coth
csc
curl
d
D
D

space of continuous functions
space of diﬀerentiable functions
space of smooth functions
n-dimensional complex space
cokernel
cosine
hyperbolic cosine
hyperbolic cotangent
cosecant
curl (see also ∇×)
ordinary derivative
derivative operator
domain

108, 576
5, 576
576
xv, 575
350
6, 89
88
91, 317
230
242
xvii, 1
342, 585
5

598

Symbol Index

det
dim
div
ds
dS
e
E
E
ex
ez
ei
erf
erfc
f
F
F
F −1
F (t, x; ξ)
G(x; ξ), Gξ (x)
G(t, x; τ, ξ)
h

Hn
m
Hnm , H
√ n
i = −1
I
Im
Jm
k
k
K
K[ u ]
ν
kij
m
K m, K

determinant
dimension
divergence (see also ∇·)
arc length element
surface area element
base of natural logarithm
energy
electric ﬁeld
exponential
complex exponential
standard basis vector
error function
complementary error function
periodic extension
function space
Fourier transform
inverse Fourier transform
fundamental solution
Green’s function
general fundamental solution
step size
Planck’s constant
Hermite polynomial

582
577
242
244
505
xvi
61, 132, 151
551
5
573
216, 577
55
302
77
575
264
265
292, 387, 481, 543
234, 240, 248, 527
297
182
6, 287, 394
311

harmonic polynomial
imaginary unit
identity matrix
imaginary part
Bessel function
frequency variable
wave number
ﬁnite element matrix
right hand side of evolution equation
elemental stiﬀness

520
571
575
571
468
264
330
401
291
417

ker
l
L2
Lk

complementary harmonic function
kernel
angular quantum number
Hilbert space
Laguerre polynomial

523
350, 577
568
106, 284
566

Ljk
L[ u ]
lim , lim

generalized Laguerre polynomial
linear function/operator
limits

566
10, 64, 585
xvi

n

x→a

n

n→∞

Symbol Index

lim ,

one-sided limits

xvi

log
m
m
M
Mr , Mxr
max
min
mod
n
n
N
O
O(h)
p
p
P
Pn
pm
n
Pnm

natural or complex logarithm
mass
magnetic quantum number
electron mass
spherical mean
maximum
minimum
modular arithmetic
principal quantum number
unit normal
natural numbers
zero matrix
Big Oh notation
pressure
option exercise price
Péclet number
Legendre polynomial
trigonometric Ferrers function
Ferrers (associated Legendre) function

xvi, 573
6
568
564
553
xvi
xvi
xvi
568
153, 244, 505
xv
575
182
3
299
311
511, 525
515
513

P (n)
ph
Q[ u ]
r
r
r
r
R
Rn
R[ u ]
Re
rng
s
S
Sm
sn
Sr , Srx
sech
sign
sin
sinh

space of polynomials of degree ≤ n
phase (argument)
quadratic function(al)
radial coordinate
cylindrical radius
spherical radius
interest rate
real numbers
n-dimensional Euclidean space
Rayleigh quotient
real part
range
arc length
surface area
spherical Bessel function
partial sum
sphere of radius r
hyperbolic secant
sign function
sine
hyperbolic sine

577
xvi, 572
362
xv, 3, 160, 572
xv, 3, 508
xv, 3, 508
299
xv
xv, 575
375
571
576
244
505
539
75, 113
553, 555
334
94, 225
6, 89
13, 88

x → a−

lim

599

x → a+

600

Symbol Index

span
supp
t
T
AT
Tν
tan
tanh
u
ux , uxx , . . .
v
v
v
v
v
V
V
v⊥
vl m n
Vλ
w
w
w
x
x
X
y
y
Y
Ym
Y m , Y m
n
Ynm

z
z
z
Z
α
βln
γ
γ
Γ

n

span
support
time
conserved density
transpose of matrix
ﬁnite element triangle
tangent
hyperbolic tangent
dependent variable
partial derivative
dependent variable
eigenvector/eigenfunction
vector
eigenvector
vector ﬁeld
vector space
potential function
perpendicular vector
atomic eigenfunction
eigenspace
dependent variable
heat ﬂux
heat ﬂux vector
Cartesian space coordinate
real part of complex number
ﬂux
Cartesian space coordinate
imaginary part of complex number
ﬂux
Bessel function of the second kind

576
407
xv, 3
38, 256
341, 578
411
1
135
xv, 3
1
xv, 3
371
xv, 575
66, 582
3, 242
575
6
244
568
371
xv, 3
122
437
xv, 3, 152, 504
571
38, 256
xv, 3, 152, 504
571
256
470

spherical harmonic
complex spherical harmonic
Cartesian space coordinate
cylindrical coordinate
complex number
integers
electron charge
radial wave function
thermal diﬀusivity
Euler–Mascheroni constant
gamma function

517
519
xv, 3, 504
xv, 3, 508
571
xv
564
568
124, 438, 535
471
454

Symbol Index

601

delta function

217, 219, 246, 247, 527

δ  , δξ
Δ

periodically extended delta function
derivative of delta function
Laplacian

229
225, 226
4, 152, 161, 243,

Δ
Δx
Δx
ΔS
ε
0
ζm,n
η
θ
θ
θ
ζ
κ
κ
λ
λ
μ0
ν
ξ
π
ρ
ρ
ρ, ρξ
ρn , ρn,ξ
ρm,n
σ
σ
σ
σ, σξ
σm,n
ϕ
ϕ
ϕk
ϕk
χ
χD

discriminant
step size
variance
spherical Laplacian
thermal energy density
permittivity constant
Bessel root
characteristic variable
polar angle
cylindrical angle
azimuthal angle
root of unity
thermal conductivity
stiﬀness or tension
eigenvalue
magniﬁcation factor
permeability constant
viscosity
characteristic variable
area of unit circle
density
spectral radius
ramp function
nth order ramp function
relative vibrational frequency
shock position
heat capacity
volatility
unit step function
spherical Bessel root
zenith angle
wave function
orthogonal or orthonormal system
basis for ﬁnite element subspace
speciﬁc heat capacity
characteristic function

504, 509
172, 173
186
287
509
122, 437
551
474
51
xv, 3, 160, 572
xv, 3, 508
xv, 3, 508
582
65, 123, 437
49
66, 371, 573
189
551
3
19, 25, 32, 51
5
49, 122, 438
584
91, 223
95, 223
495
41
65, 122, 438
299
61, 80, 222
540
xv, 3, 508
286
109
401
122, 431
485

δ, δξ
δ

602

Symbol Index

ψ
ω
Ω

time-dependent wave function
frequency
domain

394, 564
59, 330
152, 242, 504

Author Index

Abdulloev, K. O. 337, [1]
Ablowitz, M. J. 283, 292, 324, 333, 337,
338, [2]
Abraham, R. 161, [3]
Airy, G. B. 281, 327, 334, [4]
Aki, K. 549, [5]
Ames, W. F. 181, 213, 400, 410, [6]
Antman, S. S. 324, 486, 549, [7]
Apostol, T. M. 5, 20, 76, 87, 100, 105,
169, 182, 236, 242, 245, 267, 312,
437, 500, 505, [8], [9]
Atkinson, K. 519, [10]
Bank, S. B. 455, [11]
Batchelor, G. K. 3, [12]
Bateman, H. 315, [13]
Benjamin, T. B. 337, [14]
Berest, Y. 563, [15]
Bernoulli, J. xvii, 452
Berry, M. V. 329, [16]
Bessel, F. W. 111, 452
Birkhoﬀ, G. ix, 2, 11, 29, 67, 298, 305,
309, 457, [17], [18]
Black, F. 299, [19]
Blanchard, P. ix, 2, 11, 22, 25, 29, 65,
68, [20]
Bogolubsky, I. L. 337, [1]
Bohr, N. H. D. 567
Boisvert, R. F. xvi, 55, 310, 327, 364,
435, 452, 468, 511, 512, 513, 567,
573, [86]
Bona, J. L. 337, [14]
Bourget, J. 490
Boussinesq, J. 292, 333, 335, [21], [22]
Boyce, W. E. ix, 2, 11, 22, 25, 29, 65,
67, 68, 162, 169, 263, 298, 300,
309, 466, [23]
Bradie, B. ix, 135, 185, 407, 453, [24]
Bronstein, M. 267, [25]
Burgers, J. M. 315, [26]
Cantor, G. F. L. P. xvii, 64
Cantwell, B. J. 305, [27]
Carleson, L. 117, [28]
Carmichael, R. 500, [29]
Cauchy, A. L. xvii, 175, 215
Chen, G. 329, [30]
Chladni, E. F. F. 497

Clark, C. W. xvi, 55, 310, 327, 364, 435,
452, 468, 511, 512, 513, 567, 573,
[86]
Clarkson, P. A. 283, 292, 324, 333, 337,
338, [2]
Coddington, E. A. 2, [31]
Cole, J. D. 318, [32]
Courant, R. xvii, 4, 176, 177, 179, 197,
246, 255, 340, 377, 436, 440, 449,
477, 497, 541, [33], [34], [35]
Crank, J. 192
d’Alembert, J. L. R. xvii, 15, 50, 140,
149, 558
de Broglie, L. V. P. R. 287
de Coulomb, C.–A. 252, 503
Devaney, R. L. ix, 2, 11, 22, 25, 29, 65,
68, [20]
de Vries, G. 333, [65]
Dewynne, J. 299, [123]
DiPrima, R. C. ix, 2, 11, 22, 25, 29, 65,
67, 68, 162, 169, 263, 298, 300,
309, 466, [23]
Dirac, P. A. M. 217
Dirichlet, J. P. G. L. 7, 368
Drazin, P. G. 38, 292, 324, 333, 337,
338, [36]
du Bois–Reymond, P. D. G. 430
Duhamel, J. M. C. 298
Dym, H. 76, 99, 107, 115, 117, 263, 265,
275, 286, 344, [37]
Einstein, A. 19, 31, 63, 149, 504
Euler, L. xvii, 3, 454, 461, 573
Evans, L. C. xvii, 4, 314, 340, 350, 427,
436, 535, 546, [38]
Feller, W. 55, 295, [39]
Fermi, E. 333, [40]
Ferrers, N. M. 513
Feshbach, H. 170, 508, 569, [79]
Fitzpatrick, P. M. 76, 100, 102, 107,
108, 119, 217, 219, 344, [96]
Fix, G. J. 399, 400, 410, 431, [113]
Flannery, B. P. ix, 135, 181, [94]
Forsyth, A. R. 317, [41]
Fourier, J. xvii, 63, 64, 71, 114, 123,
149, 437, 452, 535, [42]
Franklin, J. 239

P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0, © Springer International Publishing Switzerland 2014

603

604
Fredholm, E. I. 350
Friedrichs, K. O. 197, [33]
Frobenius, F. G. 464
Galilei, G. 20
Gander, M. J. 497, [43]
Garabedian, P. xvii, 4, 173, 176, 246,
340, 350, 376, 377, 427, [44]
Gardner, C. S. 336, 338, [45], [76]
Gauss, J. C. F. 63
Germain, M.–S. 497
Gibbs, J. W. 84
Gonzalez, R. C. 442, [46]
Gordon, C. 487, [47]
Gradshteyn, I. S. 334, [48]
Green, G. 3, 215, 243
Greene, J. M. 336, [45]
Gregory, J. 78
Gurtin, M. E. 256, 549, [49]
Haberman, R. xvii, [50]
Hairer, E. 181, [51]
Hale, J. K. ix, 2, 11, 29, 67, [52]
Hall, G. R. ix, 2, 11, 22, 25, 29, 65, 68,
[20]
Han, W. 519, [10]
Heaviside, O. 215, 217, 551
Heisenberg, W. K. 286, 288
Henrici, P. 256, [53]
Hilbert, D. xvii, 4, 106, 176, 177, 179,
246, 255, 340, 368, 377, 436, 440,
449, 477, 497, 541, [34], [35]
Hille, E. ix, 2, 453, 457, 459, 463, [54]
Hobson, E. W. 329, [55]
Hopf, E. 318, [56]
Howison, S. vii, viii, 43, 46, 299, [57],
[123]
Hugoniot, P. H. 40
Huygens, C. 560
Hydon, P. E. 305, [58]
Ince, E. L. ix, 2, 453, 457, 459, 463, 472,
[59]
Iserles, A. ix, 181, 185, 453, [60]
Johnson, R. S. 38, 292, 324, 333, 337,
338, [36]
Jost, J. xvii, 4, 246, 255, 314, 340, 350,
427, 535, 546, [61]
Kamke, E. 453, [62]
Kaufman, R. P. 455, [11]
Keller, H. B. 185, 355, 364, [63]
Kelvin, L. (Thomson, W.) 3, 41, 139,
331
Kirchhoﬀ, G. R. 503, 558
Knobel, R. 317, [64]
Korteweg, D. J. 333, [65]

Author Index
Kovalevskaya, S. V. 175
Kruskal, M. D. 333, 336, 338, [45], [76],
[125]
Kwok, F. 497, [43]
Lagrange, J.–L. xvii, 182
Laguerre, E. N. 566
Landau, L. D. 108, 278, 288, 383, 394,
[66]
Laplace, P.–S. xvii, 152
Lebesgue, H. L. 107, 112
Legendre, A.–M. 454, 511
Leibniz, G. W. 1, 78
Levine, I. N. 569, [67]
Levinson, N. 2, [31]
Lewy, H. 197, [33]
Lie, M. S. 305
Lifshitz, E. M. 108, 278, 288, 383, 394,
[66]
Lighthill, M. J. 76, 233, 263, 286, [68]
Lin, C. C. vii, viii, [69]
Liouville, J. 363
Lozier, D. W. xvi, 55, 310, 327, 364,
435, 452, 468, 511, 512, 513, 567,
573, [86]
Lubich, C. 181, [51]
Mahony, J. J. 337, [14]
Makhankov, V. G. 337, [1]
Marsden, J. E. 161, [3]
Marzoli, I. 329, [16]
Maxwell, J. C. 551
Mayers, D. F. 181, 200, 213, 453, [80]
McKean, H. P. 76, 99, 107, 115, 117,
263, 265, 275, 286, 344, [37]
McOwen, R. C. xvii, 56, 122, 179, 246,
255, [70]
Mendeleev, D. I. 568
Merton, R. C. 299, [71]
Messiah, A. 108, 278, 288, 383, 394,
565, [72]
Miller, W., Jr. 305, [73]
Milne–Thompson, L. M. 185, [74]
Minkowski, H. 56, 560
Misner, C. W. 56, 504, [75]
Miura, R. M. 336, 338, [45], [76]
Moon, F. C. 60, [77]
Moon, P. 170, 508, [78]
Morse, P. M. 170, 508, 569, [79]
Morton, K. W. 181, 200, 213, 453, [80]
Murray, J. D. 438, [81]
Navier, C. L. M. H. 3
Neumann, C. G. 7
Newton, I. vii, 49, 63, 135, 149, 182,
252, 388, 503
Nicolson, P. 192

Author Index
Oberhettinger, F. 273, [82]
Okubo, A. 438, [84]
Olver, F. W. J. xvi, 55, 310, 327, 331,
364, 435, 452, 453, 461, 462, 468,
470, 472, 474, 511, 512, 513, 567,
573, [85], [86]
Olver, P. J. ix, xi, xv, xviii, 11, 27, 38,
39, 65, 66, 67, 73, 75, 98, 110,
162, 191, 210, 212, 300, 305, 308,
310, 311, 329, 338, 350, 357, 363,
372, 378, 390, 400, 402, 407, 408,
411, 575, 579, 581, 582, 583, 584,
585, [30], [87], [88], [89]
Oskolkov, K. I. 332, [90]
Øksendal, B. viii, 299, [83]
Parseval des Chênes, M.–A. 114
Pasta, J. 333, [40]
Pauli, W. E. 568
Pinchover, Y. xvii, [91]
Pinsky, M. A. xvii, [92]
Plancherel, M. 114
Planck, M. K. E. L. 287, 394
Poisson, S. D. 31, 152, 503, 558
Polyanin, A. D. 453, [93]
Press, W. H. ix, 135, 181, [94]
Rankine, W. J. M. 40
Ratiu, T. 161, [3]
Rayleigh, L. (Strutt, J.W.) 40, 375
Reed, M. 344, 374, 383, 565, [95]
Richards, P. G. 549, [5]
Richardson, L. F. 194
Riemann, G. F. B. xvii, 31, 63, 87, 112
Robin, V. G. 123
Rota, G.–C. ix, 2, 11, 29, 67, 298, 309,
457, [18]
Royden, H. L. 76, 100, 102, 107, 108,
119, 217, 219, 344, [96]
Rubinstein, J. xvii, [91]
Rudin, W. 5, 76, 100, 102, 105, 107,
108, 119, 169, 182, 217, 219, 256,
263, 267, 344, [97], [98]
Ryzhik, I. W. 334, [48]
Salsa, S. xvii, 4, 122, 340, 350, 427, 436,
535, 546, [99]
Sapiro, G. 442, [100]
Schleich, W. 329, [16]
Scholes, M. 299, [19]
Schrödinger, E. 394, [101]
Schumaker, L. L. 210, 400, 408, [102]
Schwartz, L. 217, [103]
Scott Russell, J. 334, [104]
Segel, L. A. vii, viii, [69]
Sethares, W. A. 144, [105]

605
Shakiban, C. ix, xv, xviii, 11, 27, 65,
66, 67, 73, 75, 98, 110, 162, 191,
210, 212, 300, 350, 357, 363, 372,
378, 390, 400, 402, 407, 408, 411,
575, 579, 581, 582, 583, 584, 585,
[89]
Siegel, C. L. 490, [106]
Simon, B. 344, 374, 383, 565, [95]
Smoller, J. 317, 427, 434, 438, [107]
Spencer, D. E. 170, 508, [78]
Stewart, J. 5, 20, 105, 236, 242, 245,
312, 407, 437, 505, [108]
Stokes, G. G. 3, 41, 331, [109], [110],
[111]
Strang, G. xvii, 357, 399, 400, 410, 431,
[112], [113]
Strauss, W. A. xvii, [114]
Strutt, J.W. see Rayleigh, L.
Sturm, F. O. R. 363
Talbot, W. H. F. 329
Taylor, B. 75
Taylor, R. L. 400, 410, 431, [126]
Teukolsky, S. A. ix, 135, 181, [94]
Thaller, B. 108, 329, 394, [115]
Thijssen, J. 565, [116]
Thomson, W. see Kelvin, L.
Thorne, K. S. 56, 504, [75]
Titchmarsh, E. C. 263, 265, 275, 286,
[117]
Ulam, S. 333, [40]
Varga, R. S. 213, 402, 411, [118]
Vetterling, W. T. ix, 135, 181, [94]
Victoria, Q. 139
von Helmholtz, H. L. F. 374
von Neumann, J. 190
Wanner, G. 181, [51]
Watson, G. N. 452, 470, 472, 474, 490,
[119]
Webb, D. L. 487, [47]
Weierstrass, K. T. W. xvii, 100, 329
Weinberger, H. F. xvii, 50, 447, 525,
[120]
Wheeler, J. A. 56, 504, [75]
Whitham, G. B. 31, 44, 46, 179, 315,
316, 317, 324, 331, 427, 434, [122]
Wiener, N. 254, [121]
Wilmott, P. 299, [123]
Winternitz, P. 563, [15]
Wolpert, S. 487, [47]
Woods, R. E. 442, [46]
Yong, D. 50, [124]
Zabusky, N. J. 333, [125]

606
Zaitsev, V. F. 453, [93]
Zienkiewicz, O. C. 400, 410, 431, [126]
Zwillinger, D. 453, [127]
Zygmund, A. 76, 99, 102, 105, 115, 117, [128]

Author Index

Subject Index

absolute
convergence 101
value 86, 94, 105, 225
zero 139
abstraction 339
acceleration 7, 49
accidental degeneracy 499
acoustics 2, 15, 31
acoustic wave 15
Acta Numerica 181
addition 571, 575
identity 575
inverse 575
adjoint 339, 341, 344, 428, 438, 505
formal 344
system 350
weighted 342
advection 322
aerodynamics 173
aﬃne 404
element 414, 416
piecewise 404, 411
afterglow 562, 563
air 15, 174, 551
airplane 15, 173, 174
Airy
diﬀerential equation 281, 459
function 327, 364, 435, 461
second kind 462
algebra 263, 273, 275, 453
linear viii, ix, 63, 215, 221, 234, 339,
353, 400, 575
numerical linear ix, 399
algebraic
diﬀerential equation 455
equation 1, 8, 11, 215, 428
function 511
multiplicity 373
algorithm 399
altitude vector 418
amplitude 334
analysis 63, 76, 99, 400, 578
complex 31, 175, 256, 263
functional 340, 350, 362
numerical ix, 181
real ix, 219
vector viii

analytic 76, 105, 158, 169, 175, 181,
521, 576
function 98, 456, 463
solution 431
angle 509
azimuthal 508, 522, 549
cylindrical 509
polar 572
right 73, 581
zenith 508, 515
angular
coordinate 130
quantum number 566, 568
animal population 2, 435, 485
anisotropic 442
annulus 170, 171, 415, 474, 480, 494,
499
ansatz 66, 124, 161, 330, 390, 394, 466,
475, 535, 538
exponential 67, 330
power 162, 464, 520
trigonometric 546
approximation vii, 9, 110, 181, 363, 406
arbitrage 299
arbitrary function 6, 21
arc 551, 556
arc-length 244
area 39, 244, 413, 414, 415, 477
equal 40, 46, 47, 431
surface 517, 529, 537, 543, 553, 555
argument (see phase) xvi, 573
arithmetic 184, 571
ﬂoating-point ix, 184
single-precision 184
asset 299
associated Legendre function 513
associativity 281, 575
astronomy 560
asymptotics 69, 284, 453, 468
atom 37, 279, 394, 497, 547, 568
eigenfunction 568, 570
orbital 503
audio 63
autonomous 24, 29, 556
average 40, 92, 131, 167, 252, 285, 521,
523, 553, 560
weighted 213

P.J. Olver, Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
DOI 10.1007/978-3-319-02099-0, © Springer International Publishing Switzerland 2014

607

608
axis
horizontal 18
vertical 18
azimuthal angle 508, 522, 549
B spline 283
back substitution 212
backward diﬀerence 182
backwards heat equation 129, 299, 442
bacteria 438, 485
ball 362, 467, 508, 528, 530, 533, 534,
537, 540, 542, 545, 547, 550, 555
balloon 549
bank 299
bar 2, 15, 49, 64, 65, 96, 122, 124, 132,
134, 138, 140, 144, 234, 239, 241,
292, 293, 298, 307, 344, 351, 357,
405, 440
barge 334
barrier 15, 173
basis ix, 112, 350, 401, 405, 430, 577,
583
eigenvector 66, 583, 584
orthogonal 73, 112, 372, 581
solution 458
standard 216, 577, 581
bath 123, 134, 154, 436
beam 2, 146, 393
equation 146, 396
light 286
bell curve 295
bend 486
Benjamin–Bona–Mahony equation 337
Bessel
boundary value problem 475, 480
diﬀerential operator 366, 374
equation 364, 452, 467, 474, 538
modiﬁed 473
function 109, 364, 374, 435, 445, 452,
468, 474, 490, 499, 508, 536, 538
second kind 470, 474
spherical 435, 503, 539, 540, 543
inequality 111, 113, 379, 380, 441
operator 367
root 474, 490, 499
spherical 540, 541, 548
best approximation 110
bi-aﬃne function 415
bidiagonal 211
bidirectional 5, 324
big Oh 182
biharmonic equation 361
bilinearity 281, 578
binomial
coeﬃcient 164
formula 163

Subject Index
biology vii, 4, 15, 63, 121, 130
black 441
Black–Scholes
equation vii, viii, 291, 299
formula 302
block 144
bidiagonal 211
tridiagonal 211
blow-up 23, 32, 37
blurring 129, 442
body 121, 341
solid 2, 15, 65, 144, 440, 503, 504,
522, 546
Bohr radius 567, 569
boiling water 138, 170, 524, 543
boost 20, 311
boundary ix, 5, 7, 152, 220, 243, 245,
254, 436, 437, 522
coeﬃcient matrix 425
condition 4, 7, 68, 217, 293, 306, 339,
400, 521
Dirichlet 7, 123, 141, 147, 153, 166,
186, 201, 343, 345, 347, 359, 364,
368, 404, 412, 436, 439, 441, 446,
486, 488, 504, 521, 522, 527, 535,
537, 540, 544, 546
impedance 154
mixed 124, 343, 345, 347, 359, 364,
368, 436, 439, 441, 486, 504, 527,
535, 544, 546
Neumann 7, 123, 144, 153, 343,
345, 347, 359, 364, 368, 374, 436,
439, 441, 486, 504, 527, 535, 544,
546
periodic 69, 124, 130, 292, 343, 345,
361
Robin 123, 134, 154, 361, 436, 439
curved 411, 412
data 207
element 424
free 486
integral 347
moving 486
node 188, 207, 411, 418, 422, 425
point 153, 169
polygonal 411, 417
solid 154, 504
term 343
value problem ix, 2, 6, 7, 74, 121,
152, 172, 215, 217, 234, 237, 255,
263, 278, 336, 339, 350, 355, 371,
376, 386, 399, 401, 402, 410, 585
Bessel 475, 480
Chebyshev 385

Subject Index
boundary value problem (continued )
Dirichlet 125, 132, 160, 207, 213,
245, 254, 383, 410, 416, 442, 450,
474, 508, 528, 531, 533, 547
elliptic 207, 216, 256, 506
Helmholtz 160, 441, 443, 445, 475,
487, 490
Legendre 511, 515
mixed 7, 134, 154, 214, 241, 245,
505, 507
Neumann 148, 238, 239, 245, 352,
387, 534
periodic 69, 373, 383, 387, 510
regular 379
singular 379
Sturm–Liouville 363, 382
bounded ix, 152, 345, 359, 504, 519, 531
bound state 278, 337, 383, 565, 568
Bourget’s hypothesis 490, 500
bow 497
bowl 362
box 508, 526, 536, 547, 550
function 60, 88, 149, 266, 283, 296
brick 536
bridge 549
Brownian motion 438
building 549
bump 169
Burgers’ equation 2, 9, 291, 315, 317
inviscid 315
potential 318
Cn 5, 456, 463, 576
piecewise 80–2, 94, 223, 233
cable equation 139, 304
calcium 569
calculus vii, viii, 63, 76, 98, 182, 221,
223, 233, 263, 275, 452
of ﬁnite diﬀerences 182, 185
multivariable 215, 437, 505
of variations 255
vector 242, 507, 571
call option 304
can 537
canal 334
cap 556, 561
capacity 65
capacitor 522, 523, 526, 533
Carleson’s Theorem 117
Cartesian coordinate viii, 242, 446, 536
Cauchy data 175
–Kovalevskaya Theorem 175
problem 175
–Schwarz inequality ix, 107, 175, 285,
526, 579, 581
sequence 107, 119

609
causality 15, 43, 433
cavity 524, 534
CD 63
ceiling 494
cell phone 63
cellar 136, 137, 545
Celsius 306
center 252
centered diﬀerence 184, 198
CFL condition 197, 205
chain 333, 452
rule viii, 20, 24, 32, 57, 161, 167, 171,
300, 305, 308, 333, 494, 509, 511,
524
change of variables viii, 51, 58, 174,
179, 318, 511
chaos 60, 393, 492
characteristic 56, 175
coordinate 57
curve 15, 24, 30, 31, 47, 49, 176–9
lifted 49
equation 177
function 485
line 21, 45, 195
variable 20, 22, 25, 30, 32, 51, 52, 552
charge 249, 252, 256, 529, 531, 564
Chebyshev
boundary value problem 385
equation 385, 473
polynomial 473
chemical 2
diﬀusion 123
reaction vii, 31, 438
chemistry vii, 435, 569, 503
Chladni ﬁgure 497
chromatography 15, 31
circle 167, 258, 450, 500, 551
civil engineering 549
clarinet 144
classical
mechanics 394, 503
solution 5, 7, 17, 51, 144, 255, 399,
410, 427, 428, 432, 535, 546, 556,
558
clockwise 243
closed ix
curve 152, 243
surface 505
coeﬃcient
constant ix, 2, 9
diﬀusion 129, 307
Fourier 71, 72, 75, 85, 102, 107, 228,
264, 441
matrix 188, 191, 350, 386
transfer 156

610
cokernel 350
co-latitude 509
cold 436
collision 54, 292, 324, 336, 337, 438
column vector 216, 578
comb 229, 233
comet 565
commutativity 575
compact ix
support 230, 430, 432
complementary
error function 277, 302, 321
harmonic function 523
complete 66, 107, 113, 119, 340, 371,
372, 378, 379, 381, 382, 383, 447,
477, 480, 517, 535, 583
system 520, 541, 565
complex
addition 571
analysis 31, 175, 256, 263
arithmetic 571
change of variables 175
conjugate 571, 580
division 572
eigenvalue 372
exponential ix, 88, 107, 109, 136, 181,
189, 265, 271, 285, 519, 573
Fourier coeﬃcient 89, 229, 284
Fourier series 89, 582
function 284, 324
inner product 118
integral 266, 461
logarithm 164, 573
multiplication 571
number viii, ix, xv, 571
plane xv, 571
solution 6
spherical harmonic 519, 526, 565
variable 163, 571
vector space 289, 571, 575, 580
zero 87
compressible 37
compression wave 36, 44
computer 184, 190, 400
algebra 273, 453
arithmetic ix
graphics 63
concentration 34
conditionally stable 190
conduction 504
conducting medium 504
conductivity 124
thermal 65, 123, 437
conductor 123
cone 56, 560, 564

Subject Index
conformal mapping 256
conic 172
conjugate symmetry 580
connected 5, 17, 141, 245, 345, 359
pathwise 5, 245
simply 243
conservation law 15, 38, 46, 131, 201, 255,
256, 295, 304, 332, 337, 431, 437
energy 61, 62, 151, 535
heat energy 122, 304
mass 41, 47, 360
momentum 61
conserved density 38, 46, 201, 256
constant
coeﬃcient ix, 2, 9
Euler–Mascheroni 471
function 285, 506
Planck 6, 287, 394, 564
separation 141, 156, 446, 510
constitutive assumption 122
contaminant 123
continuous viii, 7, 63, 80, 82, 94, 102, 231
dependence 130
function 99, 108, 117, 219, 220, 344,
576, 579
Lipschitz 29
media 390
piecewise 79, 81, 108, 127, 441
spectrum 337, 340, 374, 383, 565
continuum mechanics/physics viii, 38
contract 299
control 9, 263
convection 438
-diﬀusion equation 139, 311, 314, 438
convective ﬂow 311
convergence viii, xvi, 63, 72, 75, 76, 98,
109, 213, 383, 400, 410
in norm 98, 99, 109, 110
nonuniform 267
pointwise 99, 109, 115, 117, 231, 285,
378
radius of 453, 458
test
integral viii, 105
ratio viii, 75, 462, 468
root viii, 75
theorem 82
uniform 99, 100–102, 104, 378, 519
weak 99, 230, 270, 327, 429
convergent 584, 585
convolution 242, 281, 282, 295, 484, 544
integral 301
periodic 95
summation 284
theorem 284

Subject Index
coordinate
angular 130
Cartesian viii, 242, 446, 536
characteristic 57
curvilinear 364, 435
cylindrical viii, 3, 503, 508, 523, 527,
530, 536, 547
ellipsoidal 508
moving 15, 19
parabolic 174
parabolic spheroidal 508
polar viii, 3, 62, 160, 161, 171, 250,
383, 451, 477, 479, 490, 509, 572
rectangular viii, 161, 503
separable 508
space 3
spherical viii, 3, 503, 508, 520, 524,
528, 537, 551, 553, 565
toroidal 508
corner 81, 193, 441
node 209
corpse 129
cosine transform 274
Coulomb
potential 503, 529, 564
problem 564
counterclockwise 243, 415
crack 256, 427
Cramer’s Rule 413
Crank–Nicolson method 192
crest 331
critical point 501
cross product viii
cube 526, 534, 536, 543, 545, 550, 561
cubic 283, 408
curl viii, 13, 242, 243, 349, 507
curve 172, 176, 254, 571
bell 295
characteristic 15, 24, 30, 31, 47, 49,
176, 177, 178
lifted 49
closed 152, 243
simple 152
nodal 497, 551
curved boundary 411, 412
curvilinear coordinate 364, 435
cusp 254
cut locus 511, 515, 525
cylinder 450, 452, 467, 508, 527, 536,
537, 547, 550
cylindrical
angle 509
coordinates viii, 3, 503, 508, 523, 527,
530, 536, 547
shell 450

611
cylindrical (continued )
symmetry 517
cymbal 144
d’Alembert
formula 16, 121, 140, 146, 260, 427,
552, 558
solution 53, 201, 324, 487
dam 61
damped 62, 200
heat equation 65
transport equation 49
wave equation 207
data 363, 410
de Broglie relation 287
death 129
decay 22, 105, 276, 285, 387, 441, 443,
479, 482, 536, 544
entropic 291
exponential 127
decimal expansion 108
deep water 331
deﬂection 249
deformable body 341
deformation 121
degree 306, 509
delta
comb 229, 233
distribution 215, 217
function 63, 215, 217, 221, 223, 225,
228, 233, 246, 249, 270, 276, 277,
280, 281, 292, 293, 321, 326, 358,
379, 405, 441, 479, 481, 483, 485,
521, 544, 552, 554
three-dimensional 527
two-dimensional 246, 255
impulse 221, 234, 248, 291, 292, 387
wave 558
denoise 128, 296, 441
dense 107, 344
subspace 344, 346, 371
density 49, 122, 124, 132, 142, 253, 344,
357, 438, 486, 488, 492, 495
conserved 38, 46, 201, 256
energy 61, 122
momentum 61
probability 108, 286, 564
dependent variable 3
deposit 299
depression 31
derivative 1, 5, 105, 182, 223, 236, 275,
342, 576
of delta function 233, 277
of Fourier series 94
left-hand 81
logarithmic 336

612
derivative (continued )
normal 7, 153, 245, 250, 436, 504,
528, 529
one-sided 576
partial viii, 1, 3, 521
mixed 5, 8, 50, 242, 556
radial 554
right-hand 81
of series 101
operator 354
ordinary 1, 3
space 291
descent 503, 551, 561, 564
determinant ix
Jacobian 58, 255
deviation 287, 295
diameter 499, 500
diﬀerence
backward 182
centered 184, 198
ﬁnite vii, 9, 181, 182, 185, 186, 213,
214, 399, 407, 411, 422
forward 182
quotient 182
diﬀerentiable 80, 285
inﬁnitely 5, 75, 105, 128, 158
nowhere 329
diﬀerential
equation 1, 181, 342, 350, 427, 428,
585
stochastic viii, 299
system of 2, 66
zenith 510
see also ordinary diﬀerential equation,
partial diﬀerential equation
geometry 63
operator 9, 64, 339, 350, 371
Bessel 366, 374
partial 2, 50
Sturm–Liouville 364, 365, 480
diﬀerentiation viii, 233, 263, 276, 556
implicit 49
numerical 181
operator 286
diﬀusion 172, 299, 311, 315, 385, 388,
435, 436, 503, 535
chemical 123
coeﬃcient 129, 307
equation 123, 315, 340, 395, 438, 439,
543
nonlinear 2, 122, 291, 437, 442
of set 485
process 121, 129, 386
diﬀusive transport equation 194

Subject Index
diﬀusivity 307, 537
thermal 124, 134, 186, 293, 298, 438,
535
Dilation Theorem 271, 274
dimension 2, 112, 577
eigenspace 583
ﬁnite ix, 11, 98, 109, 215, 220, 400,
410, 430
inﬁnite ix, 11, 99, 109, 215, 340, 342,
371, 400, 577
Dirac
comb 229
delta function 217
equation vii
direct method 255
Dirichlet
boundary condition 7, 123, 141, 147,
153, 166, 186, 201, 343, 345, 347,
359, 364, 368, 404, 412, 436, 439,
441, 446, 486, 488, 504, 521, 522,
527, 535, 537, 540, 544, 546
boundary value problem 125, 132,
160, 207, 213, 245, 254, 383, 410,
416, 442, 450, 474, 508, 528, 531,
533, 547
eigenvalue 373, 377
functional 410, 416, 424
integral 368, 506
principle 368, 400, 443, 506
disconnected 17
discontinuity 37, 193, 223, 441
jump 80, 81, 82, 96, 164, 223, 233,
236, 405, 432
removable 80
discontinuous 15, 76, 102, 427
initial data 292
discrete Fourier transform 582
discriminant 172, 173, 174
disease 123
disk 121, 160, 167, 249, 251, 253, 374,
415, 427, 444, 445, 450, 467, 479,
490, 499, 500, 508
half 260, 480, 494, 496
metal 166, 479
quarter 494
semi-circular 170, 418
unit 155, 166
dislocation 256, 427
dispersion vii, 292, 324, 330
relation 330
dispersive 329, 396
equation 328, 486
medium 2
quantization 328, 329
tail 337
wave 2, 324, 459

Subject Index
displacement 49, 216, 341, 351, 486
initial 53, 59, 145, 487, 546, 547, 551,
554, 556, 557, 560, 561, 562
radial 546
dissipative 315
dissonant 144, 490, 496, 549
distance 307, 578
distribution ix, 215, 217, 553
Gaussian 295
distributive 575
disturbance 15, 22, 179, 292
diverge 98
divergence viii, 13, 242, 244, 359, 437,
439, 505, 535
operator 347
theorem viii, 505, 529
division 263
domain ix, 5, 17, 152, 207, 243, 245,
248, 253, 339, 341, 345, 359, 486,
504, 571
of dependence 59, 197, 199
of inﬂuence 56
irregular 213
rescaled 495
space 401, 585, 586
dominant
frequency 495
mode 499
dot product viii, 73, 341, 346, 354, 372,
578, 585
Hermitian 580
double
Fourier series 488
integral viii, 58, 243, 245, 248, 251,
346, 422, 432, 437, 448
weighted L2 norm 381
L2 norm 383, 525
doublet 558
doubly inﬁnite series 89, 91
driver 44
drum 63, 144, 152, 153, 214, 486, 487,
490, 495, 496
circular 154, 160, 490, 494, 496, 499
rectangular 499
square 493
du Bois–Reymond lemma 431, 434
duality 221, 247, 286, 553
Duhamel’s principle 298
DVD 63
dynamical 3
partial diﬀerential equation 291, 551
process 7, 15, 172
system 340, 385

613
dynamics 46, 47, 49, 340, 435
ﬂuid 291, 315
gas 15, 31, 315
shock 15, 431
ear 144
Earth 136, 137, 508, 530, 537, 545, 549,
560, 561
earthquake 549
echo chamber 563
ecology 15
economics vii, 4
eigenequation 66, 67, 371, 439, 583
eigenfunction 67, 74, 110, 125, 190, 340,
371, 374, 375, 376, 378, 387, 395,
439, 445, 515, 517, 535, 540, 546,
547, 549, 565, 566
atomic 568, 570
expansion 340, 379
Fourier–Bessel 477
null 70, 132, 145, 386, 387, 389, 441,
487, 547, 550
series 109, 371, 378, 386, 435, 441,
443, 535, 544
Sturm–Liouville 382
eigenmode 389, 497, 499, 546
eigensolution 66, 67, 125, 140, 389, 395,
447, 475, 487, 566, 584
series 440
eigenspace 371, 382, 517, 568, 583
eigenstate 568
eigenvalue ix, 2, 66, 67, 125, 190, 336,
340, 371, 376, 378, 387, 389, 395,
487, 517, 535, 541, 546, 549, 565,
568, 582–4
complex 372
equation 66, 67, 371, 439, 582
Dirichlet 373, 377
Helmholtz 377, 383, 446, 474, 535,
546
multiplicity 582
null 131, 439, 582
problem 130, 371, 446
simple 372, 391, 582
zero 131, 439, 582
eigenvector ix, 66, 371, 372, 375, 582,
583, 584
basis 66, 583, 584
null 582
eikonal equation vii
Einstein equation vii
elastic 550
ball 504
bar 15, 49, 234, 241, 351, 357
beam 146, 393
media 427
plate 324, 486

614
elastic (continued )
vibration 486
wave 121
elasticity vii, 2, 121, 175, 504
elastodynamics 486, 549
elastomechanics 341
electric
charge 256, 529, 531, 564
ﬁeld 341, 504, 546, 551
potential 504
electromagnetic 254, 395
potential 2, 564
vibration 2
wave 15, 121, 388, 503, 546, 551
electromagnetism vii, 121, 154, 341, 504
electromotive force 504
electron 6, 108, 249, 279, 395, 503, 564,
567, 569
electronic music 63
electrostatic 242, 249, 531
attraction 279
force 504, 529
potential 152, 249, 252, 256, 503, 522,
534
element 413, 568
aﬃne 414, 416
boundary 424
ﬁnite viii, 9, 181, 207, 340, 362, 399,
410, 411, 416, 427, 430, 431
zero 428
elemental stiﬀness 417, 418
elementary function xvi, 9, 55, 267, 310,
337, 435, 445, 452, 453, 459, 467,
539
elevation 31
ellipse 173
ellipsoid 511
ellipsoidal coordinates 508
elliptic 2, 122, 172, 173, 178, 212, 399,
404, 410
boundary value problem 207, 216,
256, 506
orbit 565
energy 39, 286, 292, 324, 565, 568, 569
conservation 61, 62, 151, 535
density 61, 122
heat 122, 246, 304, 435–7, 482
kinetic 61
level 395, 503, 567
operator 394
potential 6, 61, 152, 242, 318, 340,
362
thermal 121, 122, 132, 134, 139, 169,
295, 304
total 61, 62, 151

Subject Index
engineering vii, 2, 4, 63, 217, 263, 305,
549
enhancement 129, 442
entropic decay 291
entropy 317
condition 43, 46
envelope 230
Equal Area Rule 40, 46, 47, 431
equation
Airy 281, 459
algebraic 1, 8, 428
algebraic diﬀerential 455
backwards heat 129, 299, 442
beam 146, 396
Benjamin–Bona–Mahony 337
Bessel 364, 452, 467, 474, 538
modiﬁed 473
biharmonic 361
Black–Scholes vii, viii, 291, 299
Burgers 2, 9, 291, 315, 317
inviscid 315
potential 318
cable 139, 304
characteristic 177
Chebyshev 385, 473
convection-diﬀusion 139, 311, 314,
438
damped heat 65
damped transport 49
damped wave 207
diﬀerential 1, 181, 342, 350, 427, 428,
585
diﬀusion 123, 194, 315, 340, 395, 438,
439, 543
forced 388
diﬀusive transport 194
Dirac vii
dispersive 328, 486
eigenvalue 66, 67, 371, 439, 582
eikonal vii
Einstein vii
equilibrium 3, 5, 386
Euler vii, 3, 162, 170, 300, 315, 452,
463, 465, 510, 520
evolution 2, 64, 67, 291, 298, 324,
325, 385, 439, 442
Fitz-Hugh–Nagumo vii
functional 454
Ginzburg–Landau vii
Hamilton–Jacobi 305

Subject Index
equation (continued )
heat vii, 2, 5, 9, 11, 12, 64, 69, 121,
124, 140, 152, 172, 173, 178, 181,
186, 291, 292, 295, 301, 305, 309,
311, 315, 318, 324, 325, 339, 340,
371, 374, 385, 387, 435, 438, 443,
445, 446, 467, 474, 503, 535, 537,
543, 544
backwards 129, 299, 442
forced 134, 296, 312, 314, 442
generalized 65
lossy 139
spherical 543
Helmholtz 178, 370, 374, 378, 439,
488, 490, 508, 538, 540, 547
polar 451, 508
spherical 510, 517, 519, 565
Hermite 462
integral 350
implicit 26
indicial 464, 465, 520
integral 9, 284
Kadomstev–Petviashvili vii
Kolmogorov–Petrovsky–Piskounov vii
Korteweg–de Vries vii, 2, 292, 324,
333, 336
modiﬁed 337
Laguerre 364, 566, 569
Lamé vii
Laplace 2, 8, 13, 121, 152, 155, 166,
169, 172, 173, 175, 178, 181, 207,
213, 243, 249, 256, 291, 311, 312,
339, 399, 422, 435, 467, 503, 504,
507, 509, 520, 522, 527
Legendre 364, 511, 514, 525
linear 9, 11, 215, 216, 428
Mathieu 409
Maxwell vii, 546, 551
Maxwell–Bloch vii
minimal surface 153, 175
Navier–Stokes vii, 3, 9, 315
ordinary diﬀerential ix, 2, 6, 8, 11,
22, 65, 67, 121, 130, 140, 185, 215,
217, 263, 291, 308, 333, 363, 399,
435, 445, 452, 455, 463, 585
ﬁrst-order ix, 297
linear 11, 585
second-order ix, 297, 363, 459, 463
separable ix
partial diﬀerential vii, ix, 2, 63, 67,
130, 175, 215, 263, 305, 317, 339,
350, 394, 399, 429, 487, 582
dynamical 291, 551
elliptic 2, 122, 172, 173, 178, 212,
399, 404, 410
ﬁrst-order 176, 179, 195

615
equation, partial diﬀerential (continued )
fourth-order 486
fully nonlinear 174
hyperbolic 2, 122, 172, 173, 178,
179, 195, 385, 497
ill-posed 129, 175, 436, 442
nonlinear 2, 12, 31, 174, 222, 305,
315, 333, 486
parabolic 2, 122, 172, 173, 178, 193,
291, 315, 385, 438
quasilinear 174
second-order 121, 173
third-order 2, 324
type 172, 173
well-posed 136, 395, 535
Poisson 2, 121, 152, 169, 172, 173,
181, 207, 215, 242, 245, 248, 253,
255, 256, 339, 352, 359, 361, 368,
383, 399, 401, 410, 422, 435, 442,
503, 504, 507, 527, 530, 532, 533,
534
Poisson–Darboux 62
quadratic 172, 464, 465
reaction-diﬀusion 438
Schrödinger vii, 6, 146, 304, 340, 372,
374, 383, 385, 394, 396, 397, 503,
564
nonlinear vii
stochastic diﬀerential viii, 299
Sturm–Liouville 336, 364, 370, 404,
508
telegrapher 62, 146, 393
transcendental 1, 134
transport 2, 19, 31, 51, 65, 181, 195,
291, 315, 324, 330, 434
Tricomi 173, 178, 462
vibration 340, 388, 390
wave 2, 8, 13, 15, 50, 64, 121, 140,
149, 151, 152, 169, 172, 173, 178,
181, 195, 201, 291, 315, 324, 339,
340, 371, 374, 385, 389, 427, 434,
435, 467, 486, 488, 494, 503, 545,
551, 552, 554, 556, 558, 561
forced 56, 58, 393, 493
nonlinear 317, 333
vibration 340, 388
Vlasov vii
von Karman vii
zenith 510
equator 509, 522
equilateral triangle 260, 415, 417, 419,
424, 426
equilibrium 2, 131, 132, 172, 216, 239,
399, 435, 442, 503, 504
equation 3, 5, 386
mechanics 121

616
equilibrium (continued )
solution 132, 443
system 339, 340
temperature 133, 154, 214, 246, 435,
537
thermal 127, 152, 153, 169, 441, 448,
479, 522, 536
equivalence class 108
error 182, 184
bound 331
function 55, 277, 296, 310, 435
oscillatory 193
round-oﬀ ix, 184, 190
Euclidean
norm 169, 572, 578
space ix, 575, 578
Euler
equation 162, 170, 300, 452, 463, 465,
510, 520
equations vii, 3, 315
–Mascheroni constant 471
formula ix, 6, 88, 267, 573
European call option 300, 301
even 85, 149, 229, 268, 275, 497, 525
evolution equation 64, 67, 291, 298,
325, 385, 439, 442
third-order 2, 324
exercise
price 299
time 302
existence ix, 2, 3, 29, 130, 246, 254, 350,
355, 368, 376, 390, 436
theorem 67, 364, 457
expected value 286, 287
explicit scheme 188
explosion 551, 560, 561
exponential 445, 452
ansatz 67, 330
complex ix, 88, 107, 109, 136, 181,
189, 265, 271, 285, 519, 573
decay 127
function 66, 90, 140
integral 267
pulse 275
external
force 12, 56, 58, 152, 215, 216, 234,
291, 292, 351, 386, 390, 504
heat source 12, 152, 246, 296, 297,
312, 436, 437, 442, 485
factorial 453, 454, 468
factorization 15, 50, 62, 243, 324, 487
Fahrenheit 306, 311
feature 128
Fermi–Pasta–Ulam problem 333

Subject Index
Ferrers function 435, 513, 515, 525
trigonometric 516
ﬁeld
electric 341, 504, 546, 551
force 6, 252
gravitational 341
magnetic 341, 546, 551
scalar 242, 345, 359, 439, 505
vector 242, 243, 346, 359, 360, 439,
505, 507, 526
ﬁgure
Chladni 497
ﬁlter 291, 295, 482, 544, 545
ﬁnal
condition 300
value problem 300
ﬁnance vii, 63, 291, 299
ﬁngerprint 63
ﬁnite diﬀerence vii, 9, 181, 182, 185,
186, 213, 214, 399, 407, 411, 422
ﬁnite-dimensional ix, 11, 98, 109, 215,
220
subspace 400, 410, 430
ﬁnite element viii, 9, 181, 207, 340, 362,
399, 410, 411, 427, 430
linear system 431
matrix 418
subspace 416
ﬁre 254
ﬁrst-order 19, 182
ordinary diﬀerential equation ix, 297
partial diﬀerential equation 176, 179,
195
Fitz-Hugh–Nagumo equation vii
ﬁxed point 25, 29, 30, 584
ﬂoating-point ix, 184
ﬂood wave 15, 31
ﬂow 3, 19, 22, 139, 242, 243, 244, 304,
311, 360, 504
compressible 37
gradient 386
ideal 504
incompressible 3, 243, 244, 504
ﬂuctuation 137
ﬂuid 2, 15, 37, 152, 292, 388, 438, 504
dynamics 291, 315
ﬂux 153, 244
mechanics vii, 3, 121, 152, 153, 305,
315, 324, 341
source 360
velocity 341
ﬂute 144
ﬂux 38, 46, 201, 256, 504
ﬂuid 153, 244
heat 7, 122, 123, 139, 153, 154, 242,
246, 436, 437, 504, 535

Subject Index
focusing 179
food 136
force 49, 216, 218
electromotive 504
electrostatic 504, 529
external 12, 56, 58, 152, 215, 216,
234, 291, 292, 351, 386, 390, 504
ﬁeld 6, 252, 504
gravitational 154, 242, 487, 529, 531
restoring 169
forced
diﬀusion equation 388
heat equation 134, 296, 312, 314, 442
vibrational equation 390
wave equation 56, 58, 393, 493
forcing 4, 206
frequency 390
function 215, 255, 350, 357, 422
periodic 304, 390, 397
forensics 129
formal adjoint 344
formula
binomial 163
Black–Scholes 302
d’Alembert 16, 121, 140, 146, 260,
427, 552, 558
Euler ix, 6, 88, 267, 573
Green 244, 250, 251, 254, 432
integral 468, 521, 530
Kirchhoﬀ 503, 551, 558
Lagrange 182
Parseval 114, 118, 285, 287, 289
Plancherel 113, 118, 286, 287, 289,
379
Poisson 165, 171, 259
superposition 248, 291, 297
Taylor 75
forward
diﬀerence 182
substitution 212
foundations 64
Fourier
analysis vii, ix, 11, 181, 233, 579
–Bessel eigenfunction 477
–Bessel series 477, 492, 493
–Bessel–spherical harmonic series 541
coeﬃcient 71, 72, 75, 85, 102, 107,
228, 264, 441
complex 89, 229, 284
integral 284, 331, 383
law 123, 437, 535
mode 142, 189, 193

617
Fourier (continued )
series viii, 71, 74, 76, 82, 102, 109,
113, 121, 131, 163, 228, 231, 233,
263, 285, 328, 340, 371, 378, 402,
519
complex 89, 582
cosine 85, 87, 144, 148, 233
derivative of 94
double 488
generalized 109, 378
rescaled 495
sine 85, 87, 126, 142, 147, 157, 233,
382
transform 263, 264, 265, 269, 273,
274, 278, 282, 284, 285, 293, 297,
325, 330, 337, 340, 374, 462
discrete 582
inverse 263, 265, 269
Table 272
two-dimensional 274, 278
fractal 329, 332
Fredholm Alternative 240, 339, 350,
356, 390, 507, 527, 534
free
boundary 486
end 239
-space Green’s function 249, 530
freezer 304, 449
freezing 436
frequency 144, 263, 265, 292, 330, 546
dominant 495
forcing 390
high 127, 128, 193, 231, 296, 331, 442,
536
low 128
resonant 60, 279, 304, 391, 397, 493,
549
space 263, 284
variable 286
vibrational 389, 395, 487, 495, 548
friction 62, 315, 317, 388
Frobenius series 464, 569
fully
nonlinear 174
weak 429, 430
function xvi, 63, 76, 108, 215, 220, 263,
575
Airy 327, 364, 435, 461
second kind 462
algebraic 327, 364, 435, 461
analytic 98, 456, 463
arbitrary 6, 21
associated Legendre 513

618
function (continued )
Bessel 109, 364, 374, 435, 445, 452,
468, 474, 490, 499, 508, 536, 538
second kind 470, 474
spherical 435, 503, 539, 540, 543
bi-aﬃne 415
box 60, 88, 149, 266, 283, 296
C∞ 456, 463
characteristic 485
complementary error 277, 302, 321
complementary harmonic 523
complex 284, 324
constant 285, 506
continuous 99, 108, 117, 219, 220,
344, 576, 579
delta 63, 215, 217, 221, 223, 225, 228,
233, 246, 249, 270, 276, 277, 280,
281, 292, 293, 321, 326, 358, 379,
405, 441, 479, 481, 483, 485, 521,
544, 552, 554
three-dimensional 527
two-dimensional 246, 255
elementary xvi, 9, 55, 267, 310, 337,
435, 445, 452, 453, 459, 467, 539
error 55, 277, 296, 310, 435
even 85, 149, 229, 268, 275, 497, 525
exponential 66, 90, 140
Ferrers 435, 513, 515, 525
trigonometric 516
forcing 215, 255, 350, 357, 422
gamma 454, 455, 461, 468
Gaussian 227, 247, 273, 277, 326, 334
generalized ix, 63, 215, 223, 228, 233,
263, 270, 275, 553
Green’s vii, 3, 215, 234, 236, 239, 240,
242, 248, 253, 256, 258, 280, 291,
292, 340, 358, 360, 371, 379, 383,
388, 480, 503, 525, 527, 528, 531,
533
free-space 249, 530
modiﬁed 358, 381, 480
harmonic 121, 152, 156, 168, 169,
383, 504, 521, 530, 531
hat 88, 227, 274, 283, 405, 407, 412
hyperbolic 88, 91, 157, 241, 322, 334,
508
hypergeometric 364
inverse xvi
L2 286, 344
Legendre 364, 435, 511, 513
linear 220, 339, 341, 344, 585
odd 85, 93, 147, 268, 275, 497, 525
parabolic cylinder 310
pyramid 412, 414, 418
quadratic 340, 362, 401, 416
ramp 82, 91, 95, 223, 235, 239

Subject Index
function (continued )
rational 218, 279, 452
rescaled 96, 506
rotated 155
sawtooth 78
series of 100
sign 94, 225, 270
space 63, 74, 99, 109, 215, 220, 340,
342, 344, 386, 400, 576
special vii, 2, 9, 327, 364, 435, 453,
455, 472, 508, 511
step 29, 40, 42, 61, 105, 276, 320
unit 80, 83, 90, 102, 222, 232, 266,
329, 396
symmetric 236
trigonometric 60, 63, 70, 72, 74, 89,
109, 113, 125, 130, 189, 231, 264,
273, 374, 445, 451, 452, 457, 499,
508, 511, 519, 536
wave 108, 286, 288, 394, 396, 564,
565, 568
weight 379
zero 114, 219, 231, 281, 456, 463
zeta 87
functional 220, 399
analysis 340, 350, 362
Dirichlet 368, 400, 410, 416, 424, 443,
506
equation 454
linear 220, 553, 585
quadratic 340, 362, 400, 402
fundamental
solution 291, 292, 294, 301, 304, 326,
328, 387, 388, 435, 481, 482, 503,
543, 551, 554, 564
general 297
theorem viii, 16, 39, 223, 236, 243,
245, 267
future 130
Galilean boost 20, 311
gamma function 454, 455, 461, 468
gas 37, 123, 129, 549
dynamics 15, 31, 315
Gauss–Seidel method 212, 402, 411
Gaussian
distribution 295
Elimination ix, 210, 213, 351, 402,
407, 413, 416, 579
ﬁlter 291, 295, 482, 544, 545
function 227, 247, 273, 277, 326, 334
kernel 283
general
fundamental solution 297
relativity 31, 63, 504
solution 12, 67, 253, 390

Subject Index
generalized
Fourier series 109, 378
function ix, 63, 215, 223, 228, 233,
263, 270, 275, 553
heat equation 65
Laguerre equation 566, 569
Laguerre polynomial 567, 569
geology 549
geometric
multiplicity 371, 373, 382
sum 116
geometry 31, 63, 121, 156, 181, 423,
488, 578
geophysics 549
Gibbs phenomenon 84, 88, 99, 267
Ginzburg–Landau equation vii
glacier 15
glass 123
gradient viii, 13, 152, 172, 242, 245,
252, 278, 345, 359, 400, 416, 438,
504, 505, 526, 529
ﬂow 386
temperature 341, 437, 535
Gram–Schmidt process 372, 378
gravitation 341, 362, 504, 511, 529
ﬁeld 341
force 154, 242, 487, 529, 531
potential 152, 249, 252, 503, 529, 534
gray-scale 441, 442
Green’s
formula 244, 250, 251, 254, 432
function vii, 3, 215, 234, 236, 239,
240, 242, 248, 253, 256, 258, 280,
291, 292, 340, 358, 360, 371, 379,
383, 388, 480, 503, 525, 527, 528,
531, 533
free-space 249, 530
modiﬁed 358, 381, 480
identity 244
theorem viii, 3, 168, 215, 243, 437
Greenwich 509
Gregory’s series 78, 85, 86
grid 414, 421, 422
grounded 524
group 305, 308
velocity 292, 331
guitar 496
half
disk 260, 480, 494, 496
plane 260
Hamilton–Jacobi equation 305
Hamiltonian 394, 395, 564
hammer 55, 261, 494, 554
handbook 453

619
harmonic
function 121, 152, 156, 168, 169, 383,
504, 521, 530, 531
oscillator 397
part 256
polynomial 163, 171, 506, 520, 522,
526
spherical 109, 435, 503, 517, 519, 528,
538, 540, 549, 565, 568
harmonious 144
hat function 88, 227, 274, 283, 405, 407,
412
heat 63
bath 123, 134, 154, 436
capacity 65
conduction 504
energy 122, 246, 304, 435–7, 482
equation vii, 2, 5, 9, 11, 12, 64, 69,
121, 124, 140, 152, 172, 173, 178,
181, 186, 291, 292, 295, 301, 305,
309, 311, 315, 318, 324, 325, 339,
340, 371, 374, 385, 387, 435, 438,
443, 445, 446, 467, 474, 503, 535,
537, 543, 544
backwards 129, 299, 442
forced 312, 314, 442
generalized 65
lossy 139
spherical 543
ﬂux 7, 122, 123, 139, 153, 154, 242,
246, 436, 437, 504, 535
sensor 303
sink 504
source 292, 438, 442, 504, 543
external 12, 152, 246, 296, 297, 312,
436, 437, 442, 485
speciﬁc 122, 124, 132, 438
total 444, 485, 537
Heisenberg Uncertainty Principle 286–7
helium 564, 568
helix 164
Helmholtz
boundary value problem 160, 441,
443, 445, 475, 487, 490
eigenvalue 377, 383, 446, 474, 535,
546
equation 178, 370, 374, 378, 439, 488,
490, 508, 538, 540, 547
polar 451, 508
spherical 510, 517, 519, 565
hemisphere 522, 523, 534, 542, 561
Hermite
equation 462
polynomial 311, 462

620
Hermitian
dot product 580
inner product 89, 107, 118, 271, 285,
361, 372, 394, 580
norm 89, 394
Hessian matrix 443
high frequency 127, 128, 193, 231, 296,
331, 442, 536
highway 38
Hilbert
space ix, 106, 108, 112, 119, 263, 284,
286, 368, 394
transform 283
hole 214, 243, 286, 426
homogeneous 2, 9, 10, 172, 356, 390,
583, 586
polynomial 169, 520
Hopf–Cole transformation 318
Hôpital’s Rule 117, 229
horizontal axis 18
horse 334
Huygens’ Principle vii, 503, 551, 560,
562, 564
hydrodynamics 305
hydrogen 383, 503, 564, 568
hyperbola 173
hyperbolic 2, 122, 172, 173, 178, 179,
195, 385, 497
cosine 241
cotangent 91, 322
function 88, 508
secant 334
sine 157, 241
trajectory 565
hypergeometric function 364
hypersurface 175
ice 138, 524, 542
ideal ﬂuid ﬂow 504
identity
additive 575
matrix 584
operator 383, 384
ill-posed 129, 175, 436, 442
image xvi, 63, 441
enhancement 129, 442
method 256, 532, 534
mirror- 143, 148, 149
point 532
processing 2, 63, 129, 263
imaginary
part ix, 571
unit 571
impedance 154

Subject Index
implicit
diﬀerentiation 49
equation 26
function theorem 49
scheme 191
impulse 221, 234, 248, 291, 292, 387
boundary value problem 236
point 236
unit 215–218, 234, 280, 551, 562
incomplete matrix 583, 584
incompressible 3, 243, 244, 504
independent variable 3, 6, 15, 291, 300
index 464
indicial equation 464, 465, 520
inelastic 337
inequality
Bessel 111, 113, 379, 380, 441
Cauchy–Schwarz ix, 107, 175, 285,
526, 579, 581
triangle ix, 107, 526, 572, 579, 581
inﬁnite-dimensional ix, 11, 99, 109, 215,
340, 342, 371, 400, 577
inﬁnitely diﬀerentiable 5, 75, 105, 128,
158
inﬂation 550
inhomogeneous 2, 12, 133, 215, 217,
234, 350, 424, 427, 442, 507, 584,
586
initial
-boundary value problem 7, 121, 172,
293, 297, 306, 314, 339, 386, 440,
535, 546
periodic 328, 493
condition 4, 7, 53, 291, 292, 297, 306,
389, 453, 458, 487, 535
data 127, 175, 292, 295, 436, 481, 535
displacement 53, 59, 145, 487, 546,
547, 551, 554, 556, 557, 560, 561,
562
position 50
temperature 131, 544
value problem ix, 2, 6, 19, 50, 53,
172, 175, 263, 292, 325, 328, 371,
458, 544, 551, 560
velocity 50, 55, 59, 145, 487, 546, 547,
551, 554, 557, 560, 562
inner product ix, 107, 112, 118, 114,
339, 341, 371, 379, 400, 428, 578,
580
Hermitian 89, 107, 118, 271, 285, 361,
372, 394, 580
L2 73, 220, 230, 247, 342, 346, 358,
359, 371, 439, 477, 505, 515, 517,
525
space 350, 354, 400, 428, 429, 578
complex 118, 580

Subject Index
inner product (continued )
weighted 341, 344, 354, 358, 359, 365,
378, 438, 506, 569
inner tube 361, 493
insulated 122, 153, 154, 436, 441, 443,
504, 522, 535
insurance 299
integer xv, 453
integrable 127, 292, 324, 333, 337
square- 106, 108, 231, 284, 285, 293,
394
integral viii, 219, 236, 264, 406
boundary 347
complex 266, 461
convergence test viii, 105
convolution 301
double viii, 58, 243, 245, 248, 251,
346, 422, 432, 437, 448
equation 9, 284, 350
exponential 267
formula 468, 521, 530
Fourier 284, 331, 383
line viii, 39, 243, 245, 251, 256, 433,
437
Poisson 165, 171, 259
of series 101
surface viii, 505, 522, 529, 533
triple viii, 505, 553
integrating factor 22
integration viii, 63, 92, 101, 263, 276,
585
by parts viii, 226, 245, 288, 342, 346,
428, 430, 432, 454, 461, 505
Lebesgue ix, 107, 217, 219, 263, 270
numerical 185, 407, 422, 453
of series 92, 101
Riemann 217, 219, 270
interest 299, 304
interior
maximum 443
node 188, 207, 209, 404, 411, 412,
418, 422, 425
point 169
interpolation 27, 210, 400
interstellar space 552
interval 17, 73, 95, 102, 217, 220, 264,
284, 576
invariant 42, 291
inverse
additive 575
Fourier transform 263, 265, 269
function xvi
matrix xvi, 217
scattering 337
square law 252, 529
tangent 277

621
inversion 258, 526, 532
investment 291, 299
inviscid
Burgers’ equation 315
limit 292, 315, 317, 320
irrational 60, 107, 144, 389
time 292, 329, 332
irregular
domain 213
singular point 453, 463
irreversibility 130
irrotational 242, 360, 504
isosceles 413, 415, 417, 421
isotropic 435, 436, 437, 485, 508, 528,
535, 537
iterative 187, 453, 583
numerical method ix, 203, 212, 411
Gaussian–Seidel 212, 402, 411
Jacobi 212
successive over-relaxation (SOR)
212, 402, 411
system 190, 402, 416
Jacobi method 212
Jacobian determinant 58, 255
Jordan canonical form 67, 584
joule 287
jump 441
discontinuity 80, 81, 82, 96, 164, 223,
233, 236, 405, 432
magnitude 80, 223
rope 148, 393
Kadomstev–Petviashvili equation vii
Kelvin 311
Kepler problem 565
kernel 350, 356, 400, 586
Gaussian 283
Poisson 165, 259
kinetic energy 61
Kirchhoﬀ’s formula 503, 551, 558
Kolmogorov–Petrovsky–Piskounov equation vii
Korteweg–de Vries equation vii, 2, 292,
324, 333, 336
modiﬁed 337
L2
function 286, 344
Hermitian inner product 89, 107, 580
Hermitian norm 89
inner product 73, 220, 230, 247, 342,
346, 358, 359, 371, 439, 477, 505,
515, 517, 525
norm 89, 106, 284, 383, 394, 445, 477,
525, 579
ladder 503

622
lag 137
Lagrange’s formula 182
Laguerre
equation 364, 566, 569
polynomial 109, 566, 567, 569
Lamé equation vii
Laplace
equation 2, 8, 13, 121, 152, 155, 166,
169, 172, 175, 178, 181, 207, 213,
243, 249, 256, 291, 311, 312, 339,
399, 422, 435, 467, 503, 504, 507,
509, 520, 522, 527
transform 263
Laplacian 4, 13, 152, 171, 243, 250, 339,
359, 374, 447, 451, 507, 525, 528,
535, 537, 557, 564
spherical 510, 517, 525, 538, 543, 549,
551
laser 292
latitude 508
law
of cosines 258, 556
Fourier 123, 437, 535
inverse square 252, 529
Newton’s second 49
Lax–Wendroﬀ scheme 200
least squares 110, 363
Lebesgue integration ix, 107, 217, 219,
263, 270
left-hand
derivative 81
limit 79, 81
Legendre
boundary value problem 511, 515
equation 364, 511, 514, 525
function 364, 435, 511, 513
polynomial 511, 513, 525
lemma
du Bois–Reymond 431, 434
Riemann–Lebesgue 112, 117, 231, 276
Lie group 305
lifted characteristic curve 49
light 15, 149, 546, 551, 552, 560, 562
beam 286
cone 56, 560, 564
ray 179
sensor 561
speed 295, 551
wave 551
lightning 551, 552, 560, 563
limit viii, 63, 98, 107, 218, 219, 221, 247
inviscid 292, 315, 317, 320
left-hand 79, 81
one-sided viii
right-hand 79, 81

Subject Index
line 32, 396, 404, 563, 576
characteristic 21, 45, 195
integral viii, 39, 243, 245, 251, 256,
433, 437
shock 40, 173
spectral 395
real xv, 107, 284
linear 10, 64, 306, 394, 404, 585
algebra viii, ix, 63, 215, 221, 234, 339,
353, 400, 575
numerical ix, 399
combination 11, 12, 156, 216, 221,
401, 413, 430, 577
dependence 577
diﬀerential operator 9, 10, 64, 339,
350, 371, 586
equation 9, 11, 215, 216, 428
function 220, 339, 341, 344, 585
functional 220, 553, 585
independence ix, 112, 401, 577
operator 110, 284, 339, 340, 350, 354,
355, 386, 400, 428, 585
superposition 11, 51, 59, 67, 71, 234,
242, 293, 328, 330, 476, 481, 483,
535, 544, 551, 552, 554
system ix, 187, 191, 216, 339, 350,
386, 400, 428, 582, 586
transformation 285
linearization 317, 318
Lipschitz continuity 29
liquid 121, 123, 324, 438, 503
crystal 427
lithium 568
local maximum 507
localized 333, 503, 551
disturbance 179, 292
solution 560
logarithm 250, 452, 464
complex 164, 573
logarithmic
derivative 336
potential 253, 256, 383
singularity 383, 525
term 468
logic 64
longitude 508
lossy
convection-diﬀusion equation 139
diﬀusion equation 194
heat equation 139
low frequency 128
lower triangular matrix 211
M –test 100, 105
Mach number 174

Subject Index
magnetic
ﬁeld 341, 546, 551
quantum number 568, 569
magneto-hydrodynamics vii, 315
magniﬁcation factor 189, 192, 199, 205
magnitude 80, 221, 223
manufactured solution 210
Maple 273, 453
mass 6, 37, 39, 49, 95, 216, 333, 396,
531, 564
conservation 41, 47, 360
ﬂux 38, 47
point 249, 252, 256, 529
mass-spring system 95
materials science 175
Mathematica 273, 453
mathematics vii, ix, 2, 16, 64, 435
Mathieu equation 409
matrix 66, 216, 339, 582
boundary coeﬃcient 425
coeﬃcient 188, 191, 350, 386
convergent 584, 585
ﬁnite element 418
Hessian 443
identity 584
incomplete 583, 584
inverse xvi, 217
lower triangular 211
multiplication 341, 344, 356, 585
orthogonal 506
positive deﬁnite 342, 353, 362
self-adjoint 354, 373
symmetric 66, 74, 342, 353, 354, 362,
372, 401
tridiagonal 188, 191, 211, 407
upper triangular 211
weighted adjoint 342
zero 583
maximum 169, 507, 522
principle vii, 169, 213, 291, 312, 318,
443, 537
strong 169, 314, 522
weak 314, 507
Maxwell’s equations vii, 546, 551
Maxwell–Bloch equation vii
mean 92, 286, 287, 519, 553, 555
temperature 136
zero 78, 92
measure ix, 63, 102
zero 102, 104, 108, 114, 117
mechanics viii, 435, 578
classical 394, 503
continuum viii, 38
equilibrium 121

623
mechanics (continued )
ﬂuid vii, 3, 121, 152, 153, 305, 315,
324, 341
Newtonian 7
quantum vii, viii, x, 6, 108, 149, 263,
278, 284, 286, 288, 329, 337, 340,
383, 386, 394, 503, 564, 566, 568
solid 121, 292
medium 2, 64, 390, 427, 435, 504
membrane 15, 121, 152, 153, 154, 158,
169, 179, 249, 361, 435, 486, 487,
490, 492, 550
meridian 509
mesh 186, 195, 207, 411, 414, 418, 421,
423
metal
ball 534, 542
bar 64, 138, 292
disk 166, 479
pipe 170
plate 214, 418, 426, 435, 481, 497
shell 524
sphere 534
method
of images 256, 532, 534
Newton ix, 135
Perron 255
Richardson 194
Runge–Kutta ix
stationary phase 331
microwave 15
minimal surface 153, 175
minimization principle 255, 340, 371,
399, 427, 429, 507
minimizer 399, 400, 401, 443
minimum 168, 522
Minneapolis 137
minute 307
mirror-image 143, 148, 149
mixed
boundary condition 124, 343, 345,
347, 359, 364, 368, 436, 439, 441,
486, 504, 527, 535, 544, 546
boundary value problem 7, 134, 154,
214, 241, 245, 505, 507
partial derivatives 5, 8, 50, 242, 556
mode
Fourier 142, 189, 193
normal 389, 475, 487, 488, 489, 492,
499, 546, 565
unstable 145, 340, 547
modeling viii

624
modiﬁed
Bessel equation 473
Dirichlet integral 368
Green’s function 358, 381, 480
Korteweg–de Vries equation 337
modulus ix, 571
molecule 438, 503, 564
momentum 39, 61, 62, 286, 287
money 299
monomial 577
monotone 29, 30
Monte Carlo 181
movie xv, 63
moving
boundary 486
coordinate 15, 19
multigrid 181
multiplication 263
complex 571
matrix 341, 344, 356, 585
operator 286, 586
scalar 575
unit 575
multiplicity 371, 373, 382
multiply valued solution 36, 37
multipole 181
multiscale 442
multi-soliton solution 337
multivariable calculus 215, 437, 505
music 63, 144
N–wave 48
natural number xv
Nature 340
Navier–Stokes equations vii, 3, 9, 315
negative semi-deﬁnite 443
neon 569
Neumann
boundary condition 7, 123, 144, 153,
343, 345, 347, 359, 364, 368, 374,
436, 439, 441, 486, 504, 527, 535,
544, 546
boundary value problem 148, 238,
239, 245, 352, 387, 534
neuron 139
neuroscience vii
Newton
mechanics 7
potential 503, 504, 529, 530, 532
method ix, 135
Second Law 49
nodal
curve 497, 551
region 497, 551

Subject Index
node 186, 187, 404, 411
boundary 188, 207, 411, 418, 422, 425
corner 209
interior 188, 207, 209, 404, 411, 412,
418, 422, 425
no-ﬂux 153
noise 193, 442, 536
non-characteristic 176
nonlinear vii, 60, 222, 324, 428, 549
diﬀusion 2, 122, 291, 437, 442
fully 174
partial diﬀerential equation 2, 12, 31,
222, 305, 315, 333, 486
Schrödinger equation vii
transport 38, 46, 130, 292, 308, 315,
317, 427, 431
wave equation 317, 333
nonlinearizing 318
nonsingular 211, 216
nonsmooth 129, 399, 427
nonuniform 10
convergence 267
transport 24
norm ix, 98, 109, 344, 356, 400, 571,
578
convergence in 98, 99, 109, 110, 231
double weighted L2 381
Euclidean 169, 572, 578
L2 89, 106, 284, 383, 394, 445, 477,
525, 579
-preserving 286
unit 108, 286, 394
normal ix, 295
derivative 7, 153, 245, 250, 436, 504,
528, 529
mode 389, 475, 487, 488, 489, 492,
499, 546, 565
unit 153, 244, 433, 436, 437, 504, 505
north pole 511, 534
nowhere diﬀerentiable 329
nucleus 279, 394, 395, 503, 564, 569
null
eigenfunction 70, 132, 145, 386, 387,
389, 441, 487, 547, 550
eigenvalue 131, 439, 582
eigenvector 582
number
complex viii, ix, xv, 571
Mach 174
natural xv
Péclet 311
quantum 566–9
rational 59, 107
real xv, 98, 571

Subject Index
number (continued )
theory 31, 121, 500
wave 330
numerical
algorithm 399
analysis ix, 181
approximation vii, 9, 181, 406
Crank–Nicolson method 192
diﬀerentiation 181
domain of dependence 197, 199
explicit scheme 188
geometric method 181
implicit scheme 191
integration 185, 407, 422, 453
iterative method ix, 203, 212, 411
Gaussian–Seidel 212, 402, 411
Jacobi 212
successive over-relaxation (SOR)
212, 402, 411
Lax–Wendroﬀ scheme 200
linear algebra ix, 399
multigrid method 181
multipole method 181
Monte Carlo method 181
pseudo-spectral method 181
Richardson’s method 194
Runge–Kutta method ix
simulation 324
spectral method 181
stability 181, 190, 205
symplectic method 181
upwind scheme 200
observer 552, 556, 558, 562
obtuse 414
odd 85, 93, 147, 268, 275, 497, 525
oil 129, 549
one-sided
derivative 576
limit viii
open ix, 152
operator
Bessel 367
derivative 354
diﬀerential 9, 64, 339, 350, 371
partial 2, 50
divergence 347
identity 383, 384
linear 110, 284, 339, 340, 350, 354,
355, 386, 400, 428, 585
multiplication 286, 586
Schrödinger 565
Sturm–Liouville 364, 365, 480
wave 50, 487
optics vii, 179, 292, 329, 459
option 291, 299, 300–2, 304

625
orbit 452, 503, 565
order 1, 3, 366, 467, 511, 569
ordinary
derivative 1, 3
diﬀerential equation ix, 2, 6, 8, 11,
22, 65, 67, 121, 130, 140, 185, 215,
217, 263, 291, 308, 333, 363, 399,
435, 445, 452, 455, 463, 585
ﬁrst-order ix, 297
linear 11, 585
second-order ix, 297, 363, 459, 463
separable ix
orientation 243, 504
orthogonal ix, 73, 271, 286, 340, 350,
371, 378, 387, 428, 440, 441, 447,
462, 477, 489, 517, 525, 536, 547,
569, 581, 583
basis 73, 112, 372, 581
matrix 506
system 73, 113, 441, 515
orthonormal 109, 378, 514
basis 112, 113, 581, 583
system 73, 113, 570
oscillation 230, 267, 397, 474
oscillatory
error 193
wave 292, 327
outer space 487, 493, 531, 550
out of phase 137
oval 418, 423, 425
oven 138, 449, 480, 542
overdamped 146
overtone 63, 487, 495, 496
parabola 173, 309
parabolic 2, 122, 172, 173, 178, 193,
291, 315, 385, 438
arc 556
coordinates 174
cylinder function 310
spheroidal coordinates 508
trajectory 565
parallel 579
parameter viii, 66
relaxation 213
Parseval’s formula 114, 118, 285, 287,
289
partial
derivative viii, 1, 3, 521
mixed 5, 8, 50, 242, 556
diﬀerential equation vii, ix, 2, 63, 67,
130, 176, 215, 263, 305, 317, 339,
350, 394, 399, 429, 487, 582
dynamical 291, 551
elliptic 2, 122, 172, 173, 178, 212,
399, 404, 410

626
partial diﬀerential equation (continued )
ﬁrst-order 176, 179, 195
fourth-order 486
fully nonlinear 174
hyperbolic 2, 122, 172, 173, 178,
179, 195, 385, 497
ill-posed 129, 174, 436, 442
nonlinear 2, 12, 31, 174, 222, 305,
315, 333, 486
parabolic 2, 122, 172, 173, 178, 193,
291, 315, 385, 438
quasilinear 174
second-order 121, 172
third-order 2, 324
type 172, 173
well-posed 136, 395, 535
diﬀerential operator 2, 50
fraction 279
sum 75, 100, 110, 113, 229
particle 149, 286
quantum 6, 396
subatomic 108, 149, 279
-wave duality 55, 149
particular solution 12, 253, 390
passenger 19, 20
path-independent 256
pathwise connected 5, 245
Pauli Exclusion Principle 568
Péclet number 311
percussion 144
periodic 59, 82, 144, 147, 389, 393, 395,
489, 548
boundary condition 69, 124, 130, 292,
343, 345, 361
boundary value problem 69, 373, 383,
387, 510
convolution 95
eigenvalue problem 130
extension 77, 96, 102, 229
force 304, 390, 397
function 60, 130, 278
initial-boundary value problem 328,
493
Table 503, 568
permeability 551
permittivity 551
perpendicular 244, 581
Perron method 255
petri dish 438, 485
phase ix, xvi, 137, 572
lag 137
shift 292, 324, 334
stationary 331
velocity 330
photoelectric eﬀect 149

Subject Index
photon 108, 149, 286, 395
physical
space 263, 284, 503
variable 286
physicist 217
physics vii, viii, x, 37, 38, 55, 63, 305,
324, 339, 341, 394, 435, 509
piano 63, 144, 145, 261, 393, 496
piecewise
aﬃne 404, 411
continuous 79, 81, 108, 127, 441
cubic 408
smooth 254, 427, 505
C1 80, 82, 94, 223, 233
C2 94
Cn 81
pie wedge 170
pipe 31, 170
piston 37
pitch 495
pivot 579
Plancherel formula 113, 118, 286, 287,
289, 379
Planck’s constant 6, 287, 394, 564
plane 411, 571, 576
complex xv, 571
curve 172
half 260
quarter 24
square 255
planet 452, 530, 534, 550, 565
plasma vii, 2, 292, 388, 315
plate 2, 65, 152, 153, 246, 253, 324, 435,
441, 442, 486, 497
rectangular 154, 445
semi-circular 155
point
boundary 153, 169
charge 249, 252
critical 501
ﬁxed 25, 29, 30, 584
ﬂoating ix, 184
image 532
interior 169
mass 249, 252, 256, 529
regular 453, 457, 463
set topology viii
singular 2, 283, 363, 452, 457, 463
irregular 453, 463
regular 453, 463, 511, 569
pointwise convergence 99, 109, 115, 117,
285, 378

Subject Index
Poisson
–Darboux equation 62
equation 2, 121, 152, 169, 172, 173,
181, 207, 215, 242, 245, 248, 253,
255, 256, 339, 352, 359, 361, 368,
383, 399, 401, 410, 422, 435, 442,
503, 504, 507, 527, 530, 532, 533,
534
integral 165, 171, 259
kernel 165, 259
polar
angle 572
coordinates viii, 3, 62, 160, 161, 171,
250, 383, 451, 477, 479, 490, 509,
572
Helmholtz equation 451, 508
pole 511, 534, 549
pollutant 2, 15, 19, 23, 139, 304, 435
polygon
boundary 411, 417
vertex 414, 423
polyhedral surface 411
polynomial 1, 50, 75, 119, 400, 402, 576,
577
Chebyshev 473
Laguerre 109, 566, 567, 569
harmonic 163, 171, 506, 520, 522, 526
Hermite 311, 462
homogeneous 169, 520
Laguerre 109, 566, 569
Legendre 511, 513, 525
quadratic 316
trigonometric 71, 75, 91, 108, 111,
119, 400, 402
pond 292, 324, 331
population 2, 121, 123, 435, 438, 485
portfolio 291, 300
posed
ill 129, 175, 436, 442
well 136, 395, 535
position 50, 286, 287, 396
positive 578, 580
deﬁnite 339, 340, 355, 356, 362, 363,
371, 372, 373, 375, 376, 378, 383,
386, 387, 389, 400, 401, 429, 431,
439, 441, 442, 474, 506, 535, 546,
578
matrix 342, 353, 362
semi-deﬁnite 339, 355, 356, 371, 372,
378, 386, 387, 389, 441, 487, 506,
547, 549
potassium 569
potato 542, 543

627
potential 341
Burgers’ equation 318
Coulomb 503, 529, 564
electric 504
electromagnetic 2, 564
electrostatic 152, 249, 252, 256, 503,
522, 534
energy 6, 61, 152, 242, 318, 340, 362
gravitational 152, 249, 252, 503, 529,
534
logarithmic 253, 256, 383
Newtonian 503, 504, 529, 530, 532
theory 121
velocity 360, 504
power
ansatz 162, 464, 520
series viii, 75, 92, 98, 169, 453, 456,
458, 464, 511, 521
solution 445, 452, 463
pressure 3, 35
price 299
prime meridian 509
principal
quantum number 567, 568
value 283, 572
principle
Dirichlet 368, 400, 410, 416, 424, 443,
506
Duhamel 298
Heisenberg Uncertainty 286, 287
Huygens vii, 503, 551, 560, 562, 564
maximum vii, 169, 213, 291, 312, 318,
443, 537
strong 169, 314, 522
weak 314, 507
minimization 255, 340, 371, 399, 427,
429, 507
Pauli Exclusion 568
superposition 9, 11, 12, 215, 217, 235,
335, 586
symmetry 269, 271, 275
probability 109, 121, 181, 263, 394
density 108, 286, 564
process
diﬀusion 121, 129, 386
dynamical 7, 15, 172
Gram–Schmidt 372, 378
product 282
convolution 281
cross viii
dot viii, 73, 341, 346, 354, 372, 578,
580, 585

628
product (continued )
inner ix, 107, 112, 118, 114, 339, 341,
371, 379, 400, 428, 578
Hermitian 89, 107, 118, 271, 285,
361, 372, 394, 580
L2 73, 220, 230, 247, 342, 346, 358,
359, 371, 439, 477, 505, 515, 517,
525
weighted 341, 344, 354, 358, 359,
365, 378, 438, 506, 569
rule viii
proﬁt 299, 302
program 453
proof ix
proton 564, 569
pseudo-spectral 181
pulse 54, 266, 267, 275
put option 300, 304
pyramid function 412, 414, 418
quadrant 499
quadratic
equation 172, 464, 465
function 340, 362, 401, 416
functional 340, 362, 400, 402
polynomial 316
quadrilateral 416
quantization 287, 394, 564
dispersive 328, 329
quantum
-dynamical system 503
mechanics vii, viii, x, 6, 55, 108, 149,
263, 278, 284, 286, 288, 329, 337,
340, 383, 386, 394, 503, 564, 566,
568
number 566–9
particle 6, 396
quasar 552
quasilinear 174
quasiperiodic 60, 144, 389, 393, 395,
489, 492, 497, 546, 548, 549
quotient
Rayleigh 340, 371, 375, 376, 377, 381,
384, 387
rule viii
radial
derivative 554
displacement 546
Gaussian function 247
symmetry 249, 479, 528, 531, 541,
548
variable 249
vibration 547, 549
wave function 568
radian 509
radiation 65

Subject Index
radio 15, 63, 546
radioactive 22
radius
Bohr 567, 569
of convergence 453, 458
spectral 584
rainbow 459
ramp function 82, 91, 95, 223, 235, 239
higher order 95, 223
random 60, 129
range 585, 586
Rankine–Hugoniot condition 40, 46, 47,
317, 321, 431, 433
rarefaction wave 34, 43, 44, 309, 320,
323, 433
ratio test viii, 75, 462, 468
rational 59, 389
function 218, 279, 452
number 59, 107
time 292, 329
ray 179, 257, 499
Rayleigh quotient 340, 371, 375, 376,
377, 381, 384, 387
reaction vii, 31, 438
-diﬀusion equation 438
real 6, 371, 571
analysis ix, 219
line xv, 107, 284
number xv, 98, 571
part ix
reciprocal 572
rectangle 121, 156, 214, 248, 255, 312,
361, 374, 415, 445, 482, 488, 493,
496, 497, 499
rectangular
box 508, 526, 536, 547, 550
coordinates viii, 161, 503
drum 499
grid 422
mesh 186, 195, 207, 411, 418
plate 154, 445
pulse 266
recurrence relation 454, 456, 459, 467,
471, 525, 539, 543, 570
reduced ﬁnite element matrix 418
reﬂection 506, 571
refraction 179
refrigeration 136, 537
regular 457
boundary value problem 379
mesh 414
point 453, 457, 463
singular point 453, 463, 511, 569

Subject Index
regularization 317, 442
relative vibrational frequency 495, 548
relativity vii, 2, 31, 56, 63, 295, 504,
560
relaxation parameter 213
remainder term 182
removable discontinuity 80
reparametrization 179
repulsive 529
rescaled
domain 495
Fourier series 96, 264
function 96, 506
reservoir 123
resonant 60, 279, 304, 391, 397, 493,
549
restoring force 169
reverse shock 433
Richardson’s method 194
Riemann
hypothesis 87
geometry 31
integration 217, 219, 270
–Lebesgue Lemma 112, 117, 231, 276
sum 265
zeta function 87
right
angle 73, 581
triangle 413, 417, 419, 423
right-hand
derivative 81
limit 79, 81
rim 486
ring 69, 124, 130, 146
ripple 292
river 31
Robin boundary condition 123, 134,
154, 361, 436, 439
rock 292, 324, 549
rod 534
root ix, 316, 464, 474, 582
Bessel 474, 490, 499
spherical 540, 541, 548
cellar 136, 137, 545
test viii, 75
rope 148, 393, 494
rotated function 155
rotation 305, 506, 517, 528
rough 284, 296
round-oﬀ error ix, 184, 190
row vector 216, 578
rubber 493

629
rule
chain viii, 20, 24, 32, 57, 161, 167,
171, 300, 305, 308, 333, 494, 509,
511, 524
product viii
quotient viii
trapezoid 407
Runge–Kutta method ix
sample 27, 187, 189
sand 498
sawtooth function 78
saxophone 144
scalar
ﬁeld 242, 345, 359, 439, 505
multiplication 575
scaling 155, 305, 306, 307, 308, 323,
494, 521
symmetry 291, 307, 308, 327, 337,
479
scandium 569
scattering 337, 383, 549, 565
Schrödinger
equation vii, 6, 146, 304, 340, 372,
374, 383, 385, 394, 396, 397, 503,
564
operator 565
science vii, 2, 4
screen 286
sea 332
secant 182, 185
second-order
iterative scheme 203
ordinary diﬀerential equation ix, 297,
363, 459, 463
partial diﬀerential equation 121, 173
ramp function 95
segment 404
seismology 63, 129, 549
self-adjoint 74, 110, 237, 243, 249, 339,
340, 354, 356, 362, 371, 378, 386,
400, 429, 438, 447, 505, 517, 525,
536, 579
matrix 354, 373
semicircle 480, 499
circular disk 170, 418
circular plate 155
inﬁnite bar 136
weak formulation 429
semi-deﬁnite
negative 443
positive 339, 355, 356, 371, 372, 378,
386, 387, 389, 441, 487, 506, 547,
549
sensor 23, 303, 561

630
separable
coordinates 508
ordinary diﬀerential equation ix
solution 68, 121, 124, 141, 156, 163,
306, 320, 386, 388, 439, 463, 482,
509, 517, 520
separation
constant 141, 156, 446, 510
of variables vii, ix, 25, 68, 121, 140,
155, 161, 215, 256, 305, 340, 364,
386, 435, 447, 451, 467, 487, 503,
507, 527, 535, 538, 547, 565
sequence viii, xvi, 98, 104
Cauchy 107, 119
series viii, 2, 9, 11, 67, 68, 75, 100, 121,
156, 256, 435, 455
complex Fourier 89, 582
diﬀerentiated 101
doubly inﬁnite 89, 91
eigenfunction 109, 340, 371, 378, 379,
386, 435, 441, 443, 535, 544
eigensolution 440
Fourier viii, 71, 74, 76, 82, 102, 109,
113, 121, 131, 163, 228, 231, 233,
263, 285, 328, 340, 371, 378, 402,
519
–Bessel 477, 492, 493
–Bessel–spherical harmonic 541
complex 89, 582
cosine 85, 87, 144, 148, 233
derivative of 94
double 488
generalized 109, 378
rescaled 495
sine 85, 87, 126, 142, 147, 157, 233,
382
of functions 100
Frobenius 464, 569
Gregory 78, 85, 86
integrated 101
power viii, 75, 92, 98, 169, 453, 456,
458, 464, 511, 521
solution 445, 452, 463
Taylor 75, 169, 171, 182, 183, 193,
453, 458, 463, 521, 576
sesquilinear 372, 394, 580
set
theory 64
diﬀusion of 485
shallow water 292, 333, 435
sharpen 442
shell 324, 450, 524, 568
spherical 510, 534, 543, 549, 551
shift
phase 292, 324, 334
theorem 271, 274, 284

Subject Index
shock 39, 45, 47
dynamics 15, 431
line 40, 173
reverse 433
wave vii, 2, 5, 15, 31, 37, 130, 195,
292, 315, 316, 317, 320, 324, 399,
427, 431, 433
shore 332
signal 15, 22, 23, 128, 179, 295, 486,
552, 560
processing 63, 263, 582
sign function 94, 225, 270
silver 123
similar triangle 257, 532
similarity
solution vii, 42, 291, 305, 308, 323,
327, 337, 485
transformation 307
variable 308
simple
closed curve 152
eigenvalue 372, 391, 582
simply
connected 243
supported 146, 393
simulation 324
sine transform 274
single-precision 184
singlet 558
singular 173, 582
boundary value problem 379
point 2, 283, 363, 452, 457, 463
irregular 453, 463
regular 453, 463, 511, 569
singularity 251, 256, 427, 454, 553
logarithmic 383, 525
sink 360, 438, 442, 504
slope 21, 37, 182, 185
Smith Prize 3
smooth 5, 284, 441, 536, 576
piecewise 254, 427, 505
smoothing 128, 296, 441, 545
smoothness 7, 105, 276, 402, 410, 428,
430
snare drum 496
soap 153
soda 537
soil 137
soldering iron 292, 297, 304, 481, 534
solid 121, 388
body 2, 15, 65, 144, 440, 503, 504,
522, 546
boundary 154, 504
geometry 423
mechanics 121, 292

Subject Index
solitary wave 324, 334
soliton 292, 324, 336, 337
solute 22, 121, 438
solution 5, 6, 11, 181, 291, 350, 452
analytic 431
basis 458
classical 5, 7, 17, 51, 144, 255, 399,
410, 427, 428, 432, 535, 546, 556,
558
complex 6
d’Alembert 53, 201, 324, 487
equilibrium 132, 443
fundamental 291, 292, 294, 297, 301,
304, 326, 328, 387, 388, 435, 481,
482, 503, 543, 551, 554, 564
general 12, 67, 253, 390
localized 560
manufactured 210
multiply valued 36, 37
multi-soliton 337
particular 12, 253, 390
power series 445, 452, 463
separable 68, 121, 124, 141, 156, 163,
306, 320, 386, 388, 439, 463, 482,
509, 517, 520
similarity vii, 42, 291, 305, 308, 323,
327, 337, 485
weak 5, 43, 53, 144, 399, 427, 429,
431, 432, 433, 489, 556, 558
sonic boom 173
SOR 212, 402, 411
sound 560, 562
barrier 15, 173
speed 174
wave 121, 551, 552
source 194, 292, 360, 438, 442, 504, 543,
552
space 6, 15, 18, 121, 291, 305, 494, 504
coordinates 3
derivative 291
Euclidean ix, 575, 578
frequency 263, 284
function 63, 74, 99, 109, 215, 220,
340, 342, 344, 386, 400, 576
Hilbert ix, 106, 108, 112, 119, 263,
284, 286, 368, 394
inner product 118, 350, 354, 400, 428,
429, 578, 580
outer 487, 493, 531, 550, 552
physical 263, 284, 503
three-dimensional 503, 528, 544, 552
-time 19, 56, 560
two-dimensional 252
vector ix, 11, 98, 112, 220, 341, 371,
575, 578
span ix, 112, 401, 413, 577

631
sparse 402, 407, 410
special
function vii, 2, 9, 327, 364, 435, 453,
455, 472, 508, 511
relativity 56
speciﬁc heat 122, 124, 132, 438
spectral 181
line 395
radius 584
spectrum 395
continuous 337, 340, 374, 383, 565
speech 63
speed 331, 334
of light 295, 551
of sound 174
wave 19, 22, 24, 50, 51, 65, 140, 195,
197, 292, 330, 334, 396, 486, 488,
492, 495, 549
sphere 108, 508, 511, 517, 521, 522, 523,
526, 532, 534, 552, 555, 560
spherical
Bessel function 435, 503, 539, 540,
543
Bessel root 540, 541, 548
cap 556, 561
capacitor 522, 523, 526, 533
cavity 524, 534
coordinates viii, 3, 503, 508, 520, 524,
528, 537, 551, 553, 565
harmonic 109, 435, 503, 517, 519,
528, 538, 540, 549, 565, 568
complex 519, 526, 565
Fourier–Bessel 541
heat equation 543
Helmholtz equation 510, 517, 519,
565
Laplacian 510, 517, 525, 538, 543,
549, 551
shell 510, 534, 543, 549, 551
symmetry 531, 552
wave 552, 554
spike 218, 224, 229
spin 568
spiral 164
spline 27, 210, 283, 400, 408
spring 95, 154, 216, 333, 473
square 369, 385, 415, 418, 426, 444, 449,
479, 489, 492, 496, 499
drum 493
grid 414, 421
hole 426
-integrable 106, 108, 231, 284, 285,
293, 394
least 110, 363

632
square (continued )
mesh 421, 423
plate 255
unit 155, 158, 260, 485
stable 181, 190, 205, 340
conditionally 190
unconditionally 192, 193
von Neumann criterion 198, 199, 205
standard
basis 216, 577, 581
deviation 295
standing wave 55
star 560
state space 286
stationary 20
phase 331
point 331
wave 17
statistics vii
steepest decrease 437
step
function 29, 40, 42, 61, 105, 276, 320
unit 80, 83, 90, 102, 222, 232, 266,
329, 396
size 182, 187, 406
time 197
stiﬀness 49, 142, 234, 344, 357, 473,
486, 488, 492, 495
elemental 417, 418
stochastic diﬀerential equation viii, 299
stone 331
stopping time 128
St. Elmo’s ﬁre 254
strain 341
string 2, 13, 15, 49, 62, 96, 140, 142,
261, 391, 489
Strong Maximum Principle 169, 314,
522
structure 549
Sturm–Liouville
boundary value problem 363, 382
eigenfunction 382
equation 336, 364, 370, 404, 508
operator 364, 365, 480
subatomic particle 108, 149, 279
subsolution 255
subsonic 173, 174
subspace 576
dense 344, 346, 371
ﬁnite-dimensional 400, 410, 430
ﬁnite element 416
subterranean 129
successive over-relaxation (SOR) 212,
402, 411

Subject Index
sum
convolution 284
geometric 116
partial 75, 100, 110, 113, 229
Riemann 265
trigonometric 116
summer 136, 137, 545
sun 545, 565
supercomputer 565
superconductivity vii
supernova 551
superposition 11, 51, 59, 67, 71, 234,
242, 293, 328, 330, 476, 481, 483,
535, 544, 551, 552, 554
formula 248, 291, 297
principle 9, 11, 12, 215, 217, 235, 335,
586
supersonic 15, 173, 174
support 284, 407, 411, 414
compact 230, 430, 432
surface viii, 324, 504
area 517, 529, 537, 543, 553, 555
curve 505
integral viii, 505, 522, 529, 533
minimal 153, 175
polyhedral 411
wave 292, 333
surfer 332
symbolic program 453
symmetric 188, 211, 249, 358, 579, 583
function 236
matrix 66, 74, 342, 353, 354, 362, 372,
401
symmetry vii, ix, 170, 237, 263, 281,
291, 305, 358, 360, 494, 578
conjugate 580
cylindrical 517
principle 269, 271, 275
radial 249, 479, 528, 531, 541, 548
rotational 305
scaling 291, 307, 308, 327, 337, 479
spherical 531, 552
transformation 495
weighted 358, 380
symplectic 181
synthesizer 63, 496
system
adjoint 350
complete 520, 541, 565
of diﬀerential equations 2, 66
dynamical 340, 385
equilibrium 339, 340

Subject Index
system (continued )
linear ix, 187, 191, 216, 339, 350, 386,
400, 428, 582, 586
ﬁnite element 431
orthogonal 73, 113, 570
quantum-dynamical 503
table
of Fourier transforms 272
periodic 503, 568
tail 337
Talbot eﬀect 292, 329, 396
tangent 153, 182, 277
target xvi, 339, 341, 401, 428, 585
tautness 495
Taylor
series 75, 169, 171, 182, 183, 193, 453,
458, 463, 521, 576
formula 75
theorem viii, 203
telegrapher’s equation 62, 146, 393
television 63
temperature 7, 68, 122, 129, 136, 152,
153, 291, 312, 341, 436, 449, 504,
508, 522, 535
equilibrium 133, 154, 214, 246, 435,
537
ﬂuctuation 137
gradient 341, 437, 535
initial 131, 544
maximum 169
minimum 169
mean 136
tension 49, 142, 486
test
function 220, 225, 230, 429, 553
integral viii, 105
ratio viii, 75, 462, 468
root viii, 75
Weierstrass M 100, 105
tetrahedron 423
theorem ix
Carleson 117
Cauchy–Kovalevskaya 175
convergence 82
convolution 284
dilation 271, 274
divergence viii, 505, 529
existence 67, 364, 457
fundamental viii, 16, 39, 223, 236,
243, 245, 267
Green viii, 3, 168, 215, 243, 437
implicit function 49
shift 271, 274, 284
Taylor viii, 203
uniqueness 62, 67, 151, 169, 522

633
thermal
conductivity 65, 123, 437
diﬀusivity 124, 134, 186, 293, 298,
438, 535
energy 121, 122, 132, 134, 139, 169,
295, 304
equilibrium 127, 152, 153, 169, 441,
448, 479, 522, 536
reservoir 123
thermodynamics vii, viii, 2, 7, 12, 123,
130, 139, 242, 295, 312, 341, 436,
452
thermomechanics 121, 153
thermometer 306
thermonuclear explosion 561
third-order evolution equation 324
three-dimensional
delta function 527
space 503, 528, 544, 552
thunder 551, 552, 560
timbre 63
time 2, 3, 6, 7, 15, 18, 39, 121, 130, 286,
291, 299, 305, 307, 395, 442, 494
exercise 302
irrational 292, 329, 332
rational 292, 329
-reversible 497
space- 19, 56, 560
step 197
stopping 128
tone 63, 487
topology viii
toroidal
coordinates 508
membrane 361
torus 493
total
energy 61, 62, 151
heat 444, 485, 537
momentum 62
traﬃc 15, 31, 38, 44
train 19, 20
trajectory 565
transatlantic cable 139
transcendental equation 1, 134
transfer coeﬃcient 156
transform
cosine 274
Fourier 263, 264, 265, 269, 273, 274,
278, 282, 284, 285, 293, 297, 325,
330, 337, 340, 374, 462
discrete 582
inverse 263, 265, 269
table 272
two-dimensional 274, 278

634
transform (continued )
Hilbert 283
Laplace 263
sine 274
transformation
Hopf–Cole 318
linear 285
similarity 307
symmetry 495
translation 21, 155, 291, 295, 305, 328,
506, 528, 551
transport
equation 2, 19, 31, 51, 65, 181, 195,
291, 315, 324, 330, 434
nonlinear 38, 46, 130, 292, 308, 315,
317, 427, 431
nonuniform 24
transpose 216, 339, 341, 353, 578
transverse vibration 393
trapezoid rule 407
traveling wave 21, 51, 195, 291, 292,
305, 316, 330, 333, 334, 497
triangle 40, 58, 144, 197, 199, 205, 210,
257, 411, 414, 415, 416, 418
equilateral 260, 415, 417, 419, 424,
426
inequality ix, 107, 526, 572, 579, 581
isosceles 413, 415, 417, 421
obtuse 414
right 413, 417, 419, 423
similar 257, 532
wave 44, 48, 322
triangulation 411, 414, 417
Tricomi equation 173, 178, 462
tridiagonal 188, 191, 211, 407
trigonometric 340, 371, 388, 487, 490
ansatz 546
Ferrers function 516
function 60, 63, 70, 72, 74, 89, 109,
113, 125, 130, 189, 231, 264, 273,
374, 445, 451, 452, 457, 499, 508,
511, 519, 536
polynomial 71, 75, 91, 108, 111, 119,
400, 402
sum 116
triple integral viii, 505, 553
trumpet 63, 144
two-dimensional
delta function 246, 255
Fourier transform 274, 278
space 252
type 172, 173
Uncertainty Principle 286, 287
unconditionally stable 192, 193
underdamped 146

Subject Index
unidirectional 19, 324
uniform 50, 99, 104, 435, 436, 438, 485,
508, 528, 535, 537
convergence 99, 100–102, 104, 378,
519
uniqueness ix, 2, 29, 130, 245, 314, 340,
355, 400, 429, 458
theorem 62, 67, 151, 169, 522
unit 217, 281
circle 167, 258
disk 155, 166
imaginary 571
impulse 215–218, 234, 280, 551, 562
multiplicative 575
norm 108, 286, 394
normal 153, 244, 433, 436, 437, 504,
505
sphere 108
square 155, 158, 260, 485
step function 80, 83, 90, 102, 222,
232, 266, 329, 396
unitary 286
universe 130, 503, 553, 560, 564
unstable 136, 190, 389
mode 145, 340, 547
upper triangular matrix 211
upwind scheme 200
value of option 302
variable 1
change of viii, 51, 58, 175, 179, 318,
511
characteristic 20, 22, 25, 30, 32, 51,
52, 552
complex 163, 571
dependent 3
frequency 286
independent 3, 6, 15, 291, 300
physical 286
radial 249
separation of vii, ix, 25, 68, 121, 140,
155, 161, 215, 256, 305, 340, 364,
386, 435, 447, 451, 467, 487, 503,
507, 527, 535, 538, 547, 565
similarity 308
variance 287
vector 66, 98, 571, 575
addition 571
analysis viii
calculus 242, 507, 571
column 216, 578
ﬁeld 242, 243, 346, 359, 360, 439, 505,
507, 526
electric 341, 546, 551
magnetic 341, 546, 551
velocity 3, 152, 244, 504

Subject Index
vector (continued )
row 216, 578
space ix, 11, 98, 112, 220, 341, 371,
575, 578
complex 289, 571, 575, 580
ﬁnite-dimensional ix, 11, 98, 109,
215, 220
inﬁnite-dimensional ix, 11, 99, 109,
215, 340, 342, 371, 400, 577
vehicle 38, 44
velocity 7, 19, 35
ﬂuid 341
group 292, 331
initial 50, 55, 59, 145, 487, 546, 547,
551, 554, 557, 560, 562
phase 330
potential 360, 504
vector ﬁeld 3, 152, 244, 504
vertex 411, 415, 416
polygon 414, 423
vertical axis 18
vibration 10, 15, 49, 60, 121, 140, 142,
149, 172, 340, 385, 389, 503, 546
elastic 486
electromagnetic 2
equation 340, 388, 390
frequency 389, 395, 487, 495, 548
radial 547, 549
transverse 393
video 63
violin 13, 15, 49, 63, 144, 497
viscosity 3, 291, 315, 317
Vlasov equation vii
volatility 299, 302, 304
volume 243, 543
von Karman equation vii
von Neumann stability 198, 199, 205
water 2, 3, 15, 121, 149, 243, 331
deep 331
shallow 292, 333, 435
wave 2, 15, 54, 121, 149, 292, 324, 388
acoustic 15
compression 36, 44
delta 558
dispersive 2, 324, 459
elastic 121
electromagnetic 15, 121, 388, 503,
546, 551
equation 2, 8, 13, 15, 50, 64, 121, 140,
149, 151, 152, 169, 172, 173, 178,
181, 195, 201, 291, 315, 324, 339,
340, 371, 374, 385, 389, 427, 434,
435, 467, 486, 488, 494, 503, 545,
551, 552, 554, 556, 558, 561
damped 207

635
wave (continued )
ﬂood 15, 31
function 108, 286, 288, 394, 396, 564,
565, 568
light 551
N– 48
number 330
operator 50, 487
oscillatory 292, 327
packet 331
-particle duality 55, 149
rarefaction 34, 43, 44, 309, 320, 323,
433
shock vii, 2, 5, 15, 31, 37, 130, 195,
292, 315, 316, 317, 320, 324, 399,
427, 431, 433
solitary 324, 334
sound 121, 551, 552
speed 19, 22, 24, 50, 51, 65, 140, 195,
197, 292, 330, 334, 396, 486, 488,
492, 495, 549
spherical 552, 554
standing 55
surface 292, 333
traveling 21, 51, 195, 291, 292, 305,
316, 330, 333, 334, 497
triangle 44, 48, 322
weak
convergence 99, 230, 270, 327, 429
formulation 428, 429, 430, 432
maximum principle 314, 507
solution 5, 43, 53, 144, 399, 427, 429,
431, 432, 433, 489, 556, 558
weakening spring 473
wedge 35, 43
Weierstrass M –test 100, 105
weight function 379
weighted
adjoint matrix 342
average 213
inner product 341, 344, 354, 358, 359,
365, 378, 438, 506, 569
Sturm–Liouville diﬀerential operator
365
symmetry condition 358, 380
well-posed 136, 395, 535
white 441
width 334
wind instrument 15, 144
winter 136, 137, 545
wire 153, 158, 160, 169, 393
wolf 438
X-ray 546
xylophone 144

636
yard 307
zenith
angle 508, 515
diﬀerential equation 510
zero
complex 87
eigenvalue 131, 439, 582
element 428
function 114, 219, 231, 281, 456, 463
matrix 583
mean 78, 92
measure 102, 104, 108, 114, 117
zeta function 87

Subject Index

