OT98_LevequeFM2.qxp

6/4/2007

10:20 AM

Page 1

Finite Difference Methods
for Ordinary and Partial
Differential Equations

OT98_LevequeFM2.qxp

6/4/2007

10:20 AM

Page 2

OT98_LevequeFM2.qxp

6/4/2007

10:20 AM

Page 3

Finite Difference Methods
for Ordinary and Partial
Differential Equations
Steady-State and Time-Dependent Problems

Randall J. LeVeque
University of Washington
Seattle, Washington

Society for Industrial and Applied Mathematics • Philadelphia

OT98_LevequeFM2.qxp

6/4/2007

10:20 AM

Page 4

Copyright © 2007 by the Society for Industrial and Applied Mathematics.
10 9 8 7 6 5 4 3 2 1
All rights reserved. Printed in the United States of America. No part of this book may be
reproduced, stored, or transmitted in any manner without the written permission of the
publisher. For information, write to the Society for Industrial and Applied Mathematics, 3600
University City Science Center, Philadelphia, PA 19104-2688.
Trademarked names may be used in this book without the inclusion of a trademark symbol.
These names are used in an editorial context only; no infringement of trademark is intended.
MATLAB is a registered trademark of The MathWorks, Inc. For MATLAB product information,
please contact The MathWorks, Inc., 3 Apple Hill Drive, Natick, MA 01760-2098 USA, 508647-7000, Fax: 508-647-7101, info@mathworks.com, www.mathworks.com.
Library of Congress Cataloging-in-Publication Data
LeVeque, Randall J., 1955Finite difference methods for ordinary and partial differential equations : steady-state and
time-dependent problems / Randall J. LeVeque.
p.cm.
Includes bibliographical references and index.
ISBN 978-0-898716-29-0 (alk. paper)
1. Finite differences. 2. Differential equations. I. Title.
QA431.L548 2007
515’.35—dc22

2007061732

Partial royalties from the sale of this book are placed in a fund to help students
attend SIAM meetings and other SIAM-related activities. This fund is administered
by SIAM, and qualified individuals are encouraged to write directly to SIAM for
guidelines.

is a registered trademark.

OT98_LevequeFM2.qxp

6/4/2007

10:20 AM

Page 5

To my family,
Loyce, Ben, Bill, and Ann

i

i

i

“rjlfdm”
2007/6/1
page vii
i

Contents
Preface

xiii

I

Boundary Value Problems and Iterative Methods

1

1

Finite Difference Approximations
3
1.1
Truncation errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2
Deriving finite difference approximations . . . . . . . . . . . . . . . . 7
1.3
Second order derivatives . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.4
Higher order derivatives . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.5
A general approach to deriving the coefficients . . . . . . . . . . . . . 10

2

Steady States and Boundary Value Problems
2.1
The heat equation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2
Boundary conditions . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3
The steady-state problem . . . . . . . . . . . . . . . . . . . . . . . .
2.4
A simple finite difference method . . . . . . . . . . . . . . . . . . . .
2.5
Local truncation error . . . . . . . . . . . . . . . . . . . . . . . . . .
2.6
Global error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.7
Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.8
Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.9
Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.10
Stability in the 2-norm . . . . . . . . . . . . . . . . . . . . . . . . . .
2.11
Green’s functions and max-norm stability . . . . . . . . . . . . . . . .
2.12
Neumann boundary conditions . . . . . . . . . . . . . . . . . . . . .
2.13
Existence and uniqueness . . . . . . . . . . . . . . . . . . . . . . . .
2.14
Ordering the unknowns and equations . . . . . . . . . . . . . . . . . .
2.15
A general linear second order equation . . . . . . . . . . . . . . . . .
2.16
Nonlinear equations . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.16.1
Discretization of the nonlinear boundary value problem .
2.16.2
Nonuniqueness . . . . . . . . . . . . . . . . . . . . . . .
2.16.3
Accuracy on nonlinear equations . . . . . . . . . . . . .
2.17
Singular perturbations and boundary layers . . . . . . . . . . . . . . .
2.17.1
Interior layers . . . . . . . . . . . . . . . . . . . . . . .

13
13
14
14
15
17
18
18
19
19
20
22
29
32
34
35
37
38
40
41
43
46

vii

i

i

i

i

i

i

i

viii

Contents
2.18

Nonuniform grids . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.18.1
Adaptive mesh selection . . . . . . . . . . . . . . . . . .
Continuation methods . . . . . . . . . . . . . . . . . . . . . . . . . .
Higher order methods . . . . . . . . . . . . . . . . . . . . . . . . . .
2.20.1
Fourth order differencing . . . . . . . . . . . . . . . . .
2.20.2
Extrapolation methods . . . . . . . . . . . . . . . . . . .
2.20.3
Deferred corrections . . . . . . . . . . . . . . . . . . . .
Spectral methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

49
51
52
52
52
53
54
55

3

Elliptic Equations
3.1
Steady-state heat conduction . . . . . . . . . . . . . . . . . . . . . .
3.2
The 5-point stencil for the Laplacian . . . . . . . . . . . . . . . . . .
3.3
Ordering the unknowns and equations . . . . . . . . . . . . . . . . . .
3.4
Accuracy and stability . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5
The 9-point Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6
Other elliptic equations . . . . . . . . . . . . . . . . . . . . . . . . .
3.7
Solving the linear system . . . . . . . . . . . . . . . . . . . . . . . .
3.7.1
Sparse storage in MATLAB . . . . . . . . . . . . . . . .

59
59
60
61
63
64
66
66
68

4

Iterative Methods for Sparse Linear Systems
69
4.1
Jacobi and Gauss–Seidel . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.2
Analysis of matrix splitting methods . . . . . . . . . . . . . . . . . . 71
4.2.1
Rate of convergence . . . . . . . . . . . . . . . . . . . . 74
4.2.2
Successive overrelaxation . . . . . . . . . . . . . . . . . 76
4.3
Descent methods and conjugate gradients . . . . . . . . . . . . . . . . 78
4.3.1
The method of steepest descent . . . . . . . . . . . . . . 79
4.3.2
The A-conjugate search direction . . . . . . . . . . . . . 83
4.3.3
The conjugate-gradient algorithm . . . . . . . . . . . . . 86
4.3.4
Convergence of conjugate gradient . . . . . . . . . . . . 88
4.3.5
Preconditioners . . . . . . . . . . . . . . . . . . . . . . 93
4.3.6
Incomplete Cholesky and ILU preconditioners . . . . . . 96
4.4
The Arnoldi process and GMRES algorithm . . . . . . . . . . . . . . 96
4.4.1
Krylov methods based on three term recurrences . . . . . 99
4.4.2
Other applications of Arnoldi . . . . . . . . . . . . . . . 100
4.5
Newton–Krylov methods for nonlinear problems . . . . . . . . . . . . 101
4.6
Multigrid methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
4.6.1
Slow convergence of Jacobi . . . . . . . . . . . . . . . . 103
4.6.2
The multigrid approach . . . . . . . . . . . . . . . . . . 106

II

Initial Value Problems

5

The Initial Value Problem for Ordinary Differential Equations
113
5.1
Linear ordinary differential equations . . . . . . . . . . . . . . . . . . 114
5.1.1
Duhamel’s principle . . . . . . . . . . . . . . . . . . . . 115
5.2
Lipschitz continuity . . . . . . . . . . . . . . . . . . . . . . . . . . . 116

2.19
2.20

2.21

i

i

“rjlfdm”
2007/6/1
page viii
i

111

i

i

i

i

i

Contents

5.3
5.4
5.5
5.6
5.7
5.8
5.9

i

i

“rjlfdm”
2007/6/1
page ix
i

ix
5.2.1
Existence and uniqueness of solutions . . . . . . . . . . . 116
5.2.2
Systems of equations . . . . . . . . . . . . . . . . . . . 117
5.2.3
Significance of the Lipschitz constant . . . . . . . . . . . 118
5.2.4
Limitations . . . . . . . . . . . . . . . . . . . . . . . . . 119
Some basic numerical methods . . . . . . . . . . . . . . . . . . . . . 120
Truncation errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
One-step errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
Taylor series methods . . . . . . . . . . . . . . . . . . . . . . . . . . 123
Runge–Kutta methods . . . . . . . . . . . . . . . . . . . . . . . . . . 124
5.7.1
Embedded methods and error estimation . . . . . . . . . 128
One-step versus multistep methods . . . . . . . . . . . . . . . . . . . 130
Linear multistep methods . . . . . . . . . . . . . . . . . . . . . . . . 131
5.9.1
Local truncation error . . . . . . . . . . . . . . . . . . . 132
5.9.2
Characteristic polynomials . . . . . . . . . . . . . . . . . 133
5.9.3
Starting values . . . . . . . . . . . . . . . . . . . . . . . 134
5.9.4
Predictor-corrector methods . . . . . . . . . . . . . . . . 135

6

Zero-Stability and Convergence for Initial Value Problems
137
6.1
Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
6.2
The test problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
6.3
One-step methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
6.3.1
Euler’s method on linear problems . . . . . . . . . . . . 138
6.3.2
Relation to stability for boundary value problems . . . . . 140
6.3.3
Euler’s method on nonlinear problems . . . . . . . . . . 141
6.3.4
General one-step methods . . . . . . . . . . . . . . . . . 142
6.4
Zero-stability of linear multistep methods . . . . . . . . . . . . . . . . 143
6.4.1
Solving linear difference equations . . . . . . . . . . . . 144

7

Absolute Stability for Ordinary Differential Equations
149
7.1
Unstable computations with a zero-stable method . . . . . . . . . . . 149
7.2
Absolute stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
7.3
Stability regions for linear multistep methods . . . . . . . . . . . . . . 153
7.4
Systems of ordinary differential equations . . . . . . . . . . . . . . . 156
7.4.1
Chemical kinetics . . . . . . . . . . . . . . . . . . . . . 157
7.4.2
Linear systems . . . . . . . . . . . . . . . . . . . . . . . 158
7.4.3
Nonlinear systems . . . . . . . . . . . . . . . . . . . . . 160
7.5
Practical choice of step size . . . . . . . . . . . . . . . . . . . . . . . 161
7.6
Plotting stability regions . . . . . . . . . . . . . . . . . . . . . . . . . 162
7.6.1
The boundary locus method for linear multistep methods . 162
7.6.2
Plotting stability regions of one-step methods . . . . . . . 163
7.7
Relative stability regions and order stars . . . . . . . . . . . . . . . . 164

8

Stiff Ordinary Differential Equations
167
8.1
Numerical difficulties . . . . . . . . . . . . . . . . . . . . . . . . . . 168
8.2
Characterizations of stiffness . . . . . . . . . . . . . . . . . . . . . . 169
8.3
Numerical methods for stiff problems . . . . . . . . . . . . . . . . . . 170

i

i

i

i

i

x

Contents

8.4
8.5
8.6

i

i

“rjlfdm”
2007/6/1
page x
i

8.3.1
A-stability and A(˛)-stability . . . . . . . . . . . . . . . 171
8.3.2
L-stability . . . . . . . . . . . . . . . . . . . . . . . . . 171
BDF methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
The TR-BDF2 method . . . . . . . . . . . . . . . . . . . . . . . . . . 175
Runge–Kutta–Chebyshev explicit methods . . . . . . . . . . . . . . . 175

9

Diffusion Equations and Parabolic Problems
181
9.1
Local truncation errors and order of accuracy . . . . . . . . . . . . . . 183
9.2
Method of lines discretizations . . . . . . . . . . . . . . . . . . . . . 184
9.3
Stability theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
9.4
Stiffness of the heat equation . . . . . . . . . . . . . . . . . . . . . . 186
9.5
Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
9.5.1
PDE versus ODE stability theory . . . . . . . . . . . . . 191
9.6
Von Neumann analysis . . . . . . . . . . . . . . . . . . . . . . . . . 192
9.7
Multidimensional problems . . . . . . . . . . . . . . . . . . . . . . . 195
9.8
The locally one-dimensional method . . . . . . . . . . . . . . . . . . 197
9.8.1
Boundary conditions . . . . . . . . . . . . . . . . . . . . 198
9.8.2
The alternating direction implicit method . . . . . . . . . 199
9.9
Other discretizations . . . . . . . . . . . . . . . . . . . . . . . . . . . 200

10

Advection Equations and Hyperbolic Systems
201
10.1
Advection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
10.2
Method of lines discretization . . . . . . . . . . . . . . . . . . . . . . 203
10.2.1
Forward Euler time discretization . . . . . . . . . . . . . 204
10.2.2
Leapfrog . . . . . . . . . . . . . . . . . . . . . . . . . . 205
10.2.3
Lax–Friedrichs . . . . . . . . . . . . . . . . . . . . . . . 206
10.3
The Lax–Wendroff method . . . . . . . . . . . . . . . . . . . . . . . 207
10.3.1
Stability analysis . . . . . . . . . . . . . . . . . . . . . . 209
10.4
Upwind methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
10.4.1
Stability analysis . . . . . . . . . . . . . . . . . . . . . . 211
10.4.2
The Beam–Warming method . . . . . . . . . . . . . . . 212
10.5
Von Neumann analysis . . . . . . . . . . . . . . . . . . . . . . . . . 212
10.6
Characteristic tracing and interpolation . . . . . . . . . . . . . . . . . 214
10.7
The Courant–Friedrichs–Lewy condition . . . . . . . . . . . . . . . . 215
10.8
Some numerical results . . . . . . . . . . . . . . . . . . . . . . . . . 218
10.9
Modified equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
10.10 Hyperbolic systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
10.10.1
Characteristic variables . . . . . . . . . . . . . . . . . . 224
10.11 Numerical methods for hyperbolic systems . . . . . . . . . . . . . . . 225
10.12 Initial boundary value problems . . . . . . . . . . . . . . . . . . . . . 226
10.12.1
Analysis of upwind on the initial boundary value problem 226
10.12.2
Outflow boundary conditions . . . . . . . . . . . . . . . 228
10.13 Other discretizations . . . . . . . . . . . . . . . . . . . . . . . . . . . 230

11

Mixed Equations
233
11.1
Some examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233

i

i

i

i

i

Contents

xi

11.2
11.3
11.4
11.5
11.6

Fully coupled method of lines . . . . . . . . . . . . . . . . . . . . . . 235
Fully coupled Taylor series methods . . . . . . . . . . . . . . . . . . 236
Fractional step methods . . . . . . . . . . . . . . . . . . . . . . . . . 237
Implicit-explicit methods . . . . . . . . . . . . . . . . . . . . . . . . 239
Exponential time differencing methods . . . . . . . . . . . . . . . . . 240
11.6.1
Implementing exponential time differencing methods . . 241

III Appendices

i

i

“rjlfdm”
2007/6/1
page xi
i

243

A

Measuring Errors
245
A.1
Errors in a scalar value . . . . . . . . . . . . . . . . . . . . . . . . . . 245
A.1.1
Absolute error . . . . . . . . . . . . . . . . . . . . . . . 245
A.1.2
Relative error . . . . . . . . . . . . . . . . . . . . . . . . 246
A.2
“Big-oh” and “little-oh” notation . . . . . . . . . . . . . . . . . . . . 247
A.3
Errors in vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
A.3.1
Norm equivalence . . . . . . . . . . . . . . . . . . . . . 249
A.3.2
Matrix norms . . . . . . . . . . . . . . . . . . . . . . . . 250
A.4
Errors in functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
A.5
Errors in grid functions . . . . . . . . . . . . . . . . . . . . . . . . . 251
A.5.1
Norm equivalence . . . . . . . . . . . . . . . . . . . . . 252
A.6
Estimating errors in numerical solutions . . . . . . . . . . . . . . . . 254
A.6.1
Estimates from the true solution . . . . . . . . . . . . . . 255
A.6.2
Estimates from a fine-grid solution . . . . . . . . . . . . 256
A.6.3
Estimates from coarser solutions . . . . . . . . . . . . . 256

B

Polynomial Interpolation and Orthogonal Polynomials
259
B.1
The general interpolation problem . . . . . . . . . . . . . . . . . . . . 259
B.2
Polynomial interpolation . . . . . . . . . . . . . . . . . . . . . . . . 260
B.2.1
Monomial basis . . . . . . . . . . . . . . . . . . . . . . 260
B.2.2
Lagrange basis . . . . . . . . . . . . . . . . . . . . . . . 260
B.2.3
Newton form . . . . . . . . . . . . . . . . . . . . . . . . 260
B.2.4
Error in polynomial interpolation . . . . . . . . . . . . . 262
B.3
Orthogonal polynomials . . . . . . . . . . . . . . . . . . . . . . . . . 262
B.3.1
Legendre polynomials . . . . . . . . . . . . . . . . . . . 264
B.3.2
Chebyshev polynomials . . . . . . . . . . . . . . . . . . 265

C

Eigenvalues and Inner-Product Norms
269
C.1
Similarity transformations . . . . . . . . . . . . . . . . . . . . . . . . 270
C.2
Diagonalizable matrices . . . . . . . . . . . . . . . . . . . . . . . . . 271
C.3
The Jordan canonical form . . . . . . . . . . . . . . . . . . . . . . . 271
C.4
Symmetric and Hermitian matrices . . . . . . . . . . . . . . . . . . . 273
C.5
Skew-symmetric and skew-Hermitian matrices . . . . . . . . . . . . . 274
C.6
Normal matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
C.7
Toeplitz and circulant matrices . . . . . . . . . . . . . . . . . . . . . 275
C.8
The Gershgorin theorem . . . . . . . . . . . . . . . . . . . . . . . . . 277

i

i

i

i

i

xii

Contents
C.9
C.10

i

i

“rjlfdm”
2007/6/1
page xii
i

Inner-product norms . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
Other inner-product norms . . . . . . . . . . . . . . . . . . . . . . . 281

D

Matrix Powers and Exponentials
285
D.1
The resolvent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
D.2
Powers of matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
D.2.1
Solving linear difference equations . . . . . . . . . . . . 290
D.2.2
Resolvent estimates . . . . . . . . . . . . . . . . . . . . 291
D.3
Matrix exponentials . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
D.3.1
Solving linear differential equations . . . . . . . . . . . . 296
D.4
Nonnormal matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
D.4.1
Matrix powers . . . . . . . . . . . . . . . . . . . . . . . 297
D.4.2
Matrix exponentials . . . . . . . . . . . . . . . . . . . . 299
D.5
Pseudospectra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
D.5.1
Nonnormality of a Jordan block . . . . . . . . . . . . . . 304
D.6
Stable families of matrices and the Kreiss matrix theorem . . . . . . . 304
D.7
Variable coefficient problems . . . . . . . . . . . . . . . . . . . . . . 307

E

Partial Differential Equations
311
E.1
Classification of differential equations . . . . . . . . . . . . . . . . . 311
E.1.1
Second order equations . . . . . . . . . . . . . . . . . . 311
E.1.2
Elliptic equations . . . . . . . . . . . . . . . . . . . . . 312
E.1.3
Parabolic equations . . . . . . . . . . . . . . . . . . . . 313
E.1.4
Hyperbolic equations . . . . . . . . . . . . . . . . . . . 313
E.2
Derivation of partial differential equations from conservation principles 314
E.2.1
Advection . . . . . . . . . . . . . . . . . . . . . . . . . 315
E.2.2
Diffusion . . . . . . . . . . . . . . . . . . . . . . . . . . 316
E.2.3
Source terms . . . . . . . . . . . . . . . . . . . . . . . . 317
E.2.4
Reaction-diffusion equations . . . . . . . . . . . . . . . 317
E.3
Fourier analysis of linear partial differential equations . . . . . . . . . 317
E.3.1
Fourier transforms . . . . . . . . . . . . . . . . . . . . . 318
E.3.2
The advection equation . . . . . . . . . . . . . . . . . . 318
E.3.3
The heat equation . . . . . . . . . . . . . . . . . . . . . 320
E.3.4
The backward heat equation . . . . . . . . . . . . . . . . 322
E.3.5
More general parabolic equations . . . . . . . . . . . . . 322
E.3.6
Dispersive waves . . . . . . . . . . . . . . . . . . . . . . 323
E.3.7
Even- versus odd-order derivatives . . . . . . . . . . . . 324
E.3.8
The Schrödinger equation . . . . . . . . . . . . . . . . . 324
E.3.9
The dispersion relation . . . . . . . . . . . . . . . . . . . 325
E.3.10
Wave packets . . . . . . . . . . . . . . . . . . . . . . . . 327

Bibliography

329

Index

337

i

i

i

i

i

“rjlfdm”
2007/6/1
page xiii
i

Preface
This book evolved from lecture notes developed over the past 20+ years of teaching this material, mostly in Applied Mathematics 585–6 at the University of Washington.
The course is taken by first-year graduate students in our department, along with graduate
students from mathematics and a variety of science and engineering departments.
Exercises and student projects are an important aspect of any such course and many
have been developed in conjunction with this book. Rather than lengthening the text, they
are available on the book’s Web page:
www.siam.org/books/OT98
Along with exercises that provide practice and further exploration of the topics in each
chapter, some of the exercises introduce methods, techniques, or more advanced topics not
found in the book.
The Web page also contains MATLABr m-files that illustrate how to implement
finite difference methods, and that may serve as a starting point for further study of the
methods in exercises and projects. A number of the exercises require programming on the
part of the student, or require changes to the MATLAB programs provided. Some of these
exercises are fairly simple, designed to enable students to observe first hand the behavior
of numerical methods described in the text. Others are more open-ended and could form
the basis for a course project.
The exercises are available as PDF files. The LATEX source is also provided, along
with some hints on using LATEX for the type of mathematics used in this field. Each exercise is in a separate file so that instuctors can easily construct customized homework
assignments if desired. Students can also incorporate the source into their solutions if they
use LATEX to typeset their homework. Personally I encourage this when teaching the class,
since this is a good opportunity for them to learn a valuable skill (and also makes grading
homework considerably more pleasurable).
Organization of the Book
The book is organized into two main parts and a set of appendices. Part I deals with
steady-state boundary value problems, starting with two-point boundary value problems in
one dimension and then elliptic equations in two and three dimensions. Part I concludes
with a chapter on iterative methods for large sparse linear systems, with an emphasis on
systems arising from finite difference approximations.
xiii

i

i

i

i

i

i

i

xiv

“rjlfdm”
2007/6/1
page xiv
i

Preface

Part II concerns time-dependent problems, starting with the initial value problem
for ODEs and moving on to initial-boundary value problems for parabolic and hyperbolic
PDEs. This part concludes with a chapter on mixed equations combining features of ordinary differential equations (ODEs) and parabolic and hyperbolic equations.
Part III consists of a set of appendices covering background material that is needed at
various points in the main text. This material is collected at the end to avoid interrupting the
flow of the main text and because many concepts are repeatedly used in different contexts
in Parts I and II.
The organization of this book is somewhat different from the way courses are structured at many universities, where a course on ODEs (including both two-point boundary
value problems and the initial value problem) is followed by a course on partial differential
equations (PDEs) (including both elliptic boundary value problems and time-dependent
hyperbolic and parabolic equations). Existing textbooks are well suited to this latter approach, since many books cover numerical methods for ODEs or for PDEs, but often not
both. However, I have found over the years that the reorganization into boundary value
problems followed by initial value problems works very well. The mathematical techniques are often similar for ODEs and PDEs and depend more on the steady-state versus
time-dependent nature of the problem than on the number of dimensions involved. Concepts developed for each type of ODE are naturally extended to PDEs and the interplay
between these theories is more clearly elucidated when they are covered together.
At the University of Washington, Parts I and II of this book are used for the second
and third quarters of a year-long graduate course. Lectures are supplemented by material
from the appendices as needed. The first quarter of the sequence covers direct methods for
linear systems, eigenvalue problems, singular values, and so on. This course is currently
taught out of Trefethen and Bau [91], which also serves as a useful reference text for the
material in this book on linear algebra and iterative methods.
It should also be possible to use this book for a more traditional set of courses, teaching Chapters 1, 5, 6, 7, and 8 in an ODE course followed by Chapters 2, 3, 9, 10, and 11 in
a PDE-oriented course.
Emphasis of the Book
The emphasis is on building an understanding of the essential ideas that underlie
the development, analysis, and practical use of finite difference methods. Stability theory
necessarily plays a large role, and I have attempted to explain several key concepts, their
relation to one another, and their practical implications. I include some proofs of convergence in order to motivate the various definitions of “stability” and to show how they
relate to error estimates, but have not attempted to rigorously prove all results in complete
generality. I have also tried to give an indication of some of the more practical aspects of
the algorithms without getting too far into implementation details. My goal is to form a
foundation from which students can approach the vast literature on more advanced topics
and further explore the theory and/or use of finite difference methods according to their
interests and needs.
I am indebted to several generations of students who have worked through earlier
versions of this book, found errors and omissions, and forced me to constantly rethink
my understanding of this material and the way I present it. I am also grateful to many

i

i

i

i

i

i

i

Preface

“rjlfdm”
2007/6/1
page xv
i

xv

colleagues who have taught out of my notes and given me valuable feedback, both at the
University of Washington and at more than a dozen other universities where earlier versions
have been used in courses. I take full responsibility for the remaining errors.
I have also been influenced by other books covering these same topics, and many
excellent ones exist at all levels. Advanced books go into more detail on countless subjects
only briefly discussed here, and I give pointers to some of these in the text. There are
also a number of general introductory books that may be useful as complements to the
presentation found here, including, for example, [27], [40], [49], [72], [84], and [93].
As already mentioned, this book has evolved over the past 20 years. This is true
in part for the mundane reason that I have reworked (and perhaps improved) parts of it
each time I teach the course. But it is also true for a more exciting reason—the field itself
continues to evolve in significant ways. While some of the theory and methods in this book
were very well known when I was a student, many of the topics and methods that should
now appear in an introductory course had yet to be invented or were in their infancy. I
give at least a flavor of some of these, though many other developments have not been
mentioned. I hope that students will be inspired to further pursue the study of numerical
methods, and perhaps invent even better methods in the future.

Randall J. LeVeque

i

i

i

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page xvi
i

i

i

i

i

“rjlfdm”
2007/6/1
page 1
i

Part I

Boundary Value Problems and
Iterative Methods

i

i

i

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 2
i

i

i

i

i

“rjlfdm”
2007/6/1
page 3
i

Chapter 1

Finite Difference
Approximations

Our goal is to approximate solutions to differential equations, i.e., to find a function (or
some discrete approximation to this function) that satisfies a given relationship between
various of its derivatives on some given region of space and/or time, along with some
boundary conditions along the edges of this domain. In general this is a difficult problem,
and only rarely can an analytic formula be found for the solution. A finite difference method
proceeds by replacing the derivatives in the differential equations with finite difference
approximations. This gives a large but finite algebraic system of equations to be solved in
place of the differential equation, something that can be done on a computer.
Before tackling this problem, we first consider the more basic question of how we can
approximate the derivatives of a known function by finite difference formulas based only
on values of the function itself at discrete points. Besides providing a basis for the later
development of finite difference methods for solving differential equations, this allows us
to investigate several key concepts such as the order of accuracy of an approximation in
the simplest possible setting.
Let u.x/ represent a function of one variable that, unless otherwise stated, will always
be assumed to be smooth, meaning that we can differentiate the function several times and
each derivative is a well-defined bounded function over an interval containing a particular
point of interest x.
N
Suppose we want to approximate u0 .x/
N by a finite difference approximation based
only on values of u at a finite number of points near x.
N One obvious choice would be to
use
u.xN C h/ u.x/
N
DC u.x/
N 
(1.1)
h
for some small value of h. This is motivated by the standard definition of the derivative as
the limiting value of this expression as h ! 0. Note that DC u.x/
N is the slope of the line
interpolating u at the points xN and xN C h (see Figure 1.1).
The expression (1.1) is a one-sided approximation to u0 since u is evaluated only at
values of x  x.
N Another one-sided approximation would be
D u.x/
N 

u.x/
N

u.xN
h

h/

:

(1.2)

3

i

i

i

i

i

i

i

4

“rjlfdm”
2007/6/1
page 4
i

Chapter 1. Finite Difference Approximations

slope DC u.x/
N
slope D u.x/
N
N
slope u0.x/

N
slope D0 u.x/

xN

h

xN

xN C h

u.x/

Figure 1.1. Various approximations to u0.x/
N interpreted as the slope of secant lines.
Each of these formulas gives a first order accurate approximation to u0 .x/,
N meaning that
the size of the error is roughly proportional to h itself.
Another possibility is to use the centered approximation
D0 u.x/
N 

u.xN C h/

u.xN
2h

h/
D

1
N C D u.x//:
N
.DC u.x/
2

(1.3)

This is the slope of the line interpolating u at xN h and xN C h and is simply the average
of the two one-sided approximations defined above. From Figure 1.1 it should be clear
that we would expect D0 u.x/
N to give a better approximation than either of the one-sided
approximations. In fact this gives a second order accurate approximation—the error is
proportional to h2 and hence is much smaller than the error in a first order approximation
when h is small.
Other approximations are also possible, for example,
D3 u.x/
N 

1
Œ2u.xN C h/ C 3u.x/
N
6h

6u.xN

h/ C u.xN

2h/:

(1.4)

It may not be clear where this came from or why it should approximate u0 at all, but in fact
it turns out to be a third order accurate approximation—the error is proportional to h3 when
h is small.
Our first goal is to develop systematic ways to derive such formulas and to analyze
their accuracy and relative worth. First we will look at a typical example of how the errors
in these formulas compare.
Example 1.1. Let u.x/ D sin.x/ and xN D 1; thus we are trying to approximate
u0.1/ D cos.1/ D 0:5403023. Table 1.1 shows the error Du.x/
N
u0 .x/
N for various values
of h for each of the formulas above.
We see that DC u and D u behave similarly although one exhibits an error that is
roughly the negative of the other. This is reasonable from Figure 1.1 and explains why
D0 u, the average of the two, has an error that is much smaller than both.

i

i

i

i

i

i

i

1.1. Truncation errors

“rjlfdm”
2007/6/1
page 5
i

5

Table 1.1. Errors in various finite difference approximations to u0.x/.
N
h
1.0e
5.0e
1.0e
5.0e
1.0e

DC u.x/
N
01
02
02
03
03

4.2939e
2.1257e
4.2163e
2.1059e
4.2083e

D u.x/
N
02
02
03
03
04

4.1138e
2.0807e
4.1983e
2.1014e
4.2065e

02
02
03
03
04

D0 u.x/
N
9.0005e
2.2510e
9.0050e
2.2513e
9.0050e

D3 u.x/
N
04
04
06
06
08

6.8207e
8.6491e
6.9941e
8.7540e
6.9979e

05
06
08
09
11

We see that
DC u.x/
N

u0 .x/
N  0:42h;

D0 u.x/
N
D3 u.x/
N

u0 .x/
N  0:09h2 ;
0
u .x/
N  0:007h3;

confirming that these methods are first order, second order, and third order accurate,
respectively.
Figure 1.2 shows these errors plotted against h on a log-log scale. This is a good way
to plot errors when we expect them to behave like some power of h, since if the error E.h/
behaves like
E.h/  C hp ;
then
log jE.h/j  log jC j C p log h:
So on a log-log scale the error behaves linearly with a slope that is equal to p, the order of
accuracy.

1.1

Truncation errors

The standard approach to analyzing the error in a finite difference approximation is to
expand each of the function values of u in a Taylor series about the point x,
N e.g.,
1
1
u.xN C h/ D u.x/
N C hu0 .x/
N C h2 u00.x/
N C h3 u000.x/
N C O.h4 /;
2
6
1
1 3 000
u.xN h/ D u.x/
N
hu0.x/
N C h2 u00.x/
N
N C O.h4 /:
h u .x/
2
6

(1.5a)
(1.5b)

These expansions are valid provided that u is sufficiently smooth. Readers unfamiliar with
the “big-oh” notation O.h4 / are advised to read Section A.2 of Appendix A at this point
since this notation will be heavily used and a proper understanding of its use is critical.
Using (1.5a) allows us to compute that
DC u.x/
N D

i

i

u.xN C h/
h

u.x/
N

1
1
N C hu00.x/
N C h2 u000.x/
N C O.h3 /:
D u0.x/
2
6

i

i

i

i

i

6

“rjlfdm”
2007/6/1
page 6
i

Chapter 1. Finite Difference Approximations

−2

10

DC
−4

10

D0

−6

10

−8

10

D3
−10

10

−3

−2

10

10

−1

10

Figure 1.2. The errors in Du.x/
N from Table 1.1 plotted against h on a log-log scale.
Recall that xN is a fixed point so that u00.x/;
N u000.x/,
N etc., are fixed constants independent of
h. They depend on u of course, but the function is also fixed as we vary h.
For h sufficiently small, the error will be dominated by the first term 12 hu00.x/
N and all
the other terms will be negligible compared to this term, so we expect the error to behave
roughly like a constant times h, where the constant has the value 12 u00.x/.
N
Note that in Example 1.1, where u.x/ D sin x, we have 12 u00.1/ D 0:4207355,
which agrees with the behavior seen in Table 1.1.
Similarly, from (1.5b) we can compute that the error in D u.x/
N is
D u.x/
N

u0 .x/
N D

1 00
1
N C h2 u000.x/
N C O.h3 /;
hu .x/
2
6

which also agrees with our expectations.
Combining (1.5a) and (1.5b) shows that
u.xN C h/

u.xN

1
h/ D 2hu0.x/
N C h3 u000.x/
N C O.h5 /
3

so that

1 2 000
N C O.h4 /:
(1.6)
h u .x/
6
This confirms the second order accuracy of this approximation and again agrees with what
is seen in Table 1.1, since in the context of Example 1.1 we have
D0 u.x/
N

u0.x/
N D

1 000
N D
u .x/
6

1
cos.1/ D 0:09005038:
6

Note that all the odd order terms drop out of the Taylor series expansion (1.6) for D0 u.x/.
N
This is typical with centered approximations and typically leads to a higher order approximation.

i

i

i

i

i

i

i

1.2. Deriving finite difference approximations

7

To analyze D3 u we need to also expand u.xN
u.xN

2h/ D u.x/
N

“rjlfdm”
2007/6/1
page 7
i

2h/ as

1
2hu0.x/
N C .2h/2 u00.x/
N
2

1
.2h/3 u000.x/
N C O.h4 /:
6

(1.7)

Combining this with (1.5a) and (1.5b) shows that
D3 u.x/
N D u0.x/
N C

1 3 .4/
N C O.h4 /;
h u .x/
12

(1.8)

where u.4/ is the fourth derivative of u.

1.2

Deriving finite difference approximations

Suppose we want to derive a finite difference approximation to u0 .x/
N based on some given
set of points. We can use Taylor series to derive an appropriate formula, using the method
of undetermined coefficients.
Example 1.2. Suppose we want a one-sided approximation to u0 .x/
N based on u.x/;
N
u.xN h/, and u.xN 2h/ of the form
D2 u.x/
N D au.x/
N C bu.xN

h/ C cu.xN

2h/:

(1.9)

We can determine the coefficients a; b, and c to give the best possible accuracy by expanding in Taylor series and collecting terms. Using (1.5b) and (1.7) in (1.9) gives
D2 u.x/
N D .a C b C c/u.x/
N

1
.b C 2c/hu0 .x/
N C .b C 4c/h2 u00.x/
N
2

1
N C :
.b C 8c/h3 u000.x/
6
If this is going to agree with u0 .x/
N to high order, then we need
a C b C c D 0;
b C 2c D 1= h;

(1.10)

b C 4c D 0:
We might like to require that higher order coefficients be zero as well, but since there are
only three unknowns a; b; and c, we cannot in general hope to satisfy more than three such
conditions. Solving the linear system (1.10) gives
a D 3=2h;

b D 2= h;

c D 1=2h

so that the formula is
D2 u.x/
N D

1
Œ3u.x/
N
2h

4u.xN

h/ C u.xN

2h/:

(1.11)

This approximation is used, for example, in the system of equations (2.57) for a 2-point
boundary value problem with a Neumann boundary condition at the left boundary.

i

i

i

i

i

i

i

8

“rjlfdm”
2007/6/1
page 8
i

Chapter 1. Finite Difference Approximations
The error in this approximation is

1
N C
.b C 8c/h3 u000.x/
6
(1.12)
1
D h2 u000.x/
N C O.h3 /:
12
There are other ways to derive the same finite difference approximations. One way
is to approximate the function u.x/ by some polynomial p.x/ and then use p 0 .x/
N as an approximation to u0.x/.
N If we determine the polynomial by interpolating u at an appropriate
set of points, then we obtain the same finite difference methods as above.
Example 1.3. To derive the method of Example 1.2 in this way, let p.x/ be the
quadratic polynomial that interpolates u at x,
N xN h and xN 2h, and then compute p 0 .x/.
N
The result is exactly (1.11).
D2 u.x/
N

1.3

u0 .x/
N D

Second order derivatives

Approximations to the second derivative u00.x/ can be obtained in an analogous manner.
The standard second order centered approximation is given by
D 2 u.x/
N D

1
Œu.xN
h2

h/

2u.x/
N C u.xN C h/

(1.13)
1 2 0000
N C O.h4 /:
h u .x/
12
Again, since this is a symmetric centered approximation, all the odd order terms drop out.
This approximation can also be obtained by the method of undetermined coefficients, or
alternatively by computing the second derivative of the quadratic polynomial interpolating
u.x/ at xN h; x,
N and xN C h, as is done in Example 1.4 below for the more general case of
unequally spaced points.
Another way to derive approximations to higher order derivatives is by repeatedly
applying first order differences. Just as the second derivative is the derivative of u0, we can
view D 2 u.x/
N as being a difference of first differences. In fact,
D u00 .x/
N C

D 2 u.x/
N D DC D u.x/
N
since
1
ŒD u.xN C h/
h

1
u.xN C h/
D
h
h

DC .D u.x//
N D

D u.x/
N
 
u.x/
N
u.x/
N

u.xN
h

h/



D D 2 u.x/:
N
Alternatively, D 2 .x/
N D D DC u.x/,
N or we can also view it as a centered difference of
centered differences, if we use a step size h=2 in each centered approximation to the first
derivative. If we define
 



1
h
h
DO 0 u.x/ D
u xC
u x
;
h
2
2

i

i

i

i

i

i

i

1.4. Higher order derivatives

“rjlfdm”
2007/6/1
page 9
i

9

then we find that
1
DO 0 .DO 0 u.x//
N D
h



u.xN C h/
h

u.x/
N





u.x/
N

u.xN
h

h/


N
D D 2 u.x/:

Example 1.4. Suppose we want to approximate u00.x2 / based on data values U1 , U2 ,
and U3 , at three unequally spaced points x1 ; x2, and x3 . This approximation will be used
in Section 2.18. Let h1 D x2 x1 and h2 D x3 x2 . The approximation can be found by
interpolating by a quadratic function and differentiating twice. Using the Newton form of
the interpolating polynomial (see Section B.2.3),
p.x/ D U Œx1 C U Œx1 ; x2.x

x1 / C U Œx1 ; x2; x3.x

x1 /.x

x2 /;

we see that the second derivative is constant and equal to twice the second order divided
difference,
p 00.x2 / D 2U Œx1; x2; x3


U3 U2 U2 U1 .
D2
.h1 C h2 /
h2
h1

(1.14)

D c1U1 C c2 U2 C c3 U3 ;
where
c1 D

2
;
h1 .h1 C h2 /

c2 D

2
;
h1 h2

c3 D

2
:
h2 .h1 C h2 /

(1.15)

This would be our approximation to u00.x2 /. The same result can be found by the method
of undetermined coefficients.
To compute the error in this approximation, we can expand u.x1 / and u.x3 / in Taylor
series about x2 and find that
c1 u.x1/ C c2 u.x2 / C c3 u.x3 /
1
D .h2
3

u00.x2 /

1
h1 /u.3/ .x2 / C
12

h31 C h32
h1 C h2

!
u.4/ .x2 / C    :

(1.16)

In general, if h1 ¤ h2 , the error is proportional to max.h1 ; h2 / and this approximation is
“first order” accurate.
In the special case h1 D h2 (equally spaced points), the approximation (1.14) reduces
to the standard centered approximate D 2 u.x2 / from (1.13) with the second order error
shown there.

1.4

Higher order derivatives

Finite difference approximations to higher order derivatives can also be obtained using any
of the approaches outlined above. Repeatedly differencing approximations to lower order
derivatives is a particularly simple approach.
Example 1.5. As an example, here are two different approximations to u000.x/.
N The
first is uncentered and first order accurate:

i

i

i

i

i

i

i

10

“rjlfdm”
2007/6/1
page 10
i

Chapter 1. Finite Difference Approximations

1
.u.xN C 2h/ 3u.xN C h/ C 3u.x/
N
h3
1
D u000.x/
N C hu0000.x/
N C O.h2 /:
2
The next approximation is centered and second order accurate:
N D
DC D 2 u.x/

u.xN

h//

1
.u.xN C 2h/ 2u.xN C h/ C 2u.xN h/ u.xN 2h//
2h3
1
D u000.x/
N C h2 u00000.x/
N C O.h4 /:
4
Another way to derive finite difference approximations to higher order derivatives
is by interpolating with a sufficiently high order polynomial based on function values at
the desired stencil points and then computing the appropriate derivative of this polynomial.
This is generally a cumbersome way to do it. A simpler approach that lends itself well to
automation is to use the method of undetermined coefficients, as illustrated in Section 1.2
for an approximation to the first order derivative and explained more generally in the next
section.
D0 DC D u.x/
N D

1.5

A general approach to deriving the coefficients

The method illustrated in Section 1.2 can be extended to compute the finite difference coefficients for computing an approximation to u.k/ .x/,
N the kth derivative of u.x/ evaluated
at x,
N based on an arbitrary stencil of n  k C 1 points x1 ; : : : ; xn . Usually xN is one of the
stencil points, but not necessarily.
We assume u.x/ is sufficiently smooth, namely, at least n C 1 times continuously
differentiable in the interval containing xN and all the stencil points, so that the Taylor series
expansions below are valid. Taylor series expansions of u at each point xi in the stencil
about u.x/
N yield
1
N k u.k/ .x/
N C
(1.17)
.xi x/
k!
for i D 1; : : : ; n. We want to find a linear combination of these values that agrees with
u.k/ .x/
N as well as possible. So we want
u.xi / D u.x/
N C .xi

x/u
N 0 .x/
N CC

c1u.x1 / C c2 u.x2 / C    C cn u.xn / D u.k/ .x/
N C O.hp /;

(1.18)

where p is as large as possible. (Here h is some measure of the width of the stencil. If
we are deriving approximations on stencils with equally spaced points, then h is the mesh
width, but more generally it is some “average mesh width,” so that max1in jxi xj
N  Ch
for some small constant C .)
Following the approach of Section 1.2, we choose the coefficients cj so that

n
X
1
1 if i 1 D k;
cj .xj x/
N .i 1/ D
(1.19)
0 otherwise
.i 1/!
j D1

for i D 1; : : : ; n. Provided the points xj are distinct, this n  n Vandermonde system is
nonsingular and has a unique solution. If n  k (too few points in the stencil), then the

i

i

i

i

i

i

i

1.5. A general approach to deriving the coefficients

“rjlfdm”
2007/6/1
page 11
i

11

right-hand side and solution are both the zero vector, but for n > k the coefficients give a
suitable finite difference approximation.
How accurate is the method? The right-hand side vector has a 1 in the i D k C 1
row, which ensures that this linear combination approximates the kth derivative. The 0 in
the other component of the right-hand side ensures that the terms
0
1
n
X
@
cj .xj x/
N .i 1/ A u.i 1/ .x/
N
j D1

drop out in the linear combination of Taylor series for i 1 ¤ k. For i 1 < k this
is necessary to get even first order accuracy of the finite difference approximation. For
i 1 > k (which is possible only if n > k C 1), this gives cancellation of higher order
terms in the expansion and greater than first order accuracy. In general we expect the order
of accuracy of the finite difference approximation to be at least p  n k. It may be even
higher if higher order terms happen to cancel out as well (as often happens with centered
approximations, for example).
In MATLAB it is very easy to set up and solve this Vandermonde system. If xbar is
the point xN and x(1:n) are the desired stencil points, then the following function can be
used to compute the coefficients:
function c = fdcoeffV(k,xbar,x)
A = ones(n,n);
xrow = (x(:)-xbar)’; % displacements as a row vector.
for i=2:n
A(i,:) = (xrow .ˆ (i-1)) ./ factorial(i-1);
end
b = zeros(n,1);
% b is right hand side,
b(k+1) = 1;
% so k’th derivative term remains
c = A\b;
% solve system for coefficients
c = c’;
% row vector
If u is a column vector of n values u.xi /, then in MATLAB the resulting approximation to
u.k/ .x/
N can be computed by c*u.
This function is implemented in the MATLAB function fdcoeffV.m available on
the Web page for this book, which contains more documentation and data checking but is
essentially the same as the above code. A row vector is returned since in applications we
will often use the output of this routine as the row of a matrix approximating a differential
operator (see Section 2.18, for example).
Unfortunately, for a large number of points this Vandermonde procedure is numerically unstable because the resulting linear system can be very poorly conditioned. A more
stable procedure for calculating the weights is given by Fornberg [30], who also gives a
FORTRAN implementation. This modified procedure is implemented in the MATLAB
function fdcoeffF.m on the Web page.
Finite difference approximations of the sort derived in this chapter form the basis for
finite difference algorithms for solving differential equations. In the next chapter we begin
the study of this topic.

i

i

i

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 12
i

i

i

i

i

“rjlfdm”
2007/6/1
page 13
i

Chapter 2

Steady States and Boundary
Value Problems

We will first consider ordinary differential equations (ODEs) that are posed on some interval a < x < b, together with some boundary conditions at each end of the interval.
In the next chapter we will extend this to more than one space dimension and will study
elliptic partial differential equations (ODEs) that are posed in some region of the plane or
three-dimensional space and are solved subject to some boundary conditions specifying the
solution and/or its derivatives around the boundary of the region. The problems considered
in these two chapters are generally steady-state problems in which the solution varies only
with the spatial coordinates but not with time. (But see Section 2.16 for a case where Œa; b
is a time interval rather than an interval in space.)
Steady-state problems are often associated with some time-dependent problem that
describes the dynamic behavior, and the 2-point boundary value problem (BVP) or elliptic
equation results from considering the special case where the solution is steady in time, and
hence the time-derivative terms are equal to zero, simplifying the equations.

2.1

The heat equation

As a specific example, consider the flow of heat in a rod made out of some heat-conducting
material, subject to some external heat source along its length and some boundary conditions at each end. If we assume that the material properties, the initial temperature distribution, and the source vary only with x, the distance along the length, and not across any
cross section, then we expect the temperature distribution at any time to vary only with
x and we can model this with a differential equation in one space dimension. Since the
solution might vary with time, we let u.x; t/ denote the temperature at point x at time t,
where a < x < b along some finite length of the rod. The solution is then governed by the
heat equation
ut .x; t/ D ..x/ux .x; t//x C .x; t/;
(2.1)
where .x/ is the coefficient of heat conduction, which may vary with x, and .x; t/ is
the heat source (or sink, if < 0). See Appendix E for more discussion and a derivation.
Equation (2.1) is often called the diffusion equation since it models diffusion processes
more generally, and the diffusion of heat is just one example. It is assumed that the basic
13

i

i

i

i

i

i

i

14

“rjlfdm”
2007/6/1
page 14
i

Chapter 2. Steady States and Boundary Value Problems

theory of this equation is familiar to the reader. See standard PDE books such as [55]
for a derivation and more introduction. In general it is extremely valuable to understand
where the equation one is attempting to solve comes from, since a good understanding of
the physics (or biology, etc.) is generally essential in understanding the development and
behavior of numerical methods for solving the equation.

2.2

Boundary conditions

If the material is homogeneous, then .x/   is independent of x and the heat equation
(2.1) reduces to
ut .x; t/ D uxx .x; t/ C .x; t/:
(2.2)
Along with the equation, we need initial conditions,
u.x; 0/ D u0 .x/;
and boundary conditions, for example, the temperature might be specified at each end,
u.a; t/ D ˛.t/;

u.b; t/ D ˇ.t/:

(2.3)

Such boundary conditions, where the value of the solution itself is specified, are called
Dirichlet boundary conditions. Alternatively one end, or both ends, might be insulated, in
which case there is zero heat flux at that end, and so ux D 0 at that point. This boundary
condition, which is a condition on the derivative of u rather than on u itself, is called a
Neumann boundary condition. To begin, we will consider the Dirichlet problem for (2.2)
with boundary conditions (2.3).

2.3

The steady-state problem

In general we expect the temperature distribution to change with time. However, if .x; t/,
˛.t/, and ˇ.t/ are all time independent, then we might expect the solution to eventually
reach a steady-state solution u.x/, which then remains essentially unchanged at later times.
Typically there will be an initial transient time, as the initial data u0 .x/ approach u.x/
(unless u0 .x/  u.x/), but if we are interested only in computing the steady-state solution
itself, then we can set ut D 0 in (2.2) and obtain an ODE in x to solve for u.x/:
u00.x/ D f .x/;

(2.4)

where we introduce f .x/ D
.x/= to avoid minus signs below. This is a second order
ODE, and from basic theory we expect to need two boundary conditions to specify a unique
solution. In our case we have the boundary conditions
u.a/ D ˛;

u.b/ D ˇ:

(2.5)

Remark: Having two boundary conditions does not necessarily guarantee that there
exists a unique solution for a general second order equation—see Section 2.13.
The problem (2.4), (2.5) is called a 2-point (BVP), since one condition is specified at
each of the two endpoints of the interval where the solution is desired. If instead two data

i

i

i

i

i

i

i

2.4. A simple finite difference method

“rjlfdm”
2007/6/1
page 15
i

15

values were specified at the same point, say, u.a/ D ˛; u0.a/ D , and we want to find
the solution for t  a, then we would have an initial value problem (IVP) instead. These
problems are discussed in Chapter 5.
One approach to computing a numerical solution to a steady-state problem is to
choose some initial data and march forward in time using a numerical method for the timedependent PDE (2.2), as discussed in Chapter 9 on the solution of parabolic equations.
However, this is typically not an efficient way to compute the steady state solution if this is
all we want. Instead we can discretize and solve the 2-point BVP given by (2.4) and (2.5)
directly. This is the first BVP that we will study in detail, starting in the next section. Later
in this chapter we will consider some other BVPs, including more challenging nonlinear
equations.

2.4

A simple finite difference method

As a first example of a finite difference method for solving a differential equation, consider
the second order ODE discussed above,
u00 .x/ D f .x/ for 0 < x < 1;

(2.6)

with some given boundary conditions
u.0/ D ˛;

u.1/ D ˇ:

(2.7)

The function f .x/ is specified and we wish to determine u.x/ in the interval 0 < x < 1.
This problem is called a 2-point BVP since boundary conditions are given at two distinct
points. This problem is so simple that we can solve it explicitly (integrate f .x/ twice
and choose the two constants of integration so that the boundary conditions are satisfied),
but studying finite difference methods for this simple equation will reveal some of the
essential features of all such analysis, particularly the relation of the global error to the
local truncation error and the use of stability in making this connection.
We will attempt to compute a grid function consisting of values U0 ; U1 ; : : : ; Um ,
UmC1 , where Uj is our approximation to the solution u.xj /. Here xj D j h and h D
1=.m C 1/ is the mesh width, the distance between grid points. From the boundary
conditions we know that U0 D ˛ and UmC1 D ˇ, and so we have m unknown values
U1 ; : : : ; Um to compute. If we replace u00.x/ in (2.6) by the centered difference approximation
1
D 2 Uj D 2 .Uj 1 2Uj C Uj C1 /;
h
then we obtain a set of algebraic equations
1
.Uj 1
h2

2Uj C Uj C1 / D f .xj / for j D 1; 2; : : : ; m:

(2.8)

Note that the first equation .j D 1/ involves the value U0 D ˛ and the last equation
.j D m/ involves the value UmC1 D ˇ. We have a linear system of m equations for the m
unknowns, which can be written in the form
AU D F;

i

i

(2.9)

i

i

i

i

i

16

“rjlfdm”
2007/6/1
page 16
i

Chapter 2. Steady States and Boundary Value Problems

where U is the vector of unknowns U D ŒU1 ; U2 ; : : : ; Um T and
2
3
2
3
2 1
f .x1 / ˛= h2
6 1
7
6
7
2 1
f .x2 /
6
7
6
7
6
7
7
6
1
2
1
f
.x
/
3
1 6
7
6
7
AD 26
;
F D6
7
7 : (2.10)
:
:
:
:
::
::
::
::
7
6
7
h 6
6
7
6
7
4
5
4
5
1
2 1
f .xm 1 /
2
1
2
f .xm / ˇ= h
This tridiagonal linear system is nonsingular and can be easily solved for U from any righthand side F .
How well does U approximate the function u.x/? We know that the centered difference approximation D 2 , when applied to a known smooth function u.x/, gives a second order accurate approximation to u00.x/. But here we are doing something more complicated—
we know the values of u00 at each point and are computing a whole set of discrete values
U1 ; : : : ; Um with the property that applying D 2 to these discrete values gives the desired
values f .xj /. While we might hope that this process also gives errors that are O.h2 / (and
indeed it does), this is certainly not obvious.
First we must clarify what we mean by the error in the discrete values U1 ; : : : ; Um
relative to the true solution u.x/, which is a function. Since Uj is supposed to approximate
u.xj /, it is natural to use the pointwise errors Uj u.xj /. If we let UO be the vector of true
values
2
3
u.x1 /
6 u.x2 / 7
6
7
UO D 6
(2.11)
7;
::
4
5
:
u.xm /
then the error vector E defined by
EDU

UO

contains the errors at each grid point.
Our goal is now to obtain a bound on the magnitude of this vector, showing that it is
O.h2 / as h ! 0. To measure the magnitude of this vector we must use some norm, for
example, the max-norm
kEk1 D max jEj j D max jUj
1j m

u.xj /j:

1j m

This is just the largest error over the interval. If we can show that kEk1 D O.h2 /, then it
follows that each pointwise error must be O.h2 / as well.
Other norms are often used to measure grid functions, either because they are more
appropriate for a given problem or simply because they are easier to bound since some
mathematical techniques work only with a particular norm. Other norms that are frequently
used include the 1-norm
m
X
kEk1 D h
jEj j
j D1

i

i

i

i

i

i

i

2.5. Local truncation error

“rjlfdm”
2007/6/1
page 17
i

17

and the 2-norm

11=2

0
kEk2 D @h

m
X

jEj j2 A

:

j D1

Note the factor of h that appears in these definitions. See Appendix A for a more thorough
discussion of grid function norms and how they relate to standard vector norms.
Now let’s return to the problem of estimating the error in our finite difference solution
to BVP obtained by solving the system (2.9). The technique we will use is absolutely basic
to the analysis of finite difference methods in general. It involves two key steps. We first
compute the local truncation error (LTE) of the method and then use some form of stability
to show that the global error can be bounded in terms of the LTE.
The global error simply refers to the error U UO that we are attempting to bound.
The LTE refers to the error in our finite difference approximation of derivatives and hence
is something that can be easily estimated using Taylor series expansions, as we have seen in
Chapter 1. Stability is the magic ingredient that allows us to go from these easily computed
bounds on the local error to the estimates we really want for the global error. Let’s look at
each of these in turn.

2.5

Local truncation error

The LTE is defined by replacing Uj with the true solution u.xj / in the finite difference
formula (2.8). In general the true solution u.xj / won’t satisfy this equation exactly and the
discrepancy is the LTE, which we denote by j :
j D

1
.u.xj 1 /
h2

2u.xj / C u.xj C1 //

f .xj /

(2.12)

for j D 1; 2; : : : ; m. Of course in practice we don’t know what the true solution u.x/ is,
but if we assume it is smooth, then by the Taylor series expansions (1.5a) we know that


1
j D u00.xj / C h2 u0000.xj / C O.h4 /
f .xj /:
(2.13)
12
Using our original differential equation (2.6) this becomes
j D

1 2 0000
h u .xj / C O.h4 /:
12

Although u0000 is in general unknown, it is some fixed function independent of h, and so
j D O.h2 / as h ! 0.
If we define  to be the vector with components j , then
 D AUO

F;

where UO is the vector of true solution values (2.11), and so
AUO D F C :

i

i

(2.14)

i

i

i

i

i

18

2.6

“rjlfdm”
2007/6/1
page 18
i

Chapter 2. Steady States and Boundary Value Problems

Global error
UO , we subtract

To obtain a relation between the local error  and the global error E D U
(2.14) from (2.9) that defines U , obtaining
AE D :

(2.15)

This is simply the matrix form of the system of equations
1
.Ej 1
h2

2Ej C Ej C1 / D .xj / for j D 1; 2; : : : ; m

with the boundary conditions
E0 D EmC1 D 0
since we are using the exact boundary data U0 D ˛ and UmC1 D ˇ. We see that the
global error satisfies a set of finite difference equations that has exactly the same form as
our original difference equations for U except that the right-hand side is given by  rather
than F .
From this it should be clear why we expect the global error to be roughly the same
magnitude as the local error . We can interpret the system (2.15) as a discretization of the
ODE
e 00.x/ D .x/ for 0 < x < 1
(2.16)
with boundary conditions
e.0/ D 0;

e.1/ D 0:

1 2 0000
Since .x/  12
h u .x/, integrating twice shows that the global error should be roughly

e.x/ 

1 2 00
1
h u .x/ C h2 u00.0/ C x.u00.1/
12
12

u00.0//



and hence the error should be O.h2 /.

2.7

Stability

The above argument is not completely convincing because we are relying on the assumption that solving the difference equations gives a decent approximation to the solution of
the underlying differential equations (actually the converse now, that the solution to the differential equation (2.16) gives a good indication of the solution to the difference equations
(2.15)). Since it is exactly this assumption we are trying to prove, the reasoning is rather
circular.
Instead, let’s look directly at the discrete system (2.15), which we will rewrite in the
form
Ah E h D  h ;
(2.17)
where the superscript h indicates that we are on a grid with mesh spacing h. This serves as
a reminder that these quantities change as we refine the grid. In particular, the matrix Ah is
an m  m matrix with h D 1=.m C 1/ so that its dimension is growing as h ! 0.

i

i

i

i

i

i

i

2.9. Convergence

“rjlfdm”
2007/6/1
page 19
i

19

Let .Ah / 1 be the inverse of this matrix. Then solving the system (2.17) gives
E h D .Ah / 1  h
and taking norms gives
kE hk D k.Ah / 1  h k
 k.Ah / 1 k k h k:
We know that k h k D O.h2 / and we are hoping the same will be true of kE hk. It is
clear what we need for this to be true: we need k.Ah / 1 k to be bounded by some constant
independent of h as h ! 0:
k.Ah / 1 k  C for all h sufficiently small:
Then we will have
kE h k  C k hk

(2.18)

and so kE hk goes to zero at least as fast as k h k. This motivates the following definition
of stability for linear BVPs.
Definition 2.1. Suppose a finite difference method for a linear BVP gives a sequence of
matrix equations of the form Ah U h D F h , where h is the mesh width. We say that the
method is stable if .Ah / 1 exists for all h sufficiently small (for h < h0 , say) and if there is
a constant C , independent of h, such that
k.Ah / 1 k  C for all h < h0 :

2.8

(2.19)

Consistency

We say that a method is consistent with the differential equation and boundary conditions
if
k hk ! 0 as h ! 0:
(2.20)
This simply says that we have a sensible discretization of the problem. Typically k h k D
O.hp / for some integer p > 0, and then the method is certainly consistent.

2.9

Convergence

A method is said to be convergent if kE hk ! 0 as h ! 0. Combining the ideas introduced
above we arrive at the conclusion that
consistency C stability

H)

convergence:

(2.21)

This is easily proved by using (2.19) and (2.20) to obtain the bound
kE h k  k.Ah / 1 k k hk  C k h k ! 0 as h ! 0:

i

i

i

i

i

i

i

20

“rjlfdm”
2007/6/1
page 20
i

Chapter 2. Steady States and Boundary Value Problems

Although this has been demonstrated only for the linear BVP, in fact most analyses of finite
difference methods for differential equations follow this same two-tier approach, and the
statement (2.21) is sometimes called the fundamental theorem of finite difference methods.
In fact, as our above analysis indicates, this can generally be strengthened to say that
O.hp / local truncation error C stability

O.hp / global error.

H)

(2.22)

Consistency (and the order of accuracy) is usually the easy part to check. Verifying stability is the hard part. Even for the linear BVP just discussed it is not at all clear how to
check the condition (2.19) since these matrices become larger as h ! 0. For other problems it may not even be clear how to define stability in an appropriate way. As we will see,
there are many definitions of “stability” for different types of problems. The challenge in
analyzing finite difference methods for new classes of problems often is to find an appropriate definition of “stability” that allows one to prove convergence using (2.21) while at
the same time being sufficiently manageable that we can verify it holds for specific finite
difference methods. For nonlinear PDEs this frequently must be tuned to each particular
class of problems and relies on existing mathematical theory and techniques of analysis for
this class of problems.
Whether or not one has a formal proof of convergence for a given method, it is always
good practice to check that the computer program is giving convergent behavior, at the rate
expected. Appendix A contains a discussion of how the error in computed results can be
estimated.

2.10

Stability in the 2-norm

Returning to the BVP at the start of the chapter, let’s see how we can verify stability and
hence second order accuracy. The technique used depends on what norm we wish to consider. Here we will consider the 2-norm and see that we can show stability by explicitly
computing the eigenvectors and eigenvalues of the matrix A. In Section 2.11 we show
stability in the max-norm by different techniques.
Since the matrix A from (2.10) is symmetric, the 2-norm of A is equal to its spectral
radius (see Section A.3.2 and Section C.9):
kAk2 D .A/ D max jp j:
1pm

(Note that p refers to the pth eigenvalue of the matrix. Superscripts are used to index the
eigenvalues and eigenvectors, while subscripts on the eigenvector below refer to components of the vector.)
The matrix A 1 is also symmetric, and the eigenvalues of A 1 are simply the inverses
of the eigenvalues of A, so

kA

1

k2 D .A

1

/ D max j.p /
1pm

1

jD

 1
min jp j

:

1pm

So all we need to do is compute the eigenvalues of A and show that they are bounded
away from zero as h ! 0. Of course we have an infinite set of matrices Ah to consider,

i

i

i

i

i

i

i

2.10. Stability in the 2-norm

“rjlfdm”
2007/6/1
page 21
i

21

as h varies, but since the structure of these matrices is so simple, we can obtain a general
expression for the eigenvalues of each Ah . For more complicated problems we might
not be able to do this, but it is worth going through in detail for this problem because
one often considers model problems for which such an analysis is possible. We will also
need to know these eigenvalues for other purposes when we discuss parabolic equations
in Chapter 9. (See also Section C.7 for more general expressions for the eigenvalues of
related matrices.)
We will now focus on one particular value of h D 1=.mC1/ and drop the superscript
h to simplify the notation. Then the m eigenvalues of A are given by
p D

2
.cos.ph/
h2

1/ for p D 1; 2; : : : ; m:

(2.23)

The eigenvector up corresponding to p has components upj for j D 1; 2; : : : ; m
given by
p
uj D sin.pj h/:
(2.24)
This can be verified by checking that Aup D p up . The j th component of the vector
Aup is

1  p
p
p
.Aup /j D 2 uj 1 2uj C uj C1
h
1
D 2 .sin.p.j 1/h/ 2 sin.pj h/ C sin.p.j C 1/h//
h
1
D 2 .sin.pj h/ cos.ph/ 2 sin.pj h/ C sin.pj h/ cos.ph//
h
p
D p uj :
Note that for j D 1 and j D m the j th component of Aup looks slightly different (the
upj 1 or upjC1 term is missing) but that the above form and trigonometric manipulations are
still valid provided that we define
p

p

u0 D umC1 D 0;
as is consistent with (2.24). From (2.23) we see that the smallest eigenvalue of A (in
magnitude) is
2
.cos.h/ 1/
h2 

2
1 2 2
1 4 4
6
D 2
 h C  h C O.h /
h
2
24

1 D

D

 2 C O.h2 /:

This is clearly bounded away from zero as h ! 0, and so we see that the method is stable
in the 2-norm. Moreover we get an error bound from this:
kE hk2  k.Ah / 1 k2 k h k2 

i

i

1 h
k k2 :
2

i

i

i

i

i

22

“rjlfdm”
2007/6/1
page 22
i

Chapter 2. Steady States and Boundary Value Problems

1 2 0000
1 2
1 2
Since jh  12
h u .xj /, we expect k h k2  12
h ku0000k2 D 12
h kf 00k2 . The 2-norm of
00
the function f here means the grid-function norm of this function evaluated at the discrete
points xj , although this is approximately equal to the function space norm of f 00 defined
using (A.14).
Note that the eigenvector (2.24) is closely related to the eigenfunction of the corre@2
sponding differential operator @x
2 . The functions

up .x/ D sin.px/;

p D 1; 2; 3; : : : ;

satisfy the relation
@2 p
u .x/ D p up .x/
@x 2
with eigenvalue p D p 2  2. These functions also satisfy up .0/ D up .1/ D 0, and
@2
hence they are eigenfunctions of @x
2 on Œ0; 1 with homogeneous boundary conditions.
The discrete approximation to this operator given by the matrix A has only m eigenvalues
instead of an infinite number, and the corresponding eigenvectors (2.24) are simply the first
@2
m eigenfunctions of @x
2 evaluated at the grid points. The eigenvalue p is not exactly the
same as p , but at least for small values of p it is very nearly the same, since Taylor series
expansion of the cosine in (2.23) gives


1 2 2 2
2
1 4 4 4
p D 2
p  h C p  h C
h
2
24
D p 2  2 C O.h2 /

as h ! 0 for p fixed.

This relationship will be illustrated further when we study numerical methods for the heat
equation (2.1).

2.11

Green’s functions and max-norm stability

In Section 2.10 we demonstrated that A from (2.10) is stable in the 2-norm, and hence that
kEk2 D O.h2 /. Suppose, however, that we want a bound on the maximum error over the
interval, i.e., a bound on kEk1 D max jEj j. We can obtain one such bound directly from
the bound we have for the 2-norm. From (A.19) we know that
1
kEk1  p kEk2 D O.h3=2 / as h ! 0:
h
However, this does not show the second order accuracy that we hope to have. To show
that kEk1 D O.h2 / we will explicitly calculate the inverse of A and then show that
kA 1 k1 D O.1/, and hence
kEk1  kA 1 k1kk1 D O.h2 /
since kk1 D O.h2 /. As in the computation of the eigenvalues in the last section, we
can do this only because our model problem (2.6) is so simple. In general it would be
impossible to obtain closed form expressions for the inverse of the matrices Ah as h varies.

i

i

i

i

i

i

i

2.11. Green’s functions and max-norm stability

“rjlfdm”
2007/6/1
page 23
i

23

But again it is worth working out the details for this simple case because it gives a great
deal of insight into the nature of the inverse matrix and what it represents more generally.
Each column of the inverse matrix can be interpreted as the solution of a particular
BVP. The columns are discrete approximations to the Green’s functions that are commonly
introduced in the study of the differential equation. An understanding of this is valuable
in developing an intuition for what happens if we introduce relatively large errors at a few
points within the interval. Such difficulties arise frequently in practice, typically at the
boundary or at an internal interface where there are discontinuities in the data or solution.
We begin by reviewing the Green’s function solution to the BVP
u00.x/ D f .x/

for 0 < x < 1

(2.25)

u.1/ D ˇ:

(2.26)

with Dirichlet boundary conditions
u.0/ D ˛;

To keep the expressions simple below we assume we are on the unit interval, but everything
can be shifted to an arbitrary interval Œa; b.
For any fixed point xN 2 Œ0; 1, the Green’s function G.xI x/
N is the function of x that
solves the particular BVP of the above form with f .x/ D ı.x x/
N and ˛ D ˇ D 0.
Here ı.x x/
N is the “delta function” centered at x.
N The delta function, ı.x/, is not an
ordinary function but rather the mathematical idealization of a sharply peaked function that
is nonzero only on an interval . ; / near the origin and has the property that
Z 1

Z 
 .x/ dx D

1

 .x/ dx D 1:

(2.27)

if   x  0;
if 0  x  ;
otherwise:

(2.28)



For example, we might take
8
< . C x/=
 .x/ D
. x/=
:
0

This piecewise linear function is the “hat function” with width  and height 1=. The
exact shape of  is not important, but note that it must attain a height that is O.1=/ in
order for the integral to have the value 1. We can think of the delta function as being a
sort of limiting case of such functions as  ! 0. Delta functions naturally arise when we
differentiate functions that are discontinuous. For example, consider the Heaviside function
(or step function) H .x/ that is defined by

0 x < 0;
H .x/ D
(2.29)
1 x  0:
What is the derivative of this function? For x ¤ 0 the function is constant and so H 0 .x/ D
0. At x D 0 the derivative is not defined in the classical sense. But if we smooth out
the function a little bit, making it continuous and differentiable by changing H .x/ only on
the interval . ; /, then the new function H .x/ is differentiable everywhere and has a

i

i

i

i

i

i

i

24

“rjlfdm”
2007/6/1
page 24
i

Chapter 2. Steady States and Boundary Value Problems

derivative H0 .x/ that looks something like  .x/. The exact shape of H0 .x/ depends on
how we choose H .x/, but note that regardless of its shape, its integral must be 1, since
Z 1
Z 
H0 .x/ dx D
H0 .x/ dx
1



D H ./ H . /
D 1 0 D 1:
This explains the normalization (2.27). By letting  ! 0, we are led to define
H 0 .x/ D ı.x/:
This expression makes no sense in terms of the classical definition of derivatives, but it can
be made rigorous mathematically through the use of “distribution theory”; see, for example,
[31]. For our purposes it suffices to think of the delta function as being a very sharply
peaked function that is nonzero only on a very narrow interval but with total integral 1.
If we interpret the problem (2.25) as a steady-state heat conduction problem with
source .x/ D f .x/, then setting f .x/ D ı.x x/
N in the BVP is the mathematical
idealization of a heat sink that has unit magnitude but that is concentrated near a single
point. It might be easier to first consider the case f .x/ D ı.x x/,
N which corresponds to
a heat source localized at x,
N the idealization of a blow torch pumping heat into the rod at a
single point. With the boundary conditions u.0/ D u.1/ D 0, holding the temperature fixed
at each end, we would expect the temperature to be highest at the point xN and to fall linearly
to zero to each side (linearly because u00.x/ D 0 away from x).
N With f .x/ D ı.x x/,
N
a heat sink at x,
N we instead have the minimum temperature at x,
N rising linearly to each
side, as shown in Figure 2.1. This figure shows a typical Green’s function G.xI x/
N for one
particular choice of x.
N To complete the definition of this function we need to know the
value G.xI
N x/
N that it takes at the minimum. This value is determined by the fact that the
jump in slope at this point must be 1, since
Z xC
N
u0.xN C /

u0 .xN

u00.x/ dx

/ D
xN 
Z xC
N

ı.x

D

x/
N dx

(2.30)

xN 

D 1:

0

xN

1

Figure 2.1. The Green’s function G.xI x/
N from (2.31).

i

i

i

i

i

i

i

2.11. Green’s functions and max-norm stability

“rjlfdm”
2007/6/1
page 25
i

25

A little algebra shows that the piecewise linear function G.xI x/
N is given by

.xN 1/x for 0  x  x;
N
G.xI x/
N D
x.x
N
1/ for xN  x  1:

(2.31)

Note that by linearity, if we replaced f .x/ with cı.x x/
N for any constant c, the solution
to the BVP would be cG.xI x/.
N Moreover, any linear combination of Green’s functions at
different points xN is a solution to the BVP with the corresponding linear combination of
delta functions on the right-hand side. So if we want to solve
u00.x/ D 3ı.x

0:3/

5ı.x

0:7/;

(2.32)

for example (with u.0/ D u.1/ D 0), the solution is simply
u.x/ D 3G.xI 0:3/

5G.xI 0:7/:

(2.33)

This is a piecewise linear function with jumps in slope of magnitude 3 at x D 0:3 and 5
at x D 0:7. More generally, if the right-hand side is a sum of weighted delta functions at
any number of points,
n
X
f .x/ D
ck ı.x xk /;
(2.34)
kD1

then the solution to the BVP is
n
X

u.x/ D

ck G.xI xk /:

(2.35)

kD1

Now consider a general source f .x/ that is not a discrete sum of delta functions.
We can view this as a continuous distribution of point sources, with f .x/
N being a density
function for the weight assigned to the delta function at x,
N i.e.,
Z 1
f .x/ D

f .x/ı.x
N

x/
N d x:
N

(2.36)

0

(Note that if we smear out ı to  , then the right-hand side becomes a weighted average of
values of f very close to x.) This suggests that the solution to u00.x/ D f .x/ (still with
u.0/ D u.1/ D 0) is
Z 1
u.x/ D
f .x/G.xI
N
x/
N d x;
N
(2.37)
0

and indeed it is.
Now let’s consider more general boundary conditions. Since each Green’s function
G.xI x/
N satisfies the homogeneous boundary conditions u.0/ D u.1/ D 0, any linear
combination does as well. To incorporate the effect of nonzero boundary conditions, we
introduce two new functions G0 .x/ and G1 .x/ defined by the BVPs
G000.x/ D 0;

i

i

G0 .0/ D 1;

G0 .1/ D 0

(2.38)

i

i

i

i

i

26

“rjlfdm”
2007/6/1
page 26
i

Chapter 2. Steady States and Boundary Value Problems

and
G100 .x/ D 0;

G1 .0/ D 0;

G1 .1/ D 1:

(2.39)

The solutions are
G0 .x/ D 1 x;
G1 .x/ D x:

(2.40)

These functions give the temperature distribution for the heat conduction problem with
the temperature held at 1 at one boundary and 0 at the other with no internal heat source.
Adding a scalar multiple of G0 .x/ to the solution u.x/ of (2.37) will change the value
of u.0/ without affecting u00.x/ or u.1/, so adding ˛G0 .x/ will allow us to satisfy the
boundary condition at x D 0, and similarly adding ˇG1 .x/ will give the desired boundary
value at x D 1. The full solution to (2.25) with boundary conditions (2.26) is thus
Z 1
u.x/ D ˛G0 .x/ C ˇG1 .x/ C
f .x/G.xI
N
x/
N d x:
N
(2.41)
0

Note that using the formula (2.31), we can rewrite this as


Z x
Z 1
u.x/ D ˛
xf
N .x/
N d xN .1 x/ C ˇ C
.xN
0

!
1/f .x/
N d xN x:

(2.42)

x

Of course this simple BVP can also be solved simply by integrating the function f twice,
and the solution (2.42) can be put in this same form using integration by parts. But for
our current purposes it is the form (2.41) that is of interest, since it shows clearly how the
effect of each boundary condition and the local source at each point feeds into the global
solution. The values ˛; ˇ, and f .x/ are the data for this linear differential equation and
(2.41) writes the solution as a linear operator applied to this data, analogous to writing the
solution to the linear system AU D F as U D A 1 F .
We are finally ready to return to the study of the max-norm stability of the finite
difference method, which will be based on explicitly determining the inverse matrix for the
matrix arising in this discretization. We will work with a slightly different formulation of
the linear algebra problem in which we view U0 and UmC1 as additional “unknowns” in
the problem and introduce two new equations in the system that simply state that U0 D ˛
and umC1 D ˇ. The modified system has the form AU D F , where now
2

h2
6 1
6
6
1 6
6
AD 26
h 6
6
6
4

0
2
1

3
1
2
::
:

1
::
:
1

::

:
2
1

1
2
0

7
7
7
7
7
7;
7
7
7
1 5
h2

3
U0
6 U1 7
7
6
6 U2 7
7
6
7
6
::
U D6
7;
:
7
6
7
6 U
6 m 1 7
5
4 U
m
UmC1
2

3
˛
6 f .x1 / 7
7
6
6 f .x2 / 7
7
6
7
6
::
FD6
7:
:
7
6
7
6 f .x
/
m 1 7
6
4 f .x / 5
m
ˇ
(2.43)
2

While we could work directly with the matrix A from (2.10), this reformulation has two
advantages:

i

i

i

i

i

i

i

2.11. Green’s functions and max-norm stability

“rjlfdm”
2007/6/1
page 27
i

27

1. It separates the algebraic equations corresponding to the boundary conditions from
the algebraic equations corresponding to the ODE u00.x/ D f .x/. In the system
(2.10), the first and last equations contain a mixture of ODE and boundary conditions.
Separating these terms will make it clearer how the inverse of A relates to the Green’s
function representation of the true solution found above.
2. In the next section we will consider Neumann boundary conditions u0.0/ D  in
place of u.0/ D ˛. In this case the value U0 really is unknown and our new formulation is easily extended to this case by replacing the first row of A with a discretization
of this boundary condition.
Let B denote the .m C 2/  .m C 2/ inverse of A from (2.43), B D A 1 . We will
index the elements of B by B00 through BmC1;mC1 in the obvious manner. Let Bj denote
the j th column of B for j D 0; 1; : : : ; m C 1. Then
ABj D ej ;
where ej is the j th column of the identity matrix. We can view this as a linear system to
be solved for Bj . Note that this linear system is simply the discretization of the BVP for
a special choice of right-hand side F in which only one element of this vector is nonzero.
This is exactly analogous to the manner in which the Green’s function for the ODE is
defined. The column B0 corresponds to the problem with ˛ D 1, f .x/ D 0, and ˇ D 0,
and so we expect B0 to be a discrete approximation of the function G0 .x/. In fact, the first
(i.e., j D 0) column of B has elements obtained by simply evaluating G0 at the grid points,
Bi0 D G0 .xi / D 1

xi :

(2.44)

Since this is a linear function, the second difference operator applied at any point yields
zero. Similarly, the last (j D m C 1) column of B has elements
Bi;mC1 D G1 .xi / D xi :

(2.45)

The interior columns (1  j  m) correspond to the Green’s function for zero boundary
conditions and the source concentrated at a single point, since Fj D 1 and Fi D 0 for
i ¤ j . Note that this is a discrete version of hı.x xj / since as a grid function F is
nonzero over an interval of length h but has value 1 there, and hence total mass h. Thus
we expect that the column Bj will be a discrete approximation to the function hG.xI xj /.
In fact, it is easy to check that

h.xj 1/xi ; i D 1; 2; : : : ; j ;
Bij D hG.xi I xj / D
(2.46)
h.xi 1/xj ; i D j ; j C 1; : : : ; m:
An arbitrary right-hand side F for the linear system can be written as
m
X

F D ˛e0 C ˇemC1 C

fj ej ;

(2.47)

j D1

and the solution U D BF is
m
X

U D ˛B0 C ˇBmC1 C

fj Bj

(2.48)

j D1

i

i

i

i

i

i

i

28

“rjlfdm”
2007/6/1
page 28
i

Chapter 2. Steady States and Boundary Value Problems

with elements

m
X

Ui D ˛.1

xi / C ˇxi C h

fj G.xi I xj /:

(2.49)

j D1

This is the discrete analogue of (2.41).
In fact, something more is true: suppose we define a function v.x/ by
m
X

v.x/ D ˛.1

x/ C ˇx C h

fj G.xI xj /:

(2.50)

j D1

Then Ui D v.xi / and v.x/ is the piecewise linear function that interpolates the numerical
solution. This function v.x/ is the exact solution to the BVP
v 00.x/ D h

m
X

f .xj /ı.x

xj /;

v.0/ D ˛;

v.1/ D ˇ:

(2.51)

j D1

Thus we can interpret the discrete solution as the exact solution to a modified problem in
which the right-hand side f .x/ has been
R xreplaced by a finite sum of delta functions at the
grid points xj , with weights hf .xj /  xjj C1=2
f .x/ dx.
1=2
To verify max-norm stability of the numerical method, we must show that kBk1 is
uniformly bounded as h ! 0. The infinity norm of the matrix is given by
mC1
X

kBk1 D

max

0j mC1

jBij j;
j D0

the maximum row sum of elements in the matrix. Note that the first row of B has B00 D 1
and B0j D 0 for j > 0, and hence row sum 1. Similarly the last row contains all zeros
except for BmC1;mC1 D 1. The intermediate rows are dense and the first and last elements
(from columns B0 and BmC1 ) are bounded by 1. The other m elements of each of these
rows are all bounded by h from (2.46), and hence
mC1
X

jBij j  1 C 1 C mh < 3
j D0

since h D 1=.m C 1/. Every row sum is bounded by 3 at most, and so kA 1 k1 < 3 for all
h, and stability is proved.
While it may seem like we’ve gone to a lot of trouble to prove stability, the explicit
representation of the inverse matrix in terms of the Green’s functions is a useful thing to
have, and if it gives additional insight into the solution process. Note, however, that it
would not be a good idea to use the explicit expressions for the elements of B D A 1 to
solve the linear system by computing U D BF . Since B is a dense matrix, doing this
matrix-vector multiplication requires O.m2 / operations. We are much better off solving
the original system AU D F by Gaussian elimination. Since A is tridiagonal, this requires
only O.m/ operations.

i

i

i

i

i

i

i “rjlfdm”

2007/6/1
page 29

i

2.12. Neumann boundary conditions

29

The Green’s function representation also clearly shows the effect that each local truncation error has on the global error. Recall that the global error E is related to the local
truncation error by AE D . This continues to hold for our reformulation of the problem,
where we now define 0 and mC1 as the errors in the imposed boundary conditions, which
are typically zero for the Dirichlet problem. Solving this system gives E D B. If we
did make an error in one of the boundary conditions, setting F0 to ˛ C 0 , the effect on
the global error would be 0 B0 . The effect of this error is thus nonzero across the entire
interval, decreasing linearly from the boundary where the error is made at the other end.
Each truncation error i for 1  i  m in the difference approximation to u00 .xi / D f .xi /
likewise has an effect on the global error everywhere, although the effect is largest at the
grid point xi , where it is hG.xi I xi /i , and decays linearly toward each end. Note that since
i D O.h2 /, the contribution of this error to the global error at each point is only O.h3 /.
However, since all m local errors contribute to the global error at each point, the total effect
is O.mh3 / D O.h2 /.
As a final note on this topic, observe that we have also worked out the inverse of the
original matrix A defined in (2.10). Because the first row of B consists of zeros beyond
the first element, and the last row consists of zeros, except for the last element, it is easy
to check that the inverse of the m  m matrix from (2.10) is the m  m central block of B
consisting of B11 through Bmm . The max-norm of this matrix is bounded by 1 for all h, so
our original formulation is stable as well.

2.12

Neumann boundary conditions

Now suppose that we have one or more Neumann boundary conditions instead of Dirichlet
boundary conditions, meaning that a boundary condition on the derivative u0 is given rather
than a condition on the value of u itself. For example, in our heat conduction example we
might have one end of the rod insulated so that there is no heat flux through this end, and
hence u0 D 0 there. More generally we might have heat flux at a specified rate giving
u0 D  at this boundary.
We will see in the next section that imposing Neumann boundary conditions at both
ends gives an ill-posed problem that has either no solution or infinitely many solutions. In
this section we consider (2.25) with one Neumann condition, say,
u0 .0/ D ;

u.1/ D ˇ:

(2.52)
x

Figure 2.2 shows the solution to this problem with f .x/ D e ,  D 0, and ˇ D 0 as one
example.
To solve this problem numerically, we need to determine U0 as one of the unknowns.
If we use the formulation of (2.43), then the first row of the matrix A must be modified to
model the boundary condition (2.52).
First approach. As a first try, we might use a one-sided expression for u0 .0/,
such as
U1 U0
D :
(2.53)
h
If we use this equation in place of the first line of the system (2.43), we obtain the following
system of equations for the unknowns U0 ; U1 ; : : : ; Um ; UmC1 :

i

i

i

i

i

i

i

30

“rjlfdm”
2007/6/1
page 30
i

Chapter 2. Steady States and Boundary Value Problems
0

3.2

10

3.1
−1

3

10

2.9
−2

10

2.8
2.7

−3

10

2.6
2.5

−4

10

2.4
2.3

(a) 2.2

0

0.2

0.4

0.6

0.8

(b)

1

−2

−1

10

10

Figure 2.2. (a) Sample solution to the steady-state heat equation with a Neumann
boundary condition at the left boundary and Dirichlet at the right. The solid line is the true
solution. The plus sign shows a solution on a grid with 20 points using (2.53). The circle
shows the solution on the same grid using (2.55). (b) A log-log plot of the max-norm error
as the grid is refined is also shown for each case.
2

h
6 1
6
6
6
1 6
6
6
h2 6
6
6
6
4

h
2
1

3
3 2
U0

7 6 U1 7 6 f .x1 / 7
7
76
7 6
7 6 U2 7 6 f .x2 / 7
7
76
7 6
7 6 U3 7 6 f .x3 / 7
7
76
7 6
7:
76
7D6
::
::
7
76
7
6
:
:
7
76
7 6
7
76 U
7 6 f .x
/
m 1 7
76 m 1 7 6
1 5 4 Um 5 4 f .xm / 5
UmC1
ˇ
h2
32

1
2
1

1
2
::
:

1
::
:
1

::

:
2
1

1
2
0

(2.54)

Solving this system of equations does give an approximation to the true solution (see Figure 2.2), but checking the errors shows that this is only first order accurate. Figure 2.2 also
shows a log-log plot of the max-norm errors as we refine the grid. The problem is that the
local truncation error of the approximation (2.53) is O.h/, since
1
.hu.x1 / hu.x0 // 
h2
1
D u0.x0 / C hu00.x0 / C O.h2 /
2
1 00
D hu .x0 / C O.h2 /:
2

0 D



This translates into a global error that is only O.h/ as well.
Remark: It is sometimes possible to achieve second order accuracy even if the local
truncation error is O.h/ at a single point, as long as it is O.h2 / everywhere else. This is
true here if we made an O.h/ truncation error at a single interior point, since the effect on
the global error would be this j Bj , where Bj is the j th column of the appropriate inverse
matrix. As in the Dirichlet case, this column is given by the corresponding Green’s function
scaled by h, and so the O.h/ local error would make an O.h2 / contribution to the global
error at each point. However, introducing an O.h/ error in 0 gives a contribution of 0 B0

i

i

i

i

i

i

i

2.12. Neumann boundary conditions

“rjlfdm”
2007/6/1
page 31
i

31

to the global error, and as in the Dirichlet case this first column of B contains elements that
are O.1/, resulting in an O.h/ contribution to the global error at every point.
Second approach. To obtain a second order accurate method, we can use a centered
approximation to u0 .0/ D  instead of the one-sided approximation (2.53). We might introduce another unknown U 1 and, instead of the single equation (2.53), use the following
two equations:
1
.U 1
h2

2U0 C U1 / D f .x0 /;

1
.U1
2h

(2.55)
U 1 / D :

This results in a system of m C 3 equations.
Introducing the unknown U 1 outside the interval Œ0; 1 where the original problem
is posed may seem unsatisfactory. We can avoid this by eliminating the unknown U 1 from
the two equations (2.55), resulting in a single equation that can be written as
1
h
. U0 C U1 / D  C f .x0 /:
h
2

(2.56)

We have now reduced the system to one with only m C 2 equations for the unknowns
U0 ; U1 ; : : : ; UmC1 . The matrix is exactly the same as the matrix in (2.54), which came
from the one-sided approximation. The only difference in the linear system is that the first
element in the right-hand side of (2.54) is now changed from  to  C h2 f .x0 /. We can
interpret this as using the one-sided approximation to u0 .0/, but with a modified value for
this Neumann boundary condition that adjusts for the fact that the approximation has an
O.h/ error by introducing the same error in the data .
Alternatively, we can view the left-hand side of (2.56) as a centered approximation
to u0.x0 C h=2/ and the right-hand side as the first two terms in the Taylor series expansion
of this value,


h
h
h
u0 x0 C
D u0.x0 / C u00.x0 / C    D  C f .x0 / C    :
2
2
2
Third approach. Rather than using a second order accurate centered approximation
to the Neumann boundary condition, we could instead use a second order accurate onesided approximation based on the three unknowns U0 ; U1 , and U2 . An approximation of
this form was derived in Example 1.2, and using this as the boundary condition gives the
equation


1 3
1
U0 2U1 C U2 D :
h 2
2

i

i

i

i

i

i

i

32

“rjlfdm”
2007/6/1
page 32
i

Chapter 2. Steady States and Boundary Value Problems

This results in the linear system
2

3
3 2
U0

7 6 U1 7 6 f .x1 / 7
7
76
7 6
7 6 U2 7 6 f .x2 / 7
7
76
7 6
7 6 U3 7 6 f .x3 / 7
7
76
7 6
7:
76
7D6
::
::
7
76
7 6
:
:
7
76
7 6
7
76 U
7 6 f .x
/
m 1 7
76 m 1 7 6
5
5
4
4
1
Um
f .xm / 5
2
UmC1
ˇ
h
32

6 1
6
6
6
1 6
6
6
h2 6
6
6
6
4

2
1

1
2
1

1
2
::
:

1
::
:
1

::

:
2
1

1
2
0

(2.57)

This boundary condition is second order accurate from the error expression (1.12).
The use of this equation slightly disturbs the tridiagonal structure but adds little to the
cost of solving the system of equations and produces a second order accurate result. This
approach is often the easiest to generalize to other situations, such as higher order accurate
methods, nonuniform grids, or more complicated boundary conditions.

2.13

Existence and uniqueness

In trying to solve a mathematical problem by a numerical method, it is always a good idea
to check that the original problem has a solution and in fact that it is well posed in the sense
developed originally by Hadamard. This means that the problem should have a unique
solution that depends continuously on the data used to define the problem. In this section
we will show that even seemingly simple BVPs may fail to be well posed.
Consider the problem of Section 2.12 but now suppose we have Neumann boundary
conditions at both ends, i.e., we have (2.6) with
u0 .0/ D 0 ;

u0.1/ D 1 :

In this case the techniques of Section 2.12 would naturally lead us to the discrete system
2

h
6 1
6
6
1 6
6
6
h2 6
6
6
4

h
2
1

3 2
U0
7 6 U1 7 6
76
7 6
7 6 U2 7 6
76
7 6
7 6 U3 7 6
76
7D6
76
7 6
::
76
7 6
:
76
7 6
6
5
4
Um 5 4
1
UmC1
h
32

1
2
1

1
2
::
:

1
::
:
1

::

:
2
h

3
0 C h2 f .x0 /
7
f .x1 /
7
7
f .x2 /
7
7
f .x3 /
7:
7
::
7
7
:
7
5
f .xm /
1 C h2 f .xmC1 /
(2.58)

If we try to solve this system, however, we will soon discover that the matrix is singular,
and in general the system has no solution. (Or, if the right-hand side happens to lie in the
range of the matrix, it has infinitely many solutions.) It is easy to verify that the matrix is
singular by noting that the constant vector e D Œ1; 1; : : : ; 1T is a null vector.
This is not a failure in our numerical model. In fact it reflects that the problem we
are attempting to solve is not well posed, and the differential equation will also have either
no solution or infinitely many solutions. This can be easily understood physically by again
considering the underlying heat equation discussed in Section 2.1. First consider the case

i

i

i

i

i

i

i

2.13. Existence and uniqueness

“rjlfdm”
2007/6/1
page 33
i

33

where 0 D 1 D 0 and f .x/  0 so that both ends of the rod are insulated, there is
no heat flux through the ends, and there is no heat source within the rod. Recall that the
BVP is a simplified equation for finding the steady-state solution of the heat equation (2.2)
with some initial data u0 .x/. How does u.x; t/ behave with time? In the case now being
R1
considered the total heat energy in the rod must be conserved with time, so 0 u.x; t/ dx 
R1 0
0 u .x/ dx for all time. Diffusion of the heat tends to redistribute it until it is uniformly
distributed throughout the rod, so we expect the steady state solution u.x/ to be constant
in x,
u.x/ D c;
(2.59)
where the constant c depends on the initial data u0 .x/. In fact, by conservation of energy,
R1
c D 0 u0 .x/ dx for our rod of unit length. But notice now that any constant function
of the form (2.59) is a solution of the steady-state BVP, since it satisfies all the conditions
u00.x/  0, u0 .0/ D u0 .1/ D 0. The ODE has infinitely many solutions in this case. The
physical problem has only one solution, but in attempting to simplify it by solving for the
steady state alone, we have thrown away a crucial piece of data, which is the heat content
of the initial data for the heat equation. If at least one boundary condition is a Dirichlet
condition, then it can be shown that the steady-state solution is independent of the initial
data and we can solve the BVP uniquely, but not in the present case.
Now suppose that we have a source term f .x/ that is not identically zero, say,
f .x/ < 0 everywhere. Then we are constantly adding heat to the rod (recall that f D
in (2.4)). Since no heat can escape through the insulated ends, we expect the temperature
to keep rising without bound. In this case we never reach a steady state, and the BVP has
no solution. On the other hand, if f is positive over part of the interval and negative elsewhere, and the net effect of the heat sources and sinks exactly cancels out, then we expect
that a steady state might exist. In fact, solving the BVP exactly by integrating twice and
trying to determine the constants of integration from the boundary conditions shows that a
R1
solution exists (in the case of insulated boundaries) only if 0 f .x/ dx D 0, in which case
there are infinitely many solutions. If 0 and/or 1 are nonzero, then there is heat flow at
the boundaries and the net heat source must cancel the boundary fluxes. Since
Z 1
u0.1/ D u0.0/ C

Z 1
u00.x/ dx D

0

this requires

f .x/ dx;

(2.60)

0

Z 1
f .x/ dx D 1

0 :

(2.61)

0

Similarly, the singular linear system (2.58) has a solution (in fact infinitely many solutions)
only if the right-hand side F is orthogonal to the null space of AT . This gives the condition
m

X
h
h
f .xi / C f .xmC1 / D 1
f .x0 / C h
2
2

0 ;

(2.62)

iD1

which is the trapezoidal rule approximation to the condition (2.61).

i

i

i

i

i

i

i

34

“rjlfdm”
2007/6/1
page 34
i

Chapter 2. Steady States and Boundary Value Problems

2.14

Ordering the unknowns and equations

Note that in general we are always free to change the order of the equations in a linear system without changing the solution. Modifying the order corresponds to permuting the rows
of the matrix and right-hand side. We are also free to change the ordering of the unknowns
in the vector of unknowns, which corresponds to permuting the columns of the matrix. As
an example, consider the difference equations given by (2.9). Suppose we reordered the unknowns by listing first the unknowns at odd numbered grid points and then the unknowns
at even numbered grid points, so that UQ D ŒU1 ; U3 ; U5 ; : : : ; U2 ; U4 ; : : :T . If we also
reorder the equations in the same way, i.e., we write down first the difference equation
centered at U1 , then at U3 ; U5 , etc., then we would obtain the following system:
2

2

6
6
6
6
6
6
6
1 6
6
h2 6
6 1
6
6
6
6
6
4

1
1

2
2
::

32
1
1

:

1
::
:

2
1
1

2

:
1

2
1
1

2
1
::
:

f .x1 / ˛= h
6
f .x3 /
6
6
f .x5 /
6
6
::
6
:
6
6
f
.x
m 1/
D6
6
f
.x
2/
6
6
f
.x
4/
6
6
f
.x
6/
6
6
::
4
:
f .xm /

::

2
::

:
1
3
2

::

:

U1
U3
U5
::
:

3

7
76
7
76
7
76
7
76
7
76
7
76
7
76
7
6
1 7 6 Um 1 7
7
7 6 U2 7
7
76
7 6 U4 7
7
76
7 6 U6 7
7
76
7
76
::
5
54
:
Um
2
(2.63)

7
7
7
7
7
7
7
7
7:
7
7
7
7
7
7
7
5

ˇ= h2

This linear system has the same solution as (2.9) modulo the reordering of unknowns, but it
looks very different. For this one-dimensional problem there is no point in reordering things
this way, and the natural ordering ŒU1 ; U2 ; U3 ; : : :T clearly gives the optimal matrix
structure for the purpose of applying Gaussian elimination. By ordering the unknowns so
that those which occur in the same equation are close to one another in the vector, we keep
the nonzeros in the matrix clustered near the diagonal. In two or three space dimensions
there are more interesting consequences of choosing different orderings, a topic we return
to in Section 3.3.

i

i

i

i

i

i

i “rjlfdm”

2007/6/1
page 35

i

2.15. A general linear second order equation

2.15

35

A general linear second order equation

We now consider the more general linear equation
a.x/u00.x/ C b.x/u0 .x/ C c.x/u.x/ D f .x/;

(2.64)

together with two boundary conditions, say, the Dirichlet conditions
u.a/ D ˛;

u.b/ D ˇ:

(2.65)

This equation can be discretized to second order by




Ui 1 2Ui C UiC1
UiC1 Ui 1
C ci Ui D fi ;
ai
C bi
2h
h2

(2.66)

where, for example, ai D a.xi /. This gives the linear system AU D F , where A is the
tridiagonal matrix
3

2

.h2c1 2a1 /
6 .a
6 2 hb2 =2/
6
::
1 6
:
AD 26
6
h 6
6
4

and

2
U1
U2
::
:

.a1 C hb1 =2/
.h2c2 2a2 /
::
:

.a2 C hb2 =2/
::
:

.am 1

hbm 1 =2/

3

2

.h2 cm 1 2am 1 /
.am hbm =2/

f1

6
7
6
7
6
7
U D6
7;
6
7
4 Um 1 5
Um

6
6
6
F D6
6
4

7
7
7
7
7
7
7
7
.am 1 C hbm 1 =2/ 5
2
.h cm 2am /
(2.67)

.a1 = h2 b1 =2h/˛
f2
::
:

3
7
7
7
7:
7
5

(2.68)

fm 1
fm .am = h2 C bm =2h/ˇ
This linear system can be solved with standard techniques, assuming the matrix is nonsingular. A singular matrix would be a sign that the discrete system does not have a unique
solution, which may occur if the original problem, or a nearby problem, is not well posed
(see Section 2.13).
The discretization used above, while second order accurate, may not be the best discretization to use for certain problems of this type. Often the physical problem has certain
properties that we would like to preserve with our discretization, and it is important to understand the underlying problem and be aware of its mathematical properties before blindly
applying a numerical method. The next example illustrates this.
Example 2.1. Consider heat conduction in a rod with varying heat conduction properties, where the parameter .x/ varies with x and is always positive. The steady-state
heat-conduction problem is then
..x/u0 .x//0 D f .x/

(2.69)

together with some boundary conditions, say, the Dirichlet conditions (2.65). To discretize
this equation we might be tempted to apply the chain rule to rewrite (2.69) as
.x/u00 .x/ C  0 .x/u0 .x/ D f .x/

(2.70)

and then apply the discretization (2.67), yielding the matrix

i

i

i

i

i

i

i

36

“rjlfdm”
2007/6/1
page 36
i

Chapter 2. Steady States and Boundary Value Problems
2

6 .2
6
1 6
6
AD 26
h 6
6
4

21
h20 =2/
::
:

3

.1 C h10 =2/
22
::
:
.m 1

.2 C h20 =2/
::

0
hm
=2/
1

:

2m 1
0 =2/
.m hm

0
.m 1 C hm
1
2m

7
7
7
7
7:
7
7
=2/ 5
(2.71)

However, this is not the best approach. It is better to discretize the physical problem (2.69)
directly. This can be done by first approximating .x/u0 .x/ at points halfway between the
grid points, using a centered approximation


UiC1 Ui
0
.xiC1=2 /u .xiC1=2 / D iC1=2
h
and the analogous approximation at xi 1=2 . Differencing these then gives a centered approximation to .u0 /0 at the grid point xi :





UiC1 Ui
Ui Ui 1
1
.u0 /0 .xi / 
iC1=2
i 1=2
h
h
h
(2.72)
1
D 2 Œi 1=2 Ui 1 .i 1=2 C iC1=2 /Ui C iC1=2 UiC1 :
h
This leads to the matrix
2
6
6
1 6
6
AD 26
h 6
6
4

.1=2 C 3=2 /
3=2
::
:

3=2
.3=2 C 5=2 /
::
:
m 3=2

3
5=2
::

:

.m 3=2 C m 1=2 /
m 1=2

7
7
7
7
7:
7
7
5

m 1=2
.m 1=2 C mC1=2 /
(2.73)

Comparing (2.71) to (2.73), we see that they agree to O.h2 /, noting, for example, that
1
.xiC1=2 / D .xi / C h 0 .xi / C O.h2 / D .xiC1 /
2

1 0
h .xiC1 / C O.h2 /:
2

However, the matrix (2.73) has the advantage of being symmetric, as we would hope, since
the original differential equation is self-adjoint. Moreover, since  > 0, the matrix can be
shown to be nonsingular and negative definite. This means that all the eigenvalues are neg@
@
ative, a property also shared by the differential operator @x
.x/ @x
(see Section C.8). It is
generally desirable to have important properties such as these modeled by the discrete approximation to the differential equation. One can then show, for example, that the solution
to the difference equations satisfies a maximum principle of the same type as the solution
to the differential equation: for the homogeneous equation with f .x/  0, the values of
u.x/ lie between the values of the boundary values ˛ and ˇ everywhere, so the maximum
and minimum values of u arise on the boundaries. For the heat conduction problem this is
physically obvious: the steady-state temperature in the rod won’t exceed what’s imposed
at the boundaries if there is no heat source.

i

i

i

i

i

i

i

2.16. Nonlinear equations

“rjlfdm”
2007/6/1
page 37
i

37

When solving the resulting linear system by iterative methods (see Chapters 3 and 4)
it is also often desirable that the matrix have properties such as negative definiteness, since
some iterative methods (e.g., the conjugate-gradient (CG) method in Section 4.3) depend
on such properties.

2.16

Nonlinear equations

We next consider a nonlinear BVP to illustrate the new complications that arise in this
case. We will consider a specific example that has a simple physical interpretation which
makes it easy to understand and interpret solutions. This example also illustrates that not
all 2-point BVPs are steady-state problems.
Consider the motion of a pendulum with mass m at the end of a rigid (but massless)
bar of length L, and let .t/ be the angle of the pendulum from vertical at time t, as illustrated in Figure 2.3. Ignoring the mass of the bar and forces of friction and air resistance,
we see that the differential equation for the pendulum motion can be well approximated by
 00.t/ D

.g=L/ sin..t//;

(2.74)

where g is the gravitational constant. Taking g=L D 1 for simplicity we have
 00 .t/ D

sin..t//

(2.75)

as our model problem.
For small amplitudes of the angle  it is possible to approximate sin./   and
obtain the approximate linear differential equation
 00 .t/ D .t/

(2.76)

(b) 2
1
0
−1
−2
0

1

2

3

4

5

6

7

8

9

10

1

2

3

4

5

6

7

8

9

10

(c)2
1



0
−1

(a)

−2
0

Figure 2.3. (a) Pendulum. (b) Solutions to the linear equation (2.76) for various
initial  and zero initial velocity. (c) Solutions to the nonlinear equation (2.75) for various
initial  and zero initial velocity.

i

i

i

i

i

i

i

38

“rjlfdm”
2007/6/1
page 38
i

Chapter 2. Steady States and Boundary Value Problems

with general solutions of the form A cos.t/ C B sin.t/. The motion of a pendulum that is
oscillating only a small amount about the equilibrium at  D 0 can be well approximated
by this sinusoidal motion, which has period 2 independent of the amplitude. For largeramplitude motions, however, solving (2.76) does not give good approximations to the true
behavior. Figures 2.3(b) and (c) show some sample solutions to the two equations.
To fully describe the problem we also need to specify two auxiliary conditions in
addition to the second order differential equation (2.75). For the pendulum problem the
IVP is most natural—we set the pendulum swinging from some initial position .0/ with
some initial angular velocity  0 .0/, which gives two initial conditions that are enough to
determine a unique solution at all later times.
To obtain instead a BVP, consider the situation in which we wish to set the pendulum
swinging from some initial given location .0/ D ˛ with some unknown angular velocity
 0 .0/ in such a way that the pendulum will be at the desired location .T / D ˇ at some
specified later time T . Then we have a 2-point BVP
 00 .t/ D sin..t// for 0 < t < T;
.0/ D ˛;
.T / D ˇ:

(2.77)

Similar BVPs do arise in more practical situations, for example, trying to shoot a missile
in such a way that it hits a desired target. In fact, this latter example gives rise to the name
shooting method for another approach to solving 2-point BVPs that is discussed in [4] and
[54], for example.

2.16.1 Discretization of the nonlinear boundary value problem
We can discretize the nonlinear problem (2.75) in the obvious manner, following our approach for linear problems, to obtain the system of equations
1
.i 1
h2

2i C iC1 / C sin.i / D 0

(2.78)

for i D 1; 2; : : : ; m, where h D T =.m C 1/ and we set 0 D ˛ and mC1 D ˇ. As in
the linear case, we have a system of m equations for m unknowns. However, this is now a
nonlinear system of equations of the form
G./ D 0;

(2.79)

where G W Rm ! Rm . This cannot be solved as easily as the tridiagonal linear systems
encountered so far. Instead of a direct method we must generally use some iterative method,
such as Newton’s method. If  Œk is our approximation to  in step k, then Newton’s method
is derived via the Taylor series expansion
G. ŒkC1 / D G. Œk / C G 0 . Œk /. ŒkC1

 Œk / C    :

Setting G. ŒkC1 / D 0 as desired, and dropping the higher order terms, results in
0 D G. Œk / C G 0 . Œk /. ŒkC1

i

i

 Œk /:

i

i

i

i

i

2.16. Nonlinear equations

“rjlfdm”
2007/6/1
page 39
i

39

This gives the Newton update
 ŒkC1 D  Œk C ı Œk ;

(2.80)

where ı Œk solves the linear system
J. Œk /ı Œk D

G. Œk /:

(2.81)

Here J./  G 0 ./ 2 Rmm is the Jacobian matrix with elements
Jij ./ D

@
Gi ./;
@j

where Gi ./ is the ith component of the vector-valued function G. In our case Gi ./ is
exactly the left-hand side of (2.78), and hence
8
if j D i 1 or j D i C 1;
< 1= h2
Jij ./ D
2= h2 C cos.i / if j D i;
:
0
otherwise;
so that
2

. 2 C h2 cos.1 //
6
1
1 6
6
J. / D 2 6
h 6
4

3

1
.

2 C h2 cos.2 //
::

:

1
::
:

::

1

. 2 C h2 cos.m //

:

7
7
7
7:
7
5

(2.82)

In each iteration of Newton’s method we must solve a tridiagonal linear system similar to
the single tridiagonal system that must be solved in the linear case.
Consider the nonlinear problem with T D 2, ˛ D ˇ D 0:7. Note that the linear
problem (2.76) has infinitely many solutions in this particular case since the linearized
pendulum has period 2 independent of the amplitude of motion; see Figure 2.3. This is
not true of the nonlinear equation, however, and so we might expect a unique solution to the
full nonlinear problem. With Newton’s method we need an initial guess for the solution,
and in Figure 2.4(a) we take a particular solution to the linearized problem, the one with
Œ0
initial angular velocity 0:5, as a first approximation, i.e., i D 0:7 cos.ti / C 0:5 sin.ti /.
Figure 2.4(a) shows the different  Œk for k D 0; 1; 2; : : : that are obtained as we iterate
with Newton’s method. They rapidly converge to a solution to the nonlinear system (2.78).
(Note that the solution looks similar to the solution to the linearized equation with  0 .0/ D
0, as we should have expected, and taking this as the initial guess,  Œ0 D 0:7 cos.t/, would
have given even more rapid convergence.)
Table 2.1 shows kı Œk k1 in each iteration, which measures the change in the solution.
As expected, Newton’s method appears to be converging quadratically.
If we start with a different initial guess  Œ0 (but still close enough to this solution), we
would find that the method still converges to this same solution. For example, Figure 2.4(b)
shows the iterates  Œk for k D 0; 1; 2; : : : with a different choice of  Œ0  0:7.
Newton’s method can be shown to converge if we start with an initial guess that is
sufficiently close to a solution. How close depends on the nature of the problem. For the

i

i

i

i

i

i

i

40

“rjlfdm”
2007/6/1
page 40
i

Chapter 2. Steady States and Boundary Value Problems
1

1

0.8

0.8

0.6

0

0

0.6

0.4

0.4

0.2

0.2

0
−0.2

3,4

1

0

2

−0.2
2

(a)

−0.4

−0.4

−0.6

−0.6

−0.8

−0.8

−1

−1

0

1

2

3

4

5

6

(b)

7

0

1

1

3,4

2

3

4

5

6

7

Figure 2.4. Convergence of Newton iterates toward a solution of the pendulum
problem. The iterates  Œk for k D 1; 2; : : : are denoted by the number k in the plots. (a)
Œ0
Œ0
Starting from i D 0:7 cos.ti / C 0:5 sin.ti /. (b) Starting from i D 0:7.
Table 2.1. Change kı Œk k1 in solution in each iteration of Newton’s method.
k

Figure 2.4(a)

Figure 2.5

0
1
2
3
4
5
6
7

3.2841e
1.7518e
3.1045e
2.3739e
1.5287e
5.8197e
1.5856e

4.2047e+00
5.3899e+00
8.1993e+00
7.7111e 01
3.8154e 02
2.2490e 04
9.1667e 09
1.3395e 15

01
01
02
04
08
15
15

problem considered above one need not start very close to the solution to converge, as seen
in the examples, but for more sensitive problems one might have to start extremely close.
In such cases it may be necessary to use a technique such as continuation to find suitable
initial data; see Section 2.19.

2.16.2 Nonuniqueness
The nonlinear problem does not have an infinite family of solutions the way the linear
equation does on the interval Œ0; 2, and the solution found above is an isolated solution in
the sense that there are no other solutions very nearby (it is also said to be locally unique).
However, it does not follow that this is the unique solution to the BVP (2.77). In fact physically we should expect other solutions. The solution we found corresponds to releasing
the pendulum with nearly zero initial velocity. It swings through nearly one complete cycle
and returns to the initial position at time T .
Another possibility would be to propel the pendulum upward so that it rises toward
the top (an unstable equilibrium) at  D , before falling back down. By specifying the
correct velocity we should be able to arrange it so that the pendulum falls back to  D 0:7
again at T D 2. In fact it is possible to find such a solution for any T > 0.

i

i

i

i

i

i

i

2.16. Nonlinear equations

“rjlfdm”
2007/6/1
page 41
i

41

12

2
10

8

1

6

4

3

4,5

2
0
0
0

1

2

3

4

5

6

7

Figure 2.5. Convergence of Newton iterates toward a different solution of the
pendulum problem starting with initial guess iŒ0 D 0:7 C sin.ti =2/. The iterates k for
k D 1; 2; : : : are denoted by the number k in the plots.
Physically it seems clear that there is a second solution to the BVP. To find it numerically we can use the same iteration as before, but with a different initial guess  Œ0
that is sufficiently close to this solution. Since we are now looking for a solution where 
initially increases and then falls again, let’s try a function with this general shape. In Figure 2.5 we see the iterates  Œk generated with data iŒ0 D 0:7 C sin.ti =2/. We have gotten
lucky here on our first attempt, and we get convergence to a solution of the desired form.
(See Table 2.1.) Different guesses with the same general shape might not work. Note that
some of the iterates  Œk obtained along the way in Figure 2.5 do not make physical sense
(since  goes above  and then back down—what does this mean?), but the method still
converges.

2.16.3 Accuracy on nonlinear equations
The solutions plotted above are not exact solutions to the BVP (2.77). They are only solutions to the discrete system of (2.78) with h D 1=80. How well do they approximate
true solutions of the differential equation? Since we have used a second order accurate
centered approximation to the second derivative in (2.8), we again hope to obtain second
order accuracy as the grid is refined. In this section we will investigate this.
Note that it is very important to keep clear the distinction between the convergence
of Newton’s method to a solution of the finite difference equations and the convergence of
this finite difference approximation to the solution of the differential equation. Table 2.1
indicates that we have obtained a solution to machine accuracy (roughly 10 15 ) of the non
linear system of equations by using Newton’s method. This does not mean that our solution
agrees with the true solution of the differential equation to the same degree. This depends
on the size of h, the size of the truncation error in our finite difference approximation, and
the relation between the local truncation error and the resulting global error.
Let’s start by computing the local truncation error of the finite difference formula.
Just as in the linear case, we define this by inserting the true solution of the differential

i

i

i

i

i

i

i

42

“rjlfdm”
2007/6/1
page 42
i

Chapter 2. Steady States and Boundary Value Problems

equation into the finite difference equations. This will not satisfy the equations exactly, and
the residual is what we call the local truncation error (LTE):
i D

1
..ti 1 /
h2

2.ti / C .tiC1 // C sin..ti //

D . 00 .ti / C sin..ti /// C
D

1 2 0000
h  .ti / C O.h4 /
12

(2.83)

1 2 0000
h  .ti / C O.h4 /:
12

Note that we have used the differential equation to set  00.ti / C sin..ti // D 0, which holds
exactly since .t/ is the exact solution. The LTE is O.h2 / and has exactly the same form
as in the linear case. (For a more complicated nonlinear problem it might not work out so
O
simply, but similar expressions result.) The vector  with components i is simply G./,
O
where  is the vector made up of the true solution at each grid point. We now want to
obtain an estimate on the global error E based on this local error. We can attempt to follow
the path used in Section 2.6 for linear problems. We have
G./ D 0;
O D ;
G./
and subtracting gives
G./

O D :
G./

(2.84)

O If G were linear
We would like to derive from this a relation for the global error E D  .
O D AE,
(say, G./ D A F ), we would have G./ G.O / D A AO D A. /
O
giving an expression in terms of the global error E D  . This is what we used in
Section 2.7.
O
In the nonlinear case we cannot express G./ G.O / directly in terms of  .
However, we can use Taylor series expansions to write
O C J./E
O C O.kEk2 /;
G./ D G./
O is again the Jacobian matrix of the difference formulas, evaluated now at the
where J./
exact solution. Combining this with (2.84) gives
O
J./E
D

 C O.kEk2 /:

If we ignore the higher order terms, then we again have a linear relation between the local
and global errors.
This motivates the following definition of stability. Here we let JOh denote the Jacobian matrix of the difference formulas evaluated at the true solution on a grid with grid
spacing h.
Definition 2.2. The nonlinear difference method G./ D 0 is stable in some norm kk if the
matrices .JOh / 1 are uniformly bounded in this norm as h ! 0, i.e., there exist constants
C and h0 such that
k.JOh / 1 k  C for all h < h0 :
(2.85)

i

i

i

i

i

i

i

2.17. Singular perturbations and boundary layers

“rjlfdm”
2007/6/1
page 43
i

43

It can be shown that if the method is stable in this sense, and consistent in this norm
(k hk ! 0), then the method converges and kE hk ! 0 as h ! 0. This is not obvious
in the nonlinear case: we obtain a linear system for E only by dropping the O.kEk2 /
nonlinear terms. Since we are trying to show that E is small, we can’t necessarily assume
that these terms are negligible in the course of the proof, at least not without some care.
See [54] for a proof.
It makes sense that it is uniform boundedness of the inverse Jacobian at the exact
solution that is required for stability. After all, it is essentially this Jacobian matrix that is
used in solving linear systems in the course of Newton’s method, once we get very close to
the solution.
Warning: We state a final reminder that there is a difference between convergence
of the difference method as h ! 0 and convergence of Newton’s method, or some other
iterative method, to the solution of the difference equations for some particular h. Stability
of the difference method does not imply that Newton’s method will converge from a poor
initial guess. It can be shown, however, that with a stable method, Newton’s method will
converge from a sufficiently good initial guess; see [54]. Also, the fact that Newton’s
method has converged to a solution of the nonlinear system of difference equations, with an
error of 10 15 , say, does not mean that we have a good solution to the original differential
equation. The global error of the difference equations determines this.

2.17

Singular perturbations and boundary layers

In this section we consider some singular perturbation problems to illustrate the difficulties
that can arise when numerically solving problems with boundary layers or other regions
where the solution varies rapidly. See [55], [56] for more detailed discussions of singular
perturbation problems. In particular, the example used here is very similar to one that can
be found in [55], where solution by matched asymptotic expansions is discussed.
As a simple example we consider a steady-state advection-diffusion equation. The
time-dependent equation has the form
ut C aux D uxx C

(2.86)

in the simplest case. This models the temperature u.x; t/ of a fluid flowing through a pipe
with constant velocity a, where the fluid has constant heat diffusion coefficient  and is
a source term from heating through the walls of the tube.
If a > 0, then we naturally have a boundary condition at the left boundary (say,
x D 0),
u.0; t/ D ˛.t/;
specifying the temperature of the incoming fluid. At the right boundary (say, x D 1) the
fluid is flowing out and so it may seem that the temperature is determined only by what
is happening in the pipe, and no boundary condition is needed here. This is correct if
 D 0 since the first order advection equation needs only one boundary condition and we
are allowed to specify u only at the left boundary. However, if  > 0, then heat can diffuse
upstream, and we need to also specify u.1; t/ D ˇ.t/ to determine a unique solution.
If ˛; ˇ, and are all independent of t, then we expect a steady-state solution, which
we hope to find by solving the linear 2-point boundary value problem

i

i

i

i

i

i

i

44

“rjlfdm”
2007/6/1
page 44
i

Chapter 2. Steady States and Boundary Value Problems
au0 .x/ D u00 .x/ C .x/;
u.0/ D ˛;
u.1/ D ˇ:

(2.87)

This can be discretized using the approach of Section 2.4. If a is small relative to , then
this problem is easy to solve. In fact for a D 0 this is just the steady-state heat equation
discussed in Section 2.15, and for small a the solution appears nearly identical.
But now suppose a is large relative to  (i.e., we crank up the velocity, or we decrease
the ability of heat to diffuse with the velocity a > 0 fixed). More properly we should
work in terms of the nondimensional Péclet number, which measures the ratio of advection
velocity to transport speed due to diffusion. Here we introduce a parameter  which is like
the inverse of the Péclet number,  D =a, and rewrite (2.87) in the form
u00.x/

u0.x/ D f .x/:

(2.88)

Then taking a large relative to  (large Péclet number) corresponds to the case   1.
We should expect difficulties physically in this case where advection overwhelms
diffusion. It would be very difficult to maintain a fixed temperature at the outflow end
of the tube in this situation. If we had a thermal device that was capable of doing so by
instantaneously heating the fluid to the desired temperature as it passes the right boundary,
independent of the temperature of the fluid flowing toward this point, then we would expect
the temperature distribution to be essentially discontinuous at this boundary.
Mathematically we expect trouble as  ! 0 because in the limit  D 0 the equation
(2.88) reduces to a first order equation (the steady advection equation)
u0.x/ D f .x/;

(2.89)

which allows only one boundary condition, rather than two. For  > 0, no matter how
small, we have a second order equation that needs two conditions, but we expect to perhaps
see strange behavior at the outflow boundary as  ! 0, since in the limit we are over
specifying the problem.
Figure 2.6(a) shows how solutions to (2.88) appear for various values of  in the case
˛ D 1, ˇ D 3, and f .x/ D 1. In this case the exact solution is
!
e x= 1
u.x/ D ˛ C x C .ˇ ˛ 1/
:
(2.90)
e 1= 1
Note that as  ! 0 the solution tends toward a discontinuous function that jumps to the
value ˇ at the last possible moment. This region of rapid transition is called the boundary
layer and it can be shown that for this problem the width of this layer is O./ as  ! 0.
The equation (2.87) with 0 <   1 is called a singularly perturbed equation. It is
a small perturbation of (2.89), but this small perturbation completely changes the character
of the equation (from a first order to a second order equation). Typically any differential equation having a small parameter multiplying the highest order derivative will give a
singular perturbation problem.
By contrast, going from the pure diffusion equation uxx D f to an advection
diffusion equation uxx aux D f for very small a is a regular perturbation. Both
of these equations are second order differential equations requiring the same number of

i

i

i

i

i

i

i

2.17. Singular perturbations and boundary layers

“rjlfdm”
2007/6/1
page 45
i

45

boundary conditions. The solution of the perturbed equation looks nearly identical to the
solution of the unperturbed equation for small a, and the difference in solutions is O.a/ as
a ! 0.
Singular perturbation problems cause numerical difficulties because the solution
changes rapidly over a very small interval in space. In this region derivatives of u.x/
are large, giving rise to large errors in our finite difference approximations. Recall that
the error in our approximation to u00.x/ is proportional to h2 u0000.x/, for example. If h is
not small enough, then the local truncation error will be very large in the boundary layer.
Moreover, even if the truncation error is large only in the boundary layer, the resulting
global error may be large everywhere. (Recall that the global error E is obtained from the
truncation error  by solving a linear system AE D , which means that each element
of E depends on all elements of  since A 1 is a dense matrix.) This is clearly seen in
Figure 2.6(b), where the numerical solution with h D 1=10 is plotted. Errors are large even
in regions where the exact solution is nearly linear and u0000  0.
On finer grids the solution looks better (see Figure 2.6(c) and (d)), and as h ! 0
the method does exhibit second order accurate convergence. But it is necessary to have a
sufficiently fine grid before reasonable results are obtained; we need enough grid points to
enable the boundary layer to be well resolved.

3

3

2.5

2.5

2

2

1.5

1.5

1

(a)

0

1
0.2

0.4

0.6

0.8

1

(b)

3

3

2.5

2.5

2

2

1.5

1.5

1

(c)

0

0

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

1
0.2

0.4

0.6

0.8

1

(d)

0

Figure 2.6. (a) Solutions to the steady state advection-diffusion equation (2.88)
for different values of . The four lines correspond to  D 0:3; 0:1; 0:05, and 0:01 from
top to bottom. (b) Numerical solution with  D 0:01 and h D 1=10. (c) h D 1=25. (d)
h D 1=100.

i

i

i

i

i

i

i

46

“rjlfdm”
2007/6/1
page 46
i

Chapter 2. Steady States and Boundary Value Problems

2.17.1 Interior layers
The above example has a boundary layer, a region of rapid transition at one boundary. Other
problems may have interior layers instead. In this case the solution is smooth except for
some thin region interior to the interval where a rapid transition occurs. Such problems can
be even more difficult to solve since we often don’t know a priori where the interior layer
will be. Perturbation theory can often be used to analyze singular perturbation problems
and predict where the layers will occur, how wide they will be (as a function of the small
parameter ), and how the solution behaves. The use of perturbation theory to obtain good
approximations to problems of this type is a central theme of classical applied mathematics.
These analytic techniques can often be used to good advantage along with numerical
methods, for example, to obtain a good initial guess for Newton’s method, or to choose an
appropriate nonuniform grid as discussed in the next section. In some cases it is possible
to develop special numerical methods that have the correct singular behavior built into the
approximation in such a way that far better accuracy is achieved than with a naive numerical
method.
Example 2.2. Consider the nonlinear boundary value problem
u00 C u.u0

1/ D 0

u.a/ D ˛;

u.b/ D ˇ:

for a  x  b;

(2.91)

For small  this is a singular perturbation problem since  multiplies the highest order
derivative. Setting  D 0 gives a reduced equation
u.u0

1/ D 0

(2.92)

for which we generally can enforce only one boundary condition. Solutions to (2.92) are
u.x/  0 or u.x/ D x C C for some constant C . If the boundary condition imposed at
x D a or x D b is nonzero, then the solution has the latter form and is either
u.x/ D x C ˛

a

if u.a/ D ˛ is imposed

(2.93)

u.x/ D x C ˇ

b

if u.b/ D ˇ is imposed:

(2.94)

or
These two solutions are shown in Figure 2.7.
For 0 <   1, the full equation (2.91) has a solution that satisfies both boundary conditions, and Figure 2.7 also shows such a solution. Over most of the domain the
solution is smooth and u00 is small, in which case u00 is negligible and the solution must
nearly satisfy (2.92). Thus over most of the domain the solution follows one of the linear
solutions to the reduced equation. Both boundary conditions can be satisfied by following
one solution (2.93) near x D a and the other solution (2.94) near x D b. Connecting these
two smooth portions of the solution is a narrow zone (the interior solution) where u.x/ is
rapidly varying. In this layer u00 is very large and the u00 term of (2.91) is not negligible,
and hence u.u0 1/ may be far from zero in this region.
To determine the location and width of the interior layer, and the approximate form
of the solution in this layer, we can use perturbation theory. Focusing attention on this

i

i

i

i

i

i

i

2.17. Singular perturbations and boundary layers

“rjlfdm”
2007/6/1
page 47
i

47

1.5

1

xCˇ

b

0.5

u.x/

0

xC˛

−0.5

−1
0

0.2

0.4

0.6

a

0.8

1

Figure 2.7. Outer solutions and full solution to the singular perturbation problem
with a D 0; b D 1, ˛ D 1, and ˇ D 1:5. The solution has an interior layer centered
about xN D 0:25.
layer, which we now assume is centered at some location x D x,
N we can zoom in on the
solution by assuming that u.x/ has the approximate form
u.x/ D W ..x

x/=
N k/

(2.95)

for some power k to be determined. We are zooming in on a layer of width O. k / asymptotically, so determining k will tell us how wide the layer is. From (2.95) we compute
u0.x/ D  k W 0 ..x

x/=
N k /;

u00.x/ D  2k W 0 ..x

x/=
N k /:

(2.96)

Inserting these expressions in (2.91) gives
   2k W 00 ./ C W ./. k W 0 ./
where  D .x

1/ D 0;

x/=
N k . Multiply by  2k 1 to obtain
W 00./ C W ./. k 1 W 0./

 2k 1 / D 0:

(2.97)

By rescaling the independent variable by a factor  k , we have converted the singular perturbation problem (2.91) into a problem where the highest order derivative W 00 has coefficient
1 and the small parameter appears only in the lower order term. However, the lower order
term behaves well in the limit  ! 0 only if we take k  1. For smaller values of k
(zooming in on too large a region asymptotically), the lower order term blows up as  ! 0,
or dividing by  k 1 shows that we still have a singular perturbation problem. This gives us
some information on k.
If we fix x at any value away from x,
N then  ! ˙1 as  ! 0. So the boundary
value problem (2.97) for W ./ has boundary conditions at ˙1,
W ./ ! xN C ˛
W ./ ! xN C ˇ

i

i

a as  ! 1;
b as  ! C1:

(2.98)

i

i

i

i

i

48

“rjlfdm”
2007/6/1
page 48
i

Chapter 2. Steady States and Boundary Value Problems

The “inner solution” W ./ will then match up with the “outer solutions” given by (2.93)
and (2.94) at the edges of the layer. We also require
W 0 ./ ! 0

as  ! ˙1

(2.99)

since outside the layer the linear functions (2.93) and (2.94) have the desired slope.
For (2.97) to give a reasonable 2-point boundary value problem with these three
boundary conditions (2.98) and (2.99), we must take k D 1. We already saw that we need
k  1, but we also cannot take k > 1 since in this case the lower order term in (2.97)
vanishes as  ! 0 and the equation reduces to W 00./ D 0. In this case we are zooming
in too far on the solution near x D xN and the solution simply appears linear, as does any
sufficiently smooth function if we zoom in on its behavior at a fixed point. While this does
reveal the behavior extremely close to x,
N it does not allow us to capture the full behavior in
the interior layer. We cannot satisfy all four boundary conditions on W with a solution to
W 00 .x/ D 0.
Taking k D 1 gives the proper interior problem, and (2.97) becomes
W 00 ./ C W ./.W 0 ./

/ D 0:

(2.100)

Now letting  ! 0 we obtain
W 00./ C W ./W 0 ./ D 0:

(2.101)

This equation has solutions of the form
W ./ D w0 tanh.w0 =2/

(2.102)

for arbitrary constants w0. The boundary conditions (2.98) lead to
w0 D

1
.a
2

bCˇ

˛/

(2.103)

and

1
.a C b ˛ ˇ/:
(2.104)
2
To match this solution to the outer solutions, we require a < xN < b. If the value of
xN determined by (2.104) doesn’t satisfy this condition, then the original problem has a
boundary layer at x D a (if xN  a) or at x D b (if xN  b) instead of an interior layer. For
the remainder of this discussion we assume a < xN < b.
We can combine the inner and outer solutions to obtain an approximate solution of
the form
u.x/  u.x/
Q
 x xN C w0 tanh.w0 .x x/=2/:
N
(2.105)
xN D

Singular perturbation analysis has given us a great deal of information about the solution to the problem (2.91). We know that the solution has an interior layer of width O./
at x D xN with roughly linear solution (2.93), (2.94) outside the layer. This type of information may be all we need to know about the solution for some applications. If we want
to determine a more detailed numerical approximation to the full solution, this analytical

i

i

i

i

i

i

i

2.18. Nonuniform grids

“rjlfdm”
2007/6/1
page 49
i

49

information can be helpful in devising an accurate and efficient numerical method, as we
now consider.
The problem (2.91) can be solved numerically on a uniform grid using the finite
difference equations




UiC1 Ui 1
Ui 1 2Ui C UiC1
Gi .U /  
C Ui
1 D0
(2.106)
h2
2h
for i D 1; 2; : : : ; m with U0 D ˛ and UmC1 D ˇ (where, as usual, h D .b a/=.mC1/).
This gives a nonlinear system of equations G.U / D 0 that can be solved using Newton’s
method as described in Section 2.16.1. One way to use the singular perturbation approximation is to generate a good initial guess for Newton’s method, e.g.,
Ui D u.x
Q i /;

(2.107)

where u.x/
Q
is the approximate solution from (2.105). We then have an initial guess that is
already very accurate at nearly all grid points. Newton’s method converges rapidly from
such a guess. If the grid is fine enough that the interior layer is well resolved, then a good
approximation to the full solution is easily obtained. By contrast, starting with a more naive
initial guess such as Ui D ˛ C .x a/.ˇ ˛/=.b a/ leads to nonconvergence when  is
small.
When  is very small, highly accurate numerical results can be obtained with less
computation by using a nonuniform grid, with grid points clustered in the layer. To construct such a grid we can use the singular perturbation analysis to tell us where the points
should be clustered (near x)
N and how wide to make the clustering zone. The width of the
layer is O./ and, moreover, from (2.102) we expect that most of the transition occurs for,
say, j 12 w0 j < 2. This translates into
jx

xj
N < 4=w0;

(2.108)

where w0 is given by (2.103). The construction and use of nonuniform grids is pursued
further in the next section.

2.18

Nonuniform grids

From Figure 2.6 it is clear that we need to choose our grid to be fine enough so that several
points are within the boundary layer and we can obtain a reasonable solution. If we wanted
high accuracy within the boundary layer we would have to choose a much finer grid than
shown in this figure. With a uniform grid this means using a very large number of grid
points, the vast majority of which are in the region where the solution is very smooth and
could be represented well with far fewer points. This waste of effort may be tolerable
for simple one-dimensional problems but can easily be intolerable for more complicated
problems, particularly in more than one dimension.
Instead it is preferable to use a nonuniform grid for such calculations, with grid points
clustered in regions where they are most needed. This requires using formulas that are
sufficiently accurate on nonuniform grids. For example, a four-point stencil can be used
to obtain second order accuracy for the second derivative operator. Using this for a linear

i

i

i

i

i

i

i

50

“rjlfdm”
2007/6/1
page 50
i

Chapter 2. Steady States and Boundary Value Problems

problem would give a banded matrix with four nonzero diagonals. A little extra care is
needed at the boundaries.
One way to specify nonuniform grid points is to start with a uniform grid in some
“computational coordinate” z, which we will denote by zi D ih for i D 0; 1; : : : ; m C 1,
where h D 1=.m C 1/, and then use some appropriate grid mapping function X.z/ to
define the “physical grid points” xi D X.zi /. This is illustrated in Figure 2.8, where z is
plotted on the vertical axis and x is on the horizontal axis. The curve plotted represents
a function X.z/, although with this choice of axes it is more properly the graph of the
inverse function z D X 1 .x/. The horizontal and vertical lines indicate how the uniform
grid points on the z axis are mapped to nonuniform points in x. If the problem is posed on
the interval Œa; b, then the function X.z/ should be monotonically increasing and satisfy
X.0/ D a and X.1/ D b.
Note that grid points are clustered in regions where the curve is steepest, which means
that X.z/ varies slowly with z, and spread apart in regions where X.z/ varies rapidly
with z. Singular perturbation analysis of the sort done in the previous section may provide
guidelines for where the grid points should be clustered.
Once a set of grid points xi is chosen, it is necessary to set up and solve an appropriate
system of difference equations on this grid. In general a different set of finite difference
coefficients will be required at each grid point, depending on the spacing of the grid points
nearby.

1

0.8

0.6

z
0.4

0.2

0

x
0

0.2

0.4

0.6

0.8

1

Figure 2.8. Grid mapping from a uniform grid in 0  z  1 (vertical axis)
to the nonuniform grid in physical x-space shown on the horizontal axis. This particular
mapping may be useful for solving the singular perturbation problem illustrated in Fig. 2.7.

i

i

i

i

i

i

i

2.18. Nonuniform grids

“rjlfdm”
2007/6/1
page 51
i

51

Example 2.3. As an example, again consider the simple problem u00.x/ D f .x/
with the boundary conditions (2.52), u0 .0/ D , and u.1/ D ˇ. We would like to generalize the matrix system (2.57) to the situation where the xi are nonuniformly distributed
in the interval Œ0; 1. In MATLAB this is easily accomplished using the fdcoeffV function discussed in Section 1.5, and the code fragment below shows how this matrix can be
computed. Note that in MATLAB the vector x must be indexed from 1 to m+2 rather than
from 0 to m+1.
A = speye(m+2);
% initialize using sparse storage
% first row for Neumann BC, approximates u’(x(1))
A(1,1:3) = fdcoeffV(1, x(1), x(1:3));
% interior rows approximate u’’(x(i))
for i=2:m+1
A(i,i-1:i+1) = fdcoeffV(2, x(i), x((i-1):(i+1)));
end
% last row for Dirichlet BC, approximates u(x(m+2))
A(m+2,m:m+2) = fdcoeffV(0,x(m+2),x(m:m+2));
A complete program that uses this and tests the order of accuracy of this method is available
on the Web page.
Note that in this case the finite difference coefficients for the 3-point approximations
to u00 .xi / also can be explicitly calculated from the formula (1.14), but it is simpler to use
fdcoeffV, and the above code also can be easily generalized to higher order methods by
using more points in the stencils.
What accuracy do we expect from this method? In general if xi 1 , xi and xiC1 are
not equally spaced, then we expect an approximation to the second derivative u00.xi / based
on these three points to be only first order accurate (n D 3 and k D 2 in the terminology
of Section 1.5, so we expect p D n k D 1). This is confirmed by the error expression (1.16), and this is generally what we will observe if we take randomly spaced grid
points xi .
However, in practice we normally use grids that are smoothly varying, for example,
xi D X.zi /, where X.z/ is some smooth function, as discussed in Section 2.18. In this
case it turns out that we should expect to achieve second order accuracy with the method
just presented, and that is what is observed in practice. This can be seen from the error
expressions (1.16): the “first order” portion of the error is proportional to
h2

h1 D .xiC1

xi /

.xi

xi 1/ D X.ziC1 /

2X.zi / C X.zi 1 /  h2 X 00.zi /;

where h D z is the grid spacing in z. So we see that for a smoothly varying grid the
difference h2 h1 is actually O.h2 /. Hence the local truncation error is O.h2 / at each grid
point and we expect second order accuracy globally.

2.18.1 Adaptive mesh selection
Ideally a numerical method would work robustly on problems with interior or boundary
layers without requiring that the user know beforehand how the solution is behaving. This
can often be achieved by using methods that incorporate some form of adaptive mesh selection, which means that the method selects the mesh based on the behavior of the solution

i

i

i

i

i

i

i

52

“rjlfdm”
2007/6/1
page 52
i

Chapter 2. Steady States and Boundary Value Problems

and automatically clusters grid points in regions where they are needed. A discussion of
this topic is beyond the scope of this book. See, for example, [4].
Readers who wish to use methods on nonuniform grids are encouraged to investigate
software that will automatically choose an appropriate grid for a given problem (perhaps
with some initial guidance) and take care of all the details of discretizing on this grid. In
MATLAB, the routine bvp4c can be used and links to other software may be found on the
book’s Web page.

2.19

Continuation methods

For a difficult problem (e.g., a boundary layer or interior layer problem with   1),
an adaptive mesh refinement program may not work well unless a reasonable initial grid
is provided that already has some clustering in the appropriate layer location. Moreover,
Newton’s method may not converge unless we have a good initial guess for the solution.
We have seen how information about the layer location and width and the approximate
form of the solution can sometimes be obtained by using singular perturbation analysis.
There is another approach that is often easier in practice, known as continuation or
the homotopy method. As an example, consider again the interior layer problem considered
in Example 2.2 and suppose we want to solve this problem for a very small value of , say,
 D 10 6 . Rather than immediately tackling this problem, we could first solve the problem
with a much larger value of , say,  D 0:1, for which the solution is quite smooth and
convergence is easily obtained on a uniform grid with few points. This solution can then
be used as an initial guess for the problem with a smaller value of , say,  D 10 2 . We
can repeat this process as many times as necessary to get to the desired value of , perhaps
also adapting the grid as we go along to cluster points near the location of the interior layer
(which is independent of  and becomes clearly defined as we reduce ).
More generally, the idea of following the solution to a differential equation as some
parameter in the equation varies arises in other contexts as well. Difficulties sometimes
arise at particular parameter values, such as bifurcation points, where two paths of solutions
intersect.

2.20

Higher order methods

So far we have considered only second order methods for solving BVPs. Several approaches can be used to obtain higher order accurate methods. In this section we will
look at various approaches to achieving higher polynomial order, such as fourth order or
sixth order approximations. In Section 2.21 we briefly introduce spectral methods that can
achieve convergence at exponential rates under some conditions.

2.20.1 Fourth order differencing
The obvious approach is to use a better approximation to the second derivative operator
in place of the second order difference used in (2.8). For example, the finite difference
approximation
1
Œ Uj 2 C 16Uj 1
12h2

i

i

30Uj C 16Uj C1

Uj C2 

(2.109)

i

i

i

i

i

2.20. Higher order methods

“rjlfdm”
2007/6/1
page 53
i

53

gives a fourth order accurate approximation to u00.xj /. Note that this formula can be easily
found in MATLAB by fdcoeffV(2,0,-2:2).
For the BVP u00.x/ D f .x/ on a grid with m interior points, this approximation can
be used at grid points j D 2; 3; : : : ; m 1 but not for j D 1 or j D m. At these points
we must use methods with only one point in the stencil to the left or right, respectively.
Suitable formulas can again be found using fdcoeffV; for example,
1
Œ11U0
12h2

20U1 C 6U2 C 4U3

U4 

(2.110)

6U4 C U5 

(2.111)

is a third order accurate formula for u00.x1 / and
1
Œ10U0
12h2

15U1

4U2 C 14U3

is fourth order accurate. As in the case of the second order methods discussed above, we
can typically get away with one less order at one point near the boundary, but somewhat
better accuracy is expected if (2.111) is used.
These methods are easily extended to nonuniform grids using the same approach
as in Section 2.18. The matrix is essentially pentadiagonal except in the first and last two
rows, and using sparse matrix storage ensures that the system is solved in O.m/ operations.
Fourth order accuracy is observed as long as the grid is smoothly varying.

2.20.2 Extrapolation methods
Another approach to obtaining fourth order accuracy is to use the second order accurate
method on two different grids, with spacing h (the coarse grid) and h=2 (the fine grid), and
then to extrapolate in h to obtain a better approximation on the coarse grid that turns out to
have O.h4 / errors for this problem.
Denote the coarse grid solution by
Uj  u.j h/;

i D 1; 2; : : : ; m;

and the fine grid solution by
Vi  u.ih=2/;

i D 1; 2; : : : ; 2m C 1;

and note that both Uj and V2j approximate u.j h/. Because the method is a centered
second order accurate method, it can be shown that the error has the form of an even-order
expansion in powers of h,
Uj

u.j h/ D C2 h2 C C4 h4 C C6 h6 C    ;

(2.112)

provided u.x/ is sufficiently smooth. The coefficients C2 ; C4 ; : : : depend on high order
derivatives of u but are independent of h at each fixed point j h. (This follows from the fact
that the local truncation error has an expansion of this form and the fact that the inverse
matrix has columns that are an exact discretization of the Green’s function, as shown in
Section 2.11, but we omit the details of justifying this.)

i

i

i

i

i

i

i

54

“rjlfdm”
2007/6/1
page 54
i

Chapter 2. Steady States and Boundary Value Problems
On the fine grid we therefore have an error of the form
 2
 4
 6
h
h
h
V2j u.j h/ D C2
C C4
C C6
C
2
2
2
1
1
1
D C2 h2 C C4 h4 C C6 h6 C    :
4
16
64

(2.113)

The extrapolated value is given by
1
UN j D .4V2j
3

Uj /;

which is chosen so that the h2 term of the errors cancels out and we obtain


1 1
UN j u.j h/ D
1 C4 h4 C O.h6 /:
3 4

(2.114)

(2.115)

The result has fourth order accuracy as h is reduced and a much smaller error than either
Uj or V2j (provided C4 h2 is not larger than C2 , and usually it is much smaller).
Implementing extrapolation requires solving the problem twice, once on the coarse
grid and once on the fine grid, but to obtain similar accuracy with the second order method
alone would require a far finer grid than either of these and therefore much more work.
The extrapolation method is more complicated to implement than the fourth order
method described in Section 2.20.1, and for this simple one-dimensional boundary value
problem it is probably easier to use the fourth order method directly. For more complicated
problems, particularly in more than one dimension, developing a higher order method may
be more difficult and extrapolation is often a powerful tool.
It is also possible to extrapolate further to obtain higher order accurate approximations. If we also solve the problem on a grid with spacing h=4, then this solution can be
combined with V to obtain a fourth order accurate approximation on the .h=2/-grid. This
can be combined with UN determined above to eliminate the O.h4 / error and obtain a sixth
order accurate approximation on the original grid.

2.20.3 Deferred corrections
Another way to combine two different numerical solutions to obtain a higher order accurate
approximation, called deferred corrections, has the advantage that it solves both of the
problems on the same grid rather than refining the grid as in the extrapolation method.
We first solve the system AU D F of Section 2.4 to obtain the second order accurate
approximation U . Recall that the global error E D U UO satisfies the difference equation
(2.15),
AE D ;
(2.116)
where  is the local truncation error. Suppose we knew the vector . Then we could solve
the system (2.116) to obtain the global error E and hence obtain the exact solution UO as
UO D U E. We cannot do this exactly because the local truncation error has the form
j D

i

i

1 2 0000
h u .xj / C O.h4 /
12

i

i

i

i

i

2.21. Spectral methods

“rjlfdm”
2007/6/1
page 55
i

55

and depends on the exact solution, which we do not know. However, from the approximate
solution U we can estimate  by approximating the fourth derivative of U .
For the simple problem u00.x/ D f .x/ that we are now considering we have u0000.x/ D
00
f .x/, and so the local truncation error can be estimated directly from the given function
f .x/. In fact for this simple problem we can avoid solving the problem twice by simply
modifying the right-hand side of the original problem AU D F by setting
Fj D f .xj / C

1 2 00
h f .xj /
12

(2.117)

with boundary terms added at j D 1 and j D m. Solving AU D F then gives a fourth
order accurate solution directly. An analogue of this for the two-dimensional Poisson problem is discussed in Section 3.5.
For other problems, we would typically have to use the computed solution U to
estimate j and then solve a second problem to estimate E. This general approach is called
the method of deferred corrections. In summary, the procedure is to use the approximate
solution to estimate the local truncation error and then solve an auxiliary problem of the
form (2.116) to estimate the global error. The global error estimate can then be used to
improve the approximate solution. For more details see, e.g., [54], [4].

2.21

Spectral methods

The term spectral method generally refers to a numerical method that is capable (under
suitable smoothness conditions) of converging at a rate that is faster than polynomial in the
mesh width h. Originally the term was more precisely defined. In the classical spectral
method the solution to the differential equation is approximated by a function U.x/ that is
a linear combination of a finite set of orthogonal basis functions, say,
N
X

U.x/ D

cj j .x/;

(2.118)

j D1

and the coefficients chosen to minimize an appropriate norm of the residual function (D
U 00.x/ f .x/ for the simple BVP (2.4)). This is sometimes called a Galerkin approach.
The method we discuss in this section takes a different approach and can be viewed as
expressing U.x/ as in (2.118) but then requiring U 00.xi / D f .xi / at N 2 grid points,
along with the two boundary conditions. The differential equation will be exactly satisfied
at the grid points by the function U.x/, although in between the grid points the ODE
generally will not be satisfied. This is called collocation and the method presented below
is sometimes called a spectral collocation or pseudospectral method.
In Section 2.20.1 we observed that the second order accurate method could be extended to obtain fourth order accuracy by using more points in the stencil at every grid
point, yielding better approximations to the second derivative. We can increase the order
further by using even wider stencils.
Suppose we take this idea to its logical conclusion and use the data at all the grid
points in the domain in order to approximate the derivative at each point. This is easy to
try in MATLAB using a simple extension of the approach discussed in Example 2.3. For

i

i

i

i

i

i

i

56

“rjlfdm”
2007/6/1
page 56
i

Chapter 2. Steady States and Boundary Value Problems

the test problem considered with a Neumann boundary condition at the left boundary and
a Dirichlet condition at the right, the code from Example 2.3 can be rewritten to use all the
grid values in every stencil as
A = zeros(m+2);
% A is dense
% first row for Neumann BC, approximates u’(x(1))
A(1,:) = fdcoeffF(1, x(1), x);
% interior rows approximate u’’(x(i))
for i=2:m+1
A(i,:) = fdcoeffF(2, x(i), x);
end
% last row for Dirichlet BC, approximates u(x(m+2))
A(m+2,:) = fdcoeffF(0,x(m+2),x);
We have also switched from using fdcoeffV to the more stable fdcoeffF, as discussed
in Section 1.5.
Note that the matrix will now be dense, since each finite difference stencil involves
all the grid points. Recall that x is a vector of length m C 2 containing all the grid points,
so each vector returned by a call to fdcoeffF is a full row of the matrix A.
If we apply the resulting A to a vector U D ŒU1 U2    UmC2 T , the values in
W D AU will simply be
W1 D p 0.x1 /;
Wi D p 00.xi / for i D 2; : : : ; m C 1;

(2.119)

WmC2 D p.xmC2 /;
where we’re now using the MATLAB indexing convention as in the code and p.x/ is the
unique polynomial of degree m C 1 that interpolates the m C 2 data points in U . The same
high degree polynomial is used to approximate the derivatives at every grid point.
What sort of accuracy might we hope for? Interpolation through n points generally
gives O.h.n 2/ / accuracy for the second derivative, or one higher order if the stencil is
symmetric. We are now interpolating through m C 2 points, where m D O.1= h/, as we
refine the grid, so we might hope that the approximation is O.h1= h / accurate. Note that
h1= h approaches zero faster than any fixed power of h as h ! 0. So we might expect very
rapid convergence and small errors.
However, it is not at all clear that we will really achieve the accuracy suggested by
the argument above, since increasing the number of interpolation points spread over a fixed
interval as h ! 0 is qualitatively different than interpolating at a fixed number of points
that are all approaching a single point as h ! 0. In particular, if we take the points xi
to be equally spaced, then we generally expect to obtain disastrous results. High order
polynomial interpolation at equally spaced points on a fixed interval typically leads to a
highly oscillatory polynomial that does not approximate the underlying smooth function
well at all away from the interpolation points (the Runge phenomenon), and it becomes
exponentially worse as the grid is refined and the degree increases. Approximating second
derivatives by twice differentiating such a function would not be wise and would lead to an
unstable method.

i

i

i

i

i

i

i

2.21. Spectral methods

“rjlfdm”
2007/6/1
page 57
i

57

This idea can be saved, however, by choosing the grid points to be clustered near the
ends of the interval in a particular manner. A very popular choice, which can be shown to
be optimal in a certain sense, is to use the extreme points of the Chebyshev polynomial of
degree m C 1, shifted to the interval Œa; b. The expression (B.25) in Appendix B gives the
extreme points of Tm .x/ on the interval Œ 1; 1. Shifting to the desired interval, changing
m to m C 1, and reordering them properly gives the Chebyshev grid points
1
xi D a C .b
2

a/.1 C cos..1

zi ///

for i D 0; 1; : : : ; m C 1;

(2.120)

where the zi are again m C 2 equally spaced points in the unit interval, zi D i=.m C 1/ for
i D 0; 1; : : : ; m C 1.
The resulting method is called a Chebyshev spectral method (or pseudospectral/
spectral collocation method). For many problems these methods give remarkably good
accuracy with relatively few grid points. This is certainly true for the simple boundary
value problem u00.x/ D f .x/, as the following example illustrates.
Example 2.4. Figure 2.9 shows the error as a function of h for three methods we
have discussed on the simplest BVP of the form
u00.x/ D e x

for 0  x  3;

u.0/ D 5;

(2.121)

u.3/ D 3:

The error behaves in a textbook fashion: the errors for the second order method of Section 2.4 lie on a line with slope 2 (in this log-log plot), and those obtained with the fourth
order method of Section 2.20.1 lie on a line with slope 4. The Chebyshev pseudospectral method behaves extremely well for this problem; an error less than 10 6 is already
0

10

−2

10

−4

er r or

10

−6

10

−8

10

−10

10

2nd order
4th order

−12

10

pseudospectral

−14

10

−3

10

−2

10

−1

h

10

0

10

Figure 2.9. Error as a function of h for two finite difference methods and the
Chebyshev pseudospectral method on (2.121).

i

i

i

i

i

i

i

58

“rjlfdm”
2007/6/1
page 58
i

Chapter 2. Steady States and Boundary Value Problems

observed on the coarsest grid (with m D 10) and rounding errors become a problem by
m D 20. The finest grids used for the finite difference methods in this figure had m D 320
points.
For many problems, spectral or pseudospectral methods are well suited and should be
seriously considered, although they can have difficulties of their own on realistic nonlinear
problems in complicated geometries, or for problems where the solution is not sufficiently
smooth. In fact the solution is required to be analytic in some region of the complex plane
surrounding the interval over which the solution is being computed in order to get full
“spectral accuracy.”
Note that the polynomial p.x/ in (2.119) is exactly the function U.x/ from (2.118),
although in the way we have presented the method we do not explicitly compute the coefficients of this polynomial in terms of polynomial basis functions. One could compute this
interpolating polynomial if desired once the grid values Uj are known. This may be useful
if one needs to approximate the solution u.x/ at many more points in the interval than were
used in solving the BVP.
For some problems it is natural to use Fourier series representations for the function
U.x/ in (2.118) rather than polynomials, in particular for problems with periodic boundary
conditions. In this case the dense matrix systems that arise can generally be solved using
fast Fourier transform (FFT) algorithms. The FFT also can be used in solving problems
with Chebyshev polynomials because of the close relation between these polynomials and
trigonometric functions, as discussed briefly in Section B.3.2. In many applications, however, a spectral method uses sufficiently few grid points that using direct solvers (Gaussian
elimination) is a reasonable approach.
The analysis and proper application of pseudospectral methods goes well beyond the
scope of this book. See, for example, [10], [14], [29], [38], or [90] for more thorough
introductions to spectral methods.
Note also that spectral approximation of derivatives often is applied only in spatial
dimensions. For time-dependent problems, a time-stepping procedure is often based on
finite difference methods, of the sort developed in Part II of this book. For PDEs this time
stepping may be coupled with a spectral approximation of the spatial derivatives, a topic we
briefly touch on in Sections 9.9 and 10.13. One time-stepping procedure that has the flavor
of a spectral procedure in time is the recent spectral deferred correction method presented
in [28].

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 59
i

Chapter 3

Elliptic Equations

In more than one space dimension, the steady-state equations discussed in Chapter 2 generalize naturally to elliptic partial differential equations, as discussed in Section E.1.2. In
two space dimensions a constant-coefficient elliptic equation has the form
a1 uxx C a2 uxy C a3 uyy C a4 ux C a5 uy C a6 u D f;

(3.1)

where the coefficients a1 ; a2 ; a3 satisfy
a22

4a1 a3 < 0:

(3.2)

This equation must be satisfied for all .x; y/ in some region of the plane , together with
some boundary conditions on @, the boundary of . For example, we may have Dirichlet
boundary conditions in which case u.x; y/ is given at all points .x; y/ 2 @. If the ellipticity condition (3.2) is satisfied, then this gives a well-posed problem. If the coefficients
vary with x and y, then the ellipticity condition must be satisfied at each point in .

3.1

Steady-state heat conduction

Equations of elliptic character often arise as steady-state equations in some region of space,
associated with some time-dependent physical problem. For example, the diffusion or heat
conduction equation in two space dimensions takes the form
ut D .ux /x C .uy /y C ;

(3.3)

where .x; y/ > 0 is a diffusion or heat conduction coefficient that may vary with x and
y, and .x; y; t/ is a source term. The solution u.x; y; t/ generally will vary with time
as well as space. We also need initial conditions u.x; y; 0/ in  and boundary conditions
at each point in time at every point on the boundary of . If the boundary conditions and
source terms are independent of time, then we expect a steady state to exist, which we can
find by solving the elliptic equation
.ux /x C .uy /y D f;

(3.4)

59

i

i

i

i

i

i

i

60

“rjlfdm”
2007/6/1
page 60
i

Chapter 3. Elliptic Equations

where again we set f .x; y/ D
.x; y/, together with the boundary conditions. Note that
(3.2) is satisfied at each point, provided  > 0 everywhere.
We first consider the simplest case where   1. We then have the Poisson problem
uxx C uyy D f:

(3.5)

In the special case f  0, this reduces to Laplace’s equation,
uxx C uyy D 0:

(3.6)

We also need to specify boundary conditions all around the boundary of the region .
These could be Dirichlet conditions, where the temperature u.x; y/ is specified at each
point on the boundary, or Neumann conditions, where the normal derivative (the heat flux)
is specified. We may have Dirichlet conditions specified at some points on the boundary
and Neumann conditions at other points.
In one space dimension the corresponding Laplace’s equation u00.x/ D 0 is trivial:
the solution is a linear function connecting the two boundary values. In two dimensions
even this simple equation in nontrivial to solve, since boundary values can now be specified at every point along the curve defining the boundary. Solutions to Laplace’s equation
are called harmonic functions. You may recall from complex analysis that if g.z/ is any
complex analytic function of z D x C iy, then the real and imaginary parts of this function
are harmonic. For example, g.z/ D z 2 D .x 2 y 2 / C 2ixy is analytic and the functions
x 2 y 2 and 2xy are both harmonic.
The operator r 2 defined by
r 2 u D uxx C uyy
is called the Laplacian. The notation r 2 comes from the fact that, more generally,
.ux /x C .uy /y D r  .ru/;
where ru is the gradient of u,


ru D


ux
uy

;

(3.7)

and r is the divergence operator,

r


u
v

D ux C vy :

(3.8)

The symbol  is also often used for the Laplacian but would lead to confusion in numerical
work where x and y are often used for grid spacing.

3.2

The 5-point stencil for the Laplacian

To discuss discretizations, first consider the Poisson problem (3.5) on the unit square 0 
x  1, 0  y  1 and suppose we have Dirichlet boundary conditions. We will use a
uniform Cartesian grid consisting of grid points .xi ; yj /, where xi D ix and yj D jy.
A section of such a grid is shown in Figure 3.1.

i

i

i

i

i

i

i

3.3. Ordering the unknowns and equations
yj C2

61
yj C2

1

yj C1
1

yj

-4

yj C1
1

yj

1

yj 1

yj 1

yj 2

(a)

“rjlfdm”
2007/6/1
page 61
i

1

4

1

4

-20

4

1

4

1

xi 1

xi

yj 2
xi 2

xi 1

xi

xiC1 xiC2

(b)

xi 2

xiC1 xiC2

Figure 3.1. Portion of the computational grid for a two-dimensional elliptic equation. (a) The 5-point stencil for the Laplacian about the point .i; j / is also indicated. (b)
The 9-point stencil is indicated, which is discussed in Section 3.5.
Let uij represent an approximation to u.xi ; yj /. To discretize (3.5) we replace the
x- and y-derivatives with centered finite differences, which gives
1
.ui 1;j
.x/2

2uij C uiC1;j / C

1
.ui;j 1
.y/2

2uij C ui;j C1 / D fij :

(3.9)

For simplicity of notation we will consider the special case where x D y  h, although
it is easy to handle the general case. We can then rewrite (3.9) as
1
.ui 1;j C uiC1;j C ui;j 1 C ui;j C1
h2

4uij / D fij :

(3.10)

This finite difference scheme can be represented by the 5-point stencil shown in Figure 3.1.
We have both an unknown uij and an equation of the form (3.10) at each of m2 grid points
for i D 1; 2; : : : ; m and j D 1; 2; : : : ; m, where h D 1=.m C 1/ as in one dimension.
We thus have a linear system of m2 unknowns. The difference equations at points near the
boundary will of course involve the known boundary values, just as in the one-dimensional
case, which can be moved to the right-hand side.

3.3

Ordering the unknowns and equations

If we collect all these equations together into a matrix equation, we will have an m2 
m2 matrix that is very sparse, i.e., most of the elements are zero. Since each equation
involves at most five unknowns (fewer near the boundary), each row of the matrix has at
most five nonzeros and at least m2 5 elements that are zero. This is analogous to the
tridiagonal matrix (2.9) seen in the one-dimensional case, in which each row has at most
three nonzeros.
Recall from Section 2.14 that the structure of the matrix depends on the order we
choose to enumerate the unknowns. Unfortunately, in two space dimensions the structure of the matrix is not as compact as in one dimension, no matter how we order the

i

i

i

i

i

i

i

62

“rjlfdm”
2007/6/1
page 62
i

Chapter 3. Elliptic Equations

unknowns, and the nonzeros cannot be as nicely clustered near the main diagonal. One
obvious choice is the natural rowwise ordering, where we take the unknowns along the
bottom row, u11 ; u21 ; u31 ; : : : ; um1 , followed by the unknowns in the second row,
u12 ; u22 ; : : : ; um2, and so on, as illustrated in Figure 3.2(a). The vector of unknowns
is partitioned as
2 Œ1 3
3
2
u
u1j
6 uŒ2 7
6 u2j 7
6
7
7
6
u D 6 : 7 ; where uŒj  D 6 : 7 :
(3.11)
4 :: 5
4 :: 5
uŒm

umj

This gives a matrix equation where A has the form
2
T I
6 I T I
1 6
6
I T
I
AD 26
h 6
:
:
:
4
: ::
I

3
7
7
7
7;
:: 7
: 5
T

(3.12)

which is an m  m block tridiagonal matrix in which each block T or I is itself an m  m
matrix,
2
3
4 1
6 1
7
4 1
7
6
7
6
1
4
1
T D6
7;
6
:
:
:
::
::
:: 7
5
4
1
4
and I is the m  m identity matrix. While this has a nice structure, the 1 values in the I
matrices are separated from the diagonal by m 1 zeros, since these coefficients correspond
to grid points lying above or below the central point in the stencil and hence are in the next
or previous row of unknowns.
Another possibility, which has some advantages in the context of certain iterative
methods, is to use the red-black ordering (or checkerboard ordering) shown in Figure 3.2.
This is the two-dimensional analogue of the odd-even ordering that leads to the matrix
(2.63) in one dimension. This ordering is significant because all four neighbors of a red grid
point are black points, and vice versa, and it leads to a matrix equation with the structure

 


D H
fred
ured
D
;
(3.13)
HT D
ublack
fblack
where D D h42 I is a diagonal matrix of dimension m2 =2 and H is a banded matrix of
the same dimension with four nonzero diagonals.
When direct methods such as Gaussian elimination are used to solve the system, one
typically wants to order the equations and unknowns so as to reduce the amount of fill-in
during the elimination procedure as much as possible. This is done automatically if the
backslash operator in MATLAB is used to solve the system, provided it is set up using
sparse storage; see Section 3.7.1.

i

i

i

i

i

i

i

3.4. Accuracy and stability

“rjlfdm”
2007/6/1
page 63
i

63

13

14

15

16

15

7

16

8

9

10

11

12

5

13

6

14

5

6

7

8

11

3

12

4

1

2

3

4

1

9

2

10

(a)

(b)

Figure 3.2. (a) The natural rowwise order of unknowns and equations on a 4  4
grid. (b) The red-black ordering.

3.4

Accuracy and stability

The discretization of the two-dimensional Poisson problem can be analyzed using exactly
the same approach as we used for the one-dimensional boundary value problem. The local
truncation error ij at the .i; j / grid point is defined in the obvious way,
1
.u.xi 1 ; yj /Cu.xiC1 ; yj /Cu.xi ; yj 1 /Cu.xi ; yj C1 / 4u.xi ; yj // f .xi ; yj /;
h2
and by splitting this into the second order difference in the x- and y-directions it is clear
from previous results that

ij D

1 2
h .uxxxx C uyyyy / C O.h4 /:
12
For this linear system of equations the global error Eij D uij u.xi ; yj / then solves the
linear system
Ah E h D  h
ij D

just as in one dimension, where Ah is now the discretization matrix with mesh spacing h,
e.g., the matrix (3.12) if the rowwise ordering is used. The method will be globally second
order accurate in some norm provided that it is stable, i.e., that k.Ah / 1 k is uniformly
bounded as h ! 0.
In the 2-norm this is again easy to check for this simple problem, since we can explicitly compute the spectral radius of the matrix, as we did in one dimension in Section 2.10.
The eigenvalues and eigenvectors of A can now be indexed by two parameters p and k
corresponding to wave numbers in the x- and y-directions for p; k D 1; 2; : : : ; m. The
.p; q/ eigenvector up;q has the m2 elements
up;q
ij D sin.pih/ sin.qj h/:

(3.14)

The corresponding eigenvalue is
p;q D

i

i

2
..cos.ph/
h2

1/ C .cos.qh/

1// :

(3.15)

i

i

i

i

i

64

“rjlfdm”
2007/6/1
page 64
i

Chapter 3. Elliptic Equations

The eigenvalues are strictly negative (A is negative definite) and the one closest to the origin
is
1;1 D 2 2 C O.h2 /:
The spectral radius of .Ah / 1 , which is also the 2-norm, is thus
..Ah / 1 / D 1=1;1  1=2 2:
Hence the method is stable in the 2-norm.
While we’re at it, let’s also compute the condition number of the matrix Ah , since it
turns out that this is a critical quantity in determining how rapidly certain iterative methods
converge. Recall that the 2-norm condition number is defined by
2 .A/ D kAk2 kA 1 k2 :
We’ve just seen that k.Ah / 1 k2  1=2 2 for small h, and the norm of A is given by its
spectral radius. The largest eigenvalue of A (in magnitude) is
m;m 
and so
2 .A/ 

4
DO
 2 h2



8
h2
1
h2


as h ! 0:

(3.16)

The fact that the matrix becomes very ill-conditioned as we refine the grid is responsible
for the slow-down of iterative methods, as discussed in Chapter 4.

3.5

The 9-point Laplacian

Above we used the 5-point Laplacian, which we will denote by r52 uij , where this denotes
the left-hand side of equation (3.10). Another possible approximation is the 9-point Laplacian
r92 uij D

1
Œ4ui 1;j C 4uiC1;j C 4ui;j 1 C 4ui;j C1
6h2
C ui 1;j 1 C ui 1;j C1 C uiC1;j 1 C uiC1;j C1

(3.17)
20uij 

as indicated in Figure 3.1. If we apply this to the true solution and expand in Taylor series,
we find that
r92 u.xi ; yj / D r 2 u C

1 2
h .uxxxx C 2uxxyy C uyyyy / C O.h4 /:
12

At first glance this discretization looks no better than the 5-point discretization since the
error is still O.h2 /. However, the additional terms lead to a very nice form for the dominant
error term, since
uxxxx C 2uxxyy C uyyyy D r 2 .r 2 u/  r 4 u:

i

i

i

i

i

i

i

3.5. The 9-point Laplacian

“rjlfdm”
2007/6/1
page 65
i

65

This is the Laplacian of the Laplacian of u and r 4 is called the biharmonic operator. If we
are solving r 2 u D f , then we have
uxxxx C 2uxxyy C uyyyy D r 2 f:
Hence we can compute the dominant term in the truncation error easily from the known
function f without knowing the true solution u to the problem.
In particular, if we are solving Laplace’s equation, where f D 0, or more generally if f is a harmonic function, then this term in the local truncation error vanishes and
the 9-point Laplacian would give a fourth order accurate discretization of the differential
equation.
More generally, we can obtain a fourth order accurate method of the form
r92 uij D fij

(3.18)

for arbitrary smooth functions f .x; y/ by defining
fij D f .xi ; yj / C

h2 2
r f .xi ; yj /:
12

(3.19)

We can view this as deliberately introducing an O.h2 / error into the right-hand side of the
equation that is chosen to cancel the O.h2 / part of the local truncation error. Taylor series
expansion easily shows that the local truncation error of the method (3.18) is now O.h4 /.
This is the two-dimensional analogue of the modification (2.117) that gives fourth order
accuracy for the boundary value problem u00.x/ D f .x/.
If we have only data f .xi ; yj / at the grid points (but we know that the underlying
function is sufficiently smooth), then we can still achieve fourth order accuracy by using
fij D f .xi ; yj / C

h2 2
r f .xi ; yj /
12 5

instead of (3.19).
This is a trick that often can be used in developing numerical methods—introducing
an “error” into the equations that is carefully chosen to cancel some other error.
Note that the same trick wouldn’t work with the 5-point Laplacian, or at least not as
directly. The form of the truncation error in this method depends on uxxxx C uyyyy. There
is no way to compute this directly from the original equation without knowing u. The extra
points in the 9-point stencil convert this into the Laplacian of f , which can be computed if
f is sufficiently smooth.
On the other hand, a two-pass approach could be used with the 5-point stencil, in
which we first estimate u by solving with the standard 5-point scheme to get a second order
accurate estimate of u. We then use this estimate of u to approximate uxxxx C uyyyy and
then solve a second time with a right-hand side that is modified to eliminate the dominant
term of the local truncation error. This would be more complicated for this particular
problem, but this idea can be used much more generally than the above trick, which depends
on the special form of the Laplacian. This is the method of deferred corrections, already
discussed for one dimension in Section 2.20.3.

i

i

i

i

i

i

i

66

3.6

“rjlfdm”
2007/6/1
page 66
i

Chapter 3. Elliptic Equations

Other elliptic equations

In Chapter 2 we started with the simplest boundary value problem for the constant coefficient problem u00.x/ D f .x/ but then introduced various, more interesting problems, such
as variable coefficients, nonlinear problems, singular perturbation problems, and boundary
or interior layers.
In the multidimensional case we have discussed only the simplest Poisson problem,
which in one dimension reduces to u00.x/ D f .x/. All the further complications seen in
one dimension can also arise in multidimensional problems. For example, heat conduction
in a heterogeneous two-dimensional domain gives rise to the equation
..x; y/ux .x; y//x C ..x; y/uy .x; y//y D f .x; y/;

(3.20)

where .x; y/ is the varying heat conduction coefficient. In any number of space dimensions this equation can be written as
r  .ru/ D f:

(3.21)

These problems can be solved by generalizations of the one-dimensional methods. The
terms ..x; y/ux .x; y//x and ..x; y/uy .x; y//y can each be discretized as in the onedimensional case, again resulting in a 5-point stencil in two dimensions.
Nonlinear elliptic equations also arise in multidimensions, in which case a system of
nonlinear algebraic equations will result from the discretization. A Newton method can be
used as in one dimension, but now in each Newton iteration a large sparse linear system will
have to be solved. Typically the Jacobian matrix has a sparsity pattern similar to those seen
above for linear elliptic equations. See Section 4.5 for a brief discussion of Newton–Krylov
iterative methods for such problems.
In multidimensional problems there is an additional potential complication that is
not seen in one dimension: the domain  where the boundary value problem is posed
may not be a simple rectangle as we have supposed in our discussion so far. When the
solution exhibits boundary or interior layers, then we would also like to cluster grid points
or adaptively refine the grid in these regions. This often presents a significant challenge
that we will not tackle in this book.

3.7

Solving the linear system

Two fundamentally different approaches could be used for solving the large linear systems
that arise from discretizing elliptic equations. A direct method such as Gaussian elimination
produces an exact solution (or at least would in exact arithmetic) in a finite number of
operations. An iterative method starts with an initial guess for the solution and attempts to
improve it through some iterative procedure, halting after a sufficiently good approximation
has been obtained.
For problems with large sparse matrices, iterative methods are often the method of
choice, and Chapter 4 is devoted to a study of several iterative methods. Here we briefly
consider the operation counts for Gaussian elimination to see the potential pitfalls of this
approach.
It should be noted, however, that on current computers direct methods can be successfully used for quite large problems, provided appropriate sparse storage and efficient

i

i

i

i

i

i

i

3.7. Solving the linear system

“rjlfdm”
2007/6/1
page 67
i

67

elimination procedures are used. See Section 3.7.1 for some comments on setting up sparse
matrices such as (3.12) in MATLAB.
It is well known (see, e.g., [35], [82], [91]) that for a general N  N dense matrix
(one with few elements equal to zero), performing Gaussian elimination requires O.N 3 /
operations. (There are N .N 1/=2 D O.N 2 / elements below the diagonal to eliminate,
and eliminating each one requires O.N / operations to take a linear combination of the
rows.)
Applying a general Gaussian elimination program blindly to the matrices we are
now dealing with would be disastrous, or at best extremely wasteful of computer resources.
Suppose we are solving the three-dimensional Poisson problem on a 100100100 grid—
a modest problem these days. Then N D m3 D 106 and N 3 D 1018 . On a reasonably fast
desktop that can do on the order of 1010 floating point operations per second (10 gigaflops),
this would take on the order of 108 seconds, which is more than 3 years. More sophisticated
methods can solve this problem in seconds.
Moreover, even if speed were not an issue, memory would be. Storing the full matrix
A in order to modify the elements and produce L and U would require N 2 memory locations. In 8-byte arithmetic this requires 8 N 2 bytes. For the problem mentioned above, this
would be 8  1012 bytes, or eight terabytes. One advantage of iterative methods is that they
do not store the matrix at all and at most need to store the nonzero elements.
Of course with Gaussian elimination it would be foolish to store all the elements of
a sparse matrix, since the vast majority are zero, or to apply the procedure blindly without
taking advantage of the fact that so many elements are already zero and hence do not need
to be eliminated.
As an extreme example, consider the one-dimensional case where we have a tridiagonal matrix as in (2.9). Applying Gaussian elimination requires eliminating only the
nonzeros along the subdiagonal, only N 1 values instead of N .N 1/=2. Moreover,
when we take linear combinations of rows in the course of eliminating these values, in
most columns we will be taking linear combinations of zeros, producing zero again. If we
do not do pivoting, then only the diagonal elements are modified. Even with partial pivoting, at most we will introduce one extra superdiagonal of nonzeros in the upper triangular
U that were not present in A. As a result, it is easy to see that applying Gaussian elimination to an m  m tridiagonal system requires only O.m/ operations, not O.m3 /, and that
the storage required is O.m/ rather than O.m2 /.
Note that this is the best we could hope for in one dimension, at least in terms of the
order of magnitude. There are m unknowns and even if we had exact formulas for these
values, it would require O.m/ work to evaluate them and O.m/ storage to save them.
In two space dimensions we can also take advantage of the sparsity and structure
of the matrix to greatly reduce the storage and work required with Gaussian elimination,
although not to the minimum that one might hope to attain. On an m  m grid there are
N D m2 unknowns, so the best one could hope for is an algorithm that computes the
solution in O.N / D O.m2 / work using O.m2 / storage. Unfortunately, this cannot be
achieved with a direct method.
One approach that is better than working with the full matrix is to observe that the A
is a banded matrix with bandwidth m both above and below the diagonal. Since a general
N  N banded matrix with a nonzero bands above the diagonal and b below the diagonal

i

i

i

i

i

i

i

68

“rjlfdm”
2007/6/1
page 68
i

Chapter 3. Elliptic Equations

can be factored in O.N ab/ operations, this results in an operation count of O.m4 / for the
two-dimensional Poisson problem.
A more sophisticated approach that takes more advantage of the special structure (and
the fact that there are already many zeros within the bandwidth) is the nested dissection
algorithm [34]. This algorithm requires O.m3 / operations in two dimensions. It turns out
this is the best that can be achieved with a direct method based on Gaussian elimination.
George proved (see [34]) that any elimination method for solving this problem requires at
least O.m3 / operations.
For certain special problems, very fast direct methods can be used, which are much
better than standard Gaussian elimination. In particular, for the Poisson problem on a
rectangular domain there are fast Poisson solvers based on the fast Fourier transform that
can solve on an m  m grid in two dimensions in O.m2 log m/ operations, which is nearly
optimal. See [87] for a review of this approach.

3.7.1 Sparse storage in MATLAB
If you are going to work in MATLAB with sparse matrices arising from finite difference
methods, it is important to understand and use the sparse matrix commands that set up
matrices using sparse storage, so that only the nonzeros are stored. Type help sparse
to get started.
As one example, the matrix of (3.12) can be formed in MATLAB by the commands
I = eye(m);
e = ones(m,1);
T = spdiags([e -4*e e],[-1 0 1],m,m);
S = spdiags([e e],[-1 1],m,m);
A = (kron(I,T) + kron(S,I))/hˆ2;
The spy(A) command is also useful for looking at the nonzero structure of a matrix.
The backslash command in MATLAB can be used to solve systems using sparse
storage, and it implements highly efficient direct methods using sophisticated algorithms
for dynamically ordering the equations to minimize fill-in, as described by Davis [24].

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 69
i

Chapter 4

Iterative Methods for
Sparse Linear Systems

This chapter contains an overview of several iterative methods for solving the large sparse
linear systems that arise from discretizing elliptic equations. Large sparse linear systems
arise from many other practical problems, too, of course, and the methods discussed here
are useful in other contexts as well. Except when the matrix has very special structure and
fast direct methods of the type discussed in Section 3.7 apply, iterative methods are usually
the method of choice for large sparse linear systems.
The classical Jacobi, Gauss–Seidel, and successive overrelaxation (SOR) methods
are introduced and briefly discussed. The bulk of the chapter, however, concerns more
modern methods for solving linear systems that are typically much more effective for largescale problems: preconditioned conjugate-gradient (CG) methods, Krylov space methods
such as generalized minimum residual (GMRES), and multigrid methods.

4.1

Jacobi and Gauss–Seidel

In this section two classical iterative methods, Jacobi and Gauss–Seidel, are introduced to
illustrate the main issues. It should be stressed at the beginning that these are poor methods
in general which converge very slowly when used as standalone methods, but they have
the virtue of being simple to explain. Moreover, these methods are sometimes used as
building blocks in more sophisticated methods, e.g., Jacobi may be used as a smoother for
the multigrid method, as discussed in Section 4.6.
We again consider the Poisson problem where we have the system of equations
(3.10). We can rewrite this equation as
uij D

1
.ui 1;j C uiC1;j C ui;j 1 C ui;j C1 /
4

h2
fij :
4

(4.1)

In particular, note that for Laplace’s equation (where fij  0), this simply states that the
value of u at each grid point should be the average of its four neighbors. This is the discrete
analogue of the well-known fact that a harmonic function has the following property: the
value at any point .x; y/ is equal to the average value around a closed curve containing the
point, in the limit as the curve shrinks to the point. Physically this also makes sense if we
69

i

i

i

i

i

i

i

70

“rjlfdm”
2007/6/1
page 70
i

Chapter 4. Iterative Methods for Sparse Linear Systems

think of the heat equation. Unless the temperature at this point is equal to the average of
the temperature at neighboring points, there will be a net flow of heat toward or away from
this point.
The equation (4.1) suggests the following iterative method to produce a new estimate
uŒkC1 from a current guess uŒk :
ŒkC1

uij

D


1  Œk
Œk
Œk
Œk
ui 1;j C uiC1;j C ui;j 1 C ui;j C1
4

h2
fij :
4

(4.2)

This is the Jacobi iteration for the Poisson problem, and it can be shown that for this
particular problem it converges from any initial guess uŒ0 (although very slowly).
Here is a short section of MATLAB code that implements the main part of this iteration:
for iter=0:maxiter
for j=2:(m+1)
for i=2:(m+1)
unew(i,j) = 0.25*(u(i-1,j) + u(i+1,j) + ...
u(i,j-1) + u(i,j+1) - hˆ2 * f(i,j));
end
end
u = unew;
end
Here it is assumed that u initially contains the guess uŒ0 and that boundary data are stored
in u(1,:), u(m+2,:), u(:,1), and u(:,m+2). The indexing is off by 1 from
what might be expected since MATLAB begins arrays with index 1, not 0.
Note that one might be tempted to dispense with the variable unew and replace the
above code with
for iter=0:maxiter
for j=2:(m+1)
for i=2:(m+1)
u(i,j) = 0.25*(u(i-1,j) + u(i+1,j) + ...
u(i,j-1) + u(i,j+1) - hˆ2 * f(i,j));
end
end
end
This would not give the same results, however. In the correct code for Jacobi we
compute new values of u based entirely on old data from the previous iteration, as required
from (4.2). In the second code we have already updated u(i-1,j) and u(i,j-1)
before updating u(i,j), and these new values will be used instead of the old ones. The
latter code thus corresponds to the method
 h2
1  ŒkC1
ŒkC1
Œk
C
u
C
u
(4.3)
ui 1;j C uŒk
fij :
iC1;j
i;j 1
i;j C1
4
4
This is what is known as the Gauss–Seidel method, and it would be a lucky coding error since this method generally converges about twice as fast as Jacobi does. The Jacobi
uŒkC1
D
ij

i

i

i

i

i

i

i

4.2. Analysis of matrix splitting methods

“rjlfdm”
2007/6/1
page 71
i

71

method is sometimes called the method of simultaneous displacements, while Gauss–Seidel
is known as the method of successive displacements. Later we’ll see that Gauss–Seidel can
be improved by using SOR.
Note that if one actually wants to implement Jacobi in MATLAB, looping over i and
j is quite slow and it is much better to write the code in vectorized form, e.g.,
I = 2:(m+1);
J = 2:(m+1);
for iter=0:maxiter
u(I,J) = 0.25*(u(I-1,J) + u(I+1,J) + u(I,J-1) ...
+ u(I,J+1) - hˆ2 * f(I,J));
end
It is somewhat harder to implement Gauss–Seidel in vectorized form.
Convergence of these methods will be discussed in Section 4.2. First we note some
important features of these iterative methods:
 The matrix A is never stored. In fact, for this simple constant coefficient problem, we
don’t even store all the 5m2 nonzeros which all have the value 1= h2 or 4= h2. The
values 0:25 and h2 in the code are the only values that are “stored.” (For a variable
coefficient problem where the coefficients are different at each point, we would in
general have to store all the nonzeros.)
 Hence the storage is optimal—essentially only the m2 solution values are stored in
the Gauss–Seidel method. The above code for Jacobi uses 2m2 since unew is stored
as well as u, but one could eliminate most of this with more careful coding.
 Each iteration requires O.m2 / work. The total work required will depend on how
many iterations are required to reach the desired level of accuracy. We will see that
with these particular methods we require O.m2 log m/ iterations to reach a level of
accuracy consistent with the expected global error in the solution (as h ! 0 we
should require more accuracy in the solution to the linear system). Combining this
with the work per iteration gives a total operation count of O.m4 log m/. This looks
worse than Gaussian elimination with a banded solver, although since log m grows
so slowly with m it is not clear which is really more expensive for a realistic-size
matrix. (And the iterative method definitely saves on storage.)
Other iterative methods also typically require O.m2 / work per iteration but may
converge much faster and hence result in less overall work. The ideal would be to converge
in a number of iterations that is independent of h so that the total work is simply O.m2 /.
Multigrid methods (see Section 4.6) can achieve this, not only for Poisson’s problem but
also for many other elliptic equations.

4.2

Analysis of matrix splitting methods

In this section we study the convergence of the Jacobi and Gauss–Seidel methods. As a
simple example we will consider the one-dimensional analogue of the Poisson problem,
u00.x/ D f .x/ as discussed in Chapter 2. Then we have a tridiagonal system of equations

i

i

i

i

i

i

i

72

“rjlfdm”
2007/6/1
page 72
i

Chapter 4. Iterative Methods for Sparse Linear Systems

(2.9) to solve. In practice we would never use an iterative method for this system, since
it can be solved directly by Gaussian elimination in O.m/ operations, but it is easier to
illustrate the iterative methods in the one-dimensional case, and all the analysis done here
carries over almost unchanged to the two-dimensional and three-dimensional cases.
The Jacobi and Gauss–Seidel methods for this problem take the form

1  Œk
ŒkC1
Œk
Jacobi
ui
(4.4)
D
ui 1 C uiC1 h2 fi ;
2


1 ŒkC1
ŒkC1
Œk
Gauss–Seidel
ui
(4.5)
D
C uiC1 h2 fi :
u
2 i 1
Both methods can be analyzed by viewing them as based on a splitting of the matrix A into
ADM

N;

(4.6)

where M and N are two m  m matrices. Then the system Au D f can be written as
Mu

Nu D f

H)

M u D N u C f;

which suggests the iterative method
M uŒkC1 D N uŒk C f:
Œk

(4.7)
ŒkC1

In each iteration we assume u is known and we obtain u
by solving a linear system
with the matrix M . The basic idea is to define the splitting so that M contains as much
of A as possible (in some sense) while keeping its structure sufficiently simple that the
system (4.7) is much easier to solve than the original system with the full A. Since systems
involving diagonal, lower, or upper triangular matrices are relatively simple to solve, there
are some obvious choices for the matrix M . To discuss these in a unified framework, write
ADD

L

U

(4.8)

in general, where D is the diagonal of A, L is the strictly lower triangular part, and U
is the strictly upper triangular part. For example, the tridiagonal matrix (2.10) would give
2

2
6 0
6
1 6
6
DD 26
h 6
6
4

0
2
0

2

3
0
2
::
:

0
::
:
0

::

:
2
0

7
7
7
7
7;
7
7
0 5
2

LD

0
6 1
6
1 6
6
6
h2 6
6
4

0
0
1

3
0
0
::
:

0
::
:
1

::

:
0
1

7
7
7
7
7
7
7
0 5
0

T

with U D L being the remainder of A.
In the Jacobi method, we simply take M to be the diagonal part of A, M D D, so
that
2
3
0 1
6 1 0 1
7
6
7
6
7
1
0
1
2
1 6
7
M D
I;
N
D
L
C
U
D
D
A
D
6
7:
:
:
:
:: :: ::
7
h2
h2 6
6
7
4
1
0 1 5
1 0

i

i

i

i

i

i

i

4.2. Analysis of matrix splitting methods

“rjlfdm”
2007/6/1
page 73
i

73

The system (4.7) is then diagonal and extremely easy to solve:
2
3
0 1
6 1 0 1
7
6
7
6
7
1 0
1
16
7 Œk
ŒkC1
u
D 6
7u
:: :: ::
7
26
:
:
:
6
7
4
1
0 1 5
1 0

h2
f;
2

(4.9)

which agrees with (4.4).
In Gauss–Seidel, we take M to be the full lower triangular portion of A, so M D
D L and N D U . The system (4.7) is then solved using forward substitution, which
results in (4.5).
To analyze these methods, we derive from (4.7) the update formula
uŒkC1 D M

1

N uŒk C M

1

f
(4.10)

 GuŒk C c;
where G D M 1 N is the iteration matrix and c D M 1f .
Let u represent the true solution to the system Au D f . Then
u D Gu C c:

(4.11)

This shows that the true solution is a fixed point, or equilibrium, of the iteration (4.10),
i.e., if uŒk D u , then uŒkC1 D u as well. However, it is not clear that this is a stable
equilibrium, i.e., that we would converge toward u if we start from some incorrect initial
guess.
If e Œk D uŒk u represents the error, then subtracting (4.11) from (4.10) gives
e ŒkC1 D Ge Œk ;
and so after k steps we have
e Œk D G k e Œ0 :

(4.12)
Œ0

From this we can see that the method will converge from any initial guess u , provided
G k ! 0 (an m  m matrix of zeros) as k ! 1. When is this true?
For simplicity, assume that G is a diagonalizable matrix, so that we can write
G D RR 1 ;
where R is the matrix of right eigenvectors of G and  is a diagonal matrix of eigenvalues
1 ; 2 ; : : : ; m . Then
G k D R k R 1 ;
(4.13)
where

2
6
6
k D 6
4

3

k
1

7
7
7:
5

k
2

::

:
k
m

i

i

i

i

i

i

i

74

“rjlfdm”
2007/6/1
page 74
i

Chapter 4. Iterative Methods for Sparse Linear Systems

Clearly the method converges if j p j < 1 for all p D 1; 2; : : : ; m, i.e., if .G/ < 1, where
 is the spectral radius. See Appendix D for a more general discussion of the asymptotic
properties of matrix powers.

4.2.1 Rate of convergence
From (4.12) we can also determine how rapidly the method can be expected to converge in
cases where it is convergent. Using (4.13) in (4.12) and using the 2-norm, we obtain
ke Œk k2  k k k2 kRk2kR 1 k2 ke Œ0k2 D k 2 .R/ke Œ0 k2 ;

(4.14)

where   .G/, and 2 .R/ D kRk2kR 1 k2 is the condition number of the eigenvector
matrix.
If the matrix G is a normal matrix (see Section C.4), then the eigenvectors are orthogonal and 2 .R/ D 1. In this case we have
ke Œk k2  k ke Œ0k2 :

(4.15)

If G is nonnormal, then the spectral radius of G gives information about the asymptotic rate of convergence as k ! 1 but may not give a good indication of the behavior
of the error for small k. See Section D.4 for more discussion of powers of nonnormal matrices and see Chapters 24–27 of [92] for some discussion of iterative methods on highly
nonnormal problems.
Note: These methods are linearly convergent, in the sense that ke ŒkC1 k  ke Œkk
and it is the first power of ke Œkk that appears on the right. Recall that Newton’s method is
typically quadratically convergent, and it is the square of the previous error that appears on
the right-hand side. But Newton’s method is for a nonlinear problem and requires solving
a linear system in each iteration. Here we are looking at solving such a linear system.
Example 4.1. For the Jacobi method we have
G D D 1 .D

D 1 A:

A/ D I

If we apply this method to the boundary value problem u00 D f , then
GDIC

h2
A:
2

The eigenvectors of this matrix are the same as the eigenvectors of A, and the eigenvalues
are hence
h2
p ;
p D 1C
2
where p is given by (2.23). So
p D cos.ph/;

p D 1; 2; : : : ; m;

where h D 1=.m C 1/. The spectral radius is
.G/ D j 1j D cos.h/  1

i

i

1 2 2
 h C O.h4 /:
2

(4.16)

i

i

i

i

i

4.2. Analysis of matrix splitting methods

“rjlfdm”
2007/6/1
page 75
i

75

The spectral radius is less than 1 for any h > 0 and the Jacobi method converges. Moreover,
the G matrix for Jacobi is symmetric as seen in (4.9), and so (4.15) holds and the error is
monotonically decreasing at a rate given precisely by the spectral radius. Unfortunately,
though, for small h this value is very close to 1, resulting in very slow convergence.
How many iterations are required to obtain a good solution? Suppose we want to
reduce the error to ke Œkk  ke Œ0k (where typically ke Œ0k is on the order of 1).1 Then we
want k   and so
k  log./= log./:
(4.17)
How small should we choose ? To get full machine precision we might choose  to be
close to the machine round-off level. However, this typically would be very wasteful. For
one thing, we rarely need this many correct digits. More important, however, we should
keep in mind that even the exact solution u of the linear system Au D f is only an
approximate solution of the differential equation we are actually solving. If we are using a
second order accurate method, as in this example, then ui differs from u.xi / by something
on the order of h2 and so we cannot achieve better accuracy than this no matter how well
we solve the linear system. In practice we should thus take  to be something related to the
expected global error in the solution, e.g.,  D C h2 for some fixed C .
To estimate the order of work required asymptotically as h ! 0, we see that the
above choice gives
k D .log.C / C 2 log.h//= log./:
(4.18)
For Jacobi on the boundary value problem we have   1
1 2 2
2  h . Since h D 1=.m C 1/, using this in (4.18) gives
k D O.m2 log m/ as m ! 1:

1 2 2
 h and hence log./ 
2

(4.19)

Since each iteration requires O.m/ work in this one-dimensional problem, the total work
required to solve the problem is
total work D O.m3 log m/:
Of course this tridiagonal problem can be solved exactly in O.m/ work, so we would be
foolish to use an iterative method at all here!
For a Poisson problem in two or three dimensions it can be verified that (4.19) still
holds, although now the work required per iteration is O.m2 / or O.m3 /, respectively, if
there are m grid points in each direction. In two dimensions we would thus find that
total work D O.m4 log m/:

(4.20)

Recall from Section 3.7 that Gaussian elimination on the banded matrix requires O.m4 /
operations, while other direct methods can do much better, so Jacobi is still not competitive.
Luckily there are much better iterative methods.
1
Assuming we are using some grid function norm,
p as discussed in Appendix A. Note that for the 2-norm in
one dimension this requires introducing a factor of h in the definitions of both ke Œk k and ke Œ0 k, but these
factors cancel out in choosing an appropriate .

i

i

i

i

i

i

i

76

“rjlfdm”
2007/6/1
page 76
i

Chapter 4. Iterative Methods for Sparse Linear Systems

For the Gauss–Seidel method applied to the Poisson problem in any number of space
dimensions, it can be shown that
.G/ D 1

 2 h2 C O.h4 / as h ! 0:

(4.21)

This still approaches 1 as h ! 0, but it is better than (4.16) by a factor of 2, and the number
of iterations required to reach a given tolerance typically will be half the number required
with Jacobi. The order of magnitude figure (4.20) still holds, however, and this method
also is not widely used.

4.2.2 Successive overrelaxation
If we look at how iterates uŒk behave when Gauss–Seidel is applied to a typical problem,
ŒkC1
Œk
we will often see that ui
is closer to ui than ui was, but only by a little bit. The Gauss–
Seidel update moves ui in the right direction but is far too conservative in the amount it
allows ui to move. This suggests that we use the following two-stage update, illustrated
again for the problem u00 D f :

h2 fi ;

Œk
ui ;

uGS
i D

1  ŒkC1
Œk
C uiC1
u
2 i 1 

ŒkC1

Œk

ui

D ui C ! uGS
i

(4.22)

ŒkC1

where ! is some scalar parameter. If ! D 1, then ui
D uGS
is the Gauss–Seidel
i
update. If ! > 1, then we move farther than Gauss–Seidel suggests. In this case the
method is known as successive overrelaxation (SOR).
If ! < 1, then we would be underrelaxing, rather than overrelaxing. This would be
even less effective than Gauss–Seidel as a standalone iterative method for most problems,
although underrelaxation is sometimes used in connection with multigrid methods (see
Section 4.6).
The formulas in (4.22) can be combined to yield

!  ŒkC1
ŒkC1
Œk
Œk
ui
D
(4.23)
ui 1 C uiC1 h2 fi C .1 !/ui :
2
For a general system Au D f with A D D L U it can be shown that SOR with forward
sweeps corresponds to a matrix splitting method of the form (4.7) with
M D

1
.D
!

!L/;

N D

1
..1
!

!/D C !U /:

(4.24)

Analyzing this method is considerably trickier than with the Jacobi or Gauss–Seidel
methods because of the form of these matrices. A theorem of Ostrowski states that if A
is symmetric positive definite (SPD) and D !L is nonsingular, then the SOR method
converges for all 0 < ! < 2. Young [105] showed how to find the optimal ! to obtain
the most rapid convergence for a wide class of problems (including the Poisson problem).
This elegant theory can be found in many introductory texts. (For example, see [37], [42],
[96], [106]. See also [67] for a different introductory treatment based on Fourier series

i

i

i

i

i

i

i

4.2. Analysis of matrix splitting methods

“rjlfdm”
2007/6/1
page 77
i

77

and modified equations in the sense of Section 10.9, and see [3] for applications of this
approach to the 9-point Laplacian.)
For the Poisson problem in any number of space dimensions it can be shown that the
SOR method converges most rapidly if ! is chosen as
!opt D

2
2
1 C sin.h/

2h:

This is nearly equal to 2 for small h. One might be tempted to simply set ! D 2 in general,
but this would be a poor choice since SOR does not then converge! In fact the convergence
rate is quite sensitive to the value of ! chosen. With the optimal ! it can be shown that the
spectral radius of the corresponding G matrix is
opt D !opt

11

2h;

but if ! is changed slightly this can deteriorate substantially.
Even with the optimal ! we see that opt ! 1 as h ! 0, but only linearly in h rather
than quadratically as with Jacobi or Gauss–Seidel. This makes a substantial difference in
practice. The expected number of iterations to converge to the required O.h2 / level, the
analogue of (4.19), is now
kopt D O.m log m/:
Figure 4.1 shows some computational results for the methods described above on
the two-point boundary value problem u00 D f . The SOR method with optimal ! is

0

10

Jacobi
−1

10

Gauss−Seidel
−2

10

−3

10

SOR
−4

10

−5

10

−6

10

0

10

20

30

40

50

60

70

80

90

100

Figure 4.1. Errors versus k for three methods.

i

i

i

i

i

i

i

78

“rjlfdm”
2007/6/1
page 78
i

Chapter 4. Iterative Methods for Sparse Linear Systems

far superior to Gauss–Seidel or Jacobi, at least for this simple problem with a symmetric
coefficient matrix. For more complicated problems it can be difficult to estimate the optimal
!, however, and other approaches are usually preferred.

4.3

Descent methods and conjugate gradients

The CG method is a powerful technique for solving linear systems Au D f when the
matrix A is SPD, or negative definite since negating the system then gives an SPD matrix.
This may seem like a severe restriction, but SPD methods arise naturally in many applications, such as the discretization of elliptic equations. There are several ways to introduce
the CG method and the reader may wish to consult texts such as [39], [79], [91] for other
approaches and more analysis. Here the method is first motivated as a descent method for
solving a minimization problem.
Consider the function  W Rm ! R defined by
.u/ D

1 T
u Au
2

uT f:

(4.25)

This is a quadratic function of the variables u1 ; : : : ; um . For example, if m D 2, then
.u/ D .u1 ; u2/ D

1
.a11 u21 C 2a12u1 u2 C a22 u22 /
2

u 1 f1

u 2 f2 :

Note that since A is symmetric, a21 D a12 . If A is positive definite, then plotting .u/ as
a function of u1 and u2 gives a parabolic bowl as shown in Figure 4.2(a). There is a unique
value u that minimizes .u/ over all choices of u. At the minimum, the partial derivative
of  with respect to each component of u is zero, which gives the equations
@
D a11 u1 C a12 u2
@u1
@
D a21 u1 C a22 u2
@u2

f1 D 0;
(4.26)
f2 D 0:

This is exactly the linear system Au D f that we wish to solve. So finding u that solves
this system can equivalently be approached as finding u to minimize .u/. This is true
more generally when u 2 Rm and A 2 Rmm is SPD. The function .u/ in (4.25) has a
unique minimum at the point u , where r.u / D 0, and
r.u/ D Au

f;

(4.27)

so the minimizer solves the linear system Au D f .
If A is negative definite, then .u/ instead has a unique maximum at u , which
again solves the linear system. If A is indefinite (neither positive nor negative definite),
i.e., if the eigenvalues of A are not all of the same sign, then the function .u/ still has a
stationary point with r.u / D 0 at the solution to Au D f , but this is a saddle point
rather than a minimum or maximum, as illustrated in Figure 4.2(b). It is much harder to
find a saddle point than a minimum. An iterative method can find a minimum by always
heading downhill, but if we are looking for a saddle point, it is hard to tell if we need to

i

i

i

i

i

i

i

4.3. Descent methods and conjugate gradients

“rjlfdm”
2007/6/1
page 79
i

79
1

1
0.9
0.8
0.7

0.5

0.6
0.5
0.4
0

0.3
0.2
0.1
−0.5
0.5

0
0.5
0.5

0

(a)

0

0
−0.5

0.5
0

(b)

−0.5

−0.5

−0.5

Figure 4.2. (a) The function .u/ for m D 2 in a case where A is symmetric and
positive definite. (b) The function .u/ for m D 2 in a case where A is symmetric but
indefinite.
head uphill or downhill from the current approximation. Since the CG method is based on
minimization, it is necessary for the matrix to be SPD. By viewing CG in a different way it
is possible to generalize it and obtain methods that also work on indefinite problems, such
as the GMRES algorithm described in Section 4.4.

4.3.1 The method of steepest descent
As a prelude to studying CG, we first review the method of steepest descent for minimizing
.u/. As in all iterative methods we start with an initial guess u0 and iterate to obtain
u1 ; u2; : : :. For notational convenience we now use subscripts to denote the iteration number: uk instead of uŒk . This is potentially confusing since normally we use subscripts to
denote components of the vector, but the formulas below get too messy otherwise and we
will not need to refer to the components of the vector in the rest of this chapter.
From one estimate uk 1 to u we wish to obtain a better estimate uk by moving
downhill, based on values of .u/. It seems sensible to move in the direction in which  is
decreasing most rapidly, and go in this direction for as far as we can before .u/ starts to
increase again. This is easy to implement, since the gradient vector r.u/ always points
in the direction of most rapid increase of . So we want to set
uk D uk 1

˛k 1 r.uk 1 /

(4.28)

for some scalar ˛k 1 , chosen to solve the minimization problem
min  .uk 1
˛2R

˛r.uk 1 // :

(4.29)

We expect ˛k 1  0 and ˛k 1 D 0 only if we are already at the minimum of , i.e., only
if uk 1 D u .
For the function .u/ in (4.25), the gradient is given by (4.27) and so
r.uk 1 / D Auk 1

f 

rk 1 ;

(4.30)

where rk 1 D f Auk 1 is the residual vector based on the current approximation uk 1 .
To solve the minimization problem (4.29), we compute the derivative with respect to ˛ and
set this to zero. Note that

i

i

i

i

i

i

i

80

“rjlfdm”
2007/6/1
page 80
i

Chapter 4. Iterative Methods for Sparse Linear Systems

.u C ˛r / D

1 T
u Au
2


uT f

C ˛.r T Au

1
r T f / C ˛ 2r T Ar
2

(4.31)

and so

d.u C ˛r /
D r T Au
d˛
Setting this to zero and solving for ˛ gives
˛D

r T f C ˛r T Ar:

rT r
:
r T Ar

(4.32)

The steepest descent algorithm thus takes the form
choose a guess u0
for k D 1; 2; : : :
rk 1 D f Auk 1
if krk 1 k is less than some tolerance then stop
˛k 1 D .rkT 1 rk 1 /=.rkT 1 Ark 1 /
uk D uk 1 C ˛k 1 r k 1
end
Note that implementing this algorithm requires only that we be able to multiply a vector by
A, as with the other iterative methods discussed earlier. We do not need to store the matrix
A, and if A is very sparse, then this multiplication can be done quickly.
It appears that in each iteration we must do two matrix-vector multiplies, Auk 1 to
compute rk 1 and then Ark 1 to compute ˛k 1 . However, note that
rk D f Auk
D f A.uk 1 C ˛k 1 rk 1 /
D rk 1 ˛k 1 Ark 1 :

(4.33)

So once we have computed Ark 1 as needed for ˛k 1 , we can also use this result to compute rk . A better way to organize the computation is thus:
choose a guess u0
r0 D f Au0
for k D 1; 2; : : :
wk 1 D Ark 1
˛k 1 D .rkT 1 rk 1 /=.rkT 1 wk 1 /
uk D uk 1 C ˛k 1 r k 1
rk D rk 1 ˛k 1 wk 1
if krk k is less than some tolerance then stop
end
Figure 4.3 shows how this iteration proceeds for a typical case with m D 2. This
figure shows a contour plot of the function .u/ in the u1 -u2 plane (where u1 and u2
mean the components of u here), along with several iterates un of the steepest descent
algorithm. Note that the gradient vector is always orthogonal to the contour lines. We
move along the direction of the gradient (the “search direction” for this algorithm) to the

i

i

i

i

i

i

i

4.3. Descent methods and conjugate gradients

u2

u4
u3

“rjlfdm”
2007/6/1
page 81
i

81

u

u1
u0

Figure 4.3. Several iterates of the method of steepest descent in the case m D 2.
The concentric ellipses are level sets of .u/.
point where .u/ is minimized along this line. This will occur at the point where this line
is tangent to a contour line. Consequently, the next search direction will be orthogonal to
the current search direction, and in two dimensions we simply alternate between only two
search directions. (Which particular directions depend on the location of u0 .)
If A is SPD, then the contour lines (level sets of ) are always ellipses. How rapidly
this algorithm converges depends on the geometry of these ellipses and on the particular
starting vector u0 chosen. Figure 4.4(a) shows the best possible case, where the ellipses are
circles. In this case the iterates converge in one step from any starting guess, since the first
search direction r0 generates a line that always passes through the minimum u from any
point.
Figure 4.4(b) shows a bad case, where the ellipses are long and skinny and the iteration slowly traverses back and forth in this shallow valley searching for the minimum. In
general steepest descent is a slow algorithm, particularly when m is large, and should not
be used in practice. Shortly we will see a way to improve this algorithm dramatically.
The geometry of the level sets of .u/ is closely related to the eigenstructure of
the matrix A. In the case m D 2 as shown in Figures 4.3 and 4.4, each ellipse can be
characterized by a major and minor axis, as shown in Figure 4.5 for a typical level set.
The points v1 and v2 have the property that the gradient r.vj / lies in the direction that
connects vj to the center u , i.e.,
Avj

f D j .vj

u /

(4.34)

u /

(4.35)

for some scalar j . Since f D Au , this gives
A.vj

i

i

u / D j .vj

i

i

i

i

i

82

“rjlfdm”
2007/6/1
page 82
i

Chapter 4. Iterative Methods for Sparse Linear Systems

u
u1 u

u0

u0
(a)

(b)

Figure 4.4. (a) If A is a scalar multiple of the identity, then the level sets of .u/
are circular and steepest descent converges in one iteration from any initial guess u0 . (b)
If the level sets of .u/ are far from circular, then steepest descent may converge slowly.

v1

u
v2

Figure 4.5. The major and minor axes of the elliptical level set of .u/ point in
the directions of the eigenvectors of A.
and hence each direction vj u is an eigenvector of the matrix A, and the scalar j is an
eigenvalue.
If the eigenvalues of A are distinct, then the ellipse is noncircular and there are two
unique directions for which the relation (4.34) holds, since there are two one-dimensional
eigenspaces. Note that these two directions are always orthogonal since a symmetric matrix

i

i

i

i

i

i

i

4.3. Descent methods and conjugate gradients

“rjlfdm”
2007/6/1
page 83
i

83

A has orthogonal eigenvectors. If the eigenvalues of A are equal, 1 D 2, then every
vector is an eigenvector and the level curves of .u/ are circular. For m D 2 this happens
only if A is a multiple of the identity matrix, as in Figure 4.4(a).
The length of the major and minor axes is related to the magnitude of 1 and 2 .
Suppose that v1 and v2 lie on the level set along which .u/ D 1, for example. (Note that
.u / D 12 uT Au  0, so this is reasonable.) Then
1 T
v Avj
2 j

vjT Au D 1:

Taking the inner product of (4.35) with .vj
kvj

u k22 D

(4.36)

u / and combining with (4.36) yields
2 C uT Au
:
j

Hence the ratio of the length of the major axis to the length of the minor axis is
s
p
kv1 u k2
2
D
D 2 .A/;

kv2 u k2
1

(4.37)

(4.38)

where 1  2 and 2 .A/ is the 2-norm condition number of A. (Recall that in general
2 .A/ D maxj jj j= minj jj j when A is symmetric.)
A multiple of the identity is perfectly conditioned, 2 D 1, and has circular level
sets. Steepest descent converges in one iteration. An ill-conditioned matrix (2  1) has
long skinny level sets, and steepest descent may converge very slowly. The example shown
in Figure 4.4(b) has 2 D 50, which is not particularly ill-conditioned compared to the
matrices that often arise in solving differential equations.
When m > 2 the level sets of .u/ are ellipsoids in m-dimensional space. Again the
eigenvectors of A determine the directions of the principal axes and the spread in the size
of the eigenvalues determines how stretched the ellipse is in each direction.

4.3.2 The A-conjugate search direction
The steepest descent direction can be generalized by choosing a search direction pk 1 in
the kth iteration that might be different from the gradient direction rk 1 . Then we set
uk D uk 1 C ˛k 1 p k 1 ;

(4.39)

where ˛k 1 is chosen to minimize .uk 1 C ˛pk 1 / over all scalars ˛. In other words,
we perform a line search along the line through uk 1 in the direction pk 1 and find the
minimum of  on this line. The solution is at the point where the line is tangent to a contour
line of , and
p T rk 1
˛k 1 D Tk 1
:
(4.40)
pk 1 Apk 1
A bad choice of search direction pk 1 would be a direction orthogonal to rk 1 , since
then pk 1 would be tangent to the level set of  at uk 1 , .u/ could only increase along

i

i

i

i

i

i

i

84

“rjlfdm”
2007/6/1
page 84
i

Chapter 4. Iterative Methods for Sparse Linear Systems

u

u1
u0

Figure 4.6. The CG algorithm converges in two iterations from any initial guess
u0 in the case m D 2. The two search directions used are A-conjugate.
this line, and so uk D uk 1 . But as long as pkT 1 rk 1 ¤ 0, the new point uk will be
different from uk 1 and will satisfy .uk / < .uk 1 /.
Intuitively we might suppose that the best choice for pk 1 would be the direction
of steepest descent rk 1 , but Figure 4.4(b) illustrates that this does not always give rapid
convergence. A much better choice, if we could arrange it, would be to choose the direction
pk 1 to point directly toward the solution u , as shown in Figure 4.6. Then minimizing 
along this line would give uk D u , in which case we would have converged.
Since we don’t know u , it seems there is little hope of determining this direction in
general. But in two dimensions (m D 2) it turns out that we can take an arbitrary initial
guess u0 and initial search direction p0 and then from the next iterate u1 determine the
direction p1 that leads directly to the solution, as illustrated in Figure 4.6. Once we obtain
u1 by the formulas (4.39) and (4.40), we choose the next search direction p1 to be a vector
satisfying
p1T Ap0 D 0:
(4.41)
Below we will show that this is the optimal search direction, leading directly to u2 D u .
When m > 2 we generally cannot converge in two iterations, but we will see below that it
is possible to define an algorithm that converges in at most m iterations to the exact solution
(in exact arithmetic, at least).
Two vectors p0 and p1 that satisfy (4.41) are said to be A-conjugate. For any SPD
matrix A, the vectors u and v are A-conjugate if the inner product of u with Av is zero,
uT Av D 0. If A D I , this just means the vectors are orthogonal, and A-conjugacy is a
natural generalization of the notion of orthogonality. This concept is easily explained in
terms of the ellipses that are level sets of the function .u/ defined by (4.25). Consider

i

i

i

i

i

i

i

4.3. Descent methods and conjugate gradients

“rjlfdm”
2007/6/1
page 85
i

85

an arbitrary point on an ellipse. The direction tangent to the ellipse at this point and the
direction that points toward the center of the ellipse are always A-conjugate. This is the
fact that allows us to determine the direction toward the center once we know a tangent
direction, which has been achieved by the line search in the first iteration. If A D I then
the ellipses are circles and the direction toward the center is simply the radial direction,
which is orthogonal to the tangent direction.
To prove that the two directions shown in Figure 4.6 are A-conjugate, note that the
direction p0 is tangent to the level set of  at u1 and so p0 is orthogonal to the residual
r1 D f Au1 D A.u u1 /, which yields
p0T A.u

u1 / D 0:

(4.42)

On the other hand, u u1 D ˛p1 for some scalar ˛ ¤ 0 and using this in (4.42) gives
(4.41).
Now consider the case m D 3, from which the essential features of the general
algorithm will be more apparent. In this case the level sets of the function .u/ are concentric ellipsoids, two-dimensional surfaces in R3 for which the cross section in any twodimensional plane is an ellipse. We start at an arbitrary point u0 and choose a search
direction p0 (typically p0 D r0 , the residual at u0 ). We minimize .u/ along the onedimensional line u0 C ˛p0, which results in the choice (4.40) for ˛0 , and we set u1 D
u0 C ˛0 p0 . We now choose the search direction p1 to be A-conjugate to p0 . In the previous example with m D 2 this determined a unique direction, which pointed straight to
u . With m D 3 there is a two-dimensional space of vectors p1 that are A-conjugate to p0
(the plane orthogonal to the vector Ap0 ). In the next section we will discuss the full CG
algorithm, where a specific choice is made that is computationally convenient, but for the
moment suppose p1 is any vector that is both A-conjugate to p0 and also linearly independent from p0 . We again use (4.40) to determine ˛1 so that u2 D u1 C ˛1 p1 minimizes
.u/ along the line u1 C ˛p1 .
We now make an observation that is crucial to understanding the CG algorithm for
general m. The two vectors p0 and p1 are linearly independent and so they span a plane
that cuts through the ellipsoidal level sets of .u/, giving a set of concentric ellipses that
are the contour lines of .u/ within this plane. The fact that p0 and p1 are A-conjugate
means that the point u2 lies at the center of these ellipses. In other words, when restricted
to this plane the algorithm so far looks exactly like the m D 2 case illustrated in Figure 4.6.
This means that u2 not only minimizes .u/ over the one-dimensional line u1 C ˛p1
but in fact minimizes .u/ over the entire two-dimensional plane u0 C ˛p0 C ˇp1 for all
choices of ˛ and ˇ (with the minimum occurring at ˛ D ˛0 and ˇ D ˛1 ).
The next step of the algorithm is to choose a new search direction p2 that is Aconjugate to both p0 and p1 . It is important that it be A-conjugate to both the previous
directions, not just the most recent direction. This defines a unique direction (the line
orthogonal to the plane spanned by Ap0 and Ap1 ). We now minimize .u/ over the line
u2 C ˛p2 to obtain u3 D u2 C ˛2 p2 (with ˛2 given by (4.40)). It turns out that this
always gives u3 D u , the center of the ellipsoids and the solution to our original problem
Au D f .
In other words, the direction p2 always points from u2 directly through the center of
the concentric ellipsoids. This follows from the three-dimensional version of the result we
showed above in two dimensions, that the direction tangent to an ellipse and the direction

i

i

i

i

i

i

i

86

“rjlfdm”
2007/6/1
page 86
i

Chapter 4. Iterative Methods for Sparse Linear Systems

toward the center are always A-conjugate. In the three-dimensional case we have a plane
spanned by p0 and p1 and the point u2 that minimized .u/ over this plane. This plane
must be the tangent plane to the level set of .u/ through u2 . This tangent plane is always
A-conjugate to the line connecting u2 to u .
Another way to interpret this process is the following. After one step, u1 minimizes
.u/ over the one-dimensional line u0 C ˛p0 . After two steps, u2 minimizes .u/ over
the two-dimensional plane u0 C ˛p0 C ˇp1 . After three steps, u3 minimizes .u/ over the
three-dimensional space u0 C ˛p0 C ˇp1 C p2 . But this is all of R3 (provided p0 ; p1 ,
and p2 are linearly independent) and so u3 D u0 C ˛0 p0 C ˛1 p1 C ˛2p2 must be the
global minimizer u .
For m D 3 this procedure always converges in at most three iterations (in exact
arithmetic, at least). It may converge to u in fewer iterations. For example, if we happen
to choose an initial guess u0 that lies along one of the axes of the ellipsoids, then r0 will
already point directly toward u , and so u1 D u (although this is rather unlikely).
However, there are certain matrices A for which it will always take fewer iterations
no matter what initial guess we choose. For example, if A is a multiple of the identity
matrix, then the level sets of .u/ are concentric circles. In this case r0 points toward u
from any initial guess u0 and we always obtain convergence in one iteration. Note that in
this case all three eigenvalues of A are equal, 1 D 2 D 3 .
In the “generic” case (i.e., a random SPD matrix A), all the eigenvalues of A are
distinct and three iterations are typically required. An intermediate case is if there are only
two distinct eigenvalues, e.g., 1 D 2 ¤ 3. In this case the level sets of  appear circular
when cut by certain planes but appear elliptical when cut at other angles. As we might
suspect, it can be shown that the CG algorithm always converges in at most two iterations
in this case, from any initial u0 .
This generalizes to the following result for the analogous algorithm in m dimensions:
in exact arithmetic, an algorithm based on A-conjugate search directions as discussed above
converges in at most n iterations, where n is the number of distinct eigenvalues of the matrix
A 2 Rmm .n  m/.

4.3.3 The conjugate-gradient algorithm
In the above description of algorithms based on A-conjugate search directions we required
that each search direction pk be A-conjugate to all previous search directions, but we did
not make a specific choice for this vector. In this section the full “conjugate gradient algorithm” is presented, in which a specific recipe for each pk is given that has very nice
properties both mathematically and computationally. The CG method was first proposed
in 1952 by Hestenes and Stiefel [46], but it took some time for this and related methods to
be fully understood and widely used. See Golub and O’Leary [36] for some history of the
early developments.
This method has the feature mentioned at the end of the previous section: it always
converges to the exact solution of Au D f in a finite number of iterations n  m (in
exact arithmetic). In this sense it is not really an iterative method mathematically. We can
view it as a “direct method” like Gaussian elimination, in which a finite set of operations
produces the exact solution. If we programmed it to always take m iterations, then in
principle we would always obtain the solution, and with the same asymptotic work estimate

i

i

i

i

i

i

i

4.3. Descent methods and conjugate gradients

“rjlfdm”
2007/6/1
page 87
i

87

as for Gaussian elimination (since each iteration takes at most O.m2 / operations for matrixvector multiplies, giving O.m3 / total work). However, there are two good reasons why CG
is better viewed as an iterative method than a direct method:
 In theory it produces the exact solution in n iterations (where n is the number of distinct eigenvalues) but in finite precision arithmetic un will not be the exact solution,
and may not be substantially better than un 1 . Hence it is not clear that the algorithm
converges at all in finite precision arithmetic, and the full analysis of this turns out to
be quite subtle [39].
 On the other hand, in practice CG frequently “converges” to a sufficiently accurate
approximation to u in far less than n iterations. For example, consider solving a
Poisson problem using the 5-point Laplacian on a 100  100 grid, which gives a
linear system of dimension m D 10;000 and a matrix A that has n  5000 distinct eigenvalues. An approximation to u consistent with the truncation error of
the difference formula is obtained after approximately 150 iterations, however (after
preconditioning the matrix appropriately).
That effective convergence often is obtained in far fewer iterations is crucial to the
success and popularity of CG, since the operation count of Gaussian elimination is far too
large for most sparse problems and we wish to use an iterative method that is much quicker.
To obtain this rapid convergence it is often necessary to precondition the matrix, which
effectively moves the eigenvalues around so that they are distributed more conducively for
rapid convergence. This is discussed in Section 4.3.5, but first we present the basic CG
algorithm and explore its convergence properties more fully.
The CG algorithm takes the following form:
Choose initial guess u0 (possibly the zero vector)
r0 D f Au0
p0 D r0
for k D 1; 2; : : :
wk 1 D Apk 1
˛k 1 D .rkT 1 rk 1 /=.pkT 1 wk 1 /
uk D uk 1 C ˛k 1 p k 1
rk D rk 1 ˛k 1 wk 1
if krk k is less than some tolerance then stop
ˇk 1 D .rkT rk /=.rkT 1 rk 1 /
p k D r k C ˇk 1 p k 1
end
As with steepest descent, only one matrix-vector multiply is required at each iteration
in computing wk 1 . In addition, two inner products must be computed each iteration.
(By more careful coding than above, the inner product of each residual with itself can be
computed once and reused twice.) To arrange this, we have used the fact that
pkT 1 rk 1 D rkT 1 rk 1
to rewrite the expression (4.40).

i

i

i

i

i

i

i

88

“rjlfdm”
2007/6/1
page 88
i

Chapter 4. Iterative Methods for Sparse Linear Systems

Compare this algorithm to the steepest descent algorithm presented on page 80. Up
through the convergence check it is essentially the same except that the A-conjugate search
direction pk 1 is used in place of the steepest descent search direction rk 1 in several
places.
The final two lines in the loop determine the next search direction pk . This simple
choice gives a direction pk with the required property that pk is A-conjugate to all the
previous search directions pj for j D 0; 1; ; : : : ; k 1. This is part of the following
theorem, which is similar to Theorem 38.1 of Trefethen and Bau [91], although there it is
assumed that u0 D 0. See also Theorem 2.3.2 in Greenbaum [39].
Theorem 4.1. The vectors generated in the CG algorithm have the following properties,
provided rk ¤ 0 (if rk D 0, then we have converged):
1. pk is A-conjugate to all the previous search directions, i.e., pkT Apj D 0 for j D
0; 1; ; : : : ; k 1.
2. The residual rk is orthogonal to all previous residuals, rkT rj D 0 for j D 0; 1; ; : : : ;
k 1.
3. The following three subspaces of Rm are identical:
span.p0 ; p1 ; p2 ; : : : ; pk 1 /;
span.r0 ; Ar0 ; A2 r0 ; : : : ; Ak 1 r0 /;
2

3

(4.43)

k

span.Ae0 ; A e0 ; A e0 ; : : : ; A e0 /:

The subspace Kk D span.r0 ; Ar0 ; A2 r0 ; : : : ; Ak 1 r0 / spanned by the vector r0
and the first k 1 powers of A applied to this vector is called a Krylov space of dimension
k associated with this vector.
The iterate uk is formed by adding multiples of the search directions pj to the initial
guess u0 and hence must lie in the affine spaces u0 C Kk (i.e., the vector uk u0 is in the
linear space Kk ).
We have seen that the CG algorithm can be interpreted as minimizing the function
.u/ over the space u0 C span.p0 ; p1 ; : : : ; pk 1 / in the kth iteration, and by the theorem
above this is equivalent to minimizing .u/ over the u0 CKk . Many other iterative methods
are also based on the idea of solving problems on an expanding sequence of Krylov spaces;
see Section 4.4.

4.3.4 Convergence of conjugate gradient
The convergence theory for CG is related to the fact that uk minimizes .u/ over the affine
space u0 C Kk defined in the previous section. We now show that a certain norm of the
error is also minimized over this space, which is useful in deriving estimates about the size
of the error and rate of convergence.
Since A is assumed to be SPD, the A-norm defined by
p
kekA D e T Ae
(4.44)

i

i

i

i

i

i

i

4.3. Descent methods and conjugate gradients

“rjlfdm”
2007/6/1
page 89
i

89

satisfies the requirements of a vector norm in Section A.3, as discussed further in Section C.10. This is a natural norm to use because
kek2A D .u

u /T A.u

D uT Au

u /

2uT Au C uT Au

D 2.u/ C u

T

(4.45)



Au :

Since uT Au is a fixed number, we see that minimizing kekA is equivalent to minimizing
.u/.
Since
uk D u0 C ˛0 p 0 C ˛1 p 1 C    C ˛k 1 p k 1 ;
we find by subtracting u that
ek D e0 C ˛0 p0 C ˛1 p1 C    C ˛k 1 pk 1 :
Hence ek e0 is in Kk and by Theorem 4.1 lies in span.Ae0 ; A2 e0 ; : : : ; Ak e0 /. So ek D
e0 C c1 Ae0 C c2 A2 e0 C    C ck Ak e0 for some coefficients c1; : : : ; ck . In other words,
ek D Pk .A/e0 ;

(4.46)

Pk .A/ D I C c1 A C c2 A2 C    C ck Ak

(4.47)

where
is a polynomial in A. For a scalar value x we have
Pk .x/ D 1 C c1 x C c2 x 2 C    C ck x k

(4.48)

and Pk 2 Pk , where
Pk D fpolynomials P .x/ of degree at most k satisfying P .0/ D 1g:

(4.49)

The polynomial Pk constructed implicitly by the CG algorithm solves the minimization
problem
min kP .A/e0 kA :
(4.50)
P 2Pk

To understand how a polynomial function of a diagonalizable matrix behaves, recall that
A D VƒV

1

H)

Aj D Vƒj V

1

;

where V is the matrix of right eigenvectors, and so
Pk .A/ D VPk .ƒ/V
where

1

;

2
6
6
Pk .ƒ/ D 6
4

3
Pk .1 /

7
7
7:
5

Pk .2 /
::

:
Pk .m /

i

i

i

i

i

i

i

90

“rjlfdm”
2007/6/1
page 90
i

Chapter 4. Iterative Methods for Sparse Linear Systems

Note, in particular, that if Pk .x/ has a root at each eigenvalue 1 ; : : : ; m, then Pk .ƒ/
is the zero matrix and so ek D Pk .A/e0 D 0. If A has only n  m distinct eigenvalues
1; : : : ; n , then there is a polynomial Pn 2 Pn that has these roots, and hence the CG
algorithm converges in at most n iterations, as was previously claimed.
To get an idea of how small ke0 kA will be at some earlier point in the iteration, we
will show that for any polynomial P .x/ we have
kP .A/e0 kA
 max jP .j /j
1j m
ke0 kA

(4.51)

and then exhibit one polynomial PQk 2 Pk for which we can use this to obtain a useful
upper bound on kek kA=ke0 kk .
Since A is SPD, the eigenvectors are orthogonal and we can choose the matrix V so
that V 1 D V T and A D VƒV 1 . In this case we obtain
kP .A/e0 k2A D e0T P .A/T AP .A/e0
D e0T VP .ƒ/V T AVP .ƒ/V T e0
D e0T V diag.j P .j /2 /V T e0


 max P .j /2 e0T VƒV T e0 :

(4.52)

1j m

Taking square roots and rearranging results in (4.51).
We will now show that for a particular choice of polynomials PQk 2 Pk we can
evaluate the right-hand side of (4.51) and obtain a bound that decreases with increasing k.
Since the polynomial Pk constructed by CG solves the problem (4.50), we know that
kPk .A/e0 kA  kPQk .A/e0 kA ;
and so this will give a bound for the convergence rate of the CG algorithm.
Consider the case k D 1, after one step of CG. We choose the linear function
2x
;
m C 1

PQ1.x/ D 1

(4.53)

where we assume the eigenvalues are ordered 0 < 1  2      m. A typical case is
shown in Figure 4.7(a). The linear function PQ1.x/ D 1Cc1 x must pass through P1 .0/ D 1
and the slope c1 has been chosen so that
PQ1 .1 / D PQ1 .m /;
which gives
2
:
m C 1
If the slope were made any larger or smaller, then the value of jPQ1./j would increase at
either m or 1, respectively; see Figure 4.7(a). For this polynomial we have
1 C c1 1 D 1

c1m

H)

max jPQ1.j /j D PQ1 .1 / D 1

1j m

 1
D
;
 C1

i

i

c1 D

21
m=1 1
D
m C 1
m =1 C 1

(4.54)

i

i

i

i

i

4.3. Descent methods and conjugate gradients

(a)

91

1.5

1.5

1

1

0.5

0.5

0

0

−0.5

−0.5

−1

−1

−1.5

0

2

4

6

8

10

(b)

“rjlfdm”
2007/6/1
page 91
i

−1.5

0

2

4

6

8

10

Figure 4.7. (a) The polynomial PQ1 .x/ based on a sample set of eigenvalues
marked by dots on the x-axis. (b) The polynomial PQ2.x/ for the same set of eigenvalues.
where  D 2 .A/ is the condition number of A. This gives an upper bound on the reduction
of the error in the first step of the CG algorithm and is the best estimate we can obtain by
knowing only the distribution of eigenvalues of A. The CG algorithm constructs the actual
P1 .x/ based on e0 as well as A and may do better than this for certain initial data. For
example, if e0 D aj vj has only a single eigencomponent, then P1 .x/ D 1 x=j reduces
the error to zero in one step. This is the case where the initial guess lies on an axis of the
ellipsoid and the residual points directly to the solution u D A 1 f . But the above bound
is the best we can obtain that holds for any e0 .
Now consider the case k D 2, after two iterations of CG. Figure 4.7(b) shows the
quadratic function PQ2.x/ that has been chosen so that
PQ2.1 / D

PQ1 ..m C 1 /=2/ D PQ2 .m /:

This function equioscillates at three points in the interval Œ1; m , where the maximum
amplitude is taken. This is the polynomial from P2 that has the smallest maximum value
on this interval, i.e., it minimizes
max

jP .x/j:

1 xm

This polynomial does not necessarily solve the problem of minimizing
max jP .j /j

1j m

unless .1 C m /=2 happens to be an eigenvalue, since we could possibly reduce this
quantity by choosing a quadratic with a slightly larger magnitude near the midpoint of the
interval but a smaller magnitude at each eigenvalue. However, it has the great virtue of
being easy to compute based only on 1 and m. Moreover, we can compute the analogous
polynomial PQk .x/ for arbitrary degree k, the polynomial from PQk with the property of
minimizing the maximum amplitude over the entire interval Œ1 ; m. The resulting maximum amplitude also can be computed in terms of 1 and m and in fact depends only on
the ratio of these and hence depends only on the condition number of A. This gives an
upper bound for the convergence rate of CG in terms of the condition number of A that
often is quite realistic.

i

i

i

i

i

i

i

92

“rjlfdm”
2007/6/1
page 92
i

Chapter 4. Iterative Methods for Sparse Linear Systems

The polynomials we want are simply shifted and scaled versions of the Chebyshev
polynomials discussed in Section B.3.2. Recall that Tk .x/ equioscillates on the interval
Œ 1; 1 with the extreme values ˙1 being taken at k C 1 points, including the endpoints.
We shift this to the interval Œ1 ; m, scale it so that the value at x D 0 is 1, and obtain


1 2x
Tk mC
m 1

 :
PQk .x/ D
(4.55)
m C1
Tk m 
1

For k D 1 this gives (4.53) since T1.x/ D x. We now need only compute
max jPQk .j /j D PQk .1 /

1j m

to obtain the desired bound on kek kA . We have
T .1/
k
 D

PQk .1 / D
Tk

m C1
m 1


Tk

1

:

m C1
m 1

(4.56)

Note that

m C 1
m =1 C 1
 C1
D
D
>1
m 1
m =1 1
 1
so we need to evaluate the Chebyshev polynomial at a point outside the interval Œ 1; 1,
which according to (B.27) is
Tk .x/ D cosh.k cosh 1 x/:
We have

ez C e z
1
D .y C y 1 /
2
2
z
where y D e , so if we make the change of variables x D 12 .y C y 1 /, then cosh 1 x D z
and
e kz C e kz
1
Tk .x/ D cosh.kz/ D
D .y k C y k /:
2
2
We can find y from any given x by solving the quadratic equation y 2 2xy C 1 D 0,
yielding
p
y D x ˙ x 2 1:
cosh.z/ D

To evaluate (4.56) we need to evaluate Tk at x D . C 1/=.
s


 C1
 C1 2
yD
1
˙
 1
 1
p
 C 1 ˙ 4
D
 1
p
.  ˙ 1/2
D p
p
.  C 1/.  1/
p
p
 C1
 1
D p
or p
:
 1
 C1

i

i

1/, where we obtain

(4.57)

i

i

i

i

i

4.3. Descent methods and conjugate gradients

“rjlfdm”
2007/6/1
page 93
i

93

Either choice of y gives the same value for
" p


k  p
k #
 C1
 C1
 1
1
Tk
:
p
C p
D
 1
2
 1
 C1

(4.58)

Using this in (4.56) and combining with (4.51) gives
kP .A/e0 kA
2
ke0 kA

" p
p
k  p
k # 1
k
 C1
 1
 1
p
C p
2 p
:
 1
 C1
 C1

(4.59)

This gives an upper bound on the error when the CG algorithm is used. In practice the
error may be smaller, either because the initial error e0 happens to be deficient in some
eigencoefficients or, more likely, because the optimal polynomial Pk .x/ is much smaller at
all the eigenvalues j than our choice PQk .x/ used to obtain the above bound. This typically
happens if the eigenvalues of A are clustered near fewer than m points. Then the Pk .x/
constructed by CG will be smaller near these points and larger on other parts of the interval
Œ1 ; m where no eigenvalues lie. As an iterative method it is really the number of clusters,
not the number of mathematically distinct eigenvalues, that then determines how rapidly
CG converges in practical terms.
The bound (4.59) is realistic for many matrices, however, and shows that in general
the convergence rate depends on the size of the condition number . If  is large, then
p

k
 1
2 p
2 1
 C1

2
p


k

p

 2e 2k=  ;

(4.60)

and we p
expect that the number of iterations required to reach a desired tolerance will be
k D O. /.
For example, the standard second order discretization of the Poisson problem on
a grid with m points in each direction gives a matrix with  D O.1= h2 /, where h D
1=.m C 1/. The bound (4.60) suggests that CG will require O.m/ iterations to converge,
which is observed in practice. This is true in any number of space dimensions. In one
dimension where there are only m unknowns this does not look very good (and of course
it’s best just to solve the tridiagonal system by elimination). In two dimensions there are m2
unknowns and m2 work per iteration is required to compute Apk 1 , so CG requires O.m3 /
work to converge to a fixed tolerance, which is significantly better than Gauss elimination
and comparable to SOR with the optimal !. Of course for this problem a fast Poisson solver
could be used, requiring only O.m2 log m/ work. But for other problems, such as variable
coefficient elliptic equations with symmetric coefficient matrices, CG may still work very
well while SOR works well only if the optimal ! is found, which may be impossible, and
fast Fourier transform (FFT) methods are inapplicable. Similar comments apply in three
dimensions.

4.3.5 Preconditioners
We saw in Section 4.3.4 that the convergence rate of CG generally depends on the condition number of the matrix A. Often preconditioning the system can reduce the condition

i

i

i

i

i

i

i

94

“rjlfdm”
2007/6/1
page 94
i

Chapter 4. Iterative Methods for Sparse Linear Systems

number of the matrix involved and speed up convergence. In fact preconditioning is absolutely essential for most practical problems, and there are many papers in the literature on
the development of effective preconditioners for specific applications or general classes of
problems.
If M is any nonsingular matrix, then
Au D f

()

M

1

Au D M

1

f:

(4.61)

So we could solve the system on the right instead of the system on the left. If M is some
approximation to A, then M 1 A may have a much smaller condition number than A. If
M D A, then M 1A is perfectly conditioned but we’d still be faced with the problem of
computing M 1 f D A 1 f .
Of course in practice we don’t actually form the matrix M 1A. As we will see
below, the preconditioned conjugate gradient (PCG) algorithm has the same basic form
as CG, but a step is added in which a system of the form M z D r is solved, and it is
here that the preconditioner is “applied.” The idea is to choose an M for which M 1A is
better conditioned than A but for which systems involving M are much easier to solve than
systems involving A. Often this can be done by solving some approximation to the original
physical problem (e.g., by solving on a coarser grid and then interpolating, by solving a
nearby constant-coefficient problem).
A very simple preconditioner that is effective for some problems is simply to use
M D diag(A), a diagonal matrix for which solving linear systems is trivial. This doesn’t
help for the Poisson problem on a rectangle, where this is just a multiple of the identity
matrix, and hence doesn’t change the condition number at all, but for other problems such
as variable coefficient elliptic equations with large variation in the coefficients, this can
make a significant difference.
Another popular approach is to use an incomplete Cholesky factorization of the matrix A, as discussed briefly in Section 4.3.6. Other iterative methods are sometimes used
as a preconditioner, for example, the multigrid algorithm of Section 4.6. Other preconditioners are discussed in many places; for example, there is a list of possible approaches in
Trefethen and Bau [91].
A problem with the approach to preconditioning outlined above is that M 1 A may
not be symmetric, even if M 1 and A are, in which case CG could not be applied to the
system on the right in (4.61). Instead we can consider solving a different system, again
equivalent to the original:
.C T AC 1 /.C u/ D C T f;

(4.62)

where C is a nonsingular matrix. Write this system as
AQuQ D fQ:

(4.63)

Note that since AT D A, the matrix AQ is also symmetric even if C is not. Moreover AQ is
positive definite (provided A is) since
Q D uT C T AC 1 u D .C 1 u/T A.C 1 u/ > 0
uT Au
for any vector u ¤ 0.

i

i

i

i

i

i

i

4.3. Descent methods and conjugate gradients

“rjlfdm”
2007/6/1
page 95
i

95

Now the problem is that it may not be clear how to choose a reasonable matrix C
in this formulation. The goal is to make the condition number of AQ small, but C appears
twice in the definition of AQ so C should be chosen as some sort of “square root” of A. But
note that the condition number of AQ depends only on the eigenvalues of this matrix, and we
can apply a similarity transformation to AQ without changing its eigenvalues, e.g.,
Q D C 1 C T A D .C T C / 1 A:
C 1 AC

(4.64)

The matrix AQ thus has the same condition number as .C T C / 1 A. So if we have a sensible
way to choose a preconditioner M in (4.61) that is SPD, we could in principle determine
C by a Cholesky factorization of the matrix M .
In practice this is not necessary, however. There is a way to write the PCG algorithm
in such a form that it only requires solving systems involving M (without ever computing
C ) but that still corresponds to applying CG to the SPD system (4.63).
To see this, suppose we apply CG to (4.63) and generate vectors uQ k , pQk , wQ k , and rQk .
Now define
uk D C 1 uQ k ;

pk D C 1 pQk ;

wk D C 1 wQ k ; and rk D C T rQk :

Note that rQk is multiplied by C T , not C 1 . Here rQk is the residual when uQ k is used in the
system (4.63). Note that if uQ k approximates the solution to (4.62), then uk will approximate
the solution to the original system Au D f . Moreover, we find that
rk D C.fQ

AQuQ k / D f

Auk

and so rk is the residual for the original system. Rewriting this CG algorithm in terms of
the variables uk ; pk ; wk ; and rk , we find that it can be rewritten as the following PCG
algorithm:
r0 D f Au0
Solve M z0 D r0 for z0
p0 D z0
for k D 1; 2; : : :
wk 1 D Apk 1
˛k 1 D .rkT 1 rk 1 /=.pkT 1 wk 1 /
uk D uk 1 C ˛k 1 p k 1
rk D rk 1 ˛k 1 wk 1
if krk k is less than some tolerance then stop
Solve M zk D rk for zk
ˇk 1 D .zkT rk /=.rkT 1 rk 1 /
pk D zk C ˇk 1 pk 1
end
Note that this is essentially the same as the CG algorithm on page 87, but we solve
the system M zk D rk for zk D M 1 rk in each iteration and then use this vector in place
of rk in two places in the last two lines.

i

i

i

i

i

i

i

96

“rjlfdm”
2007/6/1
page 96
i

Chapter 4. Iterative Methods for Sparse Linear Systems

4.3.6 Incomplete Cholesky and ILU preconditioners
There is one particular preconditioning strategy where the matrix C is in fact computed and
used. Since A is SPD it has a Cholesky factorization of the form A D RT R, where R is
an upper triangular matrix (this is just a special case of the LU factorization). The problem
with computing and using this factorization to solve the original system Au D f is that
the elimination process used to compute R generates a lot of nonzeros in the R matrix, so
that it is typically much less sparse than A.
A popular preconditioner that is often very effective is to do an incomplete Cholesky
factorization of the matrix A, in which nonzeros in the factors are allowed to appear only
in positions where the corresponding element of A is nonzero, simply throwing away the
other elements as we go along. This gives an approximate factorization of the form A 
C T C . This defines a preconditioner M D C T C . To solve systems of the form M z D r
required in the PCG algorithm we use the known Cholesky factorization of M and only
need to do forward and back substitutions for these lower and upper triangular systems.
This approach can be generalized by specifying a drop tolerance and dropping only those
elements of R that are smaller than this tolerance. A smaller drop tolerance will give a
better approximation to A but a denser matrix C .
Methods for nonsymmetric linear systems (e.g., the GMRES algorithm in the next
section) also generally benefit greatly from preconditioners and this idea can be extended
to incomplete LU (ILU) factorizations as a preconditioner for nonsymmetric systems.

4.4

The Arnoldi process and GMRES algorithm

For linear systems that are not SPD, many other iterative algorithms have been developed.
We concentrate here on just one of these, the popular GMRES (generalized minimum residual) algorithm. In the course of describing this method we will also see the Arnoldi process,
which is useful in other applications.
In the kth step of GMRES a least squares problem is solved to find the best approximation to the solution of Au D f from the affine space u0 C Kk , where again Kk is the
k-dimensional Krylov space Kk D span.r0 ; Ar0 ; A2 r0 ; : : : ; Ak 1 r0 / based on the initial
residual r0 D f Au0 . To do this we build up a matrix of the form
Qk D Œq1 q2    qk  2 Rmk ;
whose columns form an orthonormal basis for the space Kk . In the kth iteration we determine the vector qkC1 by starting with some vector vj that is not in Kk and orthogonalizing
it to q1; q2 ; : : : ; qk using a Gram–Schmidt-type procedure. How should we choose vk ?
One obvious choice might be vk D Ak r0 . This is a bad choice, however. The vectors
r0 ; Ar0 ; A2 r0 ; : : :, although linearly independent and a natural choice from our definition of the Krylov space, tend to become more and more closely aligned (nearly linearly
dependent) as k grows. (In fact they converge to the eigenvector direction of the dominant
eigenvalue of A since this is just the power method.) In other words the Krylov matrix
KkC1 D Œr0 Ar0 A2 r0    Ak r0 
has rank k C 1 but has some very small singular values. Applying the orthogonalization
procedure using vk D Ak r0 would amount to doing a QR factorization of the matrix KkC1 ,

i

i

i

i

i

i

i

4.4. The Arnoldi process and GMRES algorithm

“rjlfdm”
2007/6/1
page 97
i

97

which is numerically unstable in this case. Moreover, it is not clear how we would use
the resulting basis to find the least square approximation to Au D f in the affine space
u0 C Kk .
Instead we choose vk D Aqk as the starting point in the kth step. Since qk has already been orthogonalized to all the previous basis vectors, this does not tend to be aligned
with an eigendirection. In addition, the resulting procedure can be viewed as building up
a factorization of the matrix A itself that can be directly used to solve the desired least
squares problem.
This procedure is called the Arnoldi process. This algorithm is important in other
applications as well as in the solution of linear systems, as we will see below. Here is
the basic algorithm, with an indication of where a least squares problem should be solved
in each iteration to compute the GMRES approximations uk to the solution of the linear
system:
q1 D r0 =kr0k2
for k D 1; 2; : : :
v D Aqk
for i D 1 W k
hik D qiT v
v D v hik qi
% orthogonalize to previous vectors
end
hkC1;k D kvk2
qkC1 D v= hkC1;k
% normalize
% For GMRES: Check residual of least squares problem (4.75).
% If it’s sufficiently small, halt and compute uk
end
Before discussing the least squares problem, we must investigate the form of the
matrix factorization we are building up with this algorithm. After k iterations we have
Qk D Œq1 q2    qk  2 Rmk ;

QkC1 D ŒQk qkC1  2 Rm.kC1/ ;

which form orthonormal bases for Kk and KkC1 , respectively. Let
2
h11
6 h21
6
6
Hk D 6
6
4

h12
h22
h32

h13
h23
h33
::
:




::
:

h1;k 1
h2;k 1
h3;k 1
hk;k 1

3
h1k
h2k 7
7
h3k 7
7 2 Rkk
:: 7
: 5
hkk

(4.65)

be the upper Hessenberg matrix consisting of the h values computed so far. We will also
need the matrix HQ k 2 R.kC1/k consisting of Hk with an additional row that is all zeros
except for the hkC1;k entry, also computed in the kth step of Arnoldi.
Now consider the matrix product
AQk D ŒAq1 Aq2    Aqk :

i

i

i

i

i

i

i

98

“rjlfdm”
2007/6/1
page 98
i

Chapter 4. Iterative Methods for Sparse Linear Systems

The j th column of this matrix has the form of the starting vector v used in the j th iteration
of Arnoldi, and unraveling the computations done in the j th step shows that
hj C1;j qj C1 D Aqj

h1j q1

h2j q2



hjj qj :

This can be rearranged to give
Aqj D h1j q1 C h2j q2 C    C hjj qj C hj C1;j qj C1 :

(4.66)

The left-hand side is the j th column of AQk and the right-hand side, at least for j < k, is
the j th column of the matrix Qk Hk . We find that
AQk D Qk Hk C hkC1;k qkC1 ekT :

(4.67)

In the final term the vector ekT D Œ0 0    0 1 is the vector of length k with a 1 in the last
component and hkC1;k qkC1 ekT is the m  k matrix that is all zeros except the last column,
which is hkC1;k qkC1 . This term corresponds to the last term in the expression (4.66) for
j D k. The expression (4.67) can also be rewritten as
AQk D QkC1 HQ k :

(4.68)

If we run the Arnoldi process to completion (i.e., up to k D m, the dimension of
A), then we will find in the final step that v D Aqm lies in the Krylov space Km (which is
already all of Rm ), so orthogonalizing it to each of the qi for i D 1 W m will leave us with
v D 0. So in this final step there is no hmC1;m value or qmC1 vector and setting Q D Qm
and H D Hm gives the result
AQ D QH;
which yields
QT AQ D H

or

A D QHQT :

(4.69)

We have reduced A to Hessenberg form by a similarity transformation.
Our aim at the moment is not to reduce A all the way by running the algorithm to
k D m but rather to approximate the solution to Au D f well in a few iterations. After k
iterations we have (4.67) holding. We wish to compute uk , an approximation to u D A 1 f
from the affine space u0 C Kk , by minimizing the 2-norm of the residual rk D f Auk
over this space. Since the columns of Qk form a basis for Kk , we must have
u k D u 0 C Q k yk

(4.70)

k

for some vector yk 2 R , and so the residual is
rk D f
D r0
D r0

A.u0 C Qk yk /
AQk yk
QkC1 HQ k yk ;

(4.71)

where we have used (4.68). But recall that the first column of QkC1 is just q1 D r0 =kr0k2
so we have r0 D QkC1 , where  is the vector
2
3
kr0 k2
6 0 7
6
7
D6
(4.72)
7 2 RkC1 :
::
4
5
:
0

i

i

i

i

i

i

i

4.4. The Arnoldi process and GMRES algorithm

“rjlfdm”
2007/6/1
page 99
i

99

Hence
rk D QkC1 .

HQ k yk /:

(4.73)

Since QTkC1 QkC1 D I , computing rkT rk shows that
krk k2 D k

HQ k yk k2 :

(4.74)

In the kth iteration of GMRES we choose yk to solve the least squares problem
min k

HQ k yk2 ;

y2Rk

(4.75)

and the approximation uk is then given by (4.70).
Note the following (see, e.g., Greenbaum [39] for details):
 HQ k 2 R.kC1/k and  2 RkC1 , so this is a small least squares problem when k  m.
 HQ k is already nearly upper triangular, so solving the least squares problem by computing the QR factorization of this matrix is relatively cheap.
 Moreover, in each iteration HQ k consists of HQ k 1 with one additional row and column
added. Since the QR factorization of HQ k 1 has already been computed in the previous iteration, the QR factorization of HQ k is easily computed with little additional
work.
 Once the QR factorization is known, it is possible to compute the residual in the least
squares problem (4.75) without actually solving for yk (which requires solving an
upper triangular system of size k using the R matrix from QR). So in practice only
the residual is checked each iteration and the final yk and uk are actually computed
only after the convergence criterion is satisfied.
Notice, however, one drawback of GMRES, and the Arnoldi process more generally,
for nonsymmetric matrices: in the kth iteration we must orthogonalize v to all k previous basis vectors, so we must keep all these vectors in storage. For practical problems
arising from discretizing a multidimensional partial differential equation (PDE), each of
these “vectors” is an approximation to the solution over the full grid, which may consist
of millions of grid points. Taking more than a few iterations may consume a great deal of
storage.
Often in GMRES the iteration is restarted periodically to save storage: the approximation uk at some point is used as the initial guess for a new GMRES iteration. There’s a
large literature on this and other variations of GMRES.

4.4.1 Krylov methods based on three term recurrences
Note that if A is symmetric, then so is the Hessenberg matrix H , since
H T D .QT AQ/T D QT AT Q D QT AQ D H;
and hence H must be tridiagonal. In this case the Arnoldi iteration simplifies in a very
important way: hik D 0 for i D 1; 2; : : : ; .k 2/ and in the kth iteration of Arnoldi v

i

i

i

i

i

i

i

100

“rjlfdm”
2007/6/1
page 100
i

Chapter 4. Iterative Methods for Sparse Linear Systems

only needs to be orthogonalized to the previous two basis vectors. There is a three-term
recurrence relation for each new basis vector in terms of the previous two. This means
only the two previous vectors need to be stored at any time, rather than all the previous qi
vectors, which is a dramatic improvement for systems of large dimension.
The special case of Arnoldi on a symmetric matrix (or more generally a complex
Hermitian matrix) is called the Lanczos iteration and plays an important role in many numerical algorithms, not just for linear systems but also for eigenvalue problems and other
applications.
There are also several iterative methods for nonsymmetric systems of equations that
are based on three-term recurrence relations using the idea of biorthogonalization—in addition to building up a Krylov space based on powers of the matrix A, a second Krylov
space based on powers of the matrix AH is simultaneously determined. Basis vectors vi
and wi for the two spaces are found that are not orthogonal sets separately, but are instead
“biorthogonal” in the sense that
viH wj D 0

if i ¤ j:

It turns out that there are three-term recurrence relations for these sets of basis vectors,
eliminating the need for storing long sequences of vectors. The disadvantage is that two
matrix-vector multiplies must be performed each iteration, one involving A and another
involving AH . One popular method of this form is Bi-CGSTAB (bi-conjugate gradient
stabilized), introduced by Van der Vorst [95]. See, e.g., [39], [91] for more discussion of
this method and other variants.

4.4.2 Other applications of Arnoldi
The Arnoldi process has other applications besides the approximate solution of linear systems. Note from (4.67) that
QTk AQk D Hk
(4.76)
since QTk Qk D I and QTk qkC1 D 0. This looks much like (4.69), but here Qk is a rectangular matrix (for k < m) and so this is not a similarity transformation and Hk does not have
the same eigenvalues as A (or even the same number of eigenvalues, since it has only k).
However, a very useful fact is that the eigenvalues of Hk are typically good approximations
to the dominant eigenvalues of A (those with largest magnitude). In many eigenvalue applications where A is a large sparse matrix, the primary interest is in determining the dominant
eigenvalues (e.g., in determining stability or asymptotic growth properties of matrix iterations or exponentials). In this case we can run the Arnoldi process (which requires only
matrix-vector multiplies with A) and then calculate the eigenvalues of the small matrix Hk
in the kth iteration as an approximation to the dominant eigenvalues of A. This approach
is implemented in the ARPACK software [62], which is used, for example, by the eigs
command in MATLAB.
Also note that from (4.76), by multiplying on the left by Qk and on the right by QTk
we obtain
Qk QTk AQk QTk D Qk Hk QTk :
(4.77)
If k D m, then Qk QTk D I and this is simply (4.69). For k < m, Qk QTk is the projection matrix that projects any vector z in Rm onto the k-dimensional Krylov space Kk .

i

i

i

i

i

i

i

4.5. Newton–Krylov methods for nonlinear problems

“rjlfdm”
2007/6/1
page 101
i

101

So the operator on the left of (4.77), when applied to any vector in z 2 Rm , has the following effect: the vector is first projected to Kk , then A is applied, and then the result
is again projected to Kk . The operator on the right does the same thing in a different
form: QTk z 2 Rk consists of the coefficients of the basis vectors of Qk for the projected
vector. Multiplying by Hk transforms these coefficients according to the effect of A, and
Hk QTk z are then the modified coefficients used to form a linear combination of the basis
vectors when this is multiplied by Qk . Hence we can view Hk as the restriction of A to
the k-dimensional Krylov space Kk . Thus it is not so surprising, for example, that the
eigenvalues of Hk approximate the dominant eigenvalues of A. As commented above, the
basis vectors f; Af; A2 f; : : : for Kk tend to align with the dominant eigenvectors of A,
and if an eigenvector of A lies in Kk , then it is also an eigenvector of the restriction of A
to this space.
We will see another use of Krylov space methods in Section 11.6, where we consider
exponential time differencing methods for time-dependent ordinary differential equations
(ODEs). The matrix exponential applied to a vector, e At v, arises in solving linear systems
of ODEs. This often can be effectively approximated by Qk e Hk t QTk v for k  m. More
generally, other functions .z/ can be extended to matrix arguments (using the Cauchy
integral formula (D.4), for example) and their action often approximated by .A/v 
Qk .Hk t/QTk v.

4.5

Newton–Krylov methods for nonlinear problems

So far in this chapter we have considered only linear problems and a variety of iterative
methods that can be used to solve sparse linear systems of the form Au D f . However,
many differential equations are nonlinear and these naturally give rise to nonlinear systems
of equations after discretization. In Section 2.16 we considered a nonlinear boundary value
problem and discussed the use of Newton’s method for its solution. Recall that Newton’s
method is an iterative method based on linearizing the problem about the current approximation to the solution and then solving a linear system of equations involving the Jacobian
matrix to determine the next update to the approximation. If the nonlinear system is written
as G.u/ D 0, then the Newton update is
uŒj C1 D uŒj 

ı Œj  ;

(4.78)

J Œj  ı Œj  D G.uŒj  /:

(4.79)

where ı Œj  is the solution to the linear system

Here J Œj  D G 0 .uŒj  / is the Jacobian matrix evaluated at the current iterate. For the onedimensional problem of Section 2.16 the Jacobian matrix is tridiagonal and the linear system is easily solved in each iteration by a direct method.
For a nonlinear problem in more space dimensions the Jacobian matrix typically will
have the same nonzero structure as the matrices discussed in the context of linear elliptic
equations in Chapter 3. (Of course for a linear problem Au D f we have G.u/ D Au f
and the matrix A is the Jacobian matrix.) Hence when solving a nonlinear elliptic equation
by a Newton method we must solve, in each Newton iteration, a sparse linear system of
the type we are tackling in this chapter. For practical problems the Jacobian matrix is often

i

i

i

i

i

i

i

102

“rjlfdm”
2007/6/1
page 102
i

Chapter 4. Iterative Methods for Sparse Linear Systems

nonsymmetric and Krylov space methods such as GMRES are a popular choice. This gives
an obvious way to combine Newton’s method with Krylov space methods: in each iteration
of Newton’s method determine all the elements of the Jacobian matrix J Œj  and then apply
a Krylov space method to solve the system (4.79).
However, the term Newton–Krylov method often refers to something slightly different, in which the calculation of the full Jacobian matrix is avoided in performing the
Krylov space iteration. These methods are also called Jacobian–free Newton–Krylov methods (JFNK), and a good survey of these methods and their history and applicability is given
in the review paper of Knoll and Keyes [57].
To explain the basic idea, consider a single iteration of the Newton method and drop
the superscript j for notational convenience. So we need to solve a linear system of the
form
J.u/ı D G.u/;
(4.80)
where u a fixed vector (the current Newton iterate uŒj  ).
When the GMRES algorithm (or any other iterative method requiring only matrix
vector products) is applied to the linear system (4.80), we require only the product J.u/qk
for certain vectors qk (where k is the iteration index of the linear solver). The key to
JFNK is to recognize that since J.u/ is a Jacobian matrix, the vector J.u/qk is simply
the directional derivative of the nonlinear function G at this particular u in the direction
qk . The Jacobian matrix contains all the information needed to compute the directional
derivative in any arbitrary direction, but there is no need to compute the full matrix if, in
the course of the Krylov iteration, we are only going to need the directional derivative in
relatively few directions. This is the case if we hope that the Krylov iteration will converge
in very few iterations relative to the dimension of the system.
How do we compute the directional derivative J.u/qk without knowing J.u/? The
standard approach is to use a simple finite difference approximation,
J.u/qk  .G.u C qk /

G.u//=;

(4.81)

where  is some small real number. This approximation is first order accurate in  but is
sufficiently accurate for the needs of the Krylov space method if we take  quite small.
If  is too small, however, then numerical cancellation can destroy the accuracy of the
approximation in finite precision arithmetic. For scalar problems the optimal trade-off
p
typically occurs at  D mach , the square root of the machine precision (i.e.,   10 8
for 64-bit double precision calculations). See [57] for some comments on good choices.
JFNK is particularly advantageous for problems where the derivatives required in the
Jacobian matrix cannot be easily computed analytically, for example, if the computation of
G.u/ involves table look-ups or requires solving some other nonlinear problem. A subroutine evaluating G.u/ is already needed for a Krylov space method in order to evaluate the
right-hand side of (4.80), and the JFNK method simply calls this in each iteration of the
Krylov method to compute G.u C qk /.
Good preconditioners generally are required to obtain good convergence properties
and limit the number of Krylov iterations (and hence nonlinear G evaluations) required. As
with Newton’s method in other contexts, a good initial guess is often required to achieve
convergence of the Newton iteration, regardless of how the system (4.79) is solved in each
iteration. See [57] for more comments on these and other issues.

i

i

i

i

i

i

i

4.6. Multigrid methods

4.6

“rjlfdm”
2007/6/1
page 103
i

103

Multigrid methods

We return to the solution of linear systems Au D f and discuss a totally different approach
to the solution of such systems. Multigrid methods also can be applied directly to nonlinear
problems and there is a vast literature on variations of these methods and applications to
a variety of problems. Here we concentrate on understanding the main idea of multigrid
methods in the context of the one-dimensional model problem u00.x/ D f .x/. For more
discussion, see, for example, [11], [52], [41], [101].

4.6.1 Slow convergence of Jacobi
Let
f .x/ D 20 C a 00 .x/ cos..x//

a. 0 .x//2 sin..x//;

(4.82)

where a D 0:5, .x/ D 20x 3, and consider the boundary value problem u00.x/ D f .x/
with Dirichlet boundary conditions u.0/ D 1 and u.1/ D 3. The true solution is
10x 2 C a sin..x//;

u.x/ D 1 C 12x

(4.83)

which is plotted in Figure 4.8(a). This function has been chosen because it clearly contains variations on many different spatial scales, i.e., large components of many different
frequencies.
We discretize this problem with the standard tridiagonal systems (2.10) and apply
the Jacobi iterative method of Section 4.1 to the linear initial guess u0 with components
1 C 2xi , which is also shown in Figure 4.8(a). Figure 4.8(b) shows the error e0 in this
initial guess on a grid with m D 255 grid points.
The left column of Figure 4.9 shows the approximations obtained after k D 20; 100,
and 1000 iterations of Jacobi. This method converges very slowly and it would take about
105 iterations to obtain a useful approximation to the solution. However, notice something
very interesting in Figure 4.9. The more detailed features of the solution develop relatively
quickly and it is the larger-scale features that are slow to appear. At first this may seem
counterintuitive since we might expect the small-scale features to be harder to capture.
Approximate solution after 0 Jacobi iterations

Error after 0 Jacobi iterations

5.5

1

5

0.5

4.5
0

4
−0.5

3.5
−1

3
−1.5

2.5
−2

2

−2.5

1.5

(a)

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

(b)

−3

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Figure 4.8. (a) The solution u.x/ (solid line) and initial guess u0 (circles). (b)
The error e0 in the initial guess.

i

i

i

i

i

i

i

104

“rjlfdm”
2007/6/1
page 104
i

Chapter 4. Iterative Methods for Sparse Linear Systems
Approximate solution after 20 Jacobi iterations

Error after 20 Jacobi iterations

5.5

1

5

0.5

4.5
0

4
−0.5

3.5
−1

3
−1.5

2.5
−2

2

−2.5

1.5
1
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

−3
0

0.1

0.2

0.3

Approximate solution after 100 Jacobi iterations

0.4

0.5

0.6

0.7

0.8

0.9

1

0.8

0.9

1

0.8

0.9

1

Error after 100 Jacobi iterations

5.5

1

5

0.5

4.5
0

4
−0.5

3.5
−1

3
−1.5

2.5
−2

2

−2.5

1.5
1
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

−3
0

0.1

0.2

0.3

Approximate solution after 1000 Jacobi iterations

0.4

0.5

0.6

0.7

Error after 1000 Jacobi iterations

5.5

1

5

0.5

4.5
0

4
−0.5

3.5
−1

3
−1.5

2.5
−2

2

−2.5

1.5
1
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

−3
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Figure 4.9. On the left: The solution u.x/ (solid line) and Jacobi iterate uk after
k iterations. On the right: The error ek , shown for k D 20 (top), k D 100 (middle), and
k D 1000 (bottom).
This is easier to understand if we look at the errors shown on the right. The initial error is
highly oscillatory but these oscillations are rapidly damped by the Jacobi iteration, and after
only 20 iterations the error is much smoother than the initial error. After 100 iterations it is
considerably smoother and after 1000 iterations only the smoothest components of the error
remain. This component takes nearly forever to be damped out, and it is this component
that dominates the error and renders the approximate solution worthless.

i

i

i

i

i

i

i

4.6. Multigrid methods

“rjlfdm”
2007/6/1
page 105
i

105

To understand why higher frequency components of the error are damped most rapidly,
recall from Section 4.2 that the error ek D uk D u satisfies
ek D Gek 1 ;
where, for the tridiagonal matrix A,
2

3

0
1=2
6 1=2 0 1=2
6
6
1=2 0
1=2
h2
6
GDIC AD6
:
::
:
6
2
:
:
6
4
1=2

7
7
7
7
7:
::
7
:
7
0 1=2 5
1=2 0

The ith element of ek is simply obtained by averaging the .i 1/ and .i C 1/ elements of
ek 1 and this averaging damps out higher frequencies more rapidly than low frequencies.
This can be quantified by recalling from Section 4.1 that the eigenvectors of G are the same
as the eigenvectors of A. The eigenvector up has components
p

uj D sin.pxj /

.xj D j h; j D 1; 2; : : : ; m/;

(4.84)

while the corresponding eigenvalue is
p D cos.ph/

(4.85)

for p D 1; 2; : : : ; m. If we decompose the initial error e0 into eigencomponents,
e0 D c1 u1 C c2 u2 C    C cm um ;

(4.86)

ek D c1 1k u1 C c2 2k u2 C    C cm mk um :

(4.87)

then we have
Hence the pth eigencomponent decays at the rate pk as k increases. For large k the error
is dominated by the components c1 1k u1 and cm mk um , since these eigenvalues are closest
to 1:

1 2 2
 h :
2
This determines the overall convergence rate, as discussed in Section 4.1.
Other components of the error, however, decay much more rapidly. In fact, for half
the eigenvectors, those with m=4  p  3m=4, the eigenvalue p satisfies
1 D

m 1

1
j p j  p  0:7
2
and j p j20 < 10 3 , so that 20 iterations are sufficient to reduce these components of the
error by a factor of 1000. Decomposing the error e0 as in (4.86) gives a Fourier sine series
representation of the error, since up in (4.84) is simply a discretized version of the sine

i

i

i

i

i

i

i

106

“rjlfdm”
2007/6/1
page 106
i

Chapter 4. Iterative Methods for Sparse Linear Systems

function with frequency p. Hence eigencomponents cp up for larger p represent higherfrequency components of the initial error e0 , and so we see that higher-frequency components decay more rapidly.
Actually it is the middle range of frequencies, those nearest p  m=2, that decay
most rapidly. The highest frequencies p  m decay just as slowly as the lowest frequencies
p  1. The error e0 shown in Figure 4.9 has a negligible component of these highest
frequencies, however, and we are observing the rapid decay of the intermediate frequencies
in this figure.
For this reason Jacobi is not the best method to use in the context of multigrid. A
better choice is underrelaxed Jacobi, where
ukC1 D .1

!/uk C !Guk

(4.88)

with ! D 2=3. The iteration matrix for this method is
G! D .1

!/I C !G

(4.89)

!/ C ! cos.ph/:

(4.90)

with eigenvalues
p D .1

The choice ! D 2=3 minimizes maxm=2<pm j p j, giving optimal smoothing of high
frequencies. With this choice of !, all frequencies above the midpoint p D m=2 have
j p j  1=3.
As a standalone iterative method this would be even worse than Jacobi, since lowfrequency components of the error decay even more slowly ( 1 is now 13 C 23 cos.h/ 
1 13  2h2 ), but in the context of multigrid this does not concern us. What is important is
that the upper half of the range frequencies are all damped by a factor of at least 1=3 per
iteration, giving a reduction by a factor of .1=3/3  0:037 after only three iterations, for
example.

4.6.2 The multigrid approach
We are finally ready to introduce the multigrid algorithm. If we use underrelaxed Jacobi,
then after only three iterations the high-frequency components of the error have already
decayed significantly, but convergence starts to slow down because of the lower-frequency
components. But because the error is now much smoother, we can represent the remaining
part of the problem on a coarser grid. The key idea in multigrid is to switch now to a coarser
grid to estimate the remaining error. This has two advantages. Iterating on a coarser grid
takes less work than iterating further on the original grid. This is nice but is a relatively
minor advantage. Much more important, the convergence rate for some components of the
error is greatly improved by transferring the error to a coarser grid.
For example, consider the eigencomponent p D m=4 that is not damped so much by
underrelaxed Jacobi, m=4  0:8, and after three iterations on this grid this component of
the error is damped only by a factor .0:8/3 D 0:512. The value p D m=4 is not in the
upper half of frequencies that can be represented on a grid with m points—it is right in the
middle of the lower half.
However, if we transfer this function to a grid with only half as many points, it
is suddenly at the halfway point of the frequencies we can represent on the coarser grid

i

i

i

i

i

i

i

4.6. Multigrid methods

“rjlfdm”
2007/6/1
page 107
i

107

(p  mc =2 now, where mc D .m 1/=2 is the number of grid points on the coarser grid).
Hence this same component of the error is damped by a factor of .1=3/3  0:037 after
only three iterations on this coarser grid. This is the essential feature of multigrid.
But how do we transfer the remaining part of the problem to a coarser grid? We don’t
try to solve the original problem on a coarser grid. Instead we solve an equation for the
error. Suppose we have taken  iterations on the original grid and now want to estimate
the error e D u u . This is related to the residual vector r D f Au by the linear
system
Ae D r :
(4.91)
If we can solve this equation for e , then we can subtract e from u to obtain the desired
solution u . The system (4.91) is the one we approximate on a coarsened grid. After taking
a few iterations of Jacobi on the original problem, we know that e is smoother than the
solution u to the original problem, and so it makes sense that we can approximate this
problem well on a coarser grid and then interpolate back to the original grid to obtain the
desired approximation to e . As noted above, iterating on the coarsened version of this
problem leads to much more rapid decay of some components of the error.
The basic multigrid algorithm can be informally described as follows:
1. Take a fixed number of iterations (e.g.,  D 3) of a simple iterative method (e.g.,
underrelaxed Jacobi or another choice of “smoother”) on the original m  m system
Au D f . This gives an approximation u 2 Rm .
2. Compute the residual r D f

Au 2 Rm .

3. Coarsen the residual: approximate the grid function r on a grid with mc D .m 1/=2
points to obtain rQ 2 Rmc .
4. Approximately solve the system AQeQ D rQ , where AQ is the mc  mc version of A
(the tridiagonal approximation to d 2 =dx 2 on a grid with mc points).
5. The vector eQ approximates the error in u but only at mc points on the coarse grid.
Interpolate this grid function back to the original grid with m points to obtain an
approximation to e . Subtract this from u to get a better approximation to u .
6. Using this as a starting guess, take a few more iterations (e.g.,  D 3) of a simple
iterative method (e.g., underrelaxed Jacobi) on the original m  m system Au D f
to smooth out errors introduced by this interpolation procedure.
The real power of multigrid comes from recursively applying this idea. In step 4
of the algorithm above we must approximately solve the linear system AQeQ D rQ of size
mc . As noted, some components of the error that decayed slowly when iterating on the
original system will now decay quickly. However, if mc is still quite large, then there will
be other lower-frequency components of the error that still decay abysmally slowly on this
coarsened grid. The key is to recurse. We only iterate a few times on this problem before
resorting to a coarser grid with .mc 1/=2 grid points to speed up the solution to this
problem. In other words, the entire algorithm given above is applied within step 4 to solve
the linear system AQeQ D rQ . In a recursive programming language (such as MATLAB) this
is not hard to implement and allows one to recurse back as far as possible. If m C 1 is a

i

i

i

i

i

i

i

108

“rjlfdm”
2007/6/1
page 108
i

Chapter 4. Iterative Methods for Sparse Linear Systems
Level 7 After 1 Vcycles

|| Error || = 0.11806

5.5

0.02

5
0

4.5
0.02

4
3.5

0.04

3

0.06

2.5
0.08

2
0.1

1.5

(a) 10

0.2

0.4

0.6

0.8

1

(b) 0.12 0

0.2

0.4

0.6

0.8

1

Figure 4.10. (a) The solution u.x/ (solid line) and approximate solution (circles)
obtained after one V-cycle of the multigrid algorithm with  D 3. (b) The error in this
approximation. Note the change in scale from Figure 4.9(b).
power of 2, then in principle one could recurse all the way back to a coarse grid with only
a single grid point, but in practice the recursion is generally stopped once the problem is
small enough that an iterative method converges very quickly or a direct method such as
Gaussian elimination is easily applied.
Figure 4.10 shows the results obtained when the above algorithm is used starting with
m D 28 1 D 255, using  D 3, and recursing down to a grid with three grid points, i.e.,
seven levels of grids. On each level we apply three iterations of underrelaxed Jacobi, do a
coarse grid correction, and then apply three more iterations of under-relaxed Jacobi. Hence
a total of six Jacobi iterations are used on each grid, and this is done on grids with 2j 1
points for j D 8; 7; 6; 5; 4; 3; 2, since the coarse grid correction at each level requires
doing this recursively at coarser levels. A total of 42 underrelaxed Jacobi iterations are
performed, but most of these are on relatively coarse grids. The total number of grid values
that must be updated in the course of these iterations is
8
X

6

2j  6  29 D 3072;

j D2

roughly the same amount of work as 12 iterations on the original grid would require. But
the improvement in accuracy is dramatic—compare Figure 4.10 to the results in Figure 4.9
obtained by simply iterating on the original grid with Jacobi.
More generally, suppose we start on a grid with m C 1 D 2J points and recurse all
the way down, taking  iterations of Jacobi both before and after the coarse grid correction
on each level. Then the work is proportional to the total number of grid values updated,
which is
J
X
2
2j  42J  4m D O.m/:
(4.92)
j D2

Note that this is linear in the number of grid points m, although as m increases we are using
an increasing number of coarser grids. The number of grids grows at the rate of log2 .m/
but the work on each grid is half as much as the previous finer grid and, so the total work

i

i

i

i

i

i

i

4.6. Multigrid methods

“rjlfdm”
2007/6/1
page 109
i

109

is O.m/. This is the work required for one “V-cycle” of the multigrid algorithm, starting
on the finest grid, recursing down to the coarsest grid and then back up as illustrated in
Figure 4.11(a) and (b). Taking a single V-cycle often results in a significant reduction in
the error, as illustrated in Figure 4.10, but more than one V-cycle might be required to
obtain a sufficiently accurate solution. In fact, it can be shown that for this model problem
O.log.m// V-cycles would be needed to reach a given level of error, so that the total work
would grow like O.m log m/.
We might also consider taking more that one iteration of the cycle on each of the
coarser grids to solve the coarse grid problems within each cycle on the finest grid. Suppose, for example, that we take two cycles at each stage on each of the finer grids. This
gives the W-cycle illustrated in Figure 4.11(c).
Even better results are typically obtained by using the “full multigrid” (FMG) algorithm, which consists of starting the process on the coarsest grid level instead of the finest
grid. The original problem u00 .x/ D f .x/ is discretized and solved on the coarsest level
first, using a direct solver or a few iterations of some iterative method. This approximation
to u.x/ is then interpolated to the next finer grid to obtain a good initial guess for solving
the problem on this grid. The two-level multigrid algorithm is used on this level to solve
the problem. The result is then interpolated to the next-level grid to give good initial data
there, and so on. By the time we get to the finest grid (our original grid, where we want the
solution), we have a very good initial guess to start the multigrid process described above.
This process is illustrated using the V-cycle in Figure 4.12.
This start-up phase of the computation adds relatively little work since it is mostly
iterating on coarser grids. The total work for FMG with one V-cycle is only about 50%
more than for the V-cycle alone. With this initialization process it often turns out that one
V-cycle then suffices to obtain good accuracy, regardless of the number of grid points. In

h-grid
2h-grid
4h-grid
(a)

(b)

(c)

Figure 4.11. (a) One V-cycle with two levels. (b) One V-cycle with three levels.
(c) One W-cycle with three levels.
h-grid
2h-grid
4h-grid
Figure 4.12. FMG with one V-cycle on three levels.

i

i

i

i

i

i

i

110

“rjlfdm”
2007/6/1
page 110
i

Chapter 4. Iterative Methods for Sparse Linear Systems

this case the total work is O.m/, which is optimal. For the example shown in Figure 4.10,
switching to FMG gives an error of magnitude 6  10 3 after a single V-cycle.
Of course in one dimension simply solving the tridiagonal system requires only
O.m/ work and is easier to implement, so this is not so impressive. But the same result carries over to more space dimensions. The FMG algorithm for the Poisson problem
on an m  m grid in two dimensions requires O.m2 / work, which is again optimal since
there are this many unknowns to determine. Recall that fast Poisson solvers based on the
FFT require O.m2 log m/ work, while the best possible direct method would require .m3 /.
Applying multigrid to more complicated problems can be more difficult, but optimal results
of this sort have been achieved for a wide variety of problems.
The multigrid method described here is intimately linked to the finite difference grid
being used and the natural manner in which a vector and matrix can be “coarsened” by
discretizing the same differential operator on a coarser grid. However, the ideas of multigrid
can also be applied to other sparse matrix problems arising from diverse applications where
it may not be at all clear how to coarsen the problem. This more general approach is called
algebraic multigrid (AMG); see, for example, [76], [86].

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 111
i

Part II

Initial Value Problems

i

i

i

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 112
i

i

i

i

i

“rjlfdm”
2007/6/1
page 113
i

Chapter 5

The Initial Value Problem
for Ordinary Differential
Equations
In this chapter we begin a study of time-dependent differential equations, beginning with
the initial value problem (IVP) for a time-dependent ordinary differential equation (ODE).
Standard introductory texts are Ascher and Petzold [5], Lambert [59], [60], and Gear [33].
Henrici [45] gives the details on some theoretical issues, although stiff equations are not
discussed. Butcher [12] and Hairer, Nørsett, and Wanner [43, 44] provide more recent
surveys of the field.
The IVP takes the form
u0 .t/ D f .u.t/; t/ for t > t0

(5.1)

u.t0 / D :

(5.2)

with some initial data

We will often assume t0 D 0 for simplicity.
In general, (5.1) may represent a system of ODEs, i.e., u may be a vector with
s components u1 ; : : : ; us , and then f .u; t/ also represents a vector with components
f1 .u; t/; ; : : : ; fs .u; t/, each of which can be a nonlinear function of all the components
of u.
We will consider only the first order equation (5.1), but in fact this is more general
than it appears since we can reduce higher order equations to a system of first order equations.
Example 5.1. Consider the IVP for the ODE,
v 000.t/ D v 0.t/v.t/

2t.v 00.t//2 for t > 0:

This third order equation requires three initial conditions, typically specified as
v.0/ D 1 ;
v 0 .0/ D 2 ;
v 00 .0/ D 3 :

(5.3)

113

i

i

i

i

i

i

i

114

“rjlfdm”
2007/6/1
page 114
i

Chapter 5. The Initial Value Problem for Ordinary Differential Equations

We can rewrite this as a system of the form (5.1), (5.2) by introducing the variables
u1 .t/ D v.t/;
u2 .t/ D v 0.t/;
u3 .t/ D v 00.t/:
Then the equations take the form
u01 .t/ D u2 .t/;
u02 .t/ D u3 .t/;
u03 .t/ D u1 .t/u2 .t/

2tu23.t/;

which defines the vector function f .u; t/. The initial condition is simply (5.2), where the
three components of  come from (5.3). More generally, any single equation of order m
can be reduced to m first order equations by defining uj .t/ D v .j 1/ .t/, and an mth order
system of s equations can be reduced to a system of ms first order equations.
See Section D.3.1 for an example of how this procedure can be used to determine the
general solution of an r th order linear differential equation.
It is also sometimes useful to note that any explicit dependence of f on t can be
eliminated by introducing a new variable that is simply equal to t. In the above example
we could define
u4 .t/ D t
so that
u04 .t/ D 1 and u4 .t0 / D t0 :
The system then takes the form
u0.t/ D f .u.t//
with

2

3
u2
6
7
u3
7
f .u/ D 6
4 u1 u2 2u4u2 5
3
1

(5.4)

3
1
6 2 7
7
and u.t0 / D 6
4 3 5 :
t0
2

The equation (5.4) is said to be autonomous since it does not depend explicitly on time. It
is often convenient to assume f is of this form since it simplifies notation.

5.1

Linear ordinary differential equations

The system of ODEs (5.1) is linear if
f .u; t/ D A.t/u C g.t/;

(5.5)

where A.t/ 2 Rss and g.t/ 2 Rs . An important special case is the constant coefficient
linear system
u0 .t/ D Au.t/ C g.t/;
(5.6)

i

i

i

i

i

i

i

5.1. Linear ordinary differential equations

“rjlfdm”
2007/6/1
page 115
i

115

where A 2 Rss is a constant matrix. If g.t/  0, then the equation is homogeneous. The
solution to the homogeneous system u0 D Au with data (5.2) is
u.t/ D e A.t t0/ ;

(5.7)

where the matrix exponential is defined as in Appendix D. In the scalar case we often use
 in place of A.

5.1.1 Duhamel’s principle
If g.t/ is not identically zero, then the solution to the constant coefficient system (5.6) can
be written as
Z t
u.t/ D e A.t t0/  C
e A.t  /g./ d:
(5.8)
t0

This is known as Duhamel’s principle. The matrix e A.t  / is the solution operator for the
homogeneous problem; it maps data at time  to the solution at time t when solving the
homogeneous equation. Duhamel’s principle states that the inhomogeneous term g./ at
any instant  has an effect on the solution at time t given by e A.t  /g./. Note that this is
very similar to the idea of a Green’s function for the boundary value problem (BVP).
As a special case, if A D 0, then the ODE is simply
u0.t/ D g.t/

(5.9)

and of course the solution (5.8) reduces to the integral of g:
Z t
u.t/ D  C

g./ d:

(5.10)

t0

As another special case, suppose A is constant and so is g.t/  g 2 Rs . Then (5.8)
reduces to
Z

t

u.t/ D e A.t t0/  C

e A.t  / d g:

(5.11)

t0

This integral can be computed, e.g., by expressing e A.t  / as a Taylor series as in (D.31)
and then integrating term by term. This gives
Z t


e A.t  / d D A 1 e A.t t0/


I

(5.12)


I g:

(5.13)

t0

and so


u.t/ D e A.t t0/  C A 1 e A.t t0/

This may be familiar in the scalar case and holds also for constant coefficient systems
(provided A is nonsingular). This form of the solution is used explicitly in exponential
time differencing methods; see Section 11.6.

i

i

i

i

i

i

i

116

5.2

“rjlfdm”
2007/6/1
page 116
i

Chapter 5. The Initial Value Problem for Ordinary Differential Equations

Lipschitz continuity

In the last section we considered linear ODEs, for which there is always a unique solution.
In most applications, however, we are concerned with nonlinear problems for which there
is usually no explicit formula for the solution. The standard theory for the existence of a
solution to the initial value problem
u0 .t/ D f .u; t/;

u.0/ D 

(5.14)

is discussed in many texts, e.g., [15]. To guarantee that there is a unique solution it is
necessary to require a certain amount of smoothness in the function f .u; t/ of (5.14). We
say that the function f .u; t/ is Lipschitz continuous in u over some domain
D D f.u; t/ W ju

j  a t0  t  t1 g

if there exists some constant L  0 so that
jf .u; t/

f .u ; t/j  Lju

u j

(5.15)

for all .u; t/ and .u ; t/ in D. This is slightly stronger than mere continuity, which only
requires that jf .u; t/ f .u ; t/j ! 0 as u ! u . Lipschitz continuity requires that
jf .u; t/ f .u ; t/j D O.ju u j/ as u ! u .
If f .u; t/ is differentiable with respect to u in D and this derivative fu D @f =@u is
bounded then we can take
L D max jfu.u; t/j;
.u;t /2D

since
f .u; t/ D f .u ; t/ C fu.v; t/.u

u /

for some value v between u and u .
Example 5.2. For the linear problem u0 .t/ D u.t/ C g.t/, f 0 .u/   and we can
take L D jj. This problem of course has a unique solution for any initial data  given by
(5.8) with A D .
In particular, if  D 0 then L D 0. In this case f .u; t/ D g.t/ is independent of u.
The solution is then obtained by simply integrating the function g.t/, as in (5.10).

5.2.1 Existence and uniqueness of solutions
The basic existence and uniqueness theorem states that if f is Lipschitz continuous over
some region D then there is a unique solution to the initial value problem (5.14) at least up
to time T  D min.t1 ; t0 C a=S/, where
S D max jf .u; t/j:
.u;t /2D

Note that this is the maximum modulus of the slope that the solution u.t/ can attain in this
time interval, so that up to time t0 Ca=S we know that u.t/ remains in the domain D where
(5.15) holds.

i

i

i

i

i

i

i

5.2. Lipschitz continuity

“rjlfdm”
2007/6/1
page 117
i

117

Example 5.3. Consider the initial value problem
u0 .t/ D .u.t//2 ;

u.0/ D  > 0:

The function f .u/ D u2 is independent of t and is Lipschitz continuous in u over any
finite interval ju j  a with L D 2. C a/, and the maximum slope over this interval
is S D . C a/2 . The theorem guarantees that a unique solution exists at least up to time
a=.Ca/2 . Since a is arbitrary, we can choose a to maximize this expression, which yields
a D  and so there is a solution at least up to time 1=4.
In fact this problem can be solved analytically and the unique solution is
u.t/ D

1


1

t

:

Note that u.t/ ! 1 as t ! 1=. There is no solution beyond time 1=.
If the function f is not Lipschitz continuous in any neighborhood of some point then
the initial value problem may fail to have a unique solution over any time interval if this
initial value is imposed.
Example 5.4. Consider the initial value problem
p
u0 .t/ D u.t/
with initial condition
u.0/ D 0:
p

The p
function f .u/ D
u is not Lipschitz continuous near u D 0 since f 0 .u/ D
1=.2 u/ ! 1 as u ! 0. We cannot find a constant L so that the bound (5.15) holds for
all u and u near 0.
As a result, this initial value problem does not have a unique solution. In fact it has
two distinct solutions:
u.t/  0
and
u.t/ D

1 2
t :
4

5.2.2 Systems of equations
For systems of s > 1 ordinary differential equations, u.t/ 2 Rs and f .u; t/ is a function
mapping Rs  R ! Rs . We say the function f is Lipschitz continuous in u in some norm
k  k if there is a constant L such that
kf .u; t/

f .u ; t/k  Lku

u k

(5.16)

for all .u; t/ and .u ; ; t/ in some domain D D f.u; t/ W ku k  a; t0  t  t1 g. By
the equivalence of finite-dimensional norms (Appendix A), if f is Lipschitz continuous in
one norm then it is Lipschitz continuous in any other norm, although the Lipschitz constant
may depend on the norm chosen.
The theorems on existence and uniqueness carry over to systems of equations.

i

i

i

i

i

i

i

118

“rjlfdm”
2007/6/1
page 118
i

Chapter 5. The Initial Value Problem for Ordinary Differential Equations
Example 5.5. Consider the pendulum problem from Section 2.16,
 00 .t/ D

sin..t//;

which can be rewritten as a first order system of two equations by introducing v.t/ D  0 .t/:



 

d


v
uD
;
D
:
v
sin./
dt v
Consider the max-norm. We have
ku

u k1 D max.j

  j; jv

v  j/

and
kf .u/
To bound kf .u/

f .u /k1 D max.jv

v  j; j sin./

sin.  /j/:

f .u /k1 , first note that jv

v  j  ku

u k1. We also have

sin.  /j  j

  j  ku

u k1

j sin./

since the derivative of sin./ is bounded by 1. So we have Lipschitz continuity with L D 1:
kf .u/

f .u /k1  ku

u k1:

5.2.3 Significance of the Lipschitz constant
The Lipschitz constant measures how much f .u; t/ changes if we perturb u (at some fixed
time t). Since f .u; t/ D u0 .t/, the slope of the line tangent to the solution curve through
the value u, this indicates how the slope of the solution curve will vary if we perturb u. The
significance of this is best seen through some examples.
Example 5.6. Consider the trivial equation u0.t/ D g.t/, which has Lipschitz constant L D 0 and solutions given by (5.10). Several solution curves are sketched in Figure 5.1. Note that all these curves are “parallel”; they are simply shifted depending on
the initial data. Tangent lines to the curves at any particular time are all parallel since
f .u; t/ D g.t/ is independent of u.
Example 5.7. Consider u0.t/ D u.t/ with  constant and L D jj. Then u.t/ D
u.0/ exp.t/. Two situations are shown in Figure 5.2 for negative and positive values of .
Here the slope of the solution curve does vary depending on u. The variation in the slope
with u (at fixed t) gives an indication of how rapidly the solution curves are converging
toward one another (in the case  < 0) or diverging away from one another (in the case
 > 0). If the magnitude of  were increased, the convergence or divergence would clearly
be more rapid.
The size of the Lipschitz constant is significant if we intend to solve the problem
numerically since our numerical approximation will almost certainly produce a value U n
at time tn that is not exactly equal to the true value u.tn /. Hence we are on a different
solution curve than the true solution. The best we can hope for in the future is that we stay
close to the solution curve that we are now on. The size of the Lipschitz constant gives an
indication of whether solution curves that start close together can be expected to stay close
together or might diverge rapidly.

i

i

i

i

i

i

i

5.2. Lipschitz continuity

“rjlfdm”
2007/6/1
page 119
i

119
10
9
8
7
6
5
4
3
2
1
0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Figure 5.1. Solution curves for Example 5:6, where L D 0.
3

5
4

2
3
2
1
1
0

0
−1

−1
−2
−3
−2
−4

(a)

−3
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

(b)

−5
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Figure 5.2. Solution curves for Example 5:7 with (a)  D 3 and (b)  D 3.

5.2.4 Limitations
Actually, the Lipschitz constant is not the perfect tool for this purpose, since it does not
distinguish between rapid divergence and rapid convergence of solution curves. In both
Figure 5.2(a) and Figure 5.2(b) the Lipschitz constant has the same value L D jj D 3.
But we would expect that rapidly convergent solution curves as in Figure 5.2(a) should be
easier to handle numerically than rapidly divergent ones. If we make an error at some stage,
then the effect of this error should decay at later times rather than growing. To some extent
this is true and as a result error bounds based on the Lipschitz constant may be orders of
magnitude too large in this situation.
However, rapidly converging solution curves can also give serious numerical difficulties, which one might not expect at first glance. This is discussed in detail in Chapter 8,
which covers stiff equations.
One should also keep in mind that a small value of the Lipschitz constant does not
necessarily mean that two solution curves starting close together will stay close together
forever.
Example 5.8. Consider two solutions to the pendulum problem from Example 5.5,
one with initial data
1 .0/ D  ;
v1 .0/ D 0;
and the other with

i

i

i

i

i

i

i

120

“rjlfdm”
2007/6/1
page 120
i

Chapter 5. The Initial Value Problem for Ordinary Differential Equations
2 .0/ D  C ;

v2 .0/ D 0:

The Lipschitz constant is 1 and the data differ by 2, which can be arbitrarily small, and
yet the solutions eventually diverge dramatically, as Solution 1 falls toward  D 0, while
in Solution 2 the pendulum falls the other way, toward  D 2.
In this case the IVP is very ill conditioned: small changes in the data can lead to order
1 changes in the solution. As always in numerical analysis, the solution of ill-conditioned
problems can be very hard to compute accurately.

5.3

Some basic numerical methods

We begin by listing a few standard approaches to discretizing (5.1). Note that the IVP
differs from the BVP considered before in that we are given all the data at the initial time
t0 D 0 and from this we should be able to march forward in time, computing approximations at successive times t1 ; t2 ; : : :. We will use k to denote the time step, so tn D nk for
n  0. It is convenient to use the symbol k, which is different from the spatial grid size
h, since we will soon study PDEs which involve both spatial and temporal discretizations.
Often the symbols t and x are used.
We are given initial data
U0 D 
(5.17)
and want to compute approximations U 1 ; U 2 ; : : : satisfying
U n  u.tn /:
We will use superscripts to denote the time step index, again anticipating the notation of
PDEs where we will use subscripts for spatial indices.
The simplest method is Euler’s method (also called forward Euler), based on replacing u0 .tn / with DC U n D .U nC1 U n /=k from (1.1). This gives the method
U nC1 U n
D f .U n /;
k

n D 0; 1; : : : :

(5.18)

Rather than viewing this as a system of simultaneous equations as we did for the BVP, it is
possible to solve this explicitly for U nC1 in terms of U n :
U nC1 D U n C kf .U n /:

(5.19)

From the initial data U 0 we can compute U 1 , then U 2 , and so on. This is called a timemarching method.
The backward Euler method is similar but is based on replacing u0 .tnC1 / with
nC1
D U
:
U nC1 U n
(5.20)
D f .U nC1 /
k
or
U nC1 D U n C kf .U nC1 /:
(5.21)
Again we can march forward in time since computing U nC1 requires only that we know
the previous value U n . In the backward Euler method, however, (5.21) is an equation that

i

i

i

i

i

i

i

5.4. Truncation errors

“rjlfdm”
2007/6/1
page 121
i

121

must be solved for U nC1 , and in general f .u/ is a nonlinear function. We can view this as
looking for a zero of the function
g.u/ D u

kf .u/

U n;

which can be approximated using some iterative method such as Newton’s method.
Because the backward Euler method gives an equation that must be solved for U nC1 ,
it is called an implicit method, whereas the forward Euler method (5.19) is an explicit
method.
Another implicit method is the trapezoidal method, obtained by averaging the two
Euler methods:
U nC1 U n
1
(5.22)
D .f .U n / C f .U nC1 //:
k
2
As one might expect, this symmetric approximation is second order accurate, whereas the
Euler methods are only first order accurate.
The above methods are all one-step methods, meaning that U nC1 is determined from
n
U alone and previous values of U are not needed. One way to get higher order accuracy
is to use a multistep method that involves other previous values. For example, using the
approximation
u.t C k/ u.t
2k

k/

1
D u0 .t/ C k 2 u000.t/ C O.k 3 /
6

yields the midpoint method (also called the leapfrog method),
U nC1

Un 1

D f .U n /

(5.23)

U nC1 D U n 1 C 2kf .U n /;

(5.24)

2k
or

which is a second order accurate explicit 2-step method. The approximation D2 u from
(1.11), rewritten in the form
3u.t C k/

4u.t/ C u.t
2k

k/

D u0 .t C k/ C

1 2 000
k u .t C k/ C    ;
12

yields a second order implicit 2-step method
3U nC1

4U n C U n 1
D f .U nC1 /:
2k

(5.25)

This is one of the backward differentiation formula (BDF) methods that will be discussed
further in Chapter 8.

5.4

Truncation errors

The truncation error for these methods is defined in the same way as in Chapter 2. We write
the difference equation in the form that directly models the derivatives (e.g., in the form

i

i

i

i

i

i

i

122

“rjlfdm”
2007/6/1
page 122
i

Chapter 5. The Initial Value Problem for Ordinary Differential Equations

(5.23) rather than (5.24)) and then insert the true solution to the ODE into the difference
equation. We then use Taylor series expansion and cancel out common terms.
Example 5.9. The local truncation error (LTE) of the midpoint method (5.23) is
defined by
n D

u.tnC1 /

u.tn 1 /
2k

f .u.tn //



1 2 000
0
4
D u .tn / C k u .tn / C O.k /
6
1
D k 2 u000.tn / C O.k 4 /:
6

u0 .tn /

Note that since u.t/ is the true solution of the ODE, u0 .tn / D f .u.tn //. The O.k 3 / term
drops out by symmetry. The truncation error is O.k 2 / and so we say the method is second
order accurate, although it is not yet clear that the global error will have this behavior. As
always, we need some form of stability to guarantee that the global error will exhibit the
same rate of convergence as the local truncation error. This will be discussed below.

5.5

One-step errors

In much of the literature concerning numerical methods for ODEs, a slightly different definition of the local truncation error is used that is based on the form (5.24), for example,
rather than (5.23). Denoting this value by Ln , we have
Ln D u.tnC1 / u.tn 1 / 2kf .u.tn //
1
D k 3 u000.tn / C O.k 5 /:
3

(5.26)

Since Ln D 2k n , this local error is O.k 3 / rather than O.k 2 /, but of course the global
error remains the same and will be O.k 2 /. Using this alternative definition, many standard
results in ODE theory say that a pth order accurate method should have an LTE that is
O.k pC1 /. With the notation we are using, a pth order accurate method has an LTE that is
O.k p /. The notation used here is consistent with the standard practice for PDEs and leads
to a more coherent theory, but one should be aware of this possible source of confusion.
In this book Ln will be called the one-step error, since this can be viewed as the error
that would be introduced in one time step if the past values U n ; U n 1 ; : : : were all taken
to be the exact values from u.t/. For example, in the midpoint method (5.24) we suppose
that
U n D u.tn / and U n 1 D u.tn 1 /
and we now use these values to compute U nC1 , an approximation to u.tnC1 /:
U nC1 D u.tn 1 / C 2kf .u.tn //
D u.tn 1 / C 2ku0.tn /:
Then the error is
u.tnC1 /

i

i

U nC1 D u.tnC1 /

u.tn 1 /

2ku0.tn / D Ln :

i

i

i

i

i

5.6. Taylor series methods

“rjlfdm”
2007/6/1
page 123
i

123

From (5.26) we see that in one step the error introduced is O.k 3 /. This is consistent with
second order accuracy in the global error if we think of trying to compute an approximation
to the true solution u.T / at some fixed time T > 0. To compute from time t D 0 up to
time T , we need to take T =k time steps of length k. A rough estimate of the error at time
T might be obtained by assuming that a new error of size Ln is introduced in the nth time
step and is then simply carried along in later time steps without affecting the size of future
local errors and without growing or diminishing itself. Then we would expect the resulting
global error at time T to be simply the sum of all these local errors. Since each local error
is O.k 3 / and we are adding up T =k of them, we end up with a global error that is O.k 2 /.
This viewpoint is in fact exactly right for the simplest ODE (5.9), in which f .u; t/ D
g.t/ is independent of u and the solution is simply the integral of g, but it is a bit too
simplistic for more interesting equations since the error at each time feeds back into the
computation at the next step in the case where f .u; t/ depends on u. Nonetheless, it is
essentially right in terms of the expected order of accuracy, provided the method is stable.
In fact, it is useful to think of stability as exactly what is needed to make this naive analysis
correct, by ensuring that the old errors from previous time steps do not grow too rapidly in
future time steps. This will be investigated in detail in the following chapters.

5.6

Taylor series methods

The forward Euler method (5.19) can be derived using a Taylor series expansion of u.tnC1 /
about u.tn /:
1
u.tnC1 / D u.tn / C ku0.tn / C k 2 u00.tn / C    :
(5.27)
2
If we drop all terms of order k 2 and higher and use the differential equation to replace
u0.tn / with f .u.tn /; tn /, we obtain
u.tnC1 /  u.tn / C kf .u.tn /; tn /:
This suggests the method (5.19). The 1-step error is O.k 2 / since we dropped terms of this
order.
A Taylor series method of higher accuracy can be derived by keeping more terms in
the Taylor series. If we keep the first p C 1 terms of the Taylor series expansion
1
1
u.tnC1 /  u.tn / C ku0.tn / C k 2 u00.tn / C    C k p u.p/ .tn /
2
p!
we obtain a pth order accurate method. The problem is that we are given only
u0.t/ D f .u.t/; t/
and we must compute the higher derivatives by repeated differentiation of this function.
For example, we can compute
u00.t/ D fu .u.t/; t/u0 .t/ C ft .u.t/; t/
D fu .u.t/; t/f .u.t/; t/ C ft .u.t/; t/:

i

i

(5.28)

i

i

i

i

i

124

“rjlfdm”
2007/6/1
page 124
i

Chapter 5. The Initial Value Problem for Ordinary Differential Equations

This can result in very messy expressions that must be worked out for each equation,
and as a result this approach is not often used in practice. However, it is such an obvious
approach that it is worth mentioning, and in some cases it may be useful. An example
should suffice to illustrate the technique and its limitations.
Example 5.10. Suppose we want to solve the equation
u0 .t/ D t 2 sin.u.t//:

(5.29)

Then we can compute
u00.t/ D 2t sin.u.t// C t 2 cos.u.t// u0 .t/
D 2t sin.u.t// C t 4 cos.u.t// sin.u.t//:
A second order method is given by
1
U nC1 D U n C k tn2 sin.U n / C k 2Œ2tn sin.U n / C tn4 cos.U n / sin.U n /:
2
Clearly higher order derivatives can be computed and used, but this is cumbersome even for
this simple example. For systems of equations the method becomes still more complicated.
This Taylor series approach does get used in some situations, however—for example,
in the derivation of the Lax–Wendroff method for hyperbolic PDEs; see Section 10.3. See
also Section 11.3.

5.7

Runge–Kutta methods

Most methods used in practice do not require that the user explicitly calculate higher order
derivatives. Instead a higher order finite difference approximation is designed that typically
models these terms automatically.
A multistep method of the sort we will study in Section 5.9 can achieve high accuracy by using high order polynomial interpolation through several previous values of the
solution and/or its derivative. To achieve the same effect with a 1-step method it is typically necessary to use a multistage method, where intermediate values of the solution and
its derivative are generated and used within a single time step.
Example 5.11. A two-stage explicit Runge–Kutta method is given by
1
U  D U n C kf .U n /;
2
U nC1 D U n C kf .U  /:

(5.30)

In the first stage an intermediate value is generated that approximates u.tnC1=2 / via Euler’s
method. In the second step the function f is evaluated at this midpoint to estimate the slope
over the full time step. Since this now looks like a centered approximation to the derivative
we might hope for second order accuracy, as we’ll now verify by computing the LTE.
Combining the two steps above, we can rewrite the method as


1
U nC1 D U n C kf U n C kf .U n / :
2

i

i

i

i

i

i

i

5.7. Runge–Kutta methods

“rjlfdm”
2007/6/1
page 125
i

125

Viewed this way, this is clearly a 1-step explicit method. The truncation error is


1
1
 n D .u.tnC1 / u.tn // f u.tn / C kf .u.tn // :
k
2

(5.31)

Note that




1
1
f u.tn / C kf .u.tn // Df u.tn /C ku0.tn /
2
2
1
1
Df .u.tn //C ku0.tn /f 0 .u.tn //C k 2 .u0 .tn //2 f 00.u.tn //C   :
2
8
Since f .u.tn // D u0 .tn / and differentiating gives f 0.u/u0 D u00 , we obtain


1
1
f u.tn / C kf .u.tn // D u0 .tn / C ku00.tn / C O.k 2 /:
2
2
Using this in (5.31) gives
n D

1
k



1
ku0.tn / C k 2u00.tn / C O.k 3 /
2


1
u0.tn / C ku00.tn / C O.k 2 /
2

D O.k 2 /
and the method is second order accurate. (Check the O.k 2 / term to see that this does not
vanish.)
Remark: Another way to determine the order of accuracy of this simple method is to
apply it to the special test equation u0 D u, which has solution u.tnC1 / D e k u.tn /, and
determine the error on this problem. Here we obtain


1
nC1
n
n
n
U
D U C k U C kU
2
1
D U n C .k/U n C .k/2 U n
2
D e k U n C O.k 3 /:
The one-step error is O.k 3 / and hence the LTE is O.k 2 /. Of course we have checked only
that the LTE is O.k 2 / on one particular function u.t/ D e t , not on all smooth solutions,
and for general Runge–Kutta methods for nonautonomous problems this approach gives
only an upper bound on the method’s order of accuracy. Applying a method to this special
equation is also a fundamental tool in stability analysis—see Chapter 7.
Example 5.12. The Runge–Kutta method (5.30) can be extended to nonautonomous
equations of the form u0.t/ D f .u.t/; t/:
1
U  D U n C kf .U n ; tn /;
2 

k
nC1
n
U
D U C kf U  ; tn C
:
2

i

i

(5.32)

i

i

i

i

i

126

“rjlfdm”
2007/6/1
page 126
i

Chapter 5. The Initial Value Problem for Ordinary Differential Equations

This is again second order accurate, as can be verified by expanding as above, but it is
slightly more complicated since Taylor series in two variables must be used.
Example 5.13. One simple higher order Runge–Kutta method is the fourth order
four-stage method given by
Y1 D U n ;
1
Y2 D U n C kf .Y1 ; tn /;
2


1
k
n
Y3 D U C kf Y2 ; tn C
;
2
2


k
Y4 D U n C kf Y3 ; tn C
;
2



k
k
U nC1 D U n C
f .Y1 ; tn / C 2f Y2 ; tn C
6
2



k
C2f Y3 ; tn C
C f .Y4 ; tn C k/ :
2

(5.33)

Note that if f .u; t/ D f .t/ does not depend on u, then this reduces to Simpson’s rule for
the integral. This method was particularly popular in the precomputer era, when computations were done by hand, because the coefficients are so simple. Today there is no need to
keep the coefficients simple and other Runge–Kutta methods have advantages.
A general r -stage Runge–Kutta method has the form
Y1 D U n C k

r
X

a1j f .Yj ; tn C cj k/;
j D1
r
X

Y2 D U n C k

a2j f .Yj ; tn C cj k/;
j D1

::
:
Yr D U n C k

(5.34)

r
X

arj f .Yj ; tn C cj k/;
j D1

r
X

U nC1 D U n C k

bj f .Yj ; tn C cj k/:
j D1

Consistency requires
r
X

aij D ci ;

i D 1; 2; : : : ; r;

j D1

(5.35)

r
X

bj D 1:
j D1

i

i

i

i

i

i

i

5.7. Runge–Kutta methods

“rjlfdm”
2007/6/1
page 127
i

127

If these conditions are satisfied, then the method will be at least first order accurate.
The coefficients for a Runge–Kutta method are often displayed in a so-called Butcher
tableau:
c1 a11 : : : a1r
::
::
::
:
:
:
(5.36)
cr ar 1 : : : ar r
b1

:::

br

For example, the fourth order Runge–Kutta method given in (5.33) has the following
tableau (entries not shown are all 0):
0
1/2
1/2
1

1/2
0
0

1/2
0

1

1/6

1/3

1/3

1/6

An important class of Runge–Kutta methods consists of the explicit methods for
which aij D 0 for j  i. For an explicit method, the elements on and above the diagonal in the aij portion of the Butcher tableau are all equal to zero, as, for example, with
the fourth order method displayed above. With an explicit method, each of the Yi values is
computed using only the previously computed Yj .
Fully implicit Runge–Kutta methods, in which each Yi depends on all the Yj , can be
expensive to implement on systems of ODEs. For a system of s equations (where each Yi is
in Rs ), a system of sr equations must be solved to compute the r vectors Yi simultaneously.
One subclass of implicit methods that are simpler to implement are the diagonally
implicit Runge–Kutta methods (DIRK methods) in which Yi depends on Yj for j  i, i.e.,
aij D 0 for j > i. For a system of s equations, DIRK methods require solving a sequence
of r implicit systems, each of size s, rather than a coupled set of sr equations as would
be required in a fully implicit Runge–Kutta method. DIRK methods are so named because
their tableau has zero values above the diagonal but possibly nonzero diagonal elements.
Example 5.14. A second order accurate DIRK method is given by
Y1 D U n ;




k
k
f .Y1 ; tn / C f Y2 ; tn C
;
4
2




k
k
Y3 D U n C
f .Y1 ; tn / C f Y2 ; tn C
C f .Y3 ; tn C k/ ;
3
2




k
k
U nC1 D Y3 D U n C
f .Y1 ; tn / C f Y2 ; tn C
C f .Y3 ; tn C k/ :
3
2
Y2 D U n C

(5.37)

This method is known as the TR-BDF2 method and is derived in a different form in Section 8.5. Its tableau is

i

i

i

i

i

i

i

128

“rjlfdm”
2007/6/1
page 128
i

Chapter 5. The Initial Value Problem for Ordinary Differential Equations
0
1/2
1

1/4
1/3

1/4
1/3

1/3

1/3

1/3

1/3

In addition to the conditions (5.35), a Runge–Kutta method is second order accurate
if

r
X

bj cj D
j D1

1
;
2

(5.38)

as is satisfied for the method (5.37). Third order accuracy requires two additional conditions:
r
X

bj cj2 D
j D1
r
r X
X
iD1 j D1

1
;
3
(5.39)

1
bi aij cj D :
6

Fourth order accuracy requires an additional four conditions on the coefficients, and higher
order methods require an exponentially growing number of conditions.
An r -stage explicit Runge–Kutta method can have order at most r , although for r  5
the order is strictly less than the number of stages. Among implicit Runge–Kutta methods,
r -stage methods of order 2r exist. There typically are many ways that the coefficients aij
and bj can be chosen to achieve a given accuracy, provided the number of stages is sufficiently large. Many different classes of Runge–Kutta methods have been developed over
the years with various advantages. The order conditions are quite complicated for higherorder methods and an extensive theory has been developed by Butcher for analyzing these
methods and their stability properties. For more discussion and details see, for example,
[13], [43], [44].
Using more stages to increase the order of a method is an obvious strategy. For some
problems, however, we will also see that it can be advantageous to use a large number of
stages to increase the stability of the method while keeping the order of accuracy relatively
low. This is the idea behind the Runge–Kutta–Chebyshev methods, for example, discussed
in Section 8.6.

5.7.1 Embedded methods and error estimation
Most practical software for solving ODEs does not use a fixed time step but rather adjusts
the time step during the integration process to try to achieve some specified error bound.
One common way to estimate the error in the computation is to compute using two different
methods and compare the results. Knowing something about the error behavior of each
method often allows the possibility of estimating the error in at least one of the two results.
A simple way to do this for ODEs is to take a time step with two different methods,
one of order p and one of a different order, say, p C 1. Assuming that the time step is
small enough that the higher order method is really generating a better approximation, then

i

i

i

i

i

i

i

5.7. Runge–Kutta methods

“rjlfdm”
2007/6/1
page 129
i

129

the difference between the two results will be an estimate of the one-step error in the lower
order method. This can be used as the basis for choosing an appropriate time step for the
lower order approximation. Often the time step is chosen in this manner, but then the higher
order solution is used as the actual approximation at this time and as the starting point for
the next time step. This is sometimes called local extrapolation. Once this is done there is
no estimate of the error, but presumably it is even smaller than the error in the lower order
method and so the approximation generated will be even better than the required tolerance.
For more about strategies for time step selection, see, for example, [5], [43], [78].
Note, however, that the procedure of using two different methods in every time step
could easily double the cost of the computation unless we choose the methods carefully.
Since the main cost in a Runge–Kutta method is often in evaluating the function f .u; t/,
it makes sense to reuse function values as much as possible and look for methods that
provide two approximations to U nC1 of different order based on the same set of function
evaluations, by simply taking different linear combinations of the f .Yj ; tn Ccj k/ values in
the final stage of the Runge–Kutta method (5.34). So in addition to the value U nC1 given
there we would like to also compute a value
r
X

UO nC1 D U n C k

bOj f .Yj ; tn C cj k/

(5.40)

j D1

that gives an approximation of a different order that can be used for error estimation. These
are called embedded Runge–Kutta methods and are often displayed in a tableau of the form
c1
::
:

a11
::
:

:::

a1r
::
:

cr

ar 1

:::

ar r

b1

:::

br

bO1

:::

bOr

(5.41)

As a very simple example, the second order Runge–Kutta method (5.32) could be
combined with the first order Euler method:
Y1 D U n ;
1
Y2 D U n C kf .Y1 ; tn /;
2 

k
nC1
n
U
D U C kf Y2 ; tn C
;
2
UO nC1 D U n C kf .Y1 ; tn /:

(5.42)

Note that the computation of UO nC1 reuses the value f .Y1 ; tn / obtained in computing Y2
and is essentially free. Also note that

i

i

i

i

i

i

i

130

“rjlfdm”
2007/6/1
page 130
i

Chapter 5. The Initial Value Problem for Ordinary Differential Equations
UO nC1



k
Y2 ; tn C
2

0
u .tnC1=2 /


U nC1 D k f .Y1 ; tn /
 k u0.tn /
1
 k 2 u00.tn /;
2

f

(5.43)

which is approximately the one-step error for Euler’s method.
Most software based on Runge–Kutta methods uses embedded methods of higher
order. For example, the ode45 routine in MATLAB uses a pair of embedded Runge-Kutta
methods of order 4 and 5 due to Dormand and Prince [25]. See Shampine and Reichelt [78]
for implementation details (or typeode45 in MATLAB).

5.8

One-step versus multistep methods

Taylor series and Runge–Kutta methods are one-step methods; the approximation U nC1
depends on U n but not on previous values U n 1 ; U n 2 ; : : :. In the next section we will
consider a class of multistep methods where previous values are also used (one example is
the midpoint method (5.24)).
One-step methods have several advantages over multistep methods:
 The methods are self-starting: from the initial data U 0 the desired method can be
applied immediately. Multistep methods require that some other method be used
initially, as discussed in Section 5.9.3.
 The time step k can be changed at any point, based on an error estimate, for example.
The time step can also be changed with a multistep method but more care is required
since the previous values are assumed to be equally spaced in the standard form of
these methods given below.
 If the solution u.t/ is not smooth at some isolated point t  (for example, because
f .u; t/ is discontinuous at t  ), then with a one-step method it is often possible to get
full accuracy simply by ensuring that t  is a grid point. With a multistep method that
uses data from both sides of t  in approximating derivatives, a loss of accuracy may
occur.
On the other hand, one-step methods have some disadvantages. The disadvantage of
Taylor series methods is that they require differentiating the given equation and are cumbersome and often expensive to implement. Runge–Kutta methods only use evaluations of
the function f , but a higher order multistage method requires evaluating f several times
each time step. For simple equations this may not be a problem, but if function values
are expensive to compute, then high order Runge–Kutta methods may be quite expensive
as well. This is particularly true for implicit methods, where an implicit nonlinear system
must be solved in each stage.
An alternative is to use a multistep method in which values of f already computed
in previous time steps are reused to obtain higher order accuracy. Typically only one new
f evaluation is required in each time step. The popular class of linear multistep methods
is discussed in the next section.

i

i

i

i

i

i

i

5.9. Linear multistep methods

5.9

“rjlfdm”
2007/6/1
page 131
i

131

Linear multistep methods

All the methods introduced in Section 5.3 are members of a class of methods called linear
multistep methods (LMMs). In general, an r -step LMM has the form
r
X

˛j U nCj D k

j D0

r
X

ˇj f .U nCj ; tnCj /:

(5.44)

j D0

The value U nCr is computed from this equation in terms of the previous values U nCr 1 ,
U nCr 2 , : : : ; U n and f values at these points (which can be stored and reused if f is
expensive to evaluate).
If ˇr D 0, then the method (5.44) is explicit; otherwise it is implicit. Note that we
can multiply both sides by any constant and have essentially the same method, although
the coefficients ˛j and ˇj would change. The normalization ˛r D 1 is often assumed to
fix this scale factor.
There are special classes of methods of this form that are particularly useful and have
distinctive names. These will be written out for the autonomous case where f .u; t/ D f .u/
to simplify the formulas, but each can be used more generally by replacing f .U nCj / with
f .U nCj ; tnCj / in any of the formulas.
Example 5.15. The Adams methods have the form
U nCr D U nCr 1 C k

r
X

ˇj f .U nCj /:

(5.45)

j D0

These methods all have
˛r D 1; ˛r 1 D 1; and ˛j D 0 for j < r

1:

The ˇj coefficients are chosen to maximize the order of accuracy. If we require ˇr D 0
so the method is explicit, then the r coefficients ˇ0 ; ˇ1 ; : : : ; ˇr 1 can be chosen so that
the method has order r . This can be done by using Taylor series expansion of the local
truncation error and then choosing the ˇj to eliminate as many terms as possible. This
gives the explicit Adams–Bashforth methods.
Another way to derive the Adams–Bashforth methods is by writing
Z tnCr
u.tnCr / D u.tnCr 1 / C
u0.t/ dt
tnCr

Z tnCr

1

D u.tnCr 1 / C

(5.46)
f .u.t// dt

tnCr

1

and then applying a quadrature rule to this integral to approximate
Z tnCr
r 1
X
f .u.t// dt  k
ˇj f .u.tnCj //:
tnCr

1

(5.47)

j D1

This quadrature rule can be derived by interpolating f .u.t// by a polynomial p.t/ of degree
r 1 at the points tn ; tnC1 ; : : : ; tnCr 1 and then integrating the interpolating polynomial.
Either approach gives the same r -step explicit method. The first few are given
below.

i

i

i

i

i

i

i

132

“rjlfdm”
2007/6/1
page 132
i

Chapter 5. The Initial Value Problem for Ordinary Differential Equations
Explicit Adams–Bashforth methods

1-step: U nC1 D U n C kf .U n /

(forward Euler)

2-step: U nC2 D U nC1 C

k
. f .U n / C 3f .U nC1 //
2

3-step: U nC3 D U nC2 C

k
.5f .U n /
12

16f .U nC1 / C 23f .U nC2 //

k
. 9f .U n / C 37f .U nC1 / 59f .U nC2 / C 55f .U nC3 //
24
If we allow ˇr to be nonzero, then we have one more free parameter and so we can
eliminate an additional term in the LTE. This gives an implicit method of order r C 1
called the r -step Adams–Moulton. These methods can again be derived by polynomial
interpolation, now using a polynomial p.t/ of degree r that interpolates f .u.t// at the
points tn ; tnC1 ; : : : ; tnCr and then integrating the interpolating polynomial.
4-step: U nC4 D U nC3 C

Implicit Adams–Moulton methods
k
(trapezoidal method)
.f .U n / C f .U nC1 //
2
k
2-step: U nC2 D U nC1 C . f .U n / C 8f .U nC1 / C 5f .U nC2 //
12

1-step: U nC1 D U n C

3-step: U nC3 D U nC2 C

k
.f .U n /
24

4-step: U nC4 D U nC3 C

k
. 19f .U n / C 106f .U nC1 / 264f .U nC2 /
720
C 646f .U nC3 / C 251f .U nC4 //

5f .U nC1 / C 19f .U nC2 / C 9f .U nC3 //

Example 5.16. The explicit Nyström methods have the form
U nCr D U nCr 2 C k

rX
1

ˇj f .U nCj /

j D0

with the ˇj chosen to give order r . The midpoint method (5.23) is a two-step explicit
Nyström method. A two-step implicit Nyström method is Simpson’s rule,
2k
.f .U n / C 4f .U nC1 / C f .U nC2 //:
6
This reduces to Simpson’s rule for quadrature if applied to the ODE u0.t/ D f .t/.
U nC2 D U n C

5.9.1 Local truncation error
For LMMs it is easy to derive a general formula for the LTE. We have
1
0
r
r
X
1 @X
.tnCr / D
˛j u.tnCj / k
ˇj u0 .tnCj /A :
k
j D0

i

i

j D0

i

i

i

i

i

5.9. Linear multistep methods

“rjlfdm”
2007/6/1
page 133
i

133

We have used f .u.tnCj // D u0.tnCj / since u.t/ is the exact solution of the ODE. Assuming u is smooth and expanding in Taylor series gives
1
u.tnCj / D u.tn / C j ku0 .tn / C .j k/2 u00.tn / C    ;
2
1
u0.tnCj / D u0.tn / C j ku00.tn / C .j k/2 u000.tn / C    ;
2
and so
1

0
.tnCr / D

1@
k

r
X

0

˛j A u.tn / C @

j D0

0

1
r
X

.j ˛j

j D0

1

jˇj A u00.tn /

r 
X

1 2
j ˛j
2
j D0
0
r 
X
1 q
C    C kq 1 @
j ˛j
q!
Ck@

ˇj /A u0 .tn /

j D0

1
.q

1/!

1

j q 1 ˇj A u.q/ .tn / C    :

The method is consistent if  ! 0 as k ! 0, which requires that at least the first two terms
in this expansion vanish:
r
X

r
X

˛j D 0
j D0

r
X

j ˛j D

and
j D0

ˇj :

(5.48)

j D0

If the first p C 1 terms vanish, then the method will be pth order accurate. Note that
these conditions depend only on the coefficients ˛j and ˇj of the method and not on the
particular differential equation being solved.

5.9.2 Characteristic polynomials
It is convenient at this point to introduce the so-called characteristic polynomials ./ and
./ for the LMM:
r
X

./ D
j D0

˛j  j

r
X

and

./ D

ˇj  j :

(5.49)

j D0

The first of these is a polynomial of degree P
r . So is ./ if the method isP
implicit; otherwise
its degree is less than r . Note that .1/ D ˛j and also that 0./ D j ˛j  j 1 , so that
the consistency conditions (5.48) can be written quite concisely as conditions on these two
polynomials:
.1/ D 0
and
0.1/ D .1/:
(5.50)
This, however, is not the main reason for introducing these polynomials. The location of
the roots of certain polynomials related to  and  plays a fundamental role in stability
theory as we will see in the next two chapters.

i

i

i

i

i

i

i

134

“rjlfdm”
2007/6/1
page 134
i

Chapter 5. The Initial Value Problem for Ordinary Differential Equations
Example 5.17. The two-step Adams–Moulton method
U nC2 D U nC1 C

k
. f .U n / C 8f .U nC1 / C 5f .U nC2 //
12

(5.51)

has characteristic polynomials
./ D  2

;

./ D

1
. 1 C 8 C 5 2 /:
12

(5.52)

5.9.3 Starting values
One difficulty with using LMMs if r > 1 is that we need the values U 0 ; U 1 ; : : : ; U r 1
before we can begin to apply the multistep method. The value U 0 D  is known from the
initial data for the problem, but the other values are not and typically must be generated by
some other numerical method or methods.
Example 5.18. If we want to use the midpoint method (5.23), then we need to generate U 1 by some other method before we begin to apply (5.23) with n D 1. We can obtain
U 1 from U 0 using any one-step method, such as Euler’s method or the trapezoidal method,
or a higher order Taylor series or Runge–Kutta method. Since the midpoint method is second order accurate we need to make sure that the value U 1 we generate is sufficiently
accurate so that this second order accuracy will not be lost. Our first impulse might be
to conclude that we need to use a second order accurate method such as the trapezoidal
method rather than the first order accurate Euler method, but this is wrong. The overall
method is second order in either case. The reason that we achieve second order accuracy
even if Euler is used in the first step is exactly analogous to what was observed earlier for
boundary value problems, where we found that we can often get away with one order of
accuracy lower in the local error at a single point than what we have elsewhere.
In the present context this is easiest to explain in terms of the one-step error. The
midpoint method has a one-step error that is O.k 3 / and because this method is applied
in O.T =k/ time steps, the global error is expected to be O.k 2 /. Euler’s method has a
one-step error that is O.k 2 /, but we are applying this method only once.
If U 0 D  D u.0/, then the error in U 1 obtained with Euler will be O.k 2 /. If the
midpoint method is stable, then this error will not be magnified unduly in later steps and
its contribution to the global error will be only O.k 2 /. The overall second order accuracy
will not be affected.
More generally, with an r -step method of order p, we need r starting values
U 0; U 1; : : : ; U r 1
and we need to generate these values using a method that has a one-step error that is
O.k p / (corresponding to an LTE that is O.k p 1 /). Since the number of times we apply
this method (r 1) is independent of k as k ! 0, this is sufficient to give an O.k p / global
error. Of course somewhat better accuracy (a smaller error constant) may be achieved by
using a pth order accurate method for the starting values, which takes little additional work.
In software for the IVP, multistep methods generally are implemented in a form that
allows changing the time step during the integration process, as is often required to efficiently solve the problem. Typically the order of the method is also allowed to vary,

i

i

i

i

i

i

i

5.9. Linear multistep methods

“rjlfdm”
2007/6/1
page 135
i

135

depending on how the solution is behaving. In such software it is then natural to solve the
starting-value problem by initially taking a small time step with a one-step method and then
ramping up to higher order methods and longer time steps as the integration proceeds and
more past data are available.

5.9.4 Predictor-corrector methods
The idea of comparing results obtained with methods of different order as a way to choose
the time step, discussed in Section 5.7.1 for Runge–Kutta methods, is also used with
LMMs. One approach is to use a predictor-corrector method, in which an explicit Adams–
Bashforth method of some order is used to predict a value UO nC1 and then the Adams–
Moulton method of the same order is used to “correct” this value. This is done by using
UO nC1 on the right-hand side of the Adams–Moulton method inside the f evaluation, so
that the Adams–Moulton formula is no longer implicit. For example, the one-step Adams–
Bashforth (Euler’s method) and the one-step Adams–Moulton method (the trapezoidal
method) could be combined into
UO nC1 D U n C kf .U n /;
1
U nC1 D U n C k.f .U n / C f .UO nC1 //:
2

(5.53)

It can be shown that this method is second order accurate, like the trapezoidal method,
but it also generates a lower order approximation and the difference between the two can
be used to estimate the error. The MATLAB routine ode113 uses this approach, with
Adams–Bashforth–Moulton methods of orders 1–12; see [78].

i

i

i

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 136
i

i

i

i

i

“rjlfdm”
2007/6/1
page 137
i

Chapter 6

Zero-Stability and
Convergence for Initial
Value Problems
6.1

Convergence

To discuss the convergence of a numerical method for the initial value problem, we focus
on a fixed (but arbitrary) time T > 0 and consider the error in our approximation to u.T /
computed with the method using time step k. The method converges on this problem if
this error goes to zero as k ! 0. Note that the number of time steps that we need to take
to reach time T increases as k ! 0. If we use N to denote this value (N D T =k), then
convergence means that
lim U N D u.T /:
(6.1)
k!0
N kDT

In principle a method might converge on one problem but not on another, or converge
with one set of starting values but not with another set. To speak of a method being
convergent in general, we require that it converges on all problems in a reasonably large
class with all reasonable starting values. For an r -step method we need r starting values.
These values will typically depend on k, and to make this clear we will write them as
U 0 .k/; U 1 .k/; : : : ; U r 1 .k/. While these will generally approximate u.t/ at the times
t0 D 0; t1 D k, : : : ; tr 1 D .r 1/k, respectively, as k ! 0, each of these times approaches t0 D 0. So the weakest condition we might put on our starting values is that they
converge to the correct initial value  as k ! 0:
lim U  .k/ D  for  D 0; 1; : : : ; r

k!0

1:

(6.2)

We can now state the definition of convergence.
Definition 6.1. An r -step method is said to be convergent if applying the method to any
ODE (5.1) with f .u; t/ Lipschitz continuous in u, and with any set of starting values satisfying (6.2), we obtain convergence in the sense of (6.1) for every fixed time T > 0 at which
the ODE has a unique solution.
To be convergent, a method must be consistent, meaning as before that the local
truncation error (LTE) is o.1/ as k ! 0, and also zero-stable, as described later in this
137

i

i

i

i

i

i

i

138

“rjlfdm”
2007/6/1
page 138
i

Chapter 6. Zero-Stability and Convergence for Initial Value Problems

chapter. We will begin to investigate these issues by first proving the convergence of onestep methods, which turn out to be zero-stable automatically. We start with Euler’s method
on linear problems, then consider Euler’s method on general nonlinear problems and finally
extend this to a wide class of one-step methods.

6.2

The test problem

Much of the theory presented below is based on examining what happens when a method
is applied to a simple scalar linear equation of the form
u0 .t/ D u.t/ C g.t/

(6.3)

with initial data
u.t0 / D :
The solution is then given by Duhamel’s principle (5.8),
Z t
u.t/ D e .t t0 /  C
e .t  / g./ d:

(6.4)

t0

6.3

One-step methods

6.3.1 Euler’s method on linear problems
If we apply Euler’s method to (6.3), we obtain
U nC1 D U n C k.U n C g.tn //
D .1 C k/U n C kg.tn /:
The LTE for Euler’s method is given by


u.tnC1 / u.tn /
n D
.u.tn / C g.tn //
k


1
D u0 .tn / C ku00.tn / C O.k 2 /
u0.tn /
2
1
D ku00.tn / C O.k 2 /:
2

(6.5)

(6.6)

Rewriting this equation as
u.tnC1 / D .1 C k/u.tn / C kg.tn / C k n
and subtracting this from (6.5) gives a difference equation for the global error E n :
E nC1 D .1 C k/E n

k n :

(6.7)

Note that this has exactly the same form as (6.5) but with a different nonhomogeneous term:
 n in place of g.tn /. This is analogous to equation (2.15) in the boundary value theory

i

i

i

i

i

i

i

6.3. One-step methods

“rjlfdm”
2007/6/1
page 139
i

139

and again gives the relation we need between the local truncation error  n (which is easy
to compute) and the global error E n (which we wish to bound). Note again that linearity
plays a critical role in making this connection. We will consider nonlinear problems below.
Because the equation and method we are now considering are both so simple, we
obtain an equation (6.7) that we can explicitly solve for the global error E n . Applying the
recursion (6.7) repeatedly we see what form the solution should take:
E n D .1 C k/E n 1 k n 1
D .1 C k/Œ.1 C k/E n 2
D  :

k n 2 

k n 1

By induction we can easily confirm that in general
E n D .1 C k/n E 0

n
X

.1 C k/n m  m 1 :

k

(6.8)

mD1

(Note that some of the superscripts are powers while others are indices!) This has a form
that is very analogous to the solution (6.4) of the corresponding ordinary differential equation (ODE), where now .1 C k/n m plays the role of the solution operator of the homogeneous problem—it transforms data at time tm to the solution at time tn . The expression
(6.8) is sometimes called the discrete form of Duhamel’s principle.
We are now ready to prove that Euler’s method converges on (6.3). We need only
observe that
j1 C kj  e kjj
(6.9)
and so
.1 C k/n m  e .n m/kjj  e nkjj  e jjT ;

(6.10)

provided that we restrict our attention to the finite time interval 0  t  T , so that tn D
nk  T . It then follows from (6.8) that
!
n
X
jE n j  e jjT jE 0j C k
j m 1 j
(6.11)
mD1



jjT
0
m 1
e
jE j C nk max j
j :
1mn

Let N D T =k be the number of time steps needed to reach time T and set
kk1 D

max

j n j:

0nN 1

From (6.6) we expect
1
kku00k1 D O.k/;
2
where ku00k1 is the maximum value of the function u00 over the interval Œ0; T . Then for
t D nk  T , we have from (6.11) that
kk1 

jE n j  e jjT .jE 0 j C T kk1/:

i

i

i

i

i

i

i

140

“rjlfdm”
2007/6/1
page 140
i

Chapter 6. Zero-Stability and Convergence for Initial Value Problems

If (6.2) is satisfied then E 0 ! 0 as k ! 0. In fact for this one-step method we would
generally take U 0 D u.0/ D , in which case E 0 drops out and we are left with
jE n j  e jjT T kk1 D O.k/ as k ! 0

(6.12)

and hence the method converges and is in fact first order accurate.
Note where stability comes into the picture. The one-step error Lm 1 D k m 1
introduced in the mth step contributes the term .1 C k/n m Lm 1 to the global error. The
fact that j.1 C k/n m j < e jjT is uniformly bounded as k ! 0 allows us to conclude that
each contribution to the final error can be bounded in terms of its original size as a one-step
error. Hence the “naive analysis” of Section 5.5 is valid, and the global error has the same
order of magnitude as the local truncation error.

6.3.2 Relation to stability for boundary value problems
To see how this ties in with the definition of stability used in Chapter 2 for the BVP, it
may be useful to view Euler’s method as giving a linear system in matrix form, although
this is not the way it is used computationally. If we view the equations (6.5) for n D 0;
1; : : : ; N 1 as a linear system AU D F for U D ŒU 1 ; U 2 ; : : : ; U N T , then
2
3
1
6 .1 C k/
7
1
6
7
6
7
.1
C
k/
1
16
7
AD 6
7
:
:
6
7
k
:
6
7
4
5
.1 C k/
1
.1 C k/ 1
and

2

U1
U2
U3
::
:

3

6
7
6
7
6
7
6
7
U D6
7;
6
7
6
7
4 UN 1 5
UN

2

3
.1=k C /U 0 C g.t0 /
6
7
g.t1 /
6
7
6
7
g.t
/
2
6
7
F D6
7:
::
6
7
:
6
7
4
5
g.tN 2 /
g.tN 1 /

We have divided both sides of (6.5) by k to conform to the notation of Chapter 2. Since
the matrix A is lower triangular, this system is easily solved by forward substitution, which
results in the iterative equation (6.5).
If we now let UO be the vector obtained from the true solution as in Chapter 2, then
subtracting AUO D F C  from AU D F , we obtain (2.15) (the matrix form of (6.7)) with
solution (6.8). We are then in exactly the same framework as in Chapter 2. So we have
convergence and a global error with the same magnitude as the local error provided that
the method is stable in the sense of Definition 2.1, i.e., that the inverse of the matrix A is
bounded independent of k for all k sufficiently small.
The inverse of this matrix is easy to compute. In fact we can see from the solution
(6.8) that

i

i

i

i

i

i

i

6.3. One-step methods
2
6
6
6
6
1
A D k6
6
6
4

“rjlfdm”
2007/6/1
page 141
i

141
3

1
.1 C k/
.1 C k/2
.1 C k/3
::
:

1
.1 C k/
.1 C k/2

.1 C k/N 1

.1 C k/N 2

1
.1 C k/

1
::

.1 C k/N 3



:
.1 C k/ 1

7
7
7
7
7:
7
7
5

We easily compute using (A.10a) that
kA 1 k1 D k

N
X

j.1 C k/N mj

mD1

and so
kA 1 k1  kN e jjT D T e jjT :
This is uniformly bounded as k ! 0 for fixed T . Hence the method is stable and kEk1 
kA 1 k1 kk1  T e jjT kk1, which agrees with the bound (6.12).

6.3.3 Euler’s method on nonlinear problems
So far we have focused entirely on linear equations. Practical problems are almost always
nonlinear, but for the initial value problem it turns out that it is not significantly more difficult to handle this case if we assume that f .u/ is Lipschitz continuous, which is reasonable
in light of the discussion in Section 5.2.
Euler’s method on u0 D f .u/ takes the form
U nC1 D U n C kf .U n /

(6.13)

and the truncation error is defined by
1
.u.tnC1 / u.tn //
k
1
D ku00.tn / C O.k 2 /;
2

n D

f .u.tn //

just as in the linear case. So the true solution satisfies
u.tnC1 / D u.tn / C kf .u.tn // C k n
and subtracting this from (6.13) gives
E nC1 D E n C k.f .U n /

f .u.tn ///

k n :

(6.14)

In the linear case f .U n / f .u.tn // D E n and we get the relation (6.7) for E n . In the
nonlinear case we cannot express f .U n / f .u.tn // directly in terms of the error E n in
general. However, using the Lipschitz continuity of f we can get a bound on this in terms
of E n :
jf .U n / f .u.tn //j  LjU n u.tn /j D LjE n j:

i

i

i

i

i

i

i

142

“rjlfdm”
2007/6/1
page 142
i

Chapter 6. Zero-Stability and Convergence for Initial Value Problems

Using this in (6.14) gives
jE nC1 j  jE n j C kLjE nj C kj n j D .1 C kL/jE n j C kj n j:

(6.15)

From this inequality we can show by induction that
n
X

jE n j  .1 C kL/n jE 0 j C k

.1 C kL/n m j m 1 j
mD1

and so, using the same steps as in obtaining (6.12) (and again assuming E 0 D 0), we obtain
jE n j  e LT T kk1 D O.k/ as k ! 0

(6.16)

for all n with nk  T , proving that the method converges. In the linear case L D jj and
this reduces to exactly (6.12).

6.3.4 General one-step methods
A general explicit one-step method takes the form
U nC1 D U n C k‰.U n ; tn ; k/

(6.17)

for some function ‰, which depends on f of course. We will assume that ‰.u; t; k/ is
continuous in t and k and Lipschitz continuous in u, with Lipschitz constant L0 that is
generally related to the Lipschitz constant of f .
Example 6.1. For the two-stage Runge–Kutta method of Example 5.11, we have


1
‰.u; t; k/ D f u C kf .u/ :
(6.18)
2
If f is Lipschitz continuous with Lipschitz constant L, then ‰ has Lipschitz constant
L0 D L C 12 kL2.
The one-step method (6.17) is consistent if
‰.u; t; 0/ D f .u; t/
for all u; t, and ‰ is continuous in k. The local truncation error is


u.tnC1 / u.tn /
n
 D
‰.u.tn /; tn ; k/:
k
We can show that any one-step method satisfying these conditions is convergent. We
have
u.tnC1 / D u.tn / C k‰.u.tn /; tn ; k/ C k n
and subtracting this from (6.17) gives
E nC1 D E n C k .‰.U n ; tn ; k/

‰.u.tn /; tn ; k//

k n :

Using the Lipschitz condition we obtain
jE nC1 j  jE n j C kL0jE n j C kj n j:
This has exactly the same form as (6.15) and the proof of convergence proceeds exactly as
from there.

i

i

i

i

i

i

i

6.4. Zero-stability of linear multistep methods

6.4

“rjlfdm”
2007/6/1
page 143
i

143

Zero-stability of linear multistep methods

The convergence proof of the previous section shows that for one-step methods, each one0
step error k m 1 has an effect on the global error that is bounded by e L T jk m 1j. Al0
though the error is possibly amplified by a factor e L T , this factor is bounded independent
of k as k ! 0. Consequently the method is stable: the global error can be bounded in
terms of the sum of all the one-step errors and hence has the same asymptotic behavior as
the LTE as k ! 0. This form of stability is often called zero-stability in ODE theory, to
distinguish it from other forms of stability that are of equal importance in practice. The
fact that a method is zero-stable (and converges as k ! 0) is no guarantee that it will give
reasonable results on the particular grid with k > 0 that we want to use in practice. Other
“stability” issues of a different nature will be taken up in the next chapter.
But first we will investigate the issue of zero-stability for general LMMs, where the
theory of the previous section does not apply directly. We begin with an example showing
a consistent LMM that is not convergent. Examining what goes wrong will motivate our
definition of zero-stability for LMMs.
Example 6.2. The LMM
U nC2

3U nC1 C 2U n D kf .U n /

(6.19)

has an LTE given by
n D

1
Œu.tnC2 /
k

3u.tnC1 / C 2u.tn / C ku0.tn / D

5 00
ku .tn / C O.k 2 /;
2

so the method is consistent and “first order accurate.” But in fact the global error will not
exhibit first order accuracy, or even convergence, in general. This can be seen even on the
trivial initial-value problem
u0 .t/ D 0; u.0/ D 0
(6.20)
with solution u.t/  0. In this problem, equation (6.19) takes the form
U nC2

3U nC1 C 2U n D 0:

(6.21)

We need two starting values U 0 and U 1 . If we take U 0 D U 1 D 0, then (6.21) generates
U n D 0 for all n and in this case we certainly converge to correct solution, and in fact we
get the exact solution for any k.
But in general we will not have the exact value U 1 available and will have to approximate this, introducing some error into the computation. Table 6.1 shows results obtained
by applying this method with starting data U 0 D 0, U 1 D k. Since U 1 .k/ ! 0 as k ! 0,
this is valid starting data in the context of Definition 6.1 of convergence. If the method is
convergent, we should see that U N , the computed solution at time T D 1, converges to
zero as k ! 0. Instead it blows up quite dramatically. Similar results would be seen if we
applied this method to an arbitrary equation u0 D f .u/ and used any one-step method to
compute U 1 from U 0 .
The homogeneous linear difference equation (6.21) can be solved explicitly for U n
in terms of the starting values U 0 and U 1 . We obtain
U n D 2U 0

i

i

U 1 C 2n .U 1

U 0 /:

(6.22)

i

i

i

i

i

144

“rjlfdm”
2007/6/1
page 144
i

Chapter 6. Zero-Stability and Convergence for Initial Value Problems

Table 6.1. Solution U N to (6.21) with U 0 D 0, U 1 D k and various values of k D 1=N .
N

UN

5
10
20

6.2
1023
5:4  104

It is easy to verify that this satisfies (6.21) and also the starting values. (We’ll see how to
solve general linear difference equations in the next section.)
Since u.t/ D 0, the error is E n D U n and we see that any initial errors in U 1 or
0
U are magnified by a factor 2n in the global error (except in the special case U 1 D U 0 ).
This exponential growth of the error is the instability that leads to nonconvergence. To rule
out this sort of growth of errors, we need to be able to solve a general linear difference
equation.

6.4.1 Solving linear difference equations
We briefly review one solution technique for linear difference equations; see Section D.2.1
for a different approach. Consider the general homogeneous linear difference equation
r
X

˛j U nCj D 0:

(6.23)

j D0

Eventually we will look for a particular solution satisfying given initial conditions
U 0; U 1; : : : ; U r 1;
but to begin with we will find the general solution of the difference equation in terms of r
free parameters. We will hypothesize that this equation has a solution of the form
U n D n

(6.24)

for some value of  (here  n is the nth power!). Plugging this into (6.23) gives
r
X

˛j  nCj D 0

j D0

and dividing by  n yields
r
X

˛j  j D 0:

(6.25)

j D0

We see that (6.24) is a solution of the difference equation if  satisfies (6.25), i.e., if  is a
root of the polynomial
r
X
./ D
˛j  j :
j D0

i

i

i

i

i

i

i

6.4. Zero-stability of linear multistep methods

“rjlfdm”
2007/6/1
page 145
i

145

Note that this is just the first characteristic polynomial of the LMM introduced in (5.49). In
general ./ has r roots 1 ; 2; : : : ; r and can be factored as
./ D ˛r .

1 /.

2 /    .

r /:

Since the difference equation is linear, any linear combination of solutions is again a
solution. If 1 ; 2 ; : : : ; r are distinct (i ¤ j for i ¤ j ), then the r distinct solutions in
are linearly independent and the general solution of (6.23) has the form
U n D c1 1n C c2 2n C    C cr rn ;

(6.26)

where c1 ; : : : ; cr are arbitrary constants. In this case, every solution of the difference
equation (6.23) has this form. If initial conditions U 0 ; U 1 ; : : : ; U r 1 are specified, then
the constants c1 ; : : : ; cr can be uniquely determined by solving the r  r linear system
c1 C c2 C    C cr D U 0 ;
c1 1 C c2 2 C    C cr r D U 1 ;
::: :::
c11r 1 C c2 2r 1 C    C cr rr 1 D U r 1 :

(6.27)

Example 6.3. The characteristic polynomial for the difference equation (6.21) is
./ D 2

3 C  2 D .

1/.

2/

(6.28)

with roots 1 D 1; 2 D 2. The general solution has the form
U n D c1 C c2  2n
and solving for c1 and c2 from U 0 and U 1 gives the solution (6.22).
This example indicates that if ./ has any roots that are greater than one in modulus, the method will not be convergent. It turns out that the converse is nearly true: if all
the roots have modulus no greater than one, then the method is convergent, with one proviso. There must be no repeated roots with modulus equal to one. The next two examples
illustrate this.
If the roots are not distinct, say, 1 D 2 for simplicity, then 1n and 2n are not
linearly independent and the U n given by (6.26), while still a solution, is not the most
general solution. The system (6.27) would be singular in this case. In addition to 1n there
is also a solution of the form n1n and the general solution has the form
U n D c1 1n C c2n1n C c3 3n C    C cr rn :
If in addition 3 D 1 , then the third term would be replaced by c3 n2 1n . Similar modifications are made for any other repeated roots. Note how similar this theory is to the standard
solution technique for an r th order linear ODE.
Example 6.4. Applying the consistent LMM
U nC2

i

i

2U nC1 C U n D

1
k.f .U nC2 /
2

f .U n //

(6.29)

i

i

i

i

i

146

“rjlfdm”
2007/6/1
page 146
i

Chapter 6. Zero-Stability and Convergence for Initial Value Problems

to the differential equation u0.t/ D 0 gives the difference equation
U nC2

2U nC1 C U n D 0:

The characteristic polynomial is
./ D  2

2 C 1 D .

1/2

(6.30)

so 1 D 2 D 1. The general solution is
U n D c1 C c2n:
For particular starting values U 0 and U 1 the solution is
U n D U 0 C .U 1

U 0 /n:

Again we see that the solution grows with n, although not as dramatically as in Example 6.2
(the growth is linear rather than exponential). But this growth is still enough to destroy
convergence. If we take the same starting values as before, U 0 D 0 and U 1 D k, then
U n D k n and so
lim U N D kN D T:
k!0
N kDT

The method converges to the function v.t/ D t rather than to u.t/ D 0, and hence the
LMM (6.29) is not convergent.
This example shows that if ./ has a repeated root of modulus 1, then the method
cannot be convergent.
Example 6.5. Now consider the consistent LMM
U nC3

5
2U nC2 C U nC1
4

1 n
1
U D hf .U n /:
4
4

(6.31)

Applying this to (6.20) gives
U nC3

5
2U nC2 C U nC1
4

1 n
U D0
4

and the characteristic polynomial is
./ D  3

5
2 2 C 
4

1
D .
4

1/.

0:5/2 :

(6.32)

So 1 D 1; 2 D 3 D 1=2 and the general solution is
 n
 n
1
1
n
U D c1 C c2
C c3n
:
2
2
Here there is a repeated root but with modulus less than 1. The linear growth of n is then
overwhelmed by the decay of .1=2/n .
For this three-step method we need three starting values U 0 ; U 1 , U 2 and we can
find c1 ; c2 ; c3 in terms of them by solving a linear system similar to (6.27). Each ci will

i

i

i

i

i

i

i

6.4. Zero-stability of linear multistep methods

“rjlfdm”
2007/6/1
page 147
i

147

be a linear combination of U 0 ; U 1 ; U 2 and so if U  .k/ ! 0 as k ! 0, then ci .k/ ! 0
as k ! 0 also. The value U N computed at time T with step size k (where kN D T ) has
the form
 N
 N
1
1
U N D c1 .k/ C c2 .k/
C c3.k/N
:
(6.33)
2
2
Now we see that
lim U N D 0

k!0
N kDT

and so the method (6.31) converges on u0 D 0 with arbitrary starting values U  .k/ satisfying U  .k/ ! 0 as k ! 0. (In fact, this LMM is convergent in general.)
More generally, if ./ has a root j that is repeated m times, then U N will involve
terms of the form N s jN for s D 0; 1; : : : ; m 1. This converges to zero as N ! 1
provided jj j < 1. The algebraic growth of N s is overwhelmed by the exponential decay
of jN . This shows that repeated roots are not a problem as long as they have magnitude
strictly less than 1.
With the above examples as motivation, we are ready to state the definition of zerostability.
Definition 6.2. An r-step LMM is said to be zero-stable if the roots of the characteristic
polynomial ./ defined by (5.49) satisfy the following conditions:
jj j  1 for j D 1; 2; ; : : : ; r:
(6.34)
If j is a repeated root, then jj j < 1:
If the conditions (6.34) are satisfied for all roots of , then the polynomial is said to
satisfy the root condition.
Example 6.6. The Adams methods have the form
U nCr D U nCr 1 C k

r
X

ˇj f .U nCj /

j D1

and hence
./ D  r

 r 1 D .

1/ r 1 :

The roots are 1 D 1 and 2 D    D r D 0. The root condition is clearly satisfied and all
the Adams–Bashforth and Adams–Moulton methods are zero-stable.
The given examples certainly do not prove that zero-stability as defined above is a
sufficient condition for convergence. We looked at only the simplest possible ODE u0.t/ D
0 and saw that things could go wrong if the root condition is not satisfied. It turns out,
however, that the root condition is all that is needed to prove convergence on the general
initial value problem (in the sense of Definition 6.1).
Theorem 6.3 (Dahlquist [22]). For LMMs applied to the initial value problem for u0.t/ D
f .u.t/; t/,
consistency C zero-stability () convergence:
(6.35)

i

i

i

i

i

i

i

148

“rjlfdm”
2007/6/1
page 148
i

Chapter 6. Zero-Stability and Convergence for Initial Value Problems

This is the analogue of the statement (2.21) for the BVP. A proof of this result can be
found in [43].
Note: A consistent LMM always has one root equal to 1, say, 1 D 1, called the
principal root. This follows from (5.50). Hence a consistent one-step LMM (such as
Euler, backward Euler, trapezoidal) is certainly zero-stable. More generally we have proved
in Section 6.3.4 that any consistent one-step method (that is a Lipschitz continuous) is
convergent. Such methods are automatically “zero-stable” and behave well as k ! 0. We
can think of zero-stability as meaning “stable in the limit as k ! 0.”
Although a consistent zero-stable method is convergent, it may have other stability
problems that show up if the time step k is chosen too large in an actual computation.
Additional stability considerations are the subject of the next chapter.

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 149
i

Chapter 7

Absolute Stability for
Ordinary Differential
Equations
7.1

Unstable computations with a zero-stable method

In the last chapter we investigated zero-stability, the form of stability needed to guarantee
convergence of a numerical method as the grid is refined (k ! 0). In practice, however,
we are not able to compute this limit. Instead we typically perform a single calculation
with some particular nonzero time step k (or some particular sequence of time steps with a
variable step size method). Since the expense of the computation increases as k decreases,
we generally want to choose the time step as large as possible consistent with our accuracy
requirements. How can we estimate the size of k required?
Recall that if the method is stable in an appropriate sense, then we expect the global
error to be bounded in terms of the local truncation errors at each step, and so we can often
use the local truncation error to estimate the time step needed, as illustrated below. But the
form of stability now needed is something stronger than zero-stability. We need to know
that the error is well behaved for the particular time step we are now using. It is little
help to know that things will converge in the limit “for k sufficiently small.” The potential
difficulties are best illustrated with some examples.
Example 7.1. Consider the initial value problem (IVP)
u0.t/ D

sin t;

u.0/ D 1

with solution
u.t/ D cos t:
Suppose we wish to use Euler’s method to solve this problem up to time T D 2. The local
truncation error (LTE) is
1 00
ku .t/ C O.k 2 /
2
1
D
k cos.t/ C O.k 2 /:
2

.t/ D

(7.1)

Since the function f .t/ D sin t is independent of u, it is Lipschitz continuous with
Lipschitz constant L D 0, and so the error estimate (6.12) shows that
149

i

i

i

i

i

i

i

150

“rjlfdm”
2007/6/1
page 150
i

Chapter 7. Absolute Stability for Ordinary Differential Equations
jE n j  T kk1 D k max j cos tj D k:
0t T

Suppose we want to compute a solution with jEj  10 3. Then we should be able to
take k D 10 3 and obtain a suitable solution after T =k D 2000 time steps. Indeed,
calculating using k D 10 3 gives a computed value U 2000 D 0:415692 with an error
E 2000 D U 2000 cos.2/ D 0:4548  10 3 .
Example 7.2. Now suppose we modify the above equation to
u0.t/ D .u

cos t/

sin t;

(7.2)

where  is some constant. If we take the same initial data as before, u.0/ D 1, then
the solution is also the same as before, u.t/ D cos t. As a concrete example, let’s take
 D 10. Now how small do we need to take k to get an error that is 10 3 ? Since the
LTE (7.1) depends only on the true solution u.t/, which is unchanged from Example 7.1,
we might hope that we could use the same k as in that example, k D 10 3. Solving the
problem using Euler’s method with this step size now gives U 2000 D 0:416163 with
an error E 2000 D 0:161  10 4. We are again successful. In fact, the error is considerably smaller in this case than in the previous example, for reasons that will become clear
later.
Example 7.3. Now consider the problem (7.2) with  D 2100 and the same data
as before. Again the solution is unchanged and so is the LTE. But now if we compute
with the same step size as before, k D 10 3 , we obtain U 2000 D 0:2453  1077 with
an error of magnitude 1077 . The computation behaves in an “unstable” manner, with an
error that grows exponentially in time. Since the method is zero-stable and f .u; t/ is
Lipschitz continuous in u (with Lipschitz constant L D 2100), we know that the method
is convergent, and indeed with sufficiently small time steps we achieve very good results.
Table 7.1 shows the error at time T D 2 when Euler’s method is used with various values
of k. Clearly something dramatic happens between the values k D 0:000976 and k D
0:000952. For smaller values of k we get very good results, whereas for larger values of k
there is no accuracy whatsoever.
The equation (7.2) is a linear equation of the form (6.3) and so the analysis of Section 6.3.1 applies directly to this problem. From (6.7) we see that the global error E n
satisfies the recursion relation
E nC1 D .1 C k/E n

k n ;

(7.3)

where the local error  n D .tn / from (7.1). The expression (7.3) reveals the source of
the exponential growth in the error—in each time step the previous error is multiplied by a
factor of .1 C k/. For the case  D 2100 and k D 10 3 , we have 1 C k D 1:1 and
so we expect the local error introduced in step m to grow by a factor of . 1:1/n m by the
end of n steps (recall (6.8)). After 2000 steps we expect the truncation error introduced in
the first step to have grown by a factor of roughly . 1:1/2000  1082 , which is consistent
with the error actually seen.
Note that in Example 7.2 with  D 10, we have 1 C k D 0:99, causing a decay in
the effect of previous errors in each step. This explains why we got a reasonable result in
Example 7.2 and in fact a better result than in Example 7.1, where 1 C k D 1.

i

i

i

i

i

i

i

7.2. Absolute stability

“rjlfdm”
2007/6/1
page 151
i

151

Table 7.1. Errors in the computed solution using Euler’s method for Example 7:3,
for different values of the time step k. Note the dramatic change in behavior of the error
for k < 0:000952.
k
0.001000
0.000976
0.000950
0.000800
0.000400

Error
0.145252E+77
0.588105E+36
0.321089E-06
0.792298E-07
0.396033E-07

Returning to the case  D 2100, we expect to observe exponential growth in the
error for any value of k greater than 2=2100 D 0:00095238, since for any k larger than
this we have j1 C kj > 1. For smaller time steps j1 C kj < 1 and the effect of each
local error decays exponentially with time rather than growing. This explains the dramatic
change in the behavior of the error that we see as we cross the value k D 0:00095238 in
Table 7.1.
Note that the exponential growth of errors does not contradict zero-stability or convergence of the method in any way. The method does converge as k ! 0. In fact the bound
(6.12),
jE n j  e jjT T kk1 D O.k/ as k ! 0;
that we used to prove convergence allows the possibility of exponential growth with time.
The bound is valid for all k, but since T e jjT D 2e 4200 D 101825 while kk1 D 12 k, this
bound does not guarantee any accuracy whatsoever in the solution until k < 10 1825 . This
is a good example of the fact that a mathematical convergence proof may be a far cry from
what is needed in practice.

7.2

Absolute stability

To determine whether a numerical method will produce reasonable results with a given
value of k > 0, we need a notion of stability that is different from zero-stability. There are
a wide variety of other forms of “stability” that have been studied in various contexts. The
one that is most basic and suggests itself from the above examples is absolute stability. This
notion is based on the linear test equation (6.3), although a study of the absolute stability of
a method yields information that is typically directly useful in determining an appropriate
time step in nonlinear problems as well; see Section 7.4.3.
We can look at the simplest case of the test problem in which g.t/ D 0 and we have
simply
u0 .t/ D u.t/:
Euler’s method applied to this problem gives
U nC1 D .1 C k/U n

i

i

i

i

i

i

i

152

“rjlfdm”
2007/6/1
page 152
i

Chapter 7. Absolute Stability for Ordinary Differential Equations

and we say that this method is absolutely stable when j1Ckj  1; otherwise it is unstable.
Note that there are two parameters k and , but only their product z  k matters. The
method is stable whenever 2  z  0, and we say that the interval of absolute stability
for Euler’s method is Œ 2; 0.
It is more common to speak of the region of absolute stability as a region in the
complex z plane, allowing the possibility that  is complex (of course the time step k
should be real and positive). The region of absolute stability (or simply the stability region)
for Euler’s method is the disk of radius 1 centered at the point 1, since within this disk we
have j1 C kj  1 (see Figure 7.1a). Allowing  to be complex comes from the fact that in
practice we are usually solving a system of ordinary differential equations (ODEs). In the
linear case it is the eigenvalues of the coefficient matrix that are important in determining
stability. In the nonlinear case we typically linearize (see Section 7.4.3) and consider the
eigenvalues of the Jacobian matrix. Hence  represents a typical eigenvalue and these
may be complex even if the matrix is real. For some problems, looking at the eigenvalues
is not sufficient (see Section 10.12.1, for example), but eigenanalysis is generally very
revealing.

Forward Euler

(a)

Backward Euler

2

2

1.5

1.5

1

1

0.5

0.5

0

0

−0.5

−0.5

−1

−1

−1.5

−1.5

−2
−3

−2

−1

0

1

(b)

−2
−1

0

Trapezoidal

(c)

2

1.5

1.5

1

1

0.5

0.5

0

0

−0.5

−0.5

−1

−1

−1.5

−1.5
−1

0

2

3

1

2

Midpoint

2

−2
−2

1

1

2

(d)

−2
−2

−1

0

Figure 7.1. Stability regions for (a) Euler, (b) backward Euler, (c) trapezoidal,
and (d) midpoint (a segment on imaginary axis).

i

i

i

i

i

i

i

7.3. Stability regions for linear multistep methods

7.3

“rjlfdm”
2007/6/1
page 153
i

153

Stability regions for linear multistep methods

For a general linear multistep method (LMM) of the form (5.44), the region of absolute
stability is found by applying the method to u0 D u, obtaining
r
X

˛j U

nCj

r
X

Dk

j D0

ˇj U nCj ;

j D0

which can be rewritten as
r
X

.˛j

zˇj /U nCj D 0:

(7.4)

j D0

Note again that it is only the product z D k that is important, not the values of k or 
separately, and that this is a dimensionless quantity since the decay rate  has dimensions
time 1 , while the time step has dimensions of time. This makes sense—if we change the
units of time (say, from seconds to milliseconds), then the parameter  will decrease by a
factor of 1000 and we may be able to increase the numerical value of k by a factor of 1000
and still be stable. But then we also have to solve out to time 1000T instead of to time T ,
so we haven’t really changed the numerical problem or the number of time steps required.
The recurrence (7.4) is a homogeneous linear difference equation of the same form
considered in Section 6.4.1. The solution has the
P general form (6.26), where the j are
now the roots of the characteristic polynomial rj D0 .˛j zˇj / j . This polynomial is
often called the stability polynomial and denoted by .I z/. It is a polynomial in  but its
coefficients depend on the value of z. The stability polynomial can be expressed in terms
of the characteristic polynomials for the LMM as
.I z/ D ./

z./:

(7.5)

The LMM is absolutely stable for a particular value of z if errors introduced in one time
step do not grow in future time steps. According to the theory of Section 6.4.1, this requires
that the polynomial .I z/ satisfy the root condition (6.34).
Definition 7.1. The region of absolute stability for the LMM (5.44) is the set of points z in
the complex plane for which the polynomial .I z/ satisfies the root condition (6.34).
Note that an LMM is zero-stable if and only if the origin z D 0 lies in the stability
region.
Example 7.4. For Euler’s method,
.I z/ D 

.1 C z/

with the single root 1 D 1 C z. We have already seen that the stability region is the disk
in Figure 7.1(a).
Example 7.5. For the backward Euler method (5.21),
.I z/ D .1

i

i

z/

1

i

i

i

i

i

154

Chapter 7. Absolute Stability for Ordinary Differential Equations

with root 1 D .1

z/ 1 . We have
j.1

z/ 1 j  1 () j1

“rjlfdm”
2007/6/1
page 154
i

zj  1

so the stability region is the exterior of the disk of radius 1 centered at z D 1, as shown in
Figure 7.1(b).
Example 7.6. For the trapezoidal method (5.22),




1
1
.I z/ D 1
z 
1C z
2
2
with root
1 D

1 C 12 z
1

1
z
2

:

This is a linear fractional transformation and it can be shown that
j1j  1 () Re.z/  0;
where Re.z/ is the real part. So the stability region is the left half-plane as shown in
Figure 7.1(c).
Example 7.7. For the midpoint method (5.23),
.I z/ D  2

2z

1:

p
The roots are 1;2 D z ˙ z 2 C 1. It can be shown that if z is a pure imaginary number
of the form z D i˛ with j˛j < 1, then j1 j D j2 j D 1 and 1 ¤ 2 , and hence the root
condition is satisfied. For any other z the root condition is not satisfied. In particular, if
z D ˙i, then 1 D 2 is a repeated root of modulus 1. So the stability region consists only
of the open interval from i to i on the imaginary axis, as shown in Figure 7.1(d).
Since k is always real, this means the midpoint method is useful only on the test
problem u0 D u if  is pure imaginary. The method is not very useful for scalar problems
where  is typically real, but the method is of great interest in some applications with
systems of equations. For example, if the matrix is real but skew symmetric (AT D A),
then the eigenvalues are pure imaginary. This situation arises naturally in the discretization
of hyperbolic partial differential equations (PDEs), as discussed in Chapter 10.
Example 7.8. Figures 7.2 and 7.3 show the stability regions for the r -step Adams–
Bashforth and Adams–Moulton methods for various values of r . For an r -step method the
polynomial .I z/ has degree r and there are r roots. Determining the values of z for
which the root condition is satisfied does not appear simple. However, there is a simple
technique called the boundary locus method that makes it possible to determine the regions
shown in the figures. This is briefly described in Section 7.6.1.
Note that for many methods the shape of the stability region near the origin z D 0 is
directly related to the accuracy of the method. Recall that the stability polynomial ./ for
a consistent LMM always has a principal root 1 D 1. It can be shown that for z near 0 the
polynomial .I z/ has a corresponding principal root with behavior
1 .z/ D e z C O.z pC1 /

i

i

as z ! 0

(7.6)

i

i

i

i

i

7.3. Stability regions for linear multistep methods
Stability region of Adams−Bashforth 2−step method

(a)

2

1.5

1.5

1

1

0.5

0.5

0

0

−0.5

−0.5

−1

−1

−1.5

−1.5

−2

−1

0

1

(b)

−2
−3

Stability region of Adams−Bashforth 4−step method

(c)

2

1.5

1.5

1

1

0.5

0.5

0

0

−0.5

−0.5

−1

−1

−1.5

−1.5

−2

−1

0

−2

−1

0

1

Stability region of Adams−Bashforth 5−step method

2

−2
−3

155
Stability region of Adams−Bashforth 3−step method

2

−2
−3

“rjlfdm”
2007/6/1
page 155
i

1

(d)

−2
−3

−2

−1

0

1

Figure 7.2. Stability regions for some Adams–Bashforth methods. The shaded
region just to the left of the origin is the region of absolute stability. See Section 7.6.1 for a
discussion of the other loops seen in figures (c) and (d).

if the method is pth order accurate. We can see this in the examples above for one-step
methods, e.g., for Euler’s method 1 .z/ D 1 C z D e z C O.z 2 /. It is this root that is
giving the appropriate behavior U nC1  e z U n over a time step. Since this root is on the
unit circle at the origin z D 0, and since je z j < 1 only when Re.z/ < 0, we expect the
principal root to move inside the unit circle for small z with Re.z/ < 0 and outside the
unit circle for small z with Re.z/ > 0. This suggests that if we draw a small circle around
the origin, then the left half of this circle will lie inside the stability region (unless some
other root moves outside, as happens for the midpoint method), while the right half of the
circle will lie outside the stability region. Looking at the stability regions in Figure 7.1
we see that this is indeed true for all the methods except the midpoint method. Moreover,
the higher the order of accuracy in general, the larger a circle around the origin where this
will approximately hold, and so the boundary of the stability region tends to align with the
imaginary axis farther and farther from the origin as the order of the method increases, as
observed in Figures 7.2 and 7.3. (The trapezoidal method is a bit of an anomaly, as its
stability region exactly agrees with that of e z for all z.)

i

i

i

i

i

i

i

156

Chapter 7. Absolute Stability for Ordinary Differential Equations
Stability region of Adams−Moulton 2−step method

(a)

Stability region of Adams−Moulton 3−step method

4

4

3

3

2

2

1

1

0

0

−1

−1

−2

−2

−3

−3

−4

−6

−5

−4

−3

−2

−1

0

1

(b)

−4

Stability region of Adams−Moulton 4−step method

(c)

“rjlfdm”
2007/6/1
page 156
i

4

3

3

2

2

1

1

0

0

−1

−1

−2

−2

−3

−3

−6

−5

−4

−3

−2

−1

0

1

−5

−4

−3

−2

−1

0

1

Stability region of Adams−Moulton 5−step method

4

−4

−6

(d)

−4

−6

−5

−4

−3

−2

−1

0

1

Figure 7.3. Stability regions for some Adams–Moulton methods.
See Section 7.6 for a discussion of ways in which stability regions can be determined
and plotted.

7.4

Systems of ordinary differential equations

So far we have examined stability theory only in the context of a scalar differential equation u0.t/ D f .u.t// for a scalar function u.t/. In this section we will look at how this
stability theory carries over to systems of m differential equations where u.t/ 2 Rm . For
a linear system u0 D Au, where A is an m  m matrix, the solution can be written as
u.t/ D e At u.0/ and the behavior is largely governed by the eigenvalues of A. A necessary
condition for stability is that k be in the stability region for each eigenvalue  of A. For
general nonlinear systems u0 D f .u/, the theory is more complicated, but a good rule of
thumb is that k should be in the stability region for each eigenvalue  of the Jacobian
matrix f 0 .u/. This may not be true if the Jacobian is rapidly changing with time, or even
for constant coefficient linear problems in some highly nonnormal cases (see [47] and Section 10.12.1 for an example), but most of the time eigenanalysis is surprisingly effective.

i

i

i

i

i

i

i

7.4. Systems of ordinary differential equations

“rjlfdm”
2007/6/1
page 157
i

157

Before discussing this theory further we will review the theory of chemical kinetics,
a field where the solution of systems of ODEs is very important, and where the eigenvalues
of the Jacobian matrix often have a physical interpretation in terms of reaction rates.

7.4.1 Chemical kinetics
Let A and B represent chemical compounds and consider a reaction of the form
K1

A ! B:
This represents a reaction in which A is transformed into B with rate K1 > 0. If we let u1
represent the concentration of A and u2 represent the concentration of B (often denoted by
u1 D ŒA; u2 D ŒB), then the ODEs for u1 and u2 are
u01 D K1 u1;
u02 D K1 u1:
If there is also a reverse reaction at rate K2 , we write
K1

A

K2

B

and the equations then become
u01 D K1 u1 C K2 u2;
u02 D K1 u1 K2 u2 :

(7.7)

More typically, reactions involve combinations of two or more compounds, e.g.,
K1

ACB

K2

AB:

Since A and B must combine to form AB, the rate of the forward reaction is proportional
to the product of the concentrations u1 and u2 , while the backward reaction is proportional
to u3 D ŒAB. The equations become
u01 D K1 u1 u2 C K2 u3 ;
u02 D K1 u1 u2 C K2 u3 ;
u03 D K1 u1 u2 K2 u3 :

(7.8)

Note that this is a nonlinear system of equations, while (7.7) are linear.
Often several reactions take place simultaneously, e.g.,
K1

ACB

K2

AB;

K3

2A C C

i

i

K4

A2 C:

i

i

i

i

i

158

“rjlfdm”
2007/6/1
page 158
i

Chapter 7. Absolute Stability for Ordinary Differential Equations

If we now let u4 D ŒC ; u5 D ŒA2 C , then the equations are
u01 D K1 u1 u2 C K2 u3

2K3 u21 u4 C 2K4 u5;

u02 D K1 u1 u2 C K2 u3 ;
u03 D K1 u1 u2 K2 u3 ;
u04 D K3 u21 u4 C K4 u5 ;
u05 D K3 u21 u4

(7.9)

K4 u5 :

Interesting kinetics problems can give rise to very large systems of ODEs. Frequently the
rate constants K1 ; K2 ; : : : are of vastly different orders of magnitude. This leads to stiff
systems of equations, as discussed in Chapter 8.
Example 7.9. One particularly simple system arises from the decay process
K1

K2

A ! B ! C:
Let u1 D ŒA; u2 D ŒB; u3 D ŒC . Then the system is linear and has the form u0 D Au,
where
2
3
K1
0 0
A D 4 K1
K2 0 5 :
(7.10)
0
K2 0
Note that the eigenvalues are
(assuming K1 ¤ K2 )

K1 ;

K2 , and 0. The general solution thus has the form

uj .t/ D cj 1 e K1 t C cj 2 e K2 t C cj 3 :
In fact, on physical grounds (since A decays into B which decays into C ), we expect that
u1 simply decays to 0 exponentially,
u1 .t/ D e K1 t u1 .0/
(which clearly satisfies the first ODE), and also that u2 ultimately decays to 0 (although it
may first grow if K1 is larger than K2 ), while u3 grows and asymptotically approaches the
value u1 .0/ C u2 .0/ C u3 .0/ as t ! 1. A typical solution for K1 D 3 and K2 D 1 with
u1 .0/ D 3; u2 .0/ D 4, and u3 .0/ D 2 is shown in Figure 7.4.

7.4.2 Linear systems
Consider a linear system u0 D Au, where A is a constant m  m matrix, and suppose for
simplicity that A is diagonalizable, which means that it has a complete set of m linearly
independent eigenvectors rp satisfying Arp D p rp for p D 1; 2; : : : ; m. Let R D
Œr1 ; r2; : : : ; rm be the matrix of eigenvectors and ƒ D diag.1 ; 2 ; : : : ; m / be the
diagonal matrix of eigenvectors. Then we have
A D RƒR 1 and ƒ D R 1 AR:
Now let v.t/ D R 1 u.t/. Multiplying u0 D Au by R 1 on both sides and introducing
I D RR 1 gives the equivalent equations
R 1 u0 .t/ D .R 1 AR/.R 1 u.t//;

i

i

i

i

i

i

i

7.4. Systems of ordinary differential equations

159

6

8

C

4

concentrations

“rjlfdm”
2007/6/1
page 159
i

2

B

0

A
0

2

4

6

8

time
Figure 7.4. Sample solution for the kinetics problem in Example 7:9.
i.e.,
v 0 .t/ D ƒv.t/:
This is a diagonal system of equations that decouples into m independent scalar equations,
one for each component of v. The pth such equation is
vp0 .t/ D p vp .t/:
A linear multistep method applied to the linear ODE can also be decoupled in the same
way. For example, if we apply Euler’s method, we have
U nC1 D U n C kAU n ;
which, by the same transformation, can be rewritten as
V nC1 D V n C kƒV n ;
where V n D R 1 U n . This decouples into m independent numerical methods, one for each
component of V n . These take the form
VpnC1 D .1 C kp /Vpn :
We can recover U n from V n using U n D RV n .
For the overall method to be stable, each of the scalar problems must be stable, and
this clearly requires that kp be in the stability region of Euler’s method for all values of p.
The same technique can be used more generally to show that an LMM can be absolutely stable only if kp is in the stability region of the method for each eigenvalue p of
the matrix A.

i

i

i

i

i

i

i

160

“rjlfdm”
2007/6/1
page 160
i

Chapter 7. Absolute Stability for Ordinary Differential Equations

Example 7.10. Consider the linear kinetics problem with A given by (7.10). Since
this matrix is upper triangular, the eigenvalues are the diagonal elements 1 D K1 , 2 D
K2 , and 3 D 0. The eigenvalues are all real and we expect Euler’s method to be stable
provided k max.K1 ; K2/  2. Numerical experiments easily confirm that this is exactly
correct: when this condition is satisfied the numerical solution behaves well, and if k is
slightly larger there is explosive growth of the error.
Example 7.11. Consider a linearized model for a swinging pendulum, this time with
frictional forces added,
 00 .t/ D a.t/ b 0.t/;
which is valid for small values of . If we introduce u1 D  and u2 D  0 then we obtain a
first order system u0 D Au with


0
1
AD
:
(7.11)
a
b
p

1
The eigenvalues of this matrix are
b ˙ b 2 4a . Note in particular that if
p D 2
b D 0 (no damping), then  D ˙
a are pure imaginary. For b > 0 the eigenvalues shift
into the left half-plane. In the undamped case the midpoint method would be a reasonable
choice, whereas Euler’s method might be expected to have difficulties. In the damped case
the opposite is true.

7.4.3 Nonlinear systems
Now consider a nonlinear system u0 D f .u/. The stability analysis we have developed
for the linear problem does not apply directly to this system. However, if the solution is
slowly varying relative to the time step, then over a small time interval we would expect
a linearized approximation to give a good indication of what is happening. Suppose the
solution is near some value u,
N and let v.t/ D u.t/ u.
N Then
v 0 .t/ D u0 .t/ D f .u.t// D f .v.t/ C u/:
N
Taylor series expansion about uN (assuming v is small) gives
v 0 .t/ D f .u/
N C f 0 .u/v.t/
N
C O.kvk2 /:
Dropping the O.kvk2 / terms gives a linear system
v 0 .t/ D Av.t/ C b;
where A D f 0 .u/
N is the Jacobian matrix evaluated at uN and b D f .u/.
N Examining how the
numerical method behaves on this linear system (for each relevant value of u)
N gives a good
indication of how it will behave on the nonlinear system.
Example 7.12. Consider the kinetics problem (7.8). The Jacobian matrix is
2
3
K1 u2
K1 u1
K2
K1 u1
K2 5
A D 4 K1 u2
K1 u2
K1 u1
K2

i

i

i

i

i

i

i

7.5. Practical choice of step size

“rjlfdm”
2007/6/1
page 161
i

161

with eigenvalues 1 D K1 .u1 C u2 / K2 and 2 D 3 D 0. Since u1 C u2 is simply
the total quantity of species A and B present, this can be bounded for all time in terms of
the initial data. (For example, we certainly have u1 .t/ C u2 .t/  u1.0/ C u2 .0/ C 2u3 .0/.)
So we can determine the possible range of 1 along the negative real axis and hence how
small k must be chosen so that k1 stays within the region of absolute stability.

7.5

Practical choice of step size

As the examples at the beginning of this chapter illustrated, obtaining computed results that
are within some error tolerance requires two conditions:
1. The time step k must be small enough that the local truncation error is acceptably
small. This gives a constraint of the form k  kacc , where kacc depends on several
things:
 What method is being used, which determines the expansion for the local truncation error;
 How smooth the solution is, which determines how large the high order derivatives occurring in this expansion are; and
 What accuracy is required.
2. The time step k must be small enough that the method is absolutely stable on this
particular problem. This gives a constraint of the form k  kstab that depends on the
magnitude and location of the eigenvalues of the Jacobian matrix f 0 .u/.
Typically we would like to choose our time step based on accuracy considerations,
so we hope kstab > kacc . For a given method and problem, we would like to choose
k so that the local error in each step is sufficiently small that the accumulated error will
satisfy our error tolerance, assuming some “reasonable” growth of errors. If the errors
grow exponentially with time because the method is not absolutely stable, however, then
we would have to use a smaller time step to get useful results.
If stability considerations force us to use a much smaller time step than the local
truncation error indicates should be needed, then this particular method is probably not
optimal for this problem. This happens, for example, if we try to use an explicit method on a
“stiff” problem as discussed in Chapter 8, for which special methods have been developed.
As already noted in Chapter 5, most software for solving initial value problems does
a very good job of choosing time steps dynamically as the computation proceeds, based
on the observed behavior of the solution and estimates of the local error. If a time step is
chosen for which the method is unstable, then the local error estimate will typically indicate
a large error and the step size will be automatically reduced. Details of the shape of the
stability region and estimates of the eigenvalues are typically not used in the course of a
computation to choose time steps.
However, the considerations of this chapter play a big role in determining whether a
given method or class of methods is suitable for a particular problem. We will also see in
Chapters 9 and 10 that a knowledge of the stability regions of ODE methods is necessary
in order to develop effective methods for solving time-dependent PDEs.

i

i

i

i

i

i

i

162

7.6

“rjlfdm”
2007/6/1
page 162
i

Chapter 7. Absolute Stability for Ordinary Differential Equations

Plotting stability regions

7.6.1 The boundary locus method for linear multistep methods
A point z 2 C is in the stability region S of an LMM if the stability polynomial .I z/
satisfies the root condition for this value of z. It follows that if z is on the boundary of the
stability region, then .I z/ must have at least one root j with magnitude exactly equal
to 1. This j is of the form
j D e i
for some value of  in the interval Œ0; 2. (Beware of the two different uses of .) Since
j is a root of .I z/, we have
.e i I z/ D 0
for this particular combination of z and . Recalling the definition of , this gives
.e i /
and hence
zD

z.e i / D 0

(7.12)

.e i /
:
.e i /

If we know , then we can find z from this.
Since every point z on the boundary of S must be of this form for some value of  in
Œ0; 2, we can simply plot the parametrized curve
zQ ./ 

.e i /
.e i /

(7.13)

for 0    2 to find the locus of all points which are potentially on the boundary of S.
For simple methods this yields the region S directly.
Example 7.13. For Euler’s method we have ./ D  1 and ./ D 1, and so
zQ ./ D e i

1:

This function maps Œ0; 2 to the unit circle centered at z D 1, which is exactly the
boundary of S as shown in Figure 7.1(a).
To determine which side of this curve is the interior of S, we need only evaluate the
roots of .I z/ at some random point z on one side or the other and see if the polynomial
satisfies the root condition.
Alternatively, as noted on page 155, most methods are stable just to the left of the
origin on the negative real axis and unstable just to the right of the origin on the positive real
axis. This is often enough information to determine where the stability region lies relative
to the boundary locus.
For some methods the boundary locus may cross itself. In this case we typically find
that at most one of the regions cut out of the plane corresponds to the stability region. We
can determine which region is S by evaluating the roots at some convenient point z within
each region.
Example 7.14. The five-step Adams–Bashforth method gives the boundary locus
seen in Figure 7.2(d). The stability region is the small semicircular region to the left of the

i

i

i

i

i

i

i

7.6. Plotting stability regions

“rjlfdm”
2007/6/1
page 163
i

163

origin where all roots are inside the unit circle. As we cross the boundary of this region
one root moves outside. As we cross the boundary locus again into one of the loops in the
right half-plane another root moves outside and the method is still unstable in these regions
(two roots are outside the unit circle).

7.6.2 Plotting stability regions of one-step methods
If we apply a one-step method to the test problem u0 D u, we typically obtain an expression of the form
U nC1 D R.z/U n ;
(7.14)
where R.z/ is some function of z D k (typically a polynomial for an explicit method or
a rational function for an implicit method). If the method is consistent, then R.z/ will be
an approximation to e z near z D 0, and if it is pth order accurate, then
R.z/

e z D O.z pC1 /

as z ! 0:

(7.15)

Example 7.15. The pth order Taylor series method, when applied to u0 D u, gives
(since the j th derivative of u is u.j / D j u)
1
1
U nC1 D U n C kU n C k 22 U n C    C k p p U n
2
p!


1 2
1 p
D 1 C z C z C    C z U n:
2
p!

(7.16)

In this case R.z/ is the polynomial obtained from the first p C 1 terms of the Taylor series
for e z .
Example 7.16. If the fourth order Runge–Kutta method (5.33) is applied to u0 D u,
we find that
1
1
1
R.z/ D 1 C z C z 2 C z 3 C z 4 ;
(7.17)
2
6
24
which agrees with R.z/ for the fourth order Taylor series method.
Example 7.17. For the trapezoidal method (5.22),
R.z/ D

1 C 12 z
1

(7.18)

1
z
2

is a rational approximation to e z with error O.z 3 / (the method is second order accurate).
Note that this is also the root of the linear stability polynomial that we found by viewing
this as an LMM in Example 7.6.
Example 7.18. The TR-BDF2 method (5.37) has
5
1 C 12
z

R.z/ D
1

7
1 2
z C 12
z
12

:

(7.19)

This agrees with e z to O.z 3 / near z D 0.

i

i

i

i

i

i

i

164

“rjlfdm”
2007/6/1
page 164
i

Chapter 7. Absolute Stability for Ordinary Differential Equations

From the definition of absolute stability given at the beginning of this chapter, we see
that the region of absolute stability for a one-step method is simply
S D fz 2 C W jR.z/j  1g:

(7.20)

This follows from the fact that iterating a one-step method on u0 D u gives jU n j D
jR.z/jn jU 0 j and this will be uniformly bounded in n if z lies in S.
One way to attempt to compute S would be to compute the boundary locus as described in Section 7.6.1 by setting R.z/ D e i and solving for z as  varies. This would
give the set of z for which jR.z/j D 1, the boundary of S. There’s a problem with this,
however: when R.z/ is a higher order polynomial or rational function there will be several
solutions z for each  and it is not clear how to connect these to generate the proper curve.
Another approach can be taken graphically that is more brute force, but effective. If
we have a reasonable idea of what region of the complex z-plane contains the boundary of
S, we can sample jR.z/j on a fine grid of points in this region and approximate the level
set where this function has the value 1 and plot this as the boundary of S. This is easily
done with a contour plotter, for example, using the contour command in MATLAB. Or
we can simply color each point depending on whether it is inside S or outside.
For example, Figure 7.5 shows the stability regions for the Taylor series methods of
orders 2 and 4, for which
1
R.z/ D 1 C z C z 2 ;
2
1
1
1
R.z/ D 1 C z C z 2 C z 3 C z 4 ;
2
6
24

(7.21)

respectively. These are also the stability regions of the second order Runge–Kutta method
(5.30) and the fourth order accurate Runge–Kutta method (5.33), which are easily seen to
have the same stability functions.
Note that for a one-step method of order p, the rational function R.z/ must agree
with e z to O.z pC1 /. As for LMMs, we thus expect that points very close to the origin will
lie in the stability region S for Re.z/ < 0 and outside of S for Re.z/ > 0.

7.7

Relative stability regions and order stars

Recall that for a one-step method the stability region S (more properly called the region of
absolute stability) is the region S D fz 2 C W jR.z/j  1g, where U nC1 D R.z/U n is the
relation between U n and U nC1 when the method is applied to the test problem u0 D u.
For z D k in the stability region the numerical solution does not grow, and hence the
method is absolutely stable in the sense that past errors will not grow in later time steps.
On the other hand, the true solution to this problem, u.t/ D e t u.0/, is itself exponentially growing or decaying. One might argue that if u.t/ is itself decaying, then it isn’t
good enough to simply have the past errors decaying, too—they should be decaying at a
faster rate. Or conversely, if the true solution is growing exponentially, then perhaps it is
fine for the error also to be growing, as long as it is not growing faster.
This suggests defining the region of relative stability as the set of z 2 C for which
jR.z/j  je z j. In fact this idea has not proved to be particularly useful in terms of judging

i

i

i

i

i

i

i

7.7. Relative stability regions and order stars

“rjlfdm”
2007/6/1
page 165
i

165

Figure 7.5. Stability regions for the Taylor series methods of order 2 (left) and 4 (right).
the practical stability of a method for finite-size time steps; absolute stability is the more
useful concept in this regard.
Relative stability regions also proved hard to plot in the days before good computer
graphics, and so they were not studied extensively. However, a pivotal 1978 paper by
Wanner, Hairer, and Nørsett [99] showed that these regions are very useful in proving
certain types of theorems about the relation between stability and the attainable order of
accuracy for broad classes of methods. Rather than speaking in terms of regions of relative
stability, the modern terminology concerns the order star of a rational function R.z/, which
is the set of three regions .A ; A0 ; AC /:
A D fz 2 C W jR.z/j < je z jg D fz 2 C W je z R.z/j < 1g;
A0 D fz 2 C W jR.z/j D je z jg D fz 2 C W je z R.z/j D 1g;
AC D fz 2 C W jR.z/j > je z jg D fz 2 C W je z R.z/j > 1g:

(7.22)

These sets turn out to be much more strange looking than regions of absolute stability. As
their name implies, they have a star-like quality, as seen, for example, in Figure 7.6, which
shows the order stars for the same two Taylor polynomials (7.21), and Figure 7.7, which
shows the order stars for two implicit methods. In each case the shaded region is AC ,
while the white region is A and the boundary between them is A0 . Their behavior near
the origin is directly tied to the order of accuracy of the method, i.e., the degree to which
R.z/ matches e z at the origin. If R.z/ D e z C C z pC1 C higher order terms, then since
e z  1 near the origin,
e z R.z/  1 C C z pC1 :
(7.23)
As z traces out a small circle around the origin (say, z D ıe 2 i for some small ı), the
function z pC1 D ı pC1 e 2.pC1/ i goes around a smaller circle about the origin p C1 times
and hence crosses the imaginary axis 2.p C 1/ times. Each of these crossings corresponds
to z moving across A0 . So in a disk very close to the origin the order star must consist
of p C 1 wedgelike sectors of AC separated by p C 1 sectors of A . This is apparent in
Figures 7.6 and 7.7.

i

i

i

i

i

i

i

166

(a)

“rjlfdm”
2007/6/1
page 166
i

Chapter 7. Absolute Stability for Ordinary Differential Equations

(b)
Figure 7.6. Order stars for the Taylor series methods of order (a) 2 and (b) 4.

(a)

(b)

Figure 7.7. Order stars for two A-stable implicit methods, (a) the TR-BDF2
method (5.37) with R.z/ given by (7.19), and (b) the fifth-order accurate Radau5 method
[44], for which R.z/ is a rational function with degree 2 in the numerator and 3 in the
denominator.
It can also be shown that each bounded finger of A contains at least one root of the
rational function R.z/ and each bounded finger of AC contains at least one pole. (There
are no poles for an explicit method; see Figure 7.6.) Moreover, certain stability properties
of the method can be related to the geometry of the order star, facilitating the proof of some
“barrier theorems” on the possible accuracy that might be obtained.
This is just a hint of the sort of question that can be tackled with order stars. For a
better introduction to their power and beauty, see, for example, [44], [51], [98].

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 167
i

Chapter 8

Stiff Ordinary Differential
Equations

The problem of stiffness leads to computational difficulty in many practical problems. The
classic example is the case of a stiff ordinary differential equation (ODE), which we will
examine in this chapter. In general a problem is called stiff if, roughly speaking, we are
attempting to compute a particular solution that is smooth and slowly varying (relative to
the time interval of the computation), but in a context where the nearby solution curves are
much more rapidly varying. In other words, if we perturb the solution slightly at any time,
the resulting solution curve through the perturbed data has rapid variation. Typically this
takes the form of a short-lived “transient” response that moves the solution back toward a
smooth solution.
Example 8.1. Consider the ODE (7.2) from the previous chapter,
u0 .t/ D .u

cos t/

sin t:

(8.1)

One particular solution is the function u.t/ D cos t, and this is the solution with the initial
data u.0/ D 1 considered previously. This smooth function is a solution for any value
of . If we consider initial data of the form u.t0 / D  that does not lie on this curve,
then the solution through this point is a different function, of course. However, if  < 0
(or Re./ < 0 more generally), this function approaches cos t exponentially quickly, with
decay rate . It is easy to verify that the solution is
u.t/ D e .t t0 / .

cos.t0 // C cos t:

(8.2)

Figure 8.1 shows a number of different solution curves for this equation with different
choices of t0 and , with the fairly modest value  D 1. Figure 8.1b shows the corresponding solution curves when  D 10.
In this scalar example, when we perturb the solution at some point it quickly relaxes
toward the particular solution u.t/ D cos t. In other stiff problems the solution might move
quickly toward some different smooth solution, as seen in the next example.
Example 8.2. Consider the kinetics model A ! B ! C developed in Example 7.9.
The system of equations is given by (7.10). Suppose that K1  K2 so that a typical
solution appears as in Figure 8.2(a). (Here K1 D 20 and K2 D 1. Compare this to
167

i

i

i

i

i

1

u

-2

-1

0
-2

-1

u

1

2

Chapter 8. Stiff Ordinary Differential Equations

2

168

“rjlfdm”
2007/6/1
page 168
i

2

0

(a)

0

i

i

4

time
6

8

10

2

0

(b)

4

time
6

8

10

10
8

C

4

6

8

C

(b)

6

10

concentrations

(a)

4

concentrations

Figure 8.1. Solution curves for the ODE (8.1) for various initial values. (a) With
 D 1. (b) With  D 10 and the same set of initial values.

B

2

2

B
A

0

0

0

A
2

time

4

6

8

0

2

time

4

6

8

Figure 8.2. Solution curves for the kinetics problem in Example 7:9 with K1 D 20
and K2 D 1. In (b) a perturbation has been made by adding one unit of species A at time
t D 1. Figure 8.3 shows similar solutions for the case K1 D 106 .
Figure 7.4.) Now suppose at time t D 1 we perturb the system by adding more of species
A. Then the solution behaves as shown in Figure 8.2(b). The additional A introduced is
rapidly converted into B (the fast transient response) and then slowly from B into C . After
the rapid transient the solution is again smooth, although it differs from the original solution
since the final asymptotic value of C must be higher than before by the same magnitude as
the amount of A introduced.

8.1

Numerical difficulties

Stiffness causes numerical difficulties because any finite difference method is constantly
introducing errors. The local truncation error acts as a perturbation to the system that moves
us away from the smooth solution we are trying to compute. Why does this cause more
difficulty in a stiff system than in other systems? At first glance it seems like the stiffness
might work to our advantage. If we are trying to compute the solution u.t/ D cos t to the
ODE (8.1) with initial data u.0/ D 1, for example, then the fact that any errors introduced
decay exponentially should help us. The true solution is very robust and the solution is
almost completely insensitive to errors made in the past. In fact, this stability of the true

i

i

i

i

i

i

i

8.2. Characterizations of stiffness

“rjlfdm”
2007/6/1
page 169
i

169

solution does help us, as long as the numerical method is also stable. (Recall that the results
in Example 7.2 were much better than those in Example 7.1.)
The difficulty arises from the fact that many numerical methods, including all explicit methods, are unstable (in the sense of absolute stability) unless the time step is small
relative to the time scale of the rapid transient, which in a stiff problem is much smaller
than the time scale of the smooth solution we are trying to compute. In the terminology
of Section 7.5, this means that kstab  kacc . Although the true solution is smooth and it
seems that a reasonably large time step would be appropriate, the numerical method must
always deal with the rapid transients introduced by truncation error in every time step and
may need a very small time step to do so stably.

8.2

Characterizations of stiffness

A stiff ODE can be characterized by the property that f 0 .u/ is much larger (in absolute
value or norm) than u0.t/. The latter quantity measures the smoothness of the solution
being computed, while f 0 .u/ measures how rapidly f varies as we move away from this
particular solution. Note that stiff problems typically have large Lipschitz constants too.
For systems of ODEs, stiffness is sometimes defined in terms of the “stiffness ratio”
of the system, which is the ratio
max jp j
(8.3)
min jp j
over all eigenvalues of the Jacobian matrix f 0 .u/. If this is large, then a large range of time
scales is present in the problem, a necessary component for stiffness to arise. While this is
often a useful quantity, one should not rely entirely on this measure to determine whether
a problem is stiff.
For one thing, it is possible even for a scalar problem to be stiff (as we have seen
in Example 8.1), although for a scalar problem the stiffness ratio is always 1 since there
is only one eigenvalue. Still, more than one time scale can be present. In (8.1) the fast
time scale is determined by , the eigenvalue, and the slow time scale is determined by the
inhomogeneous term sin.t/. For systems of equations there also may be additional time
scales arising from inhomogeneous forcing terms or other time-dependent coefficients that
are distinct from the scales imposed by the eigenvalues.
We also know that for highly nonnormal matrices the eigenvalues don’t always tell
the full story (see Section D.4). Often they give adequate guidance, but there are examples
where the problem is more stiff than (8.3) would suggest. An example arises when certain
spectral approximations to spatial derivatives are used in discretizing hyperbolic equations;
see Section 10.13.
On the other hand, it is also important to note that a system of ODEs which has a large
“stiffness ratio” is not necessarily stiff! If the eigenvalue with large amplitude lies close to
the imaginary axis, then it leads to highly oscillatory behavior in the solution rather than
rapid damping. If the solution is rapidly oscillating, then it will probably be necessary to
take small time steps for accuracy reasons and kacc may be roughly the same magnitude as
kstab even for explicit methods, at least with the sort of methods discussed here. (Special
methods for highly oscillatory problems have been developed that allow one to take larger
time steps; see, e.g., [50], [74].)

i

i

i

i

i

Chapter 8. Stiff Ordinary Differential Equations

6

8

10

(b)

0

0

2

4

6

8

10

concentrations

(a)

2

concentrations

170

“rjlfdm”
2007/6/1
page 170
i

4

i

i

0

2

4

time

6

8

0

2

4

time

6

8

Figure 8.3. Solution curves for the kinetics problem in Example 7:9 with K1 D
106 and K2 D 1. In (b) a perturbation has been made by adding one unit of species A at
time t D 1.
Finally, note that a particular problem may be stiff over some time intervals and
nonstiff elsewhere. In particular, if we are computing a solution that has a rapid transient,
such as the kinetics problem shown in Figure 8.3(a), then the problem is not stiff over
the initial transient period where the true solution is as rapidly varying as nearby solution
curves. Only for times greater than 10 6 or so does the problem become stiff, once the
desired solution curve is much smoother than nearby curves.
For the problem shown in Figure 8.3(b), there is another time interval just after t D 1
over which the problem is again not stiff since the solution again exhibits rapid transient
behavior and a small time step would be needed on the basis of accuracy considerations.

8.3

Numerical methods for stiff problems

Over time intervals where a problem is stiff, we would like to use a numerical method that
has a large region of absolute stability, extending far into the left half-plane. The problem
with a method like Euler’s, with a stability region that extends only out to Re./ D 2, is
that the time step k is severely limited by the eigenvalue with largest magnitude, and we
need to take k  2=jmax j. Over time intervals where this fastest time scale does not appear
in the solution, we would like to be able to take much larger time steps. For example, in the
problems shown in Figure 8.3, where K1 D 106 , we would need to take k  2  10 6 with
Euler’s method, requiring 4 million time steps to compute over the time interval shown in
the figure, although the solution is very smooth over most of this time.
An analysis of stability regions shows that there are basically two different classes of
methods: those for which the stability region is bounded and extends a distance O.1/ from
the origin, such as Euler’s method, the midpoint method, or any of the Adams methods
(see Figures 7.1 and 8.5), and those for which the stability region is unbounded, such as
backward Euler or trapezoidal. Clearly the first class of methods is inappropriate for stiff
problems.
Unfortunately, all explicit methods have bounded stability regions and hence are generally quite inefficient on stiff problems. An exception is methods such as the Runge–
Kutta–Chebyshev methods described in Section 8.6 that may work well for mildly stiff
problems. Some implicit methods also have bounded stability regions, such as the Adams–
Moulton methods, and are not useful for stiff problems.

i

i

i

i

i

i

i

8.3. Numerical methods for stiff problems

“rjlfdm”
2007/6/1
page 171
i

171

8.3.1 A-stability and A(˛)-stability
It seems like it would be optimal to have a method whose stability region contains the entire
left half-plane. Then any time step would be allowed, provided that all the eigenvalues have
negative real parts, as is often the case in practice. The backward Euler and trapezoidal
methods have this property, for example. We give methods with this property a special
name, as follows.
Definition 8.1. An ODE method is said to be A-stable if its region of absolute stability S
contains the entire left half-plane fz 2 C W Re.z/  0g.
For LMMs it turns out that this is quite restrictive. A theorem of Dahlquist [21] (the
paper in which the term A-stability was introduced) states that any A-stable LMM is at
most second order accurate, and in fact the trapezoidal method is the A-stable method with
smallest truncation error. This is Dahlquist’s second barrier theorem, proved, for example,
in [44] and by using order stars in [51].
Higher order A-stable implicit Runge–Kutta methods do exist, including diagonally
implicit (DIRK) methods; see, for example, [13], [44].
For many stiff problems the eigenvalues are far out in the left half-plane but near (or
even exactly on) the real axis. For such problems there is no reason to require that the entire
left half-plane lie in the region of absolute stability. If arg.z/ represents the argument of
z with arg.z/ D  on the negative real axis, and if the wedge  ˛  arg.z/   C ˛
is contained in the stability region, then we say the method is A(˛)-stable. An A-stable
method is A(=2)-stable. A method is A(0)-stable if the negative real axis itself lies in the
stability region. Note that in general it makes sense to require a wedge to lie in the stability
regions, since adjusting the time step k causes z D k to move in toward the origin on a
ray through each eigenvalue.

8.3.2 L-stability
Notice a major difference between the stability regions for trapezoidal and backward Euler:
the trapezoidal method is stable only in the left half-plane, whereas backward Euler is also
stable over much of the right half-plane. The point at infinity (if we view these stability regions on the Riemann sphere) lies on the boundary of the stability region for the trapezoidal
method but in the interior of the stability region for backward Euler.
These are both one-step methods and so on the test problem u0 D u we have
nC1
U
D R.z/U n , where
R.z/ D

1
.1

z/

and jR.z/j ! 0 as jzj ! 1

for backward Euler, while
R.z/ D

1 C 12 z
1

1
z
2

and jR.z/j ! 1 as jzj ! 1

for the trapezoidal method.

i

i

i

i

i

i

i

172

“rjlfdm”
2007/6/1
page 172
i

Chapter 8. Stiff Ordinary Differential Equations

This difference can have a significant effect on the quality of solutions in some situations, in particular if there are rapid transients in the solution that we are not interested
in resolving accurately with very small time steps. For these transients we want more than
just stability—we want them to be effectively damped in a single time step since we are
planning to use a time step that is much larger than the true decay time of the transient. For
this purpose a method like backward Euler will perform better than the trapezoidal method.
The backward Euler method is said to be L-stable.
Definition 8.2. A one-step method is L-stable if it is A-stable and limz!1 jR.z/j D 0,
where the stability function R.z/ is defined in Section 7.6.2.
The value of L-stability is best illustrated with an example.
Example 8.3. Consider the problem (8.1) with  D 106 . We will see how the
trapezoidal and backward Euler methods behave in two different situations.
Case 1: Take data u.0/ D 1, so that u.t/ D cos.t/ and there is no initial transient.
Then both trapezoidal and backward Euler behave reasonably and the trapezoidal method
gives smaller errors since it is second order accurate. Table 8.1 shows the errors at T D 3
with various values of k.
Case 2: Now take data u.0/ D 1:5 so there is an initial rapid transient toward u D
cos.t/ on a time scale of about 10 6 . Both methods are still absolutely stable, but the
results in Table 8.1 show that backward Euler works much better in this case.
To understand what is happening, see Figure 8.4, which shows the true and computed
solutions with each method if we use k D 0:1. The trapezoidal method is stable and the
results stay bounded, but since k D

105 we have

1

1 k
2

1C 1
2 k

D

:99996 

1 and the

initial deviation from the smooth curve cos.t/ is essentially negated in each time step.
Backward Euler, on the other hand, damps the deviation very effectively in the first
time step, since .1 C k/ 1  10 6. This is the proper behavior since the true rapid
transient decays in a period much shorter than a single time step.
If we are solving a stiff equation with initial data for which the solution is smooth
from the beginning (no rapid transients), or if we plan to compute rapid transients accurately by taking suitably small time steps in these regions, then it may be fine to use a
method such as the trapezoidal method that is not L-stable.

Table 8.1. Errors at time T D 3 for Example 8:3.

Case 1

Case 2

i

i

k
0.4
0.2
0.1
0.4
0.2
0.1

Backward Euler
4.7770e 02
9.7731e 08
4.9223e 08
4.7770e 02
9.7731e 08
4.9223e 08

Trapezoidal
4.7770e 02
4.7229e 10
1.1772e 10
4.5219e 01
4.9985e 01
4.9940e 01

i

i

i

i

i

8.4. BDF methods

173
Trapezoidal

1.5 o

o

“rjlfdm”
2007/6/1
page 173
i

Backward Euler
1.5 o

o
o
o
o

1

1

o

o

o

o

o

o

o

0.5

o

o

o
o

o

o

o

o

o

0.5

o

o
o

o
o

o

0

o

o

o

o

o

o
o

-0.5

o

0

o
o

o
o

o

o

o

o
o

o

-0.5

-1

o

o

o
o

o

o
o
o

(a)

-1.5

0

0.5

1

1.5

2

2.5

o

(b) 0
-1

3

0.5

1

1.5

2

o

o

o

o

2.5

o

o

3

Figure 8.4. Comparison of (a) trapezoidal method and (b) backward Euler on a
stiff problem with an initial transient (Case 2 of Example 8:3).

8.4

BDF methods

One class of very effective methods for stiff problems consists of the backward differentiation formula (BDF) methods. These were introduced by Curtiss and Hirschfelder [20]. See
e.g., [33], [44], or [59] for more about these methods.
These methods result from taking ./ D ˇr  r , which has all its roots at the origin,
so that the point at infinity is in the interior of the stability region. The method thus has the
form
˛0 U n C ˛1 U nC1 C    C ˛r U nCr D kˇr f .U nCr /
(8.4)
with ˇ0 D ˇ1 D    D ˇr 1 D 0. Since f .u/ D u0 , this form of method can be derived
by approximating u0.tnCr / by a backward difference approximation based on u.tnCr / and
r additional points going backward in time.
It is possible to derive an r -step method that is r th order accurate. The one-step BDF
method is simply the backward Euler method, U nC1 D U n C kf .U nC1 /, which is first
order accurate. The other useful BDF methods are below:
3U nC2

r D2W
11U nC3

r D3W

4U nC1 C U n

18U nC2 C 9U nC1

D 2kf .U nC2 /

2U n

D 6kf .U nC3 /

r D4W

25U nC4

48U nC3 C 36U nC2

16U nC1 C 3U n

D 12kf .U nC4 /

r D5W

137U nC5

300U nC4 C 300U nC3 200U nC2
C 75U nC1 12U n

D 60kf .U nC5 /

360U nC5 C 450U nC4 400U nC3
C 225U nC2 72U nC1 C 10U n

D 60kf .U nC6 /

r D6W

147U nC6

These methods have the proper behavior on eigenvalues for which Re./ is very
negative, but of course we also have other eigenvalues for which z D k is closer to the
origin, corresponding to the active time scales in the problem. So deciding its suitability
for a particular problem requires looking at the full stability region of a method. This is
shown in Figure 8.5 for each of the BDF methods.

i

i

i

i

i

i

i

174

“rjlfdm”
2007/6/1
page 174
i

Chapter 8. Stiff Ordinary Differential Equations

Stability region of 1−step BDF method

Stability region of 2−step BDF method

20

20

15

15

10

10

5

5

0

0

−5

−5

−10

−10

−15

−15

−20
−10

0

10

20

30

−20
−10

Stability region of 3−step BDF method
20

15

15

10

10

5

5

0

0

−5

−5

−10

−10

−15

−15

0

10

20

30

−20
−10

Stability region of 5−step BDF method
20

15

15

10

10

5

5

0

0

−5

−5

−10

−10

−15

−15

0

10

20

20

30

0

10

20

30

Stability region of 6−step BDF method

20

−20
−10

10

Stability region of 4−step BDF method

20

−20
−10

0

30

−20
−10

0

10

20

30

Figure 8.5. Stability regions for the BDF methods. The stability region is the
shaded region exterior to the curves.

i

i

i

i

i

i

i

8.6. Runge–Kutta–Chebyshev explicit methods

“rjlfdm”
2007/6/1
page 175
i

175

In particular, we need to make sure that the method is zero-stable. Otherwise it would
not be convergent. This is not guaranteed from our derivation of the methods, since zerostability depends only on the polynomial ./, whose coefficients ˛j are determined by
considering the local truncation error and not stability considerations. It turns out that the
BDF methods are zero-stable only for r  6. Higher order BDF methods cannot be used
in practice. For r > 2 the methods are not A-stable but are A(˛)-stable for the following
values of ˛:
r D 1 W ˛ D 90ı ;
r D 4 W ˛ D 73ı
ı
r D 2 W ˛ D 90 ;
r D 5 W ˛ D 51ı ;
(8.5)
ı
r D 3 W ˛ D 88 ;
r D 6 W ˛ D 18ı :

8.5

The TR-BDF2 method

There are often situations in which it is useful to have a one-step method that is L-stable.
The backward Euler method is one possibility, but it is only first order accurate. It is
possible to derive higher order implicit Runge–Kutta methods that are L-stable. As one
example, we mention the two-stage second order accurate diagonally implicit method
U D Un C
U nC1 D

k
.f .U n / C f .U  //;
4

1
.4U 
3

(8.6)

U n C kf .U nC1 //:

Each stage is implicit. The first stage is simply the trapezoidal method (or trapezoidal rule,
hence TR in the name) applied over time k=2. This generates a value U  at tnC1=2 . Then
the two-step BDF method is applied to the data U n and U  with time step k=2 to obtain
U nC1 . This method is written in a different form in (5.37).

8.6

Runge–Kutta–Chebyshev explicit methods

While conventional wisdom says that implicit methods should be used for stiff problems,
there’s an important class of “mildly stiff” problems for which special explicit methods
have been developed with some advantages. In this section we take a brief look at the
Runge–Kutta–Chebyshev methods that are applicable to problems where the eigenvalues
of the Jacobian matrix are on the negative real axis and not too widely distributed. This
type of problem arises, for example, when a parabolic heat equation or diffusion equation
is discretized in space, giving rise to a large system of ordinary differential equations in
time. This problem is considered in detail in Chapter 9.
The idea is to develop an explicit method whose stability region stretches out along
the negative real axis as far as possible, without much concern with what’s happening away
from the real axis. We want the region of absolute stability S to contain a “stability interval”
Œ ˇ; 0 that is as long as possible. To do this we will consider multistage explicit Runge–
Kutta methods as introduced in Section 5.7, but now as we increase the number of stages
we will choose the coefficients to make ˇ as large as possible rather than to increase the
order of accuracy of the method.

i

i

i

i

i

i

i

176

“rjlfdm”
2007/6/1
page 176
i

Chapter 8. Stiff Ordinary Differential Equations

We will consider only first order methods, because they are easiest to study. Second
order methods are commonly used in practice, and this is often quite sufficient for the applications we have in mind, such as integrating method of lines (MOL) discretizations of
the heat equation. The system of ODEs is obtained by discretizing the original partial differential equations PDE in space, and often this discretization is only second order accurate
spatially. Second order methods similar to those described here can be found in [80], [97],
[2] and higher order methods in [1], [69], for example.
Consider an explicit r -stage Runge–Kutta method of the form (5.34) with aij D 0
for i  j . If we apply this method to u0 D u we obtain
U nC1 D R.z/U n ;
where z D k and R.z/ is a polynomial of degree r in z, the stability polynomial. We will
write this as
R.z/ D d0 C d1 z C d2 z 2 C    C dr z r :
(8.7)
Consistency and first order accuracy require that
d0 D 1;

d1 D 1:

(8.8)

The region of absolute stability is
S D fz 2 C W jR.z/j  1g:

(8.9)

The coefficients dj depend on the coefficients A and b defining the Runge–Kutta method,
but for the moment suppose that for any choice of dj we can find suitable coefficients A
and b that define an explicit r -stage method with stability polynomial (8.7). Then our goal
is to choose the dj coefficients so that S contains an interval Œ ˇ; 0 of the negative real
axis with ˇ as large as possible. We also require (8.8) for a first order accurate method.
Note that the condition (8.8) amounts to requiring
R.0/ D 1 and R0 .0/ D 1:

(8.10)

Then we want to choose R.z/ as a polynomial of degree r that satisfies jR.z/j  1 for as
long as possible as we go out on the negative real axis. The solution is simply a scaled and
shifted Chebyshev polynomial. Figure 8.6 shows the optimal polynomials of degrees 3 and
6. Note that these are plots of R.x/ for x real.
We have seen several other examples where Chebyshev polynomials solve problems
of this sort, and as usual the optimal solution equioscillates at a set of points, in this case
r C 2 points (including x D ˇ and x D 0), where jR.x/j D 1. If we try to perturb the
polynomial so that jR. ˇ/j < 1 (and hence the method will be stable further out on the
negative real axis), one of the other extrema of the polynomial will move above 1, losing
stability for some z closer to the origin.
The optimal polynomial is


R.z/ D Tr 1 C z=r 2 ;
(8.11)
where Tr .z/ is the Chebyshev polynomial of degree r , as in Section B.3.2. For this choice
R. 2r 2 / D Tr . 1/ D ˙1, while jR.x/j > 1 for x < 2r 2, and so
ˇ.r / D 2r 2:

i

i

(8.12)

i

i

i

i

i

8.6. Runge–Kutta–Chebyshev explicit methods

(a)

177

1

1

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0

−0.2

−0.2

−0.4

−0.4

−0.6

−0.6

−0.8

−0.8

−1

“rjlfdm”
2007/6/1
page 177
i

−1
−70

−60

−50

−40

−30

−20

−10

0

(b)

−70

−60

−50

−40

−30

−20

−10

0

Figure 8.6. Shifted Chebyshev polynomials corresponding to R.x/ along the real
axis for the first order Runge–Kutta–Chebyshev methods. (a) r D 3. (b) r D 6.
We have optimized along the negative real axis, but it is interesting to see what the
stability region S from (8.9) looks like in the complex plane. Figure 8.7 shows the stability
region for the two polynomials shown in Figure 8.6 of degrees 3 and 6. They do include
the negative real axis out to ˇ.r /, but notice that everywhere jR.x/j D 1 on the real axis
is a point where the region S contracts to the axis. Perturbing such an x away from the axis
into the complex plane gives a point z where jR.z/j > 1. So these methods are suitable
only for problems with real eigenvalues, such as MOL discretizations of the heat equation.
Even for these problems it is generally preferable to perturb the polynomial R.z/ a
bit so that jR.z/j is bounded slightly below 1 on the real axis between ˇ and the origin,
which can be done at the expense of decreasing ˇ slightly. This is done by using the shifted
Chebyshev polynomial
R.z/ D

Tr .w0 C w1 z/
;
Tr .w0 /

where w1 D

Tr .w0 /
:
Tr0 .w0 /

Here w0 > 1 is the “damping parameter” and w1 is chosen so that (8.8) holds and hence
the method remains first order accurate. Now R.x/ alternates between ˙.Tr .w0 // 1 < 1
on the interval Œ ˇ; 0, where ˇ is now found by solving w0 ˇw1 D 1, so
ˇD

.w0 C 1/Tr0 .w0 /
:
Tr .w0 /

Verwer [97] suggests taking w0 D 1 C =r 2 with  D 0:05, for which


4
ˇ 2
 r 2  1:93r 2:
3
This gives about 5% damping with little reduction in ˇ. Figure 8.8 shows the stability
regions for the damped first order methods, again for r D 3 and r D 6.
Once we have determined the desired stability polynomial R.z/, we must still develop an r -stage Runge–Kutta method with this stability polynomial. In general there are
infinitely many such Runge–Kutta methods. Here we discuss one approach that has several
desirable properties, the Runge–Kutta–Chebyshev method introduced by van der Houwen

i

i

i

i

i

i

i

178

“rjlfdm”
2007/6/1
page 178
i

Chapter 8. Stiff Ordinary Differential Equations
10
5
0
−5

(a)

−10

−70

−60

−50

−40

−30

−20

−10

0

−70

−60

−50

−40

−30

−20

−10

0

10
5
0
−5

(b)

−10

Figure 8.7. Absolute stability regions for the first order Runge–Kutta–Chebyshev
methods. (a) r D 3. (b) r D 6.
10
5
0
−5

(a)

−10

−70

−60

−50

−40

−30

−20

−10

0

−70

−60

−50

−40

−30

−20

−10

0

10
5
0
−5

(b)

−10

Figure 8.8. Absolute stability regions for the first order Runge–Kutta–Chebyshev
methods with damping. (a) r D 3. (b) r D 6.

and Sommeijer [94]. See the review paper of Verwer [97] for a derivation of this method
and a discussion and comparison of some other approaches, and see [80] for a discussion
of software that implements these methods.
The Runge–Kutta–Chebyshev methods are based on the recurrence relation of Chebyshev polynomials and have the general form

i

i

i

i

i

i

i

8.6. Runge–Kutta–Chebyshev explicit methods

“rjlfdm”
2007/6/1
page 179
i

179

Y0 D U n ;
Y1 D Y0 C 
Q 1 kf .Y0 ; tn /;
Yj D .1 j j /Y0 C j Yj 1
(8.13)
C j Yj 2 C 
Q j kf .Yj 1 ; tn C cj 1 k/ C Q kf .Y0 ; tn /; j D 2; : : : ; r;
U nC1 D Yr :
The various parameters in these formulas are given in the references above. This is an
r -stage explicit Runge–Kutta method, although written in a different form than (5.34). An
advantage of this recursive form is that only three intermediate solution vectors Y0 ; Yj 1 ,
and Yj 2 are needed to compute the next Yj , no matter how many stages r are used. This
is important since large values of r (e.g., r D 50) are often used in practice, and for PDE
applications each Yj is a grid function over the entire (possibly multidimensional) spatial
domain.
Another advantage of the Runge–Kutta–Chebyshev methods is that they have good
internal stability properties, as discussed in the references above. We know the stability
region of the overall stability polynomial R.z/, but some r -stage methods for large r suffer from exponential growth of the solution from one stage to the next in the early stages
before ultimately decaying even when jR.z/j < 1 (analogous to the transient growth sometimes seen when iterating with a nonnormal matrix even if it is asymptotically stable; see
Section D.4). This growth can cause numerical difficulties if methods are not carefully
designed.

i

i

i

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 180
i

i

i

i

i

“rjlfdm”
2007/6/1
page 181
i

Chapter 9

Diffusion Equations and
Parabolic Problems

We now begin to study finite difference methods for time-dependent partial differential
equations (PDEs), where variations in space are related to variations in time. We begin
with the heat equation (or diffusion equation) introduced in Appendix E,
ut D uxx :

(9.1)

This is the classical example of a parabolic equation, and many of the general properties
seen here carry over to the design of numerical methods for other parabolic equations. We
will assume  D 1 for simplicity, but some comments will be made about how the results
scale to other values of  > 0. (If  < 0, then (9.1) would be a “backward heat equation,”
which is an ill-posed problem.)
Along with this equation we need initial conditions at some time t0 , which we typically take to be t0 D 0,
u.x; 0/ D .x/;
(9.2)
and also boundary conditions if we are working on a bounded domain, e.g., the Dirichlet
conditions
u.0; t/ D g0 .t/ for t > 0;
u.1; t/ D g1 .t/ for t > 0

(9.3)

if 0  x  1.
We have already studied the steady-state version of this equation and spatial discretizations of uxx in Chapter 2. We have also studied discretizations of the time derivatives and some of the stability issues that arise with these discretizations in Chapters 5
through 8. Next we will put these two types of discretizations together.
In practice we generally apply a set of finite difference equations on a discrete grid
with grid points .xi ; tn /, where
xi D ih;

tn D nk:

Here h D x is the mesh spacing on the x-axis and k D t is the time step. Let
Uin  u.xi ; tn / represent the numerical approximation at grid point .xi ; tn /.
181

i

i

i

i

i

i

i

182

“rjlfdm”
2007/6/1
page 182
i

Chapter 9. Diffusion Equations and Parabolic Problems
(a)

(b)
tnC1

tn
xj 1

xj

xj C1

Figure 9.1. Stencils for the methods (9.5) and (9.7).
Since the heat equation is an evolution equation that can be solved forward in time,
we set up our difference equations in a form where we can march forward in time, determining the values UinC1 for all i from the values Uin at the previous time level, or perhaps
using also values at earlier time levels with a multistep formula.
As an example, one natural discretization of (9.1) would be
UinC1 Uin
1
D 2 .Uin 1
k
h

n
2Uin C UiC1
/:

(9.4)

This uses our standard centered difference in space and a forward difference in time. This
is an explicit method since we can compute each UinC1 explicitly in terms of the previous
data:
k
n
UinC1 D Uin C 2 .Uin 1 2Uin C UiC1
/:
(9.5)
h
Figure 9.1(a) shows the stencil of this method. This is a one-step method in time, which is
also called a two-level method in the context of PDEs since it involves the solution at two
different time levels.
Another one-step method, which is much more useful in practice, as we will see
below, is the Crank–Nicolson method,
UinC1 Uin
1
D .D 2 Uin C D 2 UinC1 /
k
2
1
n
D 2 .Uin 1 2Uin C UiC1
C UinC1
1
2h

(9.6)
nC1
2UinC1 C UiC1
/;

which can be rewritten as
k
.U n
2h2 i 1

n
2Uin C UiC1
C UinC1
1

nC1
2UinC1 C UiC1
/

(9.7)

nC1
r UinC1
1 C .1 C 2r /Ui

nC1
r UiC1
D r Uin 1 C .1

n
2r /Uin C r UiC1
;

(9.8)

UinC1 D Uin C
or

where r D k=2h2. This is an implicit method and gives a tridiagonal system of equations
to solve for all the values UinC1 simultaneously. In matrix form this is

i

i

i

i

i

i

i

9.1. Local truncation errors and order of accuracy

“rjlfdm”
2007/6/1
page 183
i

183

2

3
U1nC1
7 6 U nC1 7
7 6 2nC1 7
7
76 U
7
76 3
7
76
:
::
7
76
7
76
5 4 U nC1 5
32

.1 C 2r /
r
6
r
.1
C
2r /
6
6
r
6
6
6
6
4
2
6
6
6
6
D6
6
6
4

r
.1 C 2r /
::
:

r
::
:
r

::

:
.1 C 2r /
r
r
.1 C 2r /

r .g0 .tn / C g0 .tnC1 // C .1 2r /U1n C r U2n
r U1n C .1 2r /U2n C r U3n
r U2n C .1 2r /U3n C r U4n
::
:
r Umn 2 C .1 2r /Umn 1 C r Umn
n
r Um 1 C .1 2r /Umn C r .g1 .tn / C g1 .tnC1 //

m 1

UmnC1
3

(9.9)

7
7
7
7
7:
7
7
5

Note how the boundary conditions u.0; t/ D g0 .t/ and u.1; t/ D g1 .t/ come into these
equations.
Since a tridiagonal system of m equations can be solved with O.m/ work, this
method is essentially as efficient per time step as an explicit method. We will see in Section 9.4 that the heat equation is “stiff,” and hence this implicit method, which allows much
larger time steps to be taken than an explicit method, is a very efficient method for the heat
equation.
Solving a parabolic equation with an implicit method requires solving a system of
equations with the same structure as the two-point boundary value problem we studied
in Chapter 2. Similarly, a multidimensional parabolic equation requires solving a problem
with the structure of a multidimensional elliptic equation in each time step; see Section 9.7.

9.1

Local truncation errors and order of accuracy

We can define the local truncation error as usual—we insert the exact solution u.x; t/ of
the PDE into the finite difference equation and determine by how much it fails to satisfy
the discrete equation.
Example 9.1. The local truncation error of the method (9.5) is based on the form
(9.4): in D .xi ; tn /, where
.x; t/ D

u.x; t C k/
k

u.x; t/

1
.u.x
h2

h; t/

2u.x; t/ C u.x C h; t//:

Again we should be careful to use the form that directly models the differential equation
in order to get powers of k and h that agree with what we hope to see in the global error.
Although we don’t know u.x; t/ in general, if we assume it is smooth and use Taylor series
expansions about u.x; t/, we find that


1
1
.x; t/ D ut C kut t C k 2 ut t t C   
2
6

i

i



1
uxx C h2 uxxxx C    :
12

i

i

i

i

i

184

“rjlfdm”
2007/6/1
page 184
i

Chapter 9. Diffusion Equations and Parabolic Problems

Since ut D uxx , the O.1/ terms drop out. By differentiating ut D uxx we find that
ut t D utxx D uxxxx and so


1
1 2
.x; t/ D
k
h uxxxx C O.k 2 C h4 /:
2
12
This method is said to be second order accurate in space and first order accurate in time
since the truncation error is O.h2 C k/.
The Crank–Nicolson method is centered in both space and time, and an analysis of
its local truncation error shows that it is second order accurate in both space and time,
.x; t/ D O.k 2 C h2 /:
A method is said to be consistent if .x; t/ ! 0 as k; h ! 0. Just as in the other
cases we have studied (boundary value problems and initial value problems for ordinary
differential equations (ODEs)), we expect that consistency, plus some form of stability, will
be enough to prove that the method converges at each fixed point .X; T / as we refine the
grid in both space and time. Moreover, we expect that for a stable method the global order
of accuracy will agree with the order of the local truncation error, e.g., for Crank–Nicolson
we expect that
Uin u.X; T / D O.k 2 C h2 /
as k; h ! 0 when ih  X and nk  T are fixed.
For linear PDEs, the fact that consistency plus stability is equivalent to convergence is
known as the Lax equivalence theorem and is discussed in Section 9.5 after an introduction
of the proper concept of stability. As usual, it is the definition and study of stability that is
the hard (and interesting) part of this theory.

9.2

Method of lines discretizations

To understand how stability theory for time-dependent PDEs relates to the stability theory
we have already developed for time-dependent ODEs, it is easiest to first consider the socalled method of lines (MOL) discretization of the PDE. In this approach we first discretize
in space alone, which gives a large system of ODEs with each component of the system
corresponding to the solution at some grid point, as a function of time. The system of ODEs
can then be solved using one of the methods for ODEs that we have previously studied.
This system of ODEs is also often called a semidiscrete method, since we have discretized in space but not yet in time.
For example, we might discretize the heat equation (9.1) in space at grid point xi by
Ui0.t/ D

1
.Ui 1 .t/
h2

2Ui .t/ C UiC1 .t// for i D 1; 2; : : : ; m;

(9.10)

where prime now means differentiation with respect to time. We can view this as a coupled
system of m ODEs for the variables Ui .t/, which vary continuously in time along the lines
shown in Figure 9.2. This system can be written as
U 0.t/ D AU.t/ C g.t/;

i

i

(9.11)

i

i

i

i

i

9.2. Method of lines discretizations

“rjlfdm”
2007/6/1
page 185
i

185

U0 .t/

U1 .t/

U2 .t/

Um 1 .t/

UmC1 .t/

x0

x1

x2

xm 1 xm

xmC1

t

Figure 9.2. Method of lines interpretation. Ui .t/ is the solution along the line
forward in time at the grid point xi .
where the tridiagonal matrix A is exactly as in (2.9) and g.t/ includes the terms needed for
the boundary conditions, U0 .t/  g0 .t/ and UmC1 .t/  g1 .t/,
2
3
2
3
2 1
g0 .t/
6 1
6 0 7
7
2 1
6
6
7
7
6
7
7
1
2 1
1 6
1 6
6 0 7
7
AD 26
;
g.t/ D 2 6 : 7 :
(9.12)
7
:
:
:
::
::
::
7
h 6
h 6 :: 7
6
7
6
7
4
4 0 5
1
2 1 5
1
2
g1 .t/
This MOL approach is sometimes used in practice by first discretizing in space and
then applying a software package for systems of ODEs. There are also packages that are
specially designed to apply MOL. This approach has the advantage of being relatively easy
to apply to a fairly general set of time-dependent PDEs, but the resulting method is often
not as efficient as specially designed methods for the PDE. See Section 11.2 for more
discussion of this.
As a tool in understanding stability theory, however, the MOL discretization is extremely valuable, and this is the main use we will make of it. We know how to analyze
the stability of ODE methods applied to a linear system of the form (9.11) based on the
eigenvalues of the matrix A, which now depend on the spatial discretization.
If we apply an ODE method to discretize the system (9.11), we will obtain a fully
discrete method which produces approximations Uin  Ui .tn / at discrete points in time
which are exactly the points .xi ; tn / of the grid that we introduced at the beginning of this
chapter.
For example, applying Euler’s method U nC1 D U n C kf .U n / to this linear system
results in the fully discrete method (9.5). Applying instead the trapezoidal method (5.22)
results in the Crank–Nicolson method (9.7). Applying a higher order linear multistep or
Runge–Kutta method would give a different method, although with the spatial discretization (9.10) the overall method would be only second order accurate in space. Replacing

i

i

i

i

i

i

i

186

“rjlfdm”
2007/6/1
page 186
i

Chapter 9. Diffusion Equations and Parabolic Problems

the right-hand side of (9.10) with a higher order approximation to uxx .xi / and then using
a higher order time discretization would give a more accurate method.

9.3

Stability theory

We can now investigate the stability of schemes like (9.5) or (9.7) since these can be interpreted as standard ODE methods applied to the linear system (9.11). We expect the method
to be stable if k 2 S, i.e., if the time step k multiplied by any eigenvalue  of A lies in the
stability region of the ODE method, as discussed in Chapter 7. (Note that A is symmetric
and hence normal, so eigenvalues are the right thing to look at.)
We have determined the eigenvalues of A in (2.23),
2
.cos.ph/ 1/ for p D 1; 2; : : : ; m;
(9.13)
h2
where again m and h are related by h D 1=.m C 1/. Note that there is a new wrinkle here
relative to the ODEs we considered in Chapter 7: the eigenvalues p depend on the mesh
width h. As we refine the grid and h ! 0, the dimension of A increases, the number of
eigenvalues we must consider increases, and the values of the eigenvalues change.
This is something we must bear in mind when we attempt to prove convergence as
k; h ! 0. To begin, however, let’s consider the simpler question of how the method
behaves for some fixed k and h, i.e., the question of absolute stability in the ODE sense.
Then it is clear that the method is absolutely stable (i.e., the effect of past errors will not
grow exponentially in future time steps) provided that kp 2 S for each p, where S is the
stability region of the ODE method, as discussed in Chapter 7.
For the matrix (9.12) coming from the heat equation, the eigenvalues lie on the negative real axis and the one farthest from the origin is m  4= h2. Hence we require that
4k= h2 2 S (assuming the stability region is connected along the negative real axis up to
the origin, as is generally the case).
Example 9.2. If we use Euler’s method to obtain the discretization (9.5), then we
must require j1 C kj  1 for each eigenvalue (see Chapter 7) and hence we require
2  4k= h2  0. This limits the time step allowed to
p D

k
1
 :
2
h
2

(9.14)

This is a severe restriction: the time step must decrease at the rate of h2 as we refine the
grid, which is much smaller than the spatial width h when h is small.
Example 9.3. If we use the trapezoidal method, we obtain the Crank–Nicolson discretization (9.6). The trapezoidal method for the ODE is absolutely stable in the whole
left half-plane and the eigenvalues (9.13) are always negative. Hence the Crank–Nicolson
method is stable for any time step k > 0. Of course it may not be accurate if k is too large.
Generally we must take k D O.h/ to obtain a reasonable solution, and the unconditional
stability allows this.

9.4

Stiffness of the heat equation

Note that the system of ODEs we are solving is quite stiff, particularly for small h. The
eigenvalues of A lie on the negative real axis with one fairly close to the origin, 1   2

i

i

i

i

i

i

i

9.4. Stiffness of the heat equation

“rjlfdm”
2007/6/1
page 187
i

187

for all h, while the largest in magnitude is m  4= h2. The “stiffness ratio” of the system
is 4= 2h2 , which grows rapidly as h ! 0. As a result the explicit Euler method is stable
only for very small time steps k  12 h2 . This is typically much smaller than what we would
like to use over physically meaningful times, and a method designed for stiff problems will
be more efficient.
The stiffness is a reflection of the very different time scales present in solutions to
the physical problem modeled by the heat equation. High frequency spatial oscillations
in the initial data will decay very rapidly due to rapid diffusion over very short distances,
while smooth data decay much more slowly since diffusion over long distances takes much
longer. This is apparent from the Fourier analysis of Section E.3.3 or is easily seen by
writing down the exact solution to the heat equation on 0  x  1 with g0 .t/ D g1 .t/  0
as a Fourier sine series:
1
X
u.x; t/ D
uO j .t/ sin.j x/:
j D1

Inserting this in the heat equation gives the ODEs
uO 0j .t/ D j 2  2uO j .t/
and so

for j D 1; 2; ; : : :
2

(9.15)

2

uO j .t/ D e j  t uO j .0/
with the uO j .0/ determined as the Fourier coefficients of the initial data .x/.
We can view (9.15) as an infinite system of ODEs, but which are decoupled so that
the coefficient matrix is diagonal, with eigenvalues j 2  2 for j D 1; 2; : : :. By choosing
data with sufficiently rapid oscillation (large j ), we can obtain arbitrarily rapid decay. For
general initial data there may be some transient period when any high wave numbers are
rapidly damped, but then the long-time behavior is dominated by the slower decay rates.
See Figure 9.3 for some examples of the time evolution with different sets of data.
If we are solving the problem over the long periods needed to track this slow diffusion, then we would ultimately (after any physical transients have decayed) like to use
rather large time steps, since typically the variation in time is then on roughly the same scale
as variations in space. We would generally like to have k  h so that we have roughly the
same resolution in time as we do in space. A method that requires k  h2 forces us to take
a much finer temporal discretization than we should need to represent smooth solutions. If
h D 0:001, for example, then if we must take k D h2 rather than k D h we would need
to take 1000 time steps to cover each time interval that should be well modeled by a single
time step. This is the same difficulty we encountered with stiff ODEs in Chapter 8.
Note: The remark above that we want k  h is reasonable assuming the method we
are using has the same order of accuracy in both space and time. The method (9.5) does not
have this property. Since the error is O.k C h2 / we might want to take k D O.h2 / just to
get the same level of accuracy in both space and time. In this sense the stability restriction
k D O.h2 / may not seem unreasonable, but this is simply another reason for not wanting
to use this particular method in practice.
Note: The general diffusion equation is ut D uxx and in practice the diffusion
coefficient  may be different from 1 by many orders of magnitude. How does this affect
our conclusions above? We would expect by scaling considerations that we should take

i

i

i

i

i

188

“rjlfdm”
2007/6/1
page 188
i

Chapter 9. Diffusion Equations and Parabolic Problems
u1 .x; t1 /

2

3

4

5

6

1.0
1

2

4

5

6

0

4

5

6

3

4

5

6

0

2

5

6

4

6

5

6

1.0

1.0

0.5

u

u

0.0

-0.5

-0.5
4

3

x

-1.0

-1.0

x

1

u3 .x; t2 /

0.5

1.0
0.5
0.0

3

5

0.5

u
2

x

-0.5

2

6

-0.5
1

u3 .x; t1 /

-1.0

1

5

-1.0
0

x

u3 .x; t0 /

0

4

1.0

1.0
u
0.0
-0.5
3

3

u2 .x; t2 /

-1.0
2

2

x

0.5

1.0
0.5
0.0
-1.0

1

1

u2 .x; t1 /

-0.5

u

3

x

u2 .x; t0 /

0

0.0

u
0

x

0.0

1

-1.0

-1.0

-0.5

-0.5

u

0.0

0.5

0.5

1.0
0.5

u

0.0
-0.5
-1.0
0

u

u1 .x; t2 /

1.0

u1 .x; t0 /

0.0

i

i

0

1

2

3

x

4

5

6

0

1

2

3

4

x

Figure 9.3. Solutions to the heat equation at three different times (columns) shown
for three different sets of initial conditions (rows). In the top row u1 .x; t0 / consists of only
a low wave number, which decays slowly. The middle row shows data consisting of a
higher wave number, which decays more quickly. The bottom row shows data u3.x; t0 / that
contains a mixture of wave numbers. The high wave numbers are most rapidly damped (an
initial rapid transient), while at later times only the lower wave numbers are still visible
and decaying slowly.
k  h= in order to achieve comparable resolution in space and time, i.e., we would like
to take k= h  1. (Note that uO j .t/ D exp. j 2  2t/uO j .0/ in this case.) With the MOL
discretization we obtain the system (9.11) but A now has a factor  in front. For stability we
thus require 4k= h2 2 S, which requires k= h2 to be order 1 for any explicit method.
This is smaller than what we wish to use by a factor of h, regardless of the magnitude of .
So our conclusions on stiffness are unchanged by . In particular, even when the diffusion
coefficient is very small it is best to use an implicit method because we then want to take
very long time steps k  h=.
These comments apply to the case of pure diffusion. If we are solving an advectiondiffusion or reaction-diffusion equation where there are other time scales determined by
other phenomena, then if the diffusive term has a very small coefficient we may be able to
use an explicit method efficiently because of other restrictions on the time step.
Note: The physical problem of diffusion is “infinitely stiff” in the sense that (9.15)
has eigenvalues j 2  2 with arbitrarily large magnitude, since j can be any integer. Luckily the discrete problem is not this stiff. It is not stiff because, once we discretize in space,
only a finite number of spatial wave numbers can be represented and we obtain the finite

i

i

i

i

i

i

i

9.5. Convergence

“rjlfdm”
2007/6/1
page 189
i

189

set of eigenvalues (9.13). As we refine the grid we can represent higher and higher wave
numbers, leading to the increasing stiffness ratio as h ! 0.

9.5

Convergence

So far we have only discussed absolute stability and determined the relation between k
and h that must be satisfied to ensure that errors do not grow exponentially as we march
forward in time on this fixed grid. We now address the question of convergence at a fixed
point .X; T / as the grid is refined. It turns out that in general exactly the same relation
between k and h must now be required to hold as we vary k and h, letting both go to zero.
In other words, we cannot let k and h go to zero at arbitrary independent rates and
necessarily expect the resulting approximations to converge to the solution of the PDE.
For a particular sequence of grids .k1 ; h1 /, .k2 ; h2 /; : : :, with kj ! 0 and hj ! 0, we
will expect convergence only if the proper relation ultimately holds for each pair. For the
method (9.5), for example, the sequence of approximations will converge only if kj = h2j 
1=2 for all j sufficiently large.
It is sometimes easiest to think of k and h as being related by some fixed rule (e.g.,
we might choose k D 0:4h2 for the method (9.5)), so that we can speak of convergence as
k ! 0 with the understanding that this relation holds on each grid.
The methods we have studied so far can be written in the form
U nC1 D B.k/U n C b n .k/

(9.16)

for some matrix B.k/ 2 Rmm on a grid with h D 1=.m C 1/ and b n .k/ 2 Rm . In general
these depend on both k and h, but we will assume some fixed rule is specified relating h to
k as k ! 0.
For example, applying forward Euler to the MOL system (9.11) gives
B.k/ D I C kA;

(9.17)

where A is the tridiagonal matrix in (9.12). The Crank–Nicolson method results from
applying the trapezoidal method to (9.11), which gives

 1

k
k
B.k/ D I
IC A :
A
(9.18)
2
2
To prove convergence we need consistency and a suitable form of stability. As usual,
consistency requires that the local truncation error vanishes as k ! 0. The form of stability
that we need is often called Lax–Richtmyer stability.
Definition 9.1. A linear method of the form (9.16) is Lax–Richtmyer stable if, for each time
T , there is a constant CT > 0 such that
kB.k/n k  CT

(9.19)

for all k > 0 and integers n for which k n  T .
Theorem 9.2 (Lax Equivalence Theorem). A consistent linear method of the form (9.16)
is convergent if and only if it is Lax–Richtmyer stable.

i

i

i

i

i

i

i

190

“rjlfdm”
2007/6/1
page 190
i

Chapter 9. Diffusion Equations and Parabolic Problems

For more discussion and a proof see [75]. The main idea is the same as our proof in
Section 6.3.1 that Euler’s method converges on a linear problem. If we apply the numerical
method to the exact solution u.x; t/, we obtain
unC1 D Bun C b n C k n ;

(9.20)

where we suppress the dependence on k for clarity and where
2
2
3
3
u.x1 ; tn /
.x1 ; tn /
6 u.x2 ; tn / 7
6 .x2 ; tn / 7
6
6
7
7
un D 6
n D 6
7;
7:
::
::
4
4
5
5
:
:
u.xm ; tn /

.xm ; tn /

Subtracting (9.20) from (9.16) gives the difference equation for the global error E n D
U n un :
E nC1 D BE n k n ;
and hence, after N time steps,
EN D BN E0

N
X

k

BN n n 1 ;

nD1

from which we obtain
kE N k  kB N kkE 0k C k

N
X

kB N n k k n 1 k:

(9.21)

nD1

If the method is Lax–Richtmyer stable, then for N k  T ,
kE N k  CT kE 0 k C T CT max k n 1 k
1nN

! 0 as k ! 0 for N k  T;
provided the method is consistent (kk ! 0) and we use appropriate initial data (kE 0k !
0 as k ! 0).
Example 9.4. For the heat equation the matrix A from (9.12) is symmetric, and
hence the 2-norm is equal to the spectral radius, and the same is true of the matrix B from
(9.17). From (9.13) we see that kB.k/k2  1, provided (9.14) is satisfied, and so the
method is Lax–Richtmyer stable and hence convergent under this restriction on the time
step. Similarly, the matrix B of (9.18) is symmetric and has eigenvalues .1 C kp =2/=.1
kp =2/, and so the Crank–Nicolson method is stable in the 2-norm for any k > 0.
For the methods considered so far we have obtained kBk  1. This is called strong
stability. But note that this is not necessary for Lax–Richtmyer stability. If there is a
constant ˛ so that a bound of the form
kB.k/k  1 C ˛k

i

i

(9.22)

i

i

i

i

i

9.5. Convergence

“rjlfdm”
2007/6/1
page 191
i

191

holds in some norm (at least for all k sufficiently small), then we will have Lax–Richtmyer
stability in this norm, since
kB.k/n k  .1 C ˛k/n  e ˛T
for nk  T . Note that the matrix B.k/ depends on k, and its dimension m D O.1= h/
grows as k; h ! 0. The general theory of stability in the sense of uniform power boundedness of such families of matrices is often nontrivial.

9.5.1 PDE versus ODE stability theory
It may bother you that the stability we need for convergence now seems to depend on
absolute stability, and on the shape of the stability region for the time-discretization, which
determines the required relationship between k and h. Recall that in the case of ODEs all
we needed for convergence was “zero-stability,” which does not depend on the shape of the
stability region except for the requirement that the point z D 0 must lie in this region.
Here is the difference: with ODEs we were studying a fixed system of ODEs of fixed
dimension, and the fixed set of eigenvalues  was independent of k. For convergence we
needed k in the stability region as k ! 0, but since these values all converge to 0 it is
only the origin that is important, at least to prove convergence as k ! 0. Hence the need
for zero-stability. With PDEs, on the other hand, in our MOL discretization the system of
ODEs grows as we refine the grid, and the eigenvalues  grow in magnitude as k and h go
to zero. So it is not clear that k will go to zero, and zero-stability is not sufficient. For the
heat equation with k= h2 fixed, these values do not go to zero as k ! 0. For convergence
we must now require that these values at least lie in the region of absolute stability as
k ! 0, and this gives the stability restriction relating k and h. If we keep k= h fixed as
k; h ! 0, then k ! 1 for the eigenvalues of the matrix A from (9.12). We must use
an implicit method that includes the entire negative real axis in its stability region.
We also notice another difference between stability theory for ODEs and PDEs that
for the ODE u0.t/ D f .u.t// we could prove convergence of standard methods for any Lipschitz continuous function f .u/. For example, the proof of convergence of Euler’s method
for the linear case, found in Section 6.3.1, was easily extended to nonlinear functions in
Section 6.3.3. In the PDE case, the Lax equivalence theorem is much more limited: it applies only to linear methods (9.16), and such methods typically only arise when discretizing
linear PDEs such as the heat equation. It is possible to prove stability of many methods for
nonlinear PDEs by showing that a suitable form of stability holds, but a variety of different
techniques must be used, depending on the character of the differential equation, and there
is no general theory of the sort obtained for ODEs.
The essential difficulty is that even a linear PDE such as the heat equation ut D @2x u
involves an operator on the right-hand side that is not Lipschitz continuous in a function
space norm of the sort introduced in Section A.4. Discretizing on a grid replaces @2x u by
f .U / D AU , which is Lipschitz continuous, but the Lipschitz constant kAk grows at the
rate of 1= h2 as the grid is refined. In the nonlinear case it is often difficult to obtain the sort
of bounds needed to prove convergence. See [40], [68], [75], or [84] for further discussions
of stability.

i

i

i

i

i

i

i

192

9.6

“rjlfdm”
2007/6/1
page 192
i

Chapter 9. Diffusion Equations and Parabolic Problems

Von Neumann analysis

Although it is useful to go through the MOL formulation to understand how stability theory
for PDEs is related to the theory for ODEs, in practice there is another approach that will
sometimes give the proper stability restrictions more easily.
The von Neumann approach to stability analysis is based on Fourier analysis and
hence is generally limited to constant coefficient linear PDEs. For simplicity it is usually
applied to the Cauchy problem, which is the PDE on all space with no boundaries, 1 <
x < 1 in the one-dimensional case. Von Neumann analysis can also be used to study
the stability of problems with periodic boundary conditions, e.g., in 0  x  1 with
u.0; t/ D u.1; t/ imposed. This is generally equivalent to a Cauchy problem with periodic
initial data.
Stability theory for PDEs with more general boundary conditions can often be quite
difficult, as the coupling between the discretization of the boundary conditions and the
discretization of the PDE can be very subtle. Von Neumann analysis addresses the issue
of stability of the PDE discretization alone. Some discussion of stability theory for initial
boundary value problems can be found in [84], [75]. See also Section 10.12.
The Cauchy problem for linear PDEs can be solved using Fourier transforms—see
Section E.3 for a review. The basic reason this works is that the functions e ix with wave
number  D constant are eigenfunctions of the differential operator @x ,
@x e ix D ie ix ;
and hence of any constant coefficient linear differential operator. Von Neumann analysis
is based on the fact that the related grid function Wj D e ij h is an eigenfunction of any
translation-invariant finite difference operator.1 For example, if we approximate v 0.xj / by
1
D0 Vj D 2h
.Vj C1 Vj 1 /, then in general the grid function D0 V is not a scalar multiple
of V . But for the special case of W , we obtain

1  i.j C1/h
e i.j 1/h
e
2h

1  ih
D
e ih e ij h
e
2h
i
D sin.h/e ij h
h
i
D sin.h/Wj :
h

D0 W j D

(9.23)

So W is an “eigengridfunction” of the operator D0 , with eigenvalue hi sin.h/.
Note the relation between these and the eigenfunctions and eigenvalues of the operator @x found earlier: Wj is simply the eigenfunction w.x/ of @x evaluated at the point xj ,
and for small h we can approximate the eigenvalue of D0 by

p
1 In this section i D

i

i

1 and the index j is used on the grid functions.

i

i

i

i

i

9.6. Von Neumann analysis

“rjlfdm”
2007/6/1
page 193
i

193



i
1 3 3
i
sin.h/ D
h
h  C O.h5  5 /
h
h
6
i 2 3
h  C :
D i
6
This agrees with the eigenvalue i of @x to O.h2  3 /.
Suppose we have a grid function Vj defined at grid points xj D j h for j D 0; ˙
1; ˙ 2; : : : , which is an l2 function in the sense that the 2-norm
0
kU k2 D @h

1
X

11=2
jUj j2 A

jD 1

is finite. Then we can express Vj as a linear combination of the grid functions e ij h for all
 in the range = h    = h. Functions with larger wave number  cannot be resolved
on this grid. We can write
1
Vj D p
2
where

Z = h
VO ./e ij h d;
= h

1
X
h
VO ./ D p
Vj e ij h :
2 j D 1

These are direct analogue of the formulas for a function v.x/ in the discrete case.
Again we have Parseval’s relation, kVO k2 D kV k2 , although the 2-norms used for
the grid function Vj and the function VO ./ defined on Œ = h; = h are different:
0
kV k2 D @h

1
X
jD 1

11=2
jVj j2 A

!1=2

Z = h
;

jVO ./j2 d

kVO k2 D

:

= h

To show that a finite difference method is stable in the 2-norm by the techniques
discussed earlier in this chapter, we would have to show that kBk2  1 C ˛k in the
notation of (9.22). This amounts to showing that there is a constant ˛ such that
kU nC1 k2  .1 C ˛k/kU n k2
for all U n . This can be difficult to attack directly because of the fact that computing kU k2
requires summing over all grid points, and each UjnC1 depends on values of U n at neighboring grid points so that all grid points are coupled together. In some cases one can work
with these infinite sums directly, but it is rare that this can be done. Alternatively one can
work with the matrix B itself, as we did above in Section 9.5, but this matrix is growing as
we refine the grid.
Using Parseval’s relation, we see that it is sufficient to instead show that
kUO nC1 k2  .1 C ˛k/kUO nk2 ;

i

i

i

i

i

i

i

194

“rjlfdm”
2007/6/1
page 194
i

Chapter 9. Diffusion Equations and Parabolic Problems

where UO n is the Fourier transform of the grid function U n . The utility of Fourier analysis
now stems from the fact that after Fourier transforming the finite difference method, we
obtain a recurrence relation for each UO n ./ that is decoupled from all other wave numbers.
For a two-level method this has the form
UO nC1 ./ D g./UO n ./:

(9.24)

The factor g./, which depends on the method, is called the amplification factor for the
method at wave number . If we can show that
jg./j  1 C ˛k;
where ˛ is independent of , then it follows that the method is stable, since then
jUO nC1 ./j  .1 C ˛k/jUO n ./j for all 
and so
kUO nC1 k2  .1 C ˛k/kUO nk2 :
Fourier analysis allows us to obtain simple scalar recursions of the form (9.24) for
each wave number separately, rather than dealing with a system of equations for Ujn that
couples together all values of j .
Note: Here we are assuming that u.x; t/ is a scalar, so that g./ is a scalar. For a
system of s equations we would find that g./ is an s s matrix for each value of , so some
analysis of matrix eigenvalues is still required to investigate stability. But the dimension
of the matrices is s, independent of the grid spacing, unlike the MOL analysis, where the
matrix dimension increases as k ! 0.
Example 9.5. Consider the method (9.5). To apply von Neumann analysis we consider how this method works on a single wave number , i.e., we set
Ujn D e ij h :

(9.25)

UjnC1 D g./e ij h ;

(9.26)

Then we expect that
where g./ is the amplification factor for this wave number. Inserting these expressions
into (9.5) gives

k 
g./e ij h D e ij h C 2 e i .j 1/h 2e ij h C e i .j C1/h
h


k  i h
i h
D 1C 2 e
e ij h ;
2Ce
h
and hence

k
g./ D 1 C 2 2 .cos.h/ 1/:
h
Since 1  cos.h/  1 for any value of , we see that
1

k
4 2  g./  1
h

for all . We can guarantee that jg./j  1 for all  if we require

i

i

i

i

i

i

i

9.7. Multidimensional problems

“rjlfdm”
2007/6/1
page 195
i

195
k
4 2  2:
h

This is exactly the stability restriction (9.14) we found earlier for this method. If this
restriction is violated, then the Fourier components with some wave number  will be
amplified (and, as expected, it is the largest wave numbers that become unstable first as k
is increased).
Example 9.6. The fact that the Crank–Nicolson method is stable for all k and h
can also be shown using von Neumann analysis. Substituting (9.25) and (9.26) into the
difference equations (9.7) and canceling the common factor of e ij h gives the following
relation for g  g./:

k 
g D 1 C 2 e i h 2 C e i h .1 C g/;
2h
and hence
gD

1 C 12 z
1

1
z
2

;

(9.27)

where
k
.e i h 2 C e i h/
h2
2k
D 2 .cos.h/ 1/:
h

zD

(9.28)

Since z  0 for all , we see that jgj  1 and the method is stable for any choice of k
and h.
Note that (9.27) agrees with the root 1 found for the trapezoidal method in Example 7.6, while the z determined in (9.28), for certain values of , is simply k times an
eigenvalue p from (9.13), the eigenvalues of the MOL matrix (9.11). In general there is a
close connection between the von Neumann approach and the MOL reduction of a periodic
problem to a system of ODEs.

9.7

Multidimensional problems

In two space dimensions the heat equation takes the form
ut D uxx C uyy

(9.29)

with initial conditions u.x; y; 0/ D .x; y/ and boundary conditions all along the boundary
of our spatial domain . We can discretize in space using a discrete Laplacian of the form
considered in Chapter 3, say, the 5-point Laplacian from Section 3.2:
rh2 Uij D

1
.Ui 1;j C UiC1;j C Ui;j 1 C Ui;j C1
h2

4Uij /:

(9.30)

If we then discretize in time using the trapezoidal method, we will obtain the two-dimensional
version of the Crank–Nicolson method,
i
kh 2 n
UijnC1 D Uijn C
(9.31)
rh Uij C rh2 UijnC1 :
2

i

i

i

i

i

i

i

196

“rjlfdm”
2007/6/1
page 196
i

Chapter 9. Diffusion Equations and Parabolic Problems

Since this method is implicit, we must solve a system of equations for all the Uij where the
matrix has the same nonzero structure as for the elliptic systems considered in Chapters 3
and 4. This matrix is large and sparse, and we generally do not want to solve the system by
a direct method such as Gaussian elimination. This is even more true for the systems we
are now considering than for the elliptic equation, because of the slightly different nature of
this system, which makes other approaches even more efficient relative to direct methods.
It is also extremely important now that we use the most efficient method possible, because
we must now solve a linear system of this form in every time step, and we may need to take
thousands of time steps to solve the time-dependent problem.
We can rewrite the equations (9.31) as




k 2
k
I
(9.32)
rh UijnC1 D I C rh2 Uijn :
2
2
The matrix for this linear system has the same pattern of nonzeros as the matrix for rh2 (see
Chapter 3), but the values are scaled by k=2 and then subtracted from the identity matrix,
so that the diagonal elements are fundamentally different. If we call this matrix A,
ADI

k 2
r ;
2 h

then we find that the eigenvalues of A are
p;q D 1

k
.cos.ph/
h2


1/ C .cos.qh/

1/

for p; q D 1; 2; : : : ; m, where we have used the expression for the eigenvalues of rh2
from Section 3.4. Now the largest eigenvalue of the matrix A thus has magnitude O.k= h2 /
while the ones closest to the origin are at 1CO.k/. As a result the condition number of A is
O.k= h2 /. By contrast, the discrete Laplacian rh2 alone has condition number O.1= h2 / as
we found in Section 3.4. The smaller condition number in the present case can be expected
to lead to faster convergence of iterative methods.
Moreover, we have an excellent starting guess for the solution U nC1 to (9.31), a fact
that we can use to good advantage with iterative methods but not with direct methods. Since
UijnC1 D Uijn C O.k/, we can use Uijn , the values from the previous time step, as initial
values UijŒ0 for an iterative method. We might do even better by extrapolating forward in
Œ0

time, using, say, Uij D 2Uijn

Uijn 1 , or by using an explicit method, say,
Œ0

Uij D .I C krh2 /Uijn :
This explicit method (forward Euler) would probably be unstable as a time-marching procedure if we used only this with the value of k we have in mind, but it can sometimes be
used successfully as a way to generate initial data for an iterative procedure.
Because of the combination of a reasonably well-conditioned system and very good
initial guess, we can often get away with taking only one or two iterations in each time
step, and still get global second order accuracy.

i

i

i

i

i

i

i

9.8. The locally one-dimensional method

9.8

“rjlfdm”
2007/6/1
page 197
i

197

The locally one-dimensional method

Rather than solving the coupled sparse matrix equation for all the unknowns on the grid
simultaneously as in (9.32), an alternative approach is to replace this fully coupled single
time step with a sequence of steps, each of which is coupled in only one space direction, resulting in a set of tridiagonal systems which can be solved much more easily. One example
is the locally one-dimensional (LOD) method:
k 2 n
.D U C Dx2 Uij /;
2 x ij
k
UijnC1 D Uij C .Dy2 Uij C Dy2 UijnC1 /;
2
Uij D Uijn C

(9.33)
(9.34)

or, in matrix form,

I

I




k 2
k 2

D U D I C Dx U n ;
2 x
2



k 2
k
nC1
2
D I C Dy U  :
D U
2 y
2

(9.35)
(9.36)

In (9.33) we apply Crank–Nicolson in the x-direction only, solving ut D uxx alone over
time k, and we call the result U  . Then in (9.34) we take this result and apply Crank–
Nicolson in the y-direction to it, solving ut D uyy alone, again over time k. Physically this
corresponds to modeling diffusion in the x- and y-directions over time k as a decoupled
process in which we first allow u to diffuse only in the x-direction and then only in the
y-direction. If the time steps are very short, then this might be expected to give similar
physical behavior and hence convergence to the correct behavior as k ! 0. In fact, for
the constant coefficient diffusion problem, it can even be shown that (in the absence of
boundaries at least) this alternating diffusion approach gives exactly the same behavior as
the original two-dimensional diffusion. Diffusing first in x alone over time k and then in
y alone over time k gives the same result as if the diffusion occurs simultaneously in both
directions. (This is because the differential operators @2x and @2y commute, as discussed
further in Example 11.1.)
Numerically there is a great advantage in using (9.35) and (9.36) rather than the fully
coupled (9.32). In (9.35) the unknowns Uij are coupled together only across each row of
the grid. For any fixed value of j we have a tridiagonal system of equations to solve for
Uij .i D 1; 2; : : : ; m/. The system obtained for each value of j is completely decoupled
from the system obtained for other values of j . Hence we have a set of m C 2 tridiagonal
systems to solve (for j D 0; 1; : : : ; m C 1), each of dimension m, rather than a single
coupled system with m2 unknowns as in (9.32). Since each of these systems is tridiagonal,
it is easily solved in O.m/ operations by Gaussian elimination and there is no need for
iterative methods. (In the next section we will see why we need to solve these for j D 0
and j D m C 1 as well as at the interior grid points.)
Similarly, (9.34) decouples into a set of m tridiagonal systems in the y-direction for
i D 1; 2; : : : ; m. Hence taking a single time step requires solving 2m C 2 tridiagonal
systems of size m, and thus O.m2 / work. Since there are m2 grid points, this is the optimal
order and no worse than an explicit method, except for a constant factor.

i

i

i

i

i

i

i

198

“rjlfdm”
2007/6/1
page 198
i

Chapter 9. Diffusion Equations and Parabolic Problems

9.8.1 Boundary conditions
nC1

In solving the second set of systems (9.34), we need boundary values Ui0
and Ui0
along
nC1

and Ui;mC1 along the top boundary, for terms that go on
the bottom boundary and Ui;mC1
the right-hand side of each tridiagonal system. The values at level n C 1 are available from
the given boundary data for the heat equation, by evaluating the boundary conditions at

time tnC1 (assuming Dirichlet boundary conditions are given). To obtain the values Ui0
we
solve (9.33) for j D 0 and j D m C 1 (along the boundaries) in addition to the systems
along each row interior to the grid.
n

To solve the first set of systems (9.33), we need boundary values U0j
and U0j
along
n

the left boundary and values UmC1;j and UmC1;j along the right boundary. The values at
level n come from the given boundary conditions, but we must determine the intermediate
boundary conditions at level  along these boundaries. It is not immediately clear what
values should be used. One might be tempted to think of level  as being halfway between
tn and tnC1 , since U  is generated in the middle of the two-step procedure used to obtain
U nC1 from U n . If this were valid, then evaluating the given boundary data at time tnC1=2 D
tn C k=2 might provide values for U  on the boundary. This is not a good idea, however,
and would lead to a degradation of accuracy. The problem is that in the first step, (9.33)
does not model the full heat equation over time k=2 but rather models part of the equation
(diffusion in x alone) over the full time step k. The values along the boundary will in
general evolve quite differently in the two different cases.


To determine proper values for U0j
and UmC1;j
, we can use (9.34) along the left and
right boundaries. At i D 0, for example, this equation gives a system of equations along

the left boundary that can be viewed as a tridiagonal linear system or the unknowns U0j
in
nC1
terms of the values U0j , which are already known from the boundary conditions at time
tnC1 . Note that we are solving this equation backward from the way it will be used in the
second step of the LOD process on the interior of the grid, and this works only because we
nC1
already know U0j
from boundary data.
Since we are solving this equation backward, we can view this as solving the diffusion equation ut D uyy over a time step of length k, backward in time. This makes
sense physically—the intermediate solution U  represents what is obtained from U n by
doing diffusion in x alone, with no diffusion yet in y. There are in principle two ways to
get this, either by starting with U n and diffusing in x or by starting with U nC1 and
“undiffusing” in y. We are using the latter approach along the boundaries to generate data
for U  .
Equivalently we can view this as solving the backward heat equation ut D uyy
over time k. This may be cause for concern, since the backward heat equation is ill posed
(see Section E.3.4). However, since we are doing this only over one time step starting with
nC1
given values U0j
in each time step, this turns out to be a stable procedure.

There is still a difficulty at the corners. To solve (9.34) for U0j
, j D 1; 2; : : : ; m,


we need to know the values of U00 and U0;mC1 that are the boundary values for this system.
These can be approximated using some sort of explicit and uncentered approximation to
either ut D uxx starting with U n , or to ut D uyy starting with U nC1 . For example, we
might use

i

i

i

i

i

i

i

9.8. The locally one-dimensional method

“rjlfdm”
2007/6/1
page 199
i

199

k
nC1
nC1
.U nC1 2U01
C U02
/;
h2 00
which uses the approximation to uyy centered at .x0 ; y1 /.
Alternatively, rather than solving the tridiagonal systems obtained from (9.34) for

U0j
, we could simply use an explicit approximation to the backward heat equation along
this boundary,
k

nC1
nC1
nC1
U0j
D U0j
.U nC1
2U0j
C U0;j
(9.37)
C1 /
h2 0;j 1
for j D 1; 2; : : : ; m. This eliminates the need for values of U  in the corners. Again,
since this is not iterated but done only starting with given (and presumably smooth) boundary data U nC1 in each time step, this yields a stable procedure.
With proper treatment of the boundary conditions, it can be shown that the LOD
method is second order accurate (see Example 11.1). It can also be shown that this method,
like full Crank–Nicolson, is unconditionally stable for any time step.

nC1
U00
D U00

9.8.2 The alternating direction implicit method
A modification of the LOD method is also often used, in which the two steps each involve
discretization in only one spatial direction at the advanced time level (giving decoupled
tridiagonal systems again) but coupled with discretization in the opposite direction at the
old time level. The classical method of this form is
k 2 n
.D U C Dx2 Uij /;
2 y ij
k
UijnC1 D Uij C .Dx2 Uij C Dy2 UijnC1 /:
2
Uij D Uijn C

(9.38)
(9.39)

This is called the alternating direction implicit (ADI) method and was first introduced by
Douglas and Rachford [26]. This again gives decoupled tridiagonal systems to solve in
each step:




k 2
k
I
(9.40)
Dx U  D I C Dy2 U n ;
2
2




k 2
k
I
(9.41)
Dy U nC1 D I C Dx2 U  :
2
2
With this method, each of the two steps involves diffusion in both the x- and the
y-direction. In the first step the diffusion in x is modeled implicitly, while diffusion in y is
modeled explicitly, with the roles reversed in the second step. In this case each of the two
steps can be shown to give a first order accurate approximation to the full heat equation
over time k=2, so that U  represents a first order accurate approximation to the solution at
time tnC1=2 . Because of the symmetry of the two steps, however, the local error introduced
in the second step almost exactly cancels the local error introduced in the first step, so that
the combined method is in fact second order accurate over the full time step.
Because U  does approximate the solution at time tnC1=2 in this case, it is possible
to simply evaluate the given boundary conditions at time tnC1=2 to generate the necessary
boundary values for U  . This will maintain second order accuracy. A better error constant

i

i

i

i

i

i

i

200

“rjlfdm”
2007/6/1
page 200
i

Chapter 9. Diffusion Equations and Parabolic Problems

can be achieved by using slightly modified boundary data which introduces the expected
error in U  into the boundary data that should be canceled out by the second step.

9.9

Other discretizations

For illustration purposes we have considered only the classic Crank–Nicolson method consisting of second order centered approximation to uxx coupled with the trapezoidal method
for time stepping. However, an infinite array of other combinations of spatial approximation and time stepping methods could be considered, some of which may be preferable.
The following are a few possibilities:
 The second order accurate spatial difference operator could be replaced by a higher
order method, such as the fourth order accurate approximations of Section 2.20.1 in
one dimension of Section 3.5 in more dimensions.
 A spectral method could be used in the spatial dimension(s), as discussed in Section 2.21. Note that in this case the linear system that must be solved in each time
step will be dense. On the other hand, for many problems it is possible to use a much
coarser grid for spectral methods, leading to relatively small linear algebra problems.
 The time-stepping procedure could be replaced by a different implicit method suitable for stiff equations, of the sort discussed in Chapter 8. In particular, for some
problems it is desirable to use an L-stable method. While the trapezoidal method
is stable, it does not handle underresolved transients well (recall Figure 8.4). For
some problems where diffusion is coupled with other processes there are constantly
high-frequency oscillations or discontinuities introduced that should be smoothed by
diffusion, and Crank–Nicolson can suffer from oscillations in time.
 The time stepping could be done by using a method such as the Runge–Kutta–
Chebyshev method described in Section 8.6. This is an explicit method that works
for mildly stiff problems with real eigenvalues, such as the heat equation.
 The time stepping could be done using the exponential time differencing (ETD)
methods described in Section 11.6. The heat equation with constant coefficients and
time-varying boundary conditions leads to a MOL discretization of the form (9.11),
where A is a constant matrix. If the centered difference approximation is used in one
dimension, then (9.12) holds, but even with other discretizations, or in more dimensions, the semidiscrete system still has the form U 0 .t/ D AU.t/ C g.t/. The exact
solution can be written in terms of the matrix exponential e At and this form is used
in the ETD methods. The manner in which this is computed depends on whether A is
large and sparse (the typical case with a finite difference discretization) or small and
dense (as it might be if a spectral discretization is used in space). See Section 11.6.1
for more discussion of this.

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 201
i

Chapter 10

Advection Equations and
Hyperbolic Systems

Hyperbolic partial differential equations (PDEs) arise in many physical problems, typically whenever wave motion is observed. Acoustic waves, electromagnetic waves, seismic waves, shock waves, and many other types of waves can be modeled by hyperbolic
equations. Often these are modeled by linear hyperbolic equations (for the propagation
of sufficiently small perturbations), but modeling large motions generally requires solving nonlinear hyperbolic equations. Hyperbolic equations also arise in advective transport,
when a substance is carried along with a flow, giving rise to an advection equation. This
is a scalar linear first order hyperbolic PDE, the simplest possible case. See Appendix E
for more discussion of hyperbolic problems and a derivation of the advection equation in
particular.
In this chapter we will primarily consider the advection equation. This is sufficient to
illustrate many (although certainly not all) of the issues that arise in the numerical solution
of hyperbolic equations. Section 10.10 contains a very brief introduction to hyperbolic systems, still in the linear case. A much more extensive discussion of hyperbolic problems and
numerical methods, including nonlinear problems and multidimensional methods, can be
found in [66]. Those interested in solving more challenging hyperbolic problems may also
look at the CLAWPACK software [64], which was designed primarily for hyperbolic problems. There are also a number of other books devoted to nonlinear hyperbolic equations
and their solution, e.g., [58], [88].

10.1

Advection

In this section we consider numerical methods for the scalar advection equation
ut C aux D 0;

(10.1)

where a is a constant. See Section E.2.1 for a discussion of this equation. For the Cauchy
problem we also need initial data
u.x; 0/ D .x/:
201

i

i

i

i

i

i

i

202

“rjlfdm”
2007/6/1
page 202
i

Chapter 10. Advection Equations and Hyperbolic Systems

This is the simplest example of a hyperbolic equation, and it is so simple that we can write
down the exact solution,
u.x; t/ D .x at/:
(10.2)
One can verify directly that this is the solution (see also Appendix E). However, many of
the issues that arise more generally in discretizing hyperbolic equations can be most easily
seen with this equation.
The first approach we might consider is the analogue of the method (9.4) for the heat
equation. Using the centered difference in space,
ux .x; t/ D

u.x C h; t/

u.x
2h

h; t/

C O.h2 /

(10.3)

and the forward difference in time results in the numerical method
UjnC1
k

Ujn

D

a
.U n
2h j C1

Ujn 1 /;

(10.4)

ak n
.U
2h j C1

Ujn 1 /:

(10.5)

which can be rewritten as
UjnC1 D Ujn

This again has the stencil shown in Figure 9.1(a). In practice this method is not useful
because of stability considerations, as we will see in the next section.
A minor modification gives a more useful method. If we replace Ujn on the righthand side of (10.5) by the average 12 .Ujn 1 C UjnC1 /, then we obtain the Lax–Friedrichs
method,
1
ak n
UjnC1 D .Ujn 1 C UjnC1 /
Ujn 1 /:
(10.6)
.U
2
2h j C1
Because of the low accuracy, this method is not commonly used in practice, but it
serves to illustrate some stability issues and so we will study this method along with (10.5)
before describing higher order methods, such as the well-known Lax–Wendroff method.
We will see in the next section that Lax–Friedrichs is Lax–Richtmyer stable (see
Section 9.5) and convergent provided
ˇ ˇ
ˇ ak ˇ
ˇ ˇ  1:
(10.7)
ˇ h ˇ
Note that this stability restriction allows us to use a time step k D O.h/ although the
method is explicit, unlike the case of the heat equation. The basic reason is that the advection equation involves only the first order derivative ux rather than uxx and so the difference
equation involves 1= h rather than 1= h2.
The time step restriction (10.7) is consistent with what we would choose anyway
based on accuracy considerations, and in this sense the advection equation is not stiff, unlike
the heat equation. This is a fundamental difference between hyperbolic equations and
parabolic equations more generally and accounts for the fact that hyperbolic equations are
typically solved with explicit methods, while the efficient solution of parabolic equations
generally requires implicit methods.

i

i

i

i

i

i

i

10.2. Method of lines discretization

“rjlfdm”
2007/6/1
page 203
i

203

To see that (10.7) gives a reasonable time step, note that
ux .x; t/ D 0.x

at/;

while
ut .x; t/ D

aux .x; t/ D a0 .x

at/:

The time derivative ut is larger in magnitude than ux by a factor of a, and so we would
expect the time step required to achieve temporal resolution consistent with the spatial
resolution h to be smaller by a factor of a. This suggests that the relation k  h=a would
be reasonable in practice. This is completely consistent with (10.7).

10.2

Method of lines discretization

To investigate stability further we will again introduce the method of lines (MOL) discretization as we did in Section 9.2 for the heat equation. To obtain a system of equations
with finite dimension we must solve the equation on some bounded domain rather than
solving the Cauchy problem. However, in a bounded domain, say, 0  x  1, the advection equation can have a boundary condition specified on only one of the two boundaries.
If a > 0, then we need a boundary condition at x D 0, say,
u.0; t/ D g0 .t/;

(10.8)

which is the inflow boundary in this case. The boundary at x D 1 is the outflow boundary
and the solution there is completely determined by what is advecting to the right from the
interior. If a < 0, we instead need a boundary condition at x D 1, which is the inflow
boundary in this case.
The symmetric 3-point methods defined above can still be used near the inflow
boundary but not at the outflow boundary. Instead the discretization will have to be coupled with some “numerical boundary condition” at the outflow boundary, say, a one-sided
discretization of the equation. This issue complicates the stability analysis and will be
discussed in Section 10.12.
For analysis purposes we can obtain a nice MOL discretization if we consider the
special case of periodic boundary conditions,
u.0; t/ D u.1; t/ for t  0:
Physically, whatever flows out at the outflow boundary flows back in at the inflow boundary.
This also models the Cauchy problem in the case where the initial data is periodic with
period 1, in which case the solution remains periodic and we need to model only a single
period 0  x  1.
In this case the value U0 .t/ D UmC1 .t/ along the boundaries is another unknown,
and we must introduce one of these into the vector U.t/. If we introduce UmC1 .t/, then we
have the vector of grid values
2
3
U1 .t/
6 U2 .t/ 7
6
7
U.t/ D 6
7:
::
4
5
:
UmC1 .t/

i

i

i

i

i

i

i

204

“rjlfdm”
2007/6/1
page 204
i

Chapter 10. Advection Equations and Hyperbolic Systems

For 2  j  m we have the ordinary differential equation (ODE)
a
.Uj C1 .t/
2h

Uj0 .t/ D

Uj 1 .t//;

while the first and last equations are modified using the periodicity:
U10 .t/ D
0
UmC1
.t/ D

a
.U2 .t/
2h
a
.U1 .t/
2h

UmC1 .t//;
Um .t//:

This system can be written as
U 0 .t/ D AU.t/

(10.9)

with
2

AD

3

0
6 1
6
a 6
6
6
2h 6
6
4
1

1
0
1

1
1
0
::
:

1
::
:
1

::

:
0
1

Note that this matrix is skew-symmetric (AT D
imaginary. In fact, the eigenvalues are
p D

7
7
7
7
7 2 R.mC1/.mC1/ :
7
7
1 5
0

(10.10)

A) and so its eigenvalues must be pure

ia
sin.2ph/ for p D 1; 2; : : : ; m C 1:
h

(10.11)

The corresponding eigenvector up has components
p

uj D e 2 ipj h for j D 1; 2; : : : ; m C 1:

(10.12)

The eigenvalues lie on the imaginary axis between ia= h and ia= h.
For absolute stability of a time discretization we need the stability region S to include
this interval. Any method that includes some interval iy; jyj < b of the imaginary axis
will lead to a stable method for the advection equation provided jak= hj  b. For example,
looking again at the stability regions plotted in Figures 7.1 through 7.3 and Figure 8.5 shows
that the midpoint method or certain Adams methods may be suitable for this problem,
whereas the backward differentiation formula (BDF) methods are not.

10.2.1 Forward Euler time discretization
The method (10.5) can be viewed as the forward Euler time discretization of the MOL
system of ODEs (10.9). We found in Section 7.3 that this method is stable only if j1Ckj 
1 and the stability region S is the unit circle centered at 1. No matter how small the ratio
k= h is, since the eigenvalues p from (10.11) are imaginary, the values kp will not lie in
S. Hence the method (10.5) is unstable for any fixed mesh ratio k= h; see Figure 10.1(a).

i

i

i

i

i

i

i

10.2. Method of lines discretization

“rjlfdm”
2007/6/1
page 205
i

205

The method (10.5) will be convergent if we let k ! 0 faster than h, since then
kp ! 0 for all p and the zero-stability of Euler’s method is enough to guarantee convergence. Taking k much smaller than h is generally not desirable and the method is not
used in practice. However, it is interesting to analyze this situation also in terms of Lax–
Richtmyer stability, since it shows an example where the Lax–Richtmyer stability uses a
weaker bound of the form (9.22), kBk  1 C ˛k, rather than kBk  1. Here B D I C kA.
Suppose we take k D h2 , for example. Then we have
j1 C kp j2  1 C .ka= h/2
for each p (using the fact that p is pure imaginary) and so
j1 C kp j2  1 C a2 h2 D 1 C a2 k:
Hence kI C kAk22  1 C a2 k and if nk  T , we have
2

k.I C kA/n k2  .1 C a2 k/n=2  e a T =2 ;
showing the uniform boundedness of kB n k (in the 2-norm) needed for Lax–Richtmyer
stability.

10.2.2 Leapfrog
A better time discretization is to use the midpoint method (5.23),
U nC1 D U n 1 C 2kAU n ;
which gives the leapfrog method for the advection equation,
UjnC1 D Ujn 1

ak n
.U
h j C1

Ujn 1 /:

(10.13)

This is a 3-level explicit method and is second order accurate in both space and time.
Recall from Section 7.3 that the stability region of the midpoint method is the interval
i˛ for 1 < ˛ < 1 of the imaginary axis. This method is hence stable on the advection
equation provided jak= hj < 1 is satisfied.
On the other hand, note that the kp will always be on the boundary of the stability
region (the stability region for midpoint has no interior). This means the method is only
marginally stable—there is no growth but also no decay of any eigenmode. The difference
equation is said to be nondissipative. In some ways this is good—the true advection equation is also nondissipative, and any initial condition simply translates unchanged, no matter
how oscillatory. Leapfrog captures this qualitative behavior well.
However, there are problems with this. All modes translate without decay, but they do
not all propagate at the correct velocity, as will be explained in Example 10.12. As a result
initial data that contains high wave number components (e.g., if the data contains steep
gradients) will disperse and can result in highly oscillatory numerical approximations.
The marginal stability of leapfrog can also turn into instability if a method of this sort
is applied to a more complicated problem with variable coefficients or nonlinearities.

i

i

i

i

i

i

i

206

“rjlfdm”
2007/6/1
page 206
i

Chapter 10. Advection Equations and Hyperbolic Systems

10.2.3 Lax–Friedrichs
Again consider the Lax–Friedrichs method (10.6). Note that we can rewrite (10.6) using
the fact that
1 n
1
C UjnC1 / D Ujn C .Ujn 1 2Ujn C UjnC1 /
.U
2 j 1
2
to obtain
ak n
.U
2h j C1

UjnC1 D Ujn

1
Ujn 1 / C .Ujn 1
2

2Ujn C UjnC1 /:

(10.14)

This can be rearranged to give
UjnC1
k

Ujn

Ca

UjnC1

Ujn 1

!

Ujn 1

h2
D
2k

2h

2Ujn C UjnC1
h2

!
:

If we compute the local truncation error from this form we see, as expected, that it is
consistent with the advection equation ut C aux D 0, since the term on the right-hand side
vanishes as k; h ! 0 (assuming k= h is fixed). However, it looks more like a discretization
of the advection-diffusion equation
ut C aux D uxx ;
where  D h2 =2k.
Later in this chapter we will study the diffusive nature of many methods for the
advection equation. For our present purposes, however, the crucial part is that we can now
view (10.14) as resulting from a forward Euler discretization of the system of ODEs
U 0 .t/ D A U.t/
with

2

A D

0
6 1
6
a 6
6
6
2h 6
6
4
1
2
2
6 1
6
 6
6
C 26
h 6
6
4
1

3
1
0
1

1
1
0
::
:

1
::
:
1

7
7
7
7
7
7
7
1 5
0

::

:
0
1

3
1
2
1

(10.15)

1
1
2
::
:

1
::
:
1

::

:
2
1

7
7
7
7
7;
7
7
1 5
2

where  D h2 =2k. The matrix A differs from the matrix A of (10.10) by the addition of
a small multiple of the second difference operator, which is symmetric rather than skewsymmetric. As a result the eigenvalues of A are shifted off the imaginary axis and now lie

i

i

i

i

i

i

i

10.3. The Lax–Wendroff method

“rjlfdm”
2007/6/1
page 207
i

207

in the left half-plane. There is now some hope that each k will lie in the stability region
of Euler’s method if k is small enough relative to h.
It can be verified that the eigenvectors (10.12) of the matrix A are also eigenvectors of
the second difference operator (with periodic boundary conditions) that appears in (10.15),
and hence these are also the eigenvectors of the full matrix A . We can easily compute that
the eigenvalues of A are
p D

ia
sin.2ph/
h

2
.1
h2

cos.2ph//:

(10.16)

The values kp are plotted in the complex plane for various different values of  in Figure 10.1. They lie on an ellipse centered at 2k= h2 with semi-axes of length 2k= h2
in the x-direction and ak= h in the y-direction. For the special case  D h2 =2k used in
Lax–Friedrichs, we have 2k= h2 D 1 and this ellipse lies entirely inside the unit circle
centered at 1, provided that jak= hj  1. (If jak= hj > 1, then the top and bottom of
the ellipse would extend outside the circle.) The forward Euler method is stable as a timediscretization, and hence the Lax–Friedrichs method is Lax–Richtmyer stable, provided
jak= hj  1.

10.3

The Lax–Wendroff method

One way to achieve second order accuracy on the advection equation is to use a second
order temporal discretization of the system of ODEs (10.9), since this system is based
on a second order spatial discretization. This can be done with the midpoint method, for
example, which gives rise to the leapfrog scheme (10.13) already discussed. However, this
is a three-level method and for various reasons it is often much more convenient to use
two-level methods for PDEs whenever possible—in more than one dimension the need to
store several levels of data may be restrictive, boundary conditions can be harder to impose,
and combining methods using fractional step procedures (as discussed in Chapter 11) may
require two-level methods for each step, to name a few reasons. Moreover, the leapfrog
method is nondissipative, leading to potential stability problems if the method is extended
to variable coefficient or nonlinear problems.
Another way to achieve second order accuracy in time would be to use the trapezoidal
method to discretize the system (10.9), as was done to derive the Crank–Nicolson method
for the heat equation. But this is an implicit method and for hyperbolic equations there is
generally no need to introduce this complication and expense.
Another possibility is to use a two-stage Runge–Kutta method such as the one in
Example 5.11 for the time discretization. This can be done, although some care must be
exercised near boundaries, and the use of a multistage method again typically requires
additional storage.
One simple way to achieve a two-level explicit method with higher accuracy is to use
the idea of Taylor series methods, as described in Section 5.6. Applying this directly to the
linear system of ODEs U 0 .t/ D AU.t/ (and using U 00 D AU 0 D A2 U ) gives the second
order method
1
U nC1 D U n C kAU n C k 2 A2 U n :
2

i

i

i

i

i

i

i

208

“rjlfdm”
2007/6/1
page 208
i

Chapter 10. Advection Equations and Hyperbolic Systems

1.5

(a)  D 0:0 (Forward Euler)

1

1

0.5

0.5

0

0

−0.5

−0.5

−1

−1

−1.5
−2.5

−2

−1.5

−1

−0.5

0

0.5

(c)  D 0:005

1.5

−1.5
−2.5

1.5

1

1

0.5

0.5

0

0

−0.5

−0.5

−1

−1

−1.5
−2.5

1.5

−2

−1.5

−1

−0.5

0

0.5

(e)  D 0:0125 (Lax-Friedrichs)

−1.5
−2.5

1

0.5

0.5

0

0

−0.5

−0.5

−1

−1

−2

−1.5

−1

−0.5

0

0.5

−2

−1.5

−1.5
−2.5

−1

−0.5

0

0.5

(d)  D 0:008 (Lax-Wendroff)

−2

−1.5

−1

−0.5

0

0.5

(f)  D 0:014

1.5

1

−1.5
−2.5

(b)  D 0:001

1.5

−2

−1.5

−1

−0.5

0

0.5

Figure 10.1. Eigenvalues of the matrix A in (10.15), for various values of , in
the case h D 1=50 and k D 0:8h, a D 1, so ak= h D 0:8. (a) shows the case  D 0
which corresponds to the forward Euler method (10.5). (d) shows the case  D a2 k=2, the
Lax–Wendroff method (10.18). (e) shows the case  D h2 =2k, the Lax–Friedrichs method
(10.6). The method is stable for  between a2 k=2 and h2 =2k, as in (d) through (e).

i

i

i

i

i

i

i

10.3. The Lax–Wendroff method

“rjlfdm”
2007/6/1
page 209
i

209

Here A is the matrix (10.10), and computing A2 and writing the method at the typical grid
point then gives
UjnC1 D Ujn

ak n
.U
2h j C1

Ujn 1 / C

a2 k 2 n
.Uj 2
8h2

2Ujn C UjnC2 /:

(10.17)

This method is second order accurate and explicit but has a 5-point stencil involving the
points Ujn 2 and UjnC2 . With periodic boundary conditions this is not a problem, but with
other boundary conditions this method needs more numerical boundary conditions than a 3point method. This makes it less convenient to use and potentially more prone to numerical
instability.
Note that the last term in (10.17) is an approximation to 12 a2 k 2 uxx using a centered
difference based on step size 2h. A simple way to achieve a second order accurate 3-point
method is to replace this term by the more standard 3-point formula. We then obtain the
standard Lax–Wendroff method:
UjnC1 D Ujn

ak n
.U
2h j C1

Ujn 1 / C

a2 k 2 n
.Uj 1
2h2

2Ujn C UjnC1 /:

(10.18)

A cleaner way to derive this method is to use Taylor series expansions directly on the
PDE ut C aux D 0, to obtain
1
u.x; t C k/ D u.x; t/ C kut .x; t/ C k 2 ut t .x; t/ C    :
2
Replacing ut by aux and ut t by a2 uxx gives
u.x; t C k/ D u.x; t/

1
kaux .x; t/ C k 2 a2 uxx .x; t/ C    :
2

If we now use the standard centered approximations to ux and uxx and drop the higher
order terms, we obtain the Lax–Wendroff method (10.18). It is also clear how we could
obtain higher order accurate explicit two-level methods by this same approach, by retaining
more terms in the series and approximating the spatial derivatives (including the higher
order spatial derivatives that will then arise) by suitably high order accurate finite difference
approximations. The same approach can also be used with other PDEs. The key is to
replace the time derivatives arising in the Taylor series expansion with spatial derivatives,
using expressions obtained by differentiating the original PDE.

10.3.1 Stability analysis
We can analyze the stability of Lax–Wendroff following the same approach used for Lax–
Friedrichs in Section 10.2. Note that with periodic boundary conditions, the Lax–Wendroff
method (10.18) can be viewed as Euler’s method applied to the linear system of ODEs
U 0 .t/ D A U.t/, where A is given by (10.15) with  D a2 k=2 (instead of the value
 D h2 =2k used in Lax–Friedrichs). The eigenvalues of A are given by (10.16) with the
appropriate value of , and multiplying by the time step k gives
 
 2
ak
ak
kp D i
.cos.ph/ 1/:
sin.ph/ C
h
h

i

i

i

i

i

i

i

210

“rjlfdm”
2007/6/1
page 210
i

Chapter 10. Advection Equations and Hyperbolic Systems

These values all lie on an ellipse centered at .ak= h/2 with semi-axes of length .ak= h/2
and jak= hj. If jak= hj  1, then all of these values lie inside the stability region of Euler’s
method. Figure 10.1(d) shows an example in the case ak= h D 0:8. The Lax–Wendroff
method is stable with exactly the same time step restriction (10.7) as required for Lax–
Friedrichs. In Section 10.7 we will see that this is a very natural stability condition to
expect for the advection equation and is the best we could hope for when a 3-point method
is used.
A close look at Figure 10.1 shows that the values kp near the origin lie much closer
to the boundary of the stability region for the Lax–Wendroff method (Figure 10.1(d)) than
for the other methods illustrated in this figure. This is a reflection of the fact that Lax–
Wendroff is second order accurate, while the others are only first order accurate. Note that
a value kp lying inside the stability region indicates that this eigenmode will be damped
as the wave propagates, which is unphysical behavior since the true solution advects with
no dissipation. For small values of p (low wave numbers, smooth components) the Lax–
Wendroff method has relatively little damping and the method is more accurate. Higher
wave numbers are still damped with Lax–Wendroff (unless jak= hj D 1, in which case
all the kp lie on the boundary of S) and resolving the behavior of these modes properly
would require a finer grid.
Comparing Figures 10.1(c), (d), and (e) shows that Lax–Wendroff has the minimal
amount of numerical damping needed to bring the values kp within the stability region.
Any less damping, as in Figure 10.1(c) would lead to instability, while more damping as
in Figure 10.1(e) gives excessive smearing of low wave numbers. Recall that the value of
 used in Lax–Wendroff was determined by doing a Taylor series expansion and requiring
second order accuracy, so this makes sense.

10.4

Upwind methods

So far we have considered methods based on symmetric approximations to derivatives. Alternatively, one might use a nonsymmetric approximation to ux in the advection equation,
e.g.,
1
ux .xj ; t/  .Uj Uj 1 /
(10.19)
h
or
1
ux .xj ; t/  .Uj C1 Uj /:
(10.20)
h
These are both one-sided approximations, since they use data only to one side or the other
of the point xj . Coupling one of these approximations with forward differencing in time
gives the following methods for the advection equation:
UjnC1 D Ujn

ak n
.U
h j

Ujn 1 /

(10.21)

or

ak n
Ujn /:
(10.22)
.U
h j C1
These methods are first order accurate in both space and time. One might wonder why we
would want to use such approximations, since centered approximations are more accurate.
UjnC1 D Ujn

i

i

i

i

i

i

i

10.4. Upwind methods

“rjlfdm”
2007/6/1
page 211
i

211

For the advection equation, however, there is an asymmetry in the equations because the
equation models translation at speed a. If a > 0, then the solution moves to the right,
while if a < 0 it moves to the left. There are situations where it is best to acknowledge this
asymmetry and use one-sided differences in the appropriate direction.
The choice between the two methods (10.21) and (10.22) should be dictated by the
sign of a. Note that the true solution over one time step can be written as
u.xj ; t C k/ D u.xj

ak; t/

so that the solution at the point xj at the next time level is given by data to the left of xj
if a > 0, whereas it is determined by data to the right of xj if a < 0. This suggests that
(10.21) might be a better choice for a > 0 and (10.22) for a < 0.
In fact the stability analysis below shows that (10.21) is stable only if
0

ak
 1:
h

(10.23)

Since k and h are positive, we see that this method can be used only if a > 0. This method
is called the upwind method when used on the advection equation with a > 0. If we view
the equation as modeling the concentration of some tracer in air blowing past us at speed
a, then we are looking in the correct upwind direction to judge how the concentration will
change with time. (This is also referred to as an upstream differencing method in some
literature.)
Conversely, (10.22) is stable only if
1

ak
0
h

(10.24)

and can be used only if a < 0. In this case (10.22) is the proper upwind method to use.

10.4.1 Stability analysis
The method (10.21) can be written as
UjnC1 D Ujn

ak n
.U
2h j C1

Ujn 1 / C

ak n
.U
2h j C1

2Ujn C Ujn 1 /;

(10.25)

which puts it in the form (10.15) with  D ah=2. We have seen previously that methods of
this form are stable provided jak= hj  1 and also 2 < 2k= h2 < 0. Since k; h > 0,
this requires in particular that  > 0. For Lax–Friedrichs and Lax–Wendroff, this condition
was always satisfied, but for upwind the value of  depends on a and we see that  > 0 only
if a > 0. If a < 0, then the eigenvalues of the MOL matrix lie on a circle that lies entirely
in the right half-plane, and the method will certainly be unstable. If a > 0, then the above
requirements lead to the stability restriction (10.23).
If we think of (10.25) as modeling an advection-diffusion equation, then we see that
a < 0 corresponds to a negative diffusion coefficient. This leads to an ill-posed equation,
as in the “backward heat equation” (see Section E.3.4).
The method (10.22) can also be written in a form similar to (10.25), but the last term
will have a minus sign in front of it. In this case we need a < 0 for any hope of stability
and then easily derive the stability restriction (10.24).

i

i

i

i

i

i

i

212

“rjlfdm”
2007/6/1
page 212
i

Chapter 10. Advection Equations and Hyperbolic Systems

The three methods, Lax–Wendroff, upwind, and Lax–Friedrichs, can all be written
in the same form (10.15) with different values of . If we call these values LW , up , and
LF , respectively, then we have
LW D

a2 k
ah
D
;
2
2

up D

ah
;
2

LF D

h2
ah
D
;
2k
2

where  D ak= h. Note that
LW D up

up D LF :

and

If 0 <  < 1, then LW < up < LF and the method is stable for any value of  between
LW and LF , as suggested by Figure 10.1.

10.4.2 The Beam–Warming method
The upwind method is only first order accurate. A second order accurate method with the
same one-sided character can be derived by following the derivation of the Lax–Wendroff
method, but using one-sided approximations to the spatial derivatives. This results in the
Beam–Warming method, which for a > 0 takes the form
UjnC1 D Ujn

ak
.3Ujn
2h

4Ujn 1 C Ujn 2 / C

a2 k 2 n
.Uj
2h2

2Ujn 1 C Ujn 2 /:

(10.26)

For a < 0 the Beam–Warming method is one-sided in the other direction:
UjnC1 D Ujn

ak
. 3Ujn C 4UjnC1
2h

UjnC2 / C

a2 k 2 n
.U
2h2 j

2UjnC1 C UjnC2 /: (10.27)

These methods are stable for 0    2 and 2    0, respectively.

10.5

Von Neumann analysis

We have analyzed the stability of various algorithms for the advection equation by viewing
them as ODE methods applied to the MOL system (10.9). The same stability criteria can
be obtained by using von Neumann analysis as described
p in Section 9.6. Recall that this
is done by replacing Ujn by g./n e ij h (where i D
1 in this section). Canceling out
common factors results in an expression for the amplification factor g./, and requiring
that this be bounded by 1 in magnitude gives the stability bounds for the method.
Also recall from Section 9.6 that this can be expected to give the same result as our
MOL analysis because of the close relation between the e ij h factor and the eigenvectors
of the matrix A. In a sense von Neumann analysis simply combines the computation of the
eigenvalues of A together with the absolute stability analysis of the time-stepping method
being used. Nonetheless we will go through this analysis explicitly for several of the methods already considered to show how it works, since for other methods it may be more
convenient to work with this approach than to interpret the method as an MOL method.
For the von Neumann analysis in this section we will simplify notation slightly by
setting  D ak= h, the Courant number.

i

i

i

i

i

i

i

10.5. Von Neumann analysis

“rjlfdm”
2007/6/1
page 213
i

213

Example 10.1. Following the procedure of Example 9.6 for the upwind method
(10.21) gives


g./ D 1  1 e i h D .1 / C e i h :
(10.28)
As the wave number  varies, g./ moves around a circle of radius  centered at 1 .
These values stay within the unit circle if and only if 0    1, the stability limit that was
also found in Section 10.4.1.
Example 10.2. Going through the same procedure for Lax–Friedrichs (10.6) gives


1  i h
 e i h
C e i h
e
2
D cos.h/ i sin.h/

g./ D


e i h
(10.29)

and so
jg./j2 D cos2 .h/ C  2 sin2 .h/;

(10.30)

which is bounded by 1 for all  only if jj  1.
Example 10.3. For the Lax–Wendroff method (10.18) we obtain

D1

 1 
1  i h
e i h C  2 e i h
 e
2
2
i sin.h/ C  2 .cos.h/ 1/

D1

iŒ2 sin.h=2/ cos.h=2/ C  2 Œ2 sin2 .h=2/;

g./ D 1


2 C e i h
(10.31)

where we have used two trigonometric identities to obtain the last line. This complex
number has modulus
jg./j2 D Œ1
D1

2 2 sin2 .h=2/2 C 4 sin2 .h=2/ cos2 .h=2/
4 2 .1

 2 / sin4 .h=2/:

(10.32)

Since 0  sin4 .h=2/  1 for all values of , we see that jg./j2  1 for all , and hence
the method is stable provided that jj  1, which again gives the expected stability bound
(10.7).
Example 10.4. The leapfrog method (10.13) involves three time levels but can still
be handled by the same basic approach. If we set Ujn D g./n e ij h in the leapfrog method
we obtain


g./nC1 e ij h D g./n 1 e ij h g./n e i .j C1/h e i .j 1/h :
(10.33)
If we now divide by g./n 1 e ij h we obtain a quadratic equation for g./,
g./2 D 1

2i sin.h/g./:

(10.34)

Examining this in the same manner as the analysis of the stability region for the midpoint
method in Example 7.7 yields the stability limit jj < 1.

i

i

i

i

i

i

i

214

“rjlfdm”
2007/6/1
page 214
i

Chapter 10. Advection Equations and Hyperbolic Systems

It is important to note the severe limitations of the von Neumann approach just presented. It is strictly applicable only in the constant coefficient linear case (with periodic
boundary conditions or on the Cauchy problem). Applying von Neumann analysis to the
“frozen coefficient” problem locally often gives good guidance to the stability properties
of a method more generally, but it cannot always be relied on. A great deal of work has
been done on proving that methods stable for frozen coefficient problems remain stable for
variable coefficient or nonlinear problems when everything is sufficiently smooth; see, for
example, [40], [75]. In the nonlinear case, where the solution can contain shocks, a nonlinear stability theory is needed that employs techniques very different from von Neumann
analysis; see, e.g., [66].

10.6

Characteristic tracing and interpolation

The solution to the advection equation is given by (10.2). The value of u is constant along
each characteristic, which for this example is a straight line with constant slope. Over a
single time step we have
u.xj ; tnC1 / D u.xj ak; tn /:
(10.35)
Tracing this characteristic back over time step k from the grid point xj results in the picture
shown in Figure 10.2(a). Note that if 0 < ak= h < 1, then the point xj ak lies between
xj 1 and xj . If we carefully choose k and h so that ak= h D 1 exactly, then xj ak D
xj 1 and we would find that u.xj ; tnC1 / D u.xj 1 ; tn /. The solution should just shift one
grid cell to the right in each time step. We could compute the exact solution numerically
with the method
UjnC1 D Ujn 1 :
(10.36)
Actually, all the two-level methods that we have considered so far reduce to the formula
(10.36) in this special case ak D h, and each of these methods happens to be exact in this
case.

h

h

tnC1

tn
xj

xj 1
(a)

ak

xj C1

xj
(b)

ak

Figure 10.2. Tracing the characteristic of the advection equation back in time
from the point .xj ; tnC1 / to compute the solution according to (10.35). Interpolating the
value at this point from neighboring grid values gives the upwind method (for linear interpolation) or the Lax–Wendroff or Beam–Warming methods (quadratic interpolation). (a)
shows the case a > 0, (b) shows the case a < 0.

i

i

i

i

i

i

i

10.7. The Courant–Friedrichs–Lewy condition

“rjlfdm”
2007/6/1
page 215
i

215

If ak= h < 1, then the point xj ak is not exactly at a grid point, as illustrated
in Figure 10.2. However, we might attempt to use the relation (10.35) as the basis for a
numerical method by computing an approximation to u.xj ak; tn / based on interpolation
from the grid values Uin at nearby grid points. For example, we might perform simple
linear interpolation between Ujn 1 and Ujn . Fitting a linear function to these points gives
the function
!
n
n
U
U
j
j
1
p.x/ D Ujn C .x xj /
:
(10.37)
h
Evaluating this at xj

ak and using this to define UjnC1 gives

ak n
Ujn 1 /:
.U
h j
This is precisely the first order upwind method (10.21). Note that this also can be interpreted as a linear combination of the two values Ujn 1 and Ujn :


ak
ak n
UjnC1 D 1
(10.38)
Ujn C
U :
h
h j 1
UjnC1 D p.xj

ak/ D Ujn

Moreover, this is a convex combination (i.e., the coefficients of Ujn and Ujn 1 are both
nonnegative and sum to 1) provided the stability condition (10.23) is satisfied, which is
also the condition required to ensure that xj ak lies between the two points xj 1 and
xj . In this case we are interpolating between these points with the function p.x/. If the
stability condition is violated, then we would be using p.x/ to extrapolate outside of the
interval where the data lies. It is easy to see that this sort of extrapolation can lead to
instability—consider what happens if the data U n is oscillatory with Ujn D . 1/j , for
example.
To obtain better accuracy, we might try using a higher order interpolating polynomial
based on more data points. If we define a quadratic polynomial p.x/ by interpolating
the values Ujn 1 , Ujn , and UjnC1 , and then define UjnC1 by evaluating p.xj ak/, we
simply obtain the Lax–Wendroff method (10.18). Note that in this case we are properly
interpolating provided that the stability restriction jak= hj  1 is satisfied. If we instead
base our quadratic interpolation on the three points Ujn 2 , Ujn 1 , and Ujn , then we obtain
the Beam–Warming method (10.26), and we are properly interpolating provided 0  ak=
h  2.

10.7

The Courant–Friedrichs–Lewy condition

The discussion of Section 10.6 suggests that for the advection equation, the point xj ak
must be bracketed by points used in the stencil of the finite difference method if the method
is to be stable and convergent. This turns out to be a necessary condition in general for any
method developed for the advection equation: if UjnC1 is computed based on values UjnCp ,
UjnCpC1 ; : : : ; UjnCq with p  q (negative values are allowed for p and q), then we must
have xj Cp  xj ak  xj Cq or the method cannot be convergent. Since xi D ih, this
requires
ak
q
 p:
h

i

i

i

i

i

i

i

216

“rjlfdm”
2007/6/1
page 216
i

Chapter 10. Advection Equations and Hyperbolic Systems

This result for the advection equation is one special case of a much more general principle
that is called the CFL condition. This condition is named after Courant, Friedrichs, and
Lewy, who wrote a fundamental paper in 1928 that was the first paper on the stability and
convergence of finite difference methods for PDEs. (The original paper [17] is in German
but an English translation is available in [18].) The value  D ak= h is often called the
Courant number.
To understand this general condition, we must discuss the domain of dependence of
a time-dependent PDE. (See, e.g., [55], [66] for more details.) For the advection equation,
the solution u.X; T / at some fixed point .X; T / depends on the initial data  at only a
single point: u.X; T / D u.X aT /. We say that the domain of dependence of the point
.X; T / is the point X aT :
D.X; T / D fX

aT g:

If we modify the data  at this point, then the solution u.X; T / will change, while modifying the data at any other point will have no effect on the solution at this point.
This is a rather unusual situation for a PDE. More generally we might expect the
solution at .X; T / to depend on the data at several points or over a whole interval. In
Section 10.10 we consider hyperbolic systems of equations of the form ut C Aux D 0,
where u 2 Rs and A 2 Rss is a matrix with real eigenvalues 1 ; 2; : : : ; s . If these
values are distinct then we will see that the solution u.X; T / depends on the data at the s
distinct points X 1T; : : :, X s T , and hence
D.X; T / D fX

p T for p D 1; 2; : : : ; sg:

(10.39)

The heat equation ut D uxx has a much larger domain of dependence. For this
equation the solution at any point .X; T / depends on the data everywhere and the domain
of dependence is the whole real line,
D.X; T / D . 1; 1/:
This equation is said to have infinite propagation speed, since data at any point affects the
solution everywhere at any small time in the future (although its effect of course decays
exponentially away from this point, as seen from the Green’s function (E.37)).
A finite difference method also has a domain of dependence. On a particular fixed
grid we define the domain of dependence of a grid point .xj ; tn / to be the set of grid points
xi at the initial time t D 0 with the property that the data Ui0 at xi has an effect on the
solution Ujn . For example, with the Lax–Wendroff method (10.18) or any other 3-point
method, the value Ujn depends on Ujn 11 , Ujn 1 , and UjnC11 . These values depend in turn on
Ujn 22 through UjnC22 . Tracing back to the initial time we obtain a triangular array of grid
points as seen in Figure 10.3(a), and we see that Ujn depends on the initial data at the points
xj n ; : : : ; xj Cn .
Now consider what happens if we refine the grid, keeping k= h fixed. Figure 10.3(b)
shows the situation when k and h are reduced by a factor of 2, focusing on the same value
2n
of .X; T / which now corresponds to U2j
on the finer grid. This value depends on twice
as many values of the initial data, but these values all lie within the same interval and are
merely twice as dense.

i

i

i

i

i

i

i

10.7. The Courant–Friedrichs–Lewy condition

t2

t0
(a)

“rjlfdm”
2007/6/1
page 217
i

217

t4

xj 2

xj

xj C2

t0
(b)

xj 4

xj

xj C4

Figure 10.3. (a) Numerical domain of dependence of a grid point when using a
3-point explicit method. (b) On a finer grid.
If the grid is refined further with k= h  r fixed, then clearly the numerical domain of
dependence of the point .X; T / will fill in the interval ŒX T =r; X C T =r . As we refine
the grid, we hope that our computed solution at .X; T / will converge to the true solution
u.X; T / D .X aT /. Clearly this can be possible only if
X

T =r  X

aT  X C T =r:

(10.40)

Otherwise, the true solution will depend only on a value .X aT / that is never seen by
the numerical method, no matter how fine a grid we take. We could change the data at
this point and hence change the true solution without having any effect on the numerical
solution, so the method cannot be convergent for general initial data.
Note that the condition (10.40) translates into jaj  1=r and hence jak= hj  1.
This can also be written as jakj  h, which just says that over a single time step the
characteristic we trace back must lie within one grid point of xj . (Recall the discussion of
interpolation versus extrapolation in Section 10.6.)
The CFL condition generalizes this idea:
The CFL condition: A numerical method can be convergent only if its numerical
domain of dependence contains the true domain of dependence of the PDE, at least in the
limit as k and h go to zero.
For the Lax–Friedrichs, leapfrog, and Lax–Wendroff methods the condition on k and
h required by the CFL condition is exactly the stability restriction we derived earlier in this
chapter. But it is important to note that in general the CFL condition is only a necessary
condition. If it is violated, then the method cannot be convergent. If it is satisfied, then the
method might be convergent, but a proper stability analysis is required to prove this or to
determine the proper stability restriction on k and h. (And of course consistency is also
required for convergence—stability alone is not enough.)
Example 10.5. The 3-point method (10.5) has the same stencil and numerical domain
of dependence as Lax–Wendroff but is unstable for any fixed value of k= h even though the
CFL condition is satisfied for jak= hj  1.
Example 10.6. The upwind methods (10.21) and (10.22) each have a 2-point stencil and the stability restrictions of these methods, (10.23) and (10.24), respectively, agree
precisely with what the CFL condition requires.

i

i

i

i

i

i

i

218

“rjlfdm”
2007/6/1
page 218
i

Chapter 10. Advection Equations and Hyperbolic Systems

Example 10.7. The Beam–Warming method (10.26) has a 3-point one-sided stencil.
The CFL condition is satisfied if 0  ak= h  2. When a < 0 the method (10.27) is used
and the CFL condition requires 2  ak= h  0. These are also the stability regions for
the methods, which must be verified by appropriate stability analysis.
Example 10.8. For the heat equation the true domain of dependence is the whole real
line. It appears that any 3-point explicit method violates the CFL condition, and indeed
it does if we fix k= h as the grid is refined. However, recall from Section 10.2.1 that
the 3-point explicit method (9.5) is convergent as we refine the grid, provided we have
k= h2  1=2. In this case when we make the grid finer by a factor of 2 in space it will
become finer by a factor of 4 in time, and hence the numerical domain of dependence will
cover a wider interval at time t D 0. As k ! 0 the numerical domain of dependence
will spread to cover the entire real line, and hence the CFL condition is satisfied in this
case.
An implicit method such as the Crank–Nicolson method (9.7) satisfies the CFL condition for any time step k. In this case the numerical domain of dependence is the entire
real line because the tridiagonal linear system couples together all points in such a manner that the solution at each point depends on the data at all points (i.e., the inverse of a
tridiagonal matrix is dense).

10.8

Some numerical results

Figure 10.4 shows typical numerical results obtained with three of the methods discussed
in the previous sections. The initial data at time t D 0, shown in Figure 10.4(a), are smooth
and consist of two Gaussian peaks, one sharper than the other:
u.x; 0/ D .x/ D exp. 20.x

2/2 / C exp. .x

5/2 /:

(10.41)

The remaining frames in this figure show the results obtained when solving the advection
equation ut C ux D 0 up to time t D 17, so the exact solution is simply the initial
data shifted by 17 units. Note that only part of the computational domain is shown; the
computation was done on the interval 0  x  25. The grid spacing h D 0:05 was used,
with time step k D 0:8h so the Courant number is ak= h D 0:8. On this grid one peak is
fairly well resolved and the other is poorly resolved.
Figure 10.4(b) shows the result obtained with the upwind method (10.21) and illustrates the extreme numerical dissipation of this method. Figure 10.4(c) shows the result
obtained with Lax–Wendroff. The broader peak remains well resolved, while the dispersive nature of Lax–Wendroff is apparent near the sharper peak. Dispersion is even more
apparent when the leapfrog method is used, as seen in Figure 10.4(d).
The “modified equation” analysis of the next section sheds more light on these results.

10.9

Modified equations

Our standard tool for estimating the accuracy of a finite difference method has been the
“local truncation error.” Seeing how well the true solution of the PDE satisfies the difference equation gives an indication of the accuracy of the difference equation. Now we will
study a slightly different approach that can be very illuminating since it reveals much more
about the structure and behavior of the numerical solution.

i

i

i

i

i

i

i

10.9. Modified equations

219

initial data

Upwind

1.5

1.5

1

1

0.5

0.5

0

0

(a)0.5 0

2

4

6

8

10

(b)0.5 15

20

LaxWendroff

25

Leapfrog

1.5

1.5

1

1

0.5

0.5

0

0

(c)0.5 15

“rjlfdm”
2007/6/1
page 219
i

20

25

(d)0.5 15

20

25

Figure 10.4. The numerical experiments on the advection equation described in
Section 10:8.
The idea is to ask the following question: is there a PDE vt D    such that our
numerical approximation Ujn is actually the exact solution to this PDE, Ujn D v.xj ; tn /?
Or, less ambitiously, can we at least find a PDE that is better satisfied by Ujn than the
original PDE we were attempting to model? If so, then studying the behavior of solutions
to this PDE should tell us much about how the numerical approximation is behaving. This
can be advantageous because it is often easier to study the behavior of PDEs than of finite
difference formulas.
In fact it is possible to find a PDE that is exactly satisfied by the Ujn by doing Taylor
series expansions as we do to compute the local truncation error. However, this PDE will
have an infinite number of terms involving higher and higher powers of k and h. By
truncating this series at some point we will obtain a PDE that is simple enough to study and
yet gives a good indication of the behavior of the Ujn .
The procedure of determining a modified equation is best illustrated with an example.
See [100] for a more detailed discussion of the derivation of modified equations.
Example 10.9. Consider the upwind method (10.21) for the advection equation ut C
aux D 0 in the case a > 0,
UjnC1 D Ujn

i

i

ak n
.U
h j

Ujn 1 /:

(10.42)

i

i

i

i

i

220

“rjlfdm”
2007/6/1
page 220
i

Chapter 10. Advection Equations and Hyperbolic Systems

The process of deriving the modified equation is very similar to computing the local truncation error, only now we insert the formula v.x; t/ into the difference equation. This is
supposed to be a function that agrees exactly with Ujn at the grid points and so, unlike
u.x; t/, the function v.x; t/ satisfies (10.42) exactly:
v.x; t C k/ D v.x; t/

ak
.v.x; t/
h

v.x

h; t//:

Expanding these terms in Taylor series about .x; t/ and simplifying gives




1
1 2
1
1 2
vt C kvt t C k vt t t C    C a vx
hvxx C h vxxx C    D 0:
2
6
2
6
We can rewrite this as
vt C avx D

1
.ahvxx
2

1
kvt t / C .ah2 vxxx
6

k 2 vt t t / C    :

This is the PDE that v satisfies. If we take k= h fixed, then the terms on the right-hand side
are O.k/; O.k 2 /; etc., so that for small k we can truncate this series to get a PDE that is
quite well satisfied by the Ujn .
If we drop all the terms on the right-hand side, we just recover the original advection
equation. Since we have then dropped terms of O.k/, we expect that Ujn satisfies this
equation to O.k/, as we know to be true since this upwind method is first order accurate.
If we keep the O.k/ terms, then we get something more interesting:
vt C avx D

1
.ahvxx
2

kvt t /:

(10.43)

This involves second derivatives in both x and t, but we can derive a slightly different
modified equation with the same accuracy by differentiating (10.43) with respect to t to
obtain
1
vt t D avxt C .ahvxxt kvt t t /
2
and with respect to x to obtain
1
vtx D avxx C .ahvxxx
2

kvt tx /:

Combining these gives
vt t D a2 vxx C O.k/:
Inserting this in (10.43) gives
vt C avx D

1
.ahvxx
2

a2 kvxx / C O.k 2 /:

Since we have already decided to drop terms of O.k 2 /, we can drop these terms here also
to obtain


1
ak
vt C avx D ah 1
(10.44)
vxx :
2
h

i

i

i

i

i

i

i

10.9. Modified equations

“rjlfdm”
2007/6/1
page 221
i

221

This is now a familiar advection-diffusion equation. The grid values Ujn can be viewed as
giving a second order accurate approximation to the true solution of this equation (whereas
they give only first order accurate approximations to the true solution of the advection
equation).
The fact that the modified equation is an advection-diffusion equation tells us a great
deal about how the numerical solution behaves. Solutions to the advection-diffusion equation translate at the proper speed a but also diffuse and are smeared out. This is clearly
visible in Figure 10.4(b).
Note that the diffusion coefficient in (10.44) is 12 .ah a2 k/, which vanishes in the
special case ak D h. In this case we already know that the exact solution to the advection
equation is recovered by the upwind method.
Also note that the diffusion coefficient is positive only if 0 < ak= h < 1. This is
precisely the stability limit of upwind. If this is violated, then the diffusion coefficient in
the modified equation is negative, giving an ill-posed problem with exponentially growing
solutions. Hence we see that even some information about stability can be extracted from
the modified equation.
Example 10.10. If the same procedure is followed for the Lax–Wendroff method, we
find that all O.k/ terms drop out of the modified equation, as is expected since this method
is second order accurate on the advection equation. The modified equation obtained by
retaining the O.k 2 / term and then replacing time derivatives by spatial derivatives is
 2 !
ak
1 2
vt C avx C ah 1
vxxx D 0:
(10.45)
6
h
The Lax–Wendroff method produces a third order accurate solution to this equation. This
equation has a very different character from (10.43). The vxxx term leads to dispersive
behavior rather than diffusion. This is clearly seen in Figure 10.4(c), where the Ujn computed with Lax–Wendroff are compared to the true solution of the advection equation. The
magnitude of the error is smaller than with the upwind method for a given set of k and h,
since it is a higher order method, but the dispersive term leads to an oscillating solution and
also a shift in the location of the main peak, a phase error. This is similar to the dispersive
behavior seen in Figure E.1 for an equation very similar to (10.45).
In Section E.3.6 the propagation properties of dispersive waves is analyzed in terms
of the dispersion relation of the PDE and the phase and group velocities of different wave
numbers. Following the discussion there, we find that for the modified equation (10.45),
the group velocity for wave number  is
 2 !
ak
1 2
cg D a
 2;
ah 1
2
h
which is less than a for all wave numbers. As a result the numerical result can be expected
to develop a train of oscillations behind the peak, with the high wave numbers lagging
farthest behind the correct location.
Some care must be used here, however, when looking at highly oscillatory waves
(relative to the grid, i.e., waves for which h is far from 0). For h sufficiently small
the modified equation (10.45) is a reasonable model, but for larger h the terms we have

i

i

i

i

i

i

i

222

“rjlfdm”
2007/6/1
page 222
i

Chapter 10. Advection Equations and Hyperbolic Systems

neglected in this modified equation may play an equally important role. Rather than determining the dispersion relation for a method from its modified equation, it is more reliable
to determine it directly from the numerical method, which is essentially what we have done
in von Neumann stability analysis. This is pursued further in Example 10.13 below.
If we retain one more term in the modified equation for Lax–Wendroff, we would
find that the Ujn are fourth order accurate solutions to an equation of the form
 2 !
ak
1 2
vt C avx C ah 1
vxxx D vxxxx ;
(10.46)
6
h
where the  in the fourth order dissipative term is O.k 3 Ch3 / and positive when the stability
bound holds. This higher order dissipation causes the highest wave numbers to be damped
(see Section E.3.7), so that there is a limit to the oscillations seen in practice.
The fact that this method can produce oscillatory approximations is one of the reasons
that the first order upwind method is sometimes preferable in practice. In some situations
nonphysical oscillations may be disastrous, for example, if the value of u represents a
concentration that cannot become negative or exceed some limit without difficulties arising
elsewhere in the modeling process.
Example 10.11. The Beam–Warming method (10.26) has a similar modified equation,
 2 !
3ak
1 2
ak
vt C avx D ah 2
vxxx :
(10.47)
C
6
h
h
In this case the group velocity is greater than a for all wave numbers in the case 0 <
ak= h < 1, so that the oscillations move ahead of the main hump. If 1 < ak= h < 2,
then the group velocity is less than a and the oscillations fall behind. (Again the dispersion
relation for (10.47) gives an accurate idea of the dispersive properties of the numerical
method only for h sufficiently small.)
Example 10.12. The modified equation for the leapfrog method (10.13) can be derived by writing




v.x; t C k/ v.x; t k/
v.x C h; t/ v.x h; t/
Da
D0
(10.48)
2k
2h
and expanding in Taylor series. As in Example 10.9 we then further differentiate the resulting equation (which has an infinite number of terms) to express higher spatial derivatives
of v in terms of temporal derivatives. The dominant terms look just like Lax–Wendroff,
and (10.45) is again obtained.
However, from the symmetric form of (10.48) in both x and t we see that all evenorder derivatives drop out. If we derive the next term in the modified equation we will find
an equation of the form
 2 !
ak
1 2
vt C avx C ah 1
vxxx D vxxxxx C   
(10.49)
6
h
for some  D O.h4 C k 4 /, and higher order modified equations will also involve only oddorder derivatives and even powers of h and k. Hence the numerical solution produced with
the leapfrog method is a fourth order accurate solution to the modified equation (10.45).

i

i

i

i

i

i

i

10.9. Modified equations

“rjlfdm”
2007/6/1
page 223
i

223

Moreover, recall from Section E.3.6 that all higher order odd-order derivatives give
dispersive terms. We conclude that the leapfrog method is nondissipative at all orders. This
conclusion is consistent with the observation in Section 10.2.2 that kp is on the boundary
of the stability region for all eigenvalues of A from (10.10), and so we see neither growth
nor decay of any mode. However, we also see from the form of (10.49) that high wave
number modes will not propagate with the correct velocity. This was also true of Lax–
Wendroff, but there the fourth order dissipation damps out the worst offenders, whereas
with leapfrog this dispersion of often much more apparent in computational results, as
observed in Figure 10.4(d).
Example 10.13. Since leapfrog is nondissipative it serves as a nice example for
calculating the true dispersion relation of the numerical method. The approach is very
similar to the von Neumann stability analysis of Section 10.5, only now we use e i.xj !tn /
as our Ansatz (so that the g from von Neumann analysis is replaced by e i!k ). Following
the same procedure as in Example 10.4, we find that
e i!k D e i!k

ak  i h
e
h


e i h ;

(10.50)

which can be simplified to yield
sin.!k/ D

ak
sin.h/:
h

(10.51)

This is the dispersion relation relating ! to . Note that jhj   for waves that can
be resolved on our grid and that for each such h there are two corresponding values of
!k. The dispersion relation is multivalued because leapfrog is a three-level method, and
different temporal behavior of the same spatial wave can be seen, depending on the relation
between the initial data chosen on the two initial levels. For well-resolved waves (jhj
small) and reasonable initial data we expect !k also near zero (not near ˙, where the
other solution is in this case).
Solving for ! as a function of  and expanding in Taylor series for small h would
show this agrees with the dispersion relation of the infinite modified equation (10.49). We
do not need to do this, however, if our goal is to compute the group velocity for the leapfrog
method. We can differentiate (10.51) with respect to  and solve for
d!
a cos.h/
a cos.h/
D
D ˙q
;
d
cos.!k/
1  sin2 .h/

(10.52)

where  D ak= h is the Courant number and again the ˙ arises from the multivalued
dispersion relation. The velocity observed would depend on how the initial two levels are
set.
Note that the group velocity can be negative and near a for jhj  . This is
not surprising since the leapfrog method has a 3-point centered stencil, and it is possible
for numerical waves to travel from right to left although physically there is advection only
to the right. This can be observed in some computations, for example, in Figure 10.5 as
discussed in Example 10.14.

i

i

i

i

i

i

i

224

10.10

“rjlfdm”
2007/6/1
page 224
i

Chapter 10. Advection Equations and Hyperbolic Systems

Hyperbolic systems

The advection equation ut C aux D 0 can be generalized to a first order linear system of
equations of the form
ut C Aux D 0;
u.x; 0/ D .x/;

(10.53)

where u W R  R ! Rs and A 2 Rss is a constant matrix. (Note that this is not the matrix
A from earlier in this chapter, e.g., (10.10).)
This is a system of conservation laws (see Section E.2) with the flux function f .u/ D
Au. This system is called hyperbolic if A is diagonalizable with real eigenvalues, so that
we can decompose
A D RƒR 1 ;
(10.54)
where ƒ D diag.1 ; 2 ; : : : ; s / is a diagonal matrix of eigenvalues and R D Œr1 jr2 j    jrs 
is the matrix of right eigenvectors. Note that AR D Rƒ, i.e.,
Arp D p rp

for p D 1; 2; : : : ; s:

(10.55)

The system is called strictly hyperbolic if the eigenvalues are distinct.

10.10.1 Characteristic variables
We can solve (10.53) by changing to the “characteristic variables”
w D R 1u

(10.56)

in much the same way we solved linear systems of ODEs in Section 7.4.2. Multiplying
(10.53) by R 1 and using (10.54) gives
R 1 ut C ƒR 1 ux D 0

(10.57)

wt C ƒwx D 0:

(10.58)

or, since R 1 is constant,
Since ƒ is diagonal, this decouples into s independent scalar equations
.wp /t C p .wp /x D 0;

p D 1; 2; : : : ; s:

(10.59)

Each of these is a constant coefficient linear advection equation with solution
wp .x; t/ D wp .x

p t; 0/:

(10.60)

Since w D R 1 u, the initial data for wp is simply the pth component of the vector
w.x; 0/ D R 1 .x/:

(10.61)

The solution to the original system is finally recovered via (10.56):
u.x; t/ D Rw.x; t/:

i

i

(10.62)

i

i

i

i

i

10.11. Numerical methods for hyperbolic systems

“rjlfdm”
2007/6/1
page 225
i

225

Note that the value wp .x; t/ is the coefficient of rp in an eigenvector expansion of
the vector u.x; t/, i.e., (10.62) can be written out as
s
X

u.x; t/ D

wp .x; t/rp :

(10.63)

pD1

Combining this with the solutions (10.60) of the decoupled scalar equations gives
s
X

u.x; t/ D

wp .x

p t; 0/rp :

(10.64)

pD1

Note that u.x; t/ depends only on the initial data at the s points x p t. This set of points
is the domain of dependent D.x; t/ of (10.39).
The curves x D x0 C p t satisfying x 0.t/ D p are the “characteristics of the
pth family,” or simply “p-characteristics.” These are straight lines in the case of a constant
coefficient system. Note that for a strictly hyperbolic system, s distinct characteristic curves
pass through each point in the x-t plane. The coefficient wp .x; t/ of the eigenvector rp in
the eigenvector expansion (10.63) of u.x; t/ is constant along any p-characteristic.

10.11

Numerical methods for hyperbolic systems

Most of the methods discussed earlier for the advection equation can be extended directly
to a general hyperbolic system by replacing a with A in the formulas. For example, the
Lax–Wendroff method becomes
UjnC1 D Ujn

k
A.UjnC1
2h

Ujn 1 / C

k2 2 n
A .Uj 1
2h2

2Ujn C UjnC1 /:

(10.65)

This is second order accurate and is stable provided the Courant number is no larger than
1, where the Courant number is defined to be
 D max jp k= hj:

(10.66)

1ps

For the scalar advection equation, there is only one eigenvalue equal to a, and the Courant
number is simply jak= hj. The Lax–Friedrichs and leapfrog methods can be generalized in
the same way to systems of equations and remain stable for   1.
The upwind method for the scalar advection equation is based on a one-sided approximation to ux , using data in the upwind direction. The one-sided formulas (10.21) and
(10.22) generalize naturally to
UjnC1 D Ujn

k
A.Ujn
h

Ujn 1 /

(10.67)

UjnC1 D Ujn

k
A.UjnC1
h

Ujn /:

(10.68)

and

i

i

i

i

i

i

i

226

“rjlfdm”
2007/6/1
page 226
i

Chapter 10. Advection Equations and Hyperbolic Systems

For a system of equations, however, neither of these is useful unless all the eigenvalues of A
have the same sign, so that the upwind direction is the same for all characteristic variables.
The method (10.67) is stable only if
0

kp
1
h

for all p D 1; 2; : : : ; s;

(10.69)

for all p D 1; 2; : : : ; s:

(10.70)

while (10.68) is stable only if
1

kp
0
h

It is possible to generalize the upwind method to more general systems with eigenvalues of
both signs, but to do so requires decomposing the system into the characteristic variables
and upwinding each of these in the appropriate direction. The resulting method can also be
generalized to nonlinear hyperbolic systems and generally goes by the name of Godunov’s
method. These methods are described in much more detail in [66].

10.12

Initial boundary value problems

So far we have studied only numerical methods for hyperbolic problems on a domain with
periodic boundary conditions (or the Cauchy problem, if we could use a grid with an infinite
number of grid points).
Most practical problems are posed on a bounded domain with nonperiodic boundary
conditions, which must be specified in addition to the initial conditions to march forward
in time. These problems are called initial boundary value problems (IBVPs).
Consider the advection equation ut C aux D 0 with a > 0, corresponding to flow
to the right, on the domain 0  x  1 where some initial conditions u.x; 0/ D .x/
are given. This data completely determine the solution via (10.2) in the triangular region
0  x at  1 of the x-t plane. Outside this region, however, the solution is determined
only if we also impose boundary conditions at x D 0, say, (10.8). Then the solution is

.x at/
if 0  x at  1;
u.x; t/ D
(10.71)
g0 .t x=a/ otherwise:
Note that boundary data are required only at the inflow boundary x D 0, not at the outflow
boundary x D 1, where the solution is determined via (10.71). Trying to impose a different
value on u.1; t/ would lead to a problem with no solution.
If a < 0 in the advection equation, then x D 1 is the inflow boundary, the solution is
transported to the left, and x D 0 is the outflow boundary.

10.12.1 Analysis of upwind on the initial boundary value problem
Now suppose we apply the upwind method (10.21) to this IBVP with a > 0 on a grid with
h D 1=.m C 1/ and xi D ih for i D 0; 1; : : : ; m C 1. The formula (10.21) can be
applied for i D 1; : : : ; m C 1 in each time step, while U0n D g.tn / is set by the boundary
condition. Hence the method is completely specified.

i

i

i

i

i

i

i

10.12. Initial boundary value problems

“rjlfdm”
2007/6/1
page 227
i

227

When is this method stable? Intuitively we expect it to be stable if 0  ak= h  1.
This is the stability condition for the problem with periodic boundary conditions, and here
we are using the same method at every point except i D 0, where the exact solution is
being set in each time step. Our intuition is correct in this case and the method is stable if
0  ak= h  1.
Notice, however, that von Neumann analysis cannot be used in this case, as discussed
already in Section 10.5: the Fourier modes e i h are no longer eigengridfunctions. But
von Neumann analysis is still useful because it generally gives a necessary condition for
stability. In most cases a method that is unstable on the periodic domain or Cauchy problem
will not be useful on a bounded domain either, since locally on a fine grid, away from the
boundaries, any instability indicated by von Neumann analysis is bound to show up.
Instead we can use MOL stability analysis, although it is sometimes subtle to do so
correctly. We have a system of ODEs similar to (10.9) for the vector U.t/, which again has
m C 1 components corresponding to u.xi ; t/. But now we must incorporate the boundary
conditions, and so we have a system of the form
U 0 .t/ D AU.t/ C g.t/;

(10.72)

where
2

AD

6
a6
6
6
h6
4

3
1
1

1
1

7
7
7
7;
7
5

1
::
:

3
g0 .t/a= h
7
6
0
7
6
7
6
0
g.t/ D 6
7:
7
6
::
5
4
:

1 1

2

(10.73)

0

The upwind method corresponds to using Euler’s method on the ODE (10.72).
The change from the matrix of (10.10) to (10.73) may seem trivial but it completely
changes the character of the matrix. The matrix (10.10) is circulant and normal and has
eigenvalues uniformly distributed about the circle of radius a= h in the complex plane centered at z D a= h. The matrix (10.73) is a defective Jordan block with all its eigenvalues
at the point a= h. The eigenvalues have moved distance a= h ! 1 as h ! 0.
Suppose we attempt to apply the usual stability analysis of Chapter 7 to this system
and require that kp 2 S for all eigenvalues of A, where S is the stability region for
Euler’s method. Since S contains the interval Œ 2; 0 on the real axis, this would suggest
the stability restriction
0  ak= h  2
(10.74)
for the upwind method on the IBVP. This is wrong by a factor of 2. It is a necessary
condition but not sufficient.
The problem is that A in (10.73) is highly nonnormal. It is essentially a Jordan block
of the sort discussed in Section D.5.1, and on a fine grid its -pseudospectra roughly fill up
the circle of radius a= h about a= h, even for very small . This is a case where we need
to apply a more stringent requirement than simply requiring that k be inside the stability
region for all eigenvalues; we also need to require that
dist.k ; S/  C

i

i

(10.75)

i

i

i

i

i

228

“rjlfdm”
2007/6/1
page 228
i

Chapter 10. Advection Equations and Hyperbolic Systems

holds for the -pseudoeigenvalues (see Section D.5), where C is a modest constant, as suggested in [47], [92]. Requiring (10.75) shows that the expected requirement 0  ak= h  1
is needed rather than (10.74).

10.12.2 Outflow boundary conditions
When the upwind method is used for the advection equation, as in the previous section,
the outflow boundary poses no problem. The finite difference formula is one sided and the
n
value UmC1
at the rightmost grid point is computed by the same formula that is used in the
interior.
However, if we use a finite difference method whose stencil extends to the right as
well as the left, we will need to use a different formula at the rightmost point in the domain.
This is called a numerical boundary condition or artificial boundary condition since it is
required by the method, not for the PDE. Numerical boundary conditions may also be
required at boundaries where a physical boundary condition is imposed, if the numerical
method requires more conditions than the equation. For example, if we use the Beam–
Warming method for advection, which has a stencil that extends two grid points in the
upwind direction, then we can use this only for updating U2nC1 ; U3nC1 ; : : :. The value
U0nC1 will be set by the physical boundary condition but U1nC1 will have to be set by some
other method, which can be viewed as a numerical boundary condition for Beam–Warming.
Naturally some care must be used in choosing numerical boundary conditions, both
in terms of the accuracy and stability of the resulting method. This is a difficult topic,
particularly the stability analysis of numerical methods for IBVPs, and we will not pursue
it here. See, for example, [40], [84], [89].
Example 10.14. We will look at one example simply to give a flavor of the potential
difficulties. Suppose we use the leapfrog method to solve the IBVP for the advection
equation ut C aux D 0. At the left (inflow) boundary we can use the given boundary
condition, but at the right we will need a numerical boundary condition. Suppose we use
the first order upwind method at this point,
nC1
n
UmC1
D UmC1

ak n
.U
h mC1

Umn /;

(10.76)

which is also consistent with the advection equation. Figure 10.5 shows four snapshots of
the solution when the initial data is u.x; 0/ D .x/ D exp. 5.x 2/2 / and the first two
time levels for leapfrog are initialized based on the exact solution u.x; t/ D .x at/. We
see that as the wave passes out the right boundary, a reflection is generated that moves to
the left, back into the domain. The dispersion relation for the leapfrog method found in
Example 10.13 shows that waves with h   can move to the left with group velocity
approximately equal to a, and this wave number corresponds exactly to the sawtooth
wave seen in the figure.
For an interesting discussion of the relation of numerical dispersion relations and
group velocity to the stability of numerical boundary conditions, see [89].
Outflow boundaries are often particularly troublesome. Even if a method is formally
stable (as the leapfrog method in the previous example is with the upwind boundary condition), it is often hard to avoid spurious reflections. We have seen this even for the advection
equation, where in principle flow is entirely to the right, and it can be even more difficult

i

i

i

i

i

i

i

10.12. Initial boundary value problems

“rjlfdm”
2007/6/1
page 229
i

229

1

0.5

0
4

5

6

7

8

9

10

5

6

7

8

9

10

5

6

7

8

9

10

5

6

7

8

9

10

1

0.5

0
4

1

0.5

0
4

1

0.5

0
4

Figure 10.5. Numerical solution of the advection equation using the leapfrog
method in the interior and the upwind method at the right boundary. The solution is shown
at four equally spaced times, illustrating the generation and leftward propagation of a
sawtooth mode.

to develop appropriate boundary conditions for wave propagation or fluid dynamics equations that admit wave motion in all directions. Yet in practice we always have to compute
over a finite domain and this often requires setting artificial boundaries around the region
of interest. The assumption is that the phenomena of interest happen within this region,
and the hope is that any waves hitting the artificial boundary will leave the domain with
no reflection. Numerical boundary conditions that attempt to achieve this are often called
nonreflecting or absorbing boundary conditions.

i

i

i

i

i

i

i

230

“rjlfdm”
2007/6/1
page 230
i

Chapter 10. Advection Equations and Hyperbolic Systems

10.13

Other discretizations

As in the previous chapter on parabolic equations, we have concentrated on a few basic
methods in order to explore some fundamental ideas. We have also considered only the
simplest case of constant coefficient linear hyperbolic equations, whereas in practice most
hyperbolic problems of interest have variable coefficients (e.g., linear wave propagation in
heterogeneous media) or are nonlinear. These problems give rise to a host of new difficulties, not least of which is the fact that the solutions of interest are often discontinuous since
nonlinearity can lead to shock formation. There is a well-developed theory of numerical
methods for such problems that we will not delve into here; see, for example, [66].
Even in the case of constant coefficient linear problems, there are many other discretizations possible beyond the ones presented here. We end with a brief overview of just
a few of these:
 Higher order discretizations of ux can be used in place of the discretizations considered so far. If we write the MOL system for the advection equation as
Uj0 .t/ D aWj .t/;

(10.77)

where Wj .t/ is some approximation to ux .xj ; t/, then there are many ways to approximate Wj .t/ from the Uj values beyond the centered approximation (10.3).
One-sided approximations are one possibility, as in the upwind method. For sufficiently smooth solutions we might instead use higher order accurate centered approximations, e.g.,




4 Uj C1 Uj 1
1 Uj C2 Uj 2
Wj D
:
(10.78)
3
2h
3
4h
This and other approximations can be determined using the fdcoeffV.m routine
discussed in Section 1.5; e.g., fdcoeffV(1,0,-2:2) produces (10.78). Centered approximations such as (10.78) generally lead to skew-symmetric matrices
with pure imaginary eigenvalues, at least when applied to the problem with periodic
boundary conditions. In practice most problems are on a finite domain with nonperiodic boundary conditions, and other issues arise as already seen in Section 10.12.
Note that the discretization (10.78) requires more numerical boundary conditions
than the upwind or second order centered operator.
 An interesting approach to obtaining better accuracy in Wj is to use a so-called
compact method, in which the Wj are determined by solving a linear system rather
than explicitly. A simple example is


1
1
3 Uj C1 Uj 1
Wj 1 C Wj C Wj C1 D
:
(10.79)
4
4
2
2h
This gives a tridiagonal system of equations to solve for the Wj values, and it can be
shown that the resulting values will be O.h4 / approximations to ux .xj ; t/. Higher
order methods of this form also exist; see Lele [63] for an in-depth discussion. In
addition to giving higher order of accuracy with a compact stencil, these approximations also typically have much better dispersion properties than standard finite
difference approximations of the same order.

i

i

i

i

i

i

i

10.13. Other discretizations

“rjlfdm”
2007/6/1
page 231
i

231

 Spectral approximations to the first derivative can be used, based on the same ideas
as in Section 2.21. In this case W D DU is used to obtain the approximations to the
first derivative, where D is the dense spectral differentiation matrix. These methods
can also be generalized to variable coefficient and even nonlinear problems and often
work very well for problems with smooth solutions.
Stability analysis of these methods can be tricky. To obtain reasonable results a
nonuniform distribution of grid points must be used, such as the Chebyshev extreme
points as discussed in Section 2.21. In this case the eigenvalues of the matrix D turn
out to be O.1= h2 /, rather than O.1= h/ as is expected for a fixed-stencil discretization of the first derivative. If instead we use the roots of the Legendre polynomial
(see Section B.3.1), another popular choice of grid points for spectral methods, it can
be shown that the eigenvalues are O.1= h/, which appears to be better. In both cases,
however, the matrix D is highly nonnormal and the eigenvalues are misleading, and
in fact a time step k D O.h2 / is generally required if an explicit method is used for
either choice of grid points; see, e.g., [92].
 Other time discretizations can be used in place of the ones discussed in this chapter.
In particular, for spectral methods the MOL system is stiff and it may be beneficial
to use an implicit method as discussed in Chapters 8 and 9. Another possibility is to
use an exponential time differencing method, as discussed in Section 11.6.
 For conservation laws (see Section E.2) numerical methods are often more naturally
derived using the integral form (E.9) than by using finite difference approximations to
derivatives. Such methods are particularly important for nonlinear hyperbolic problems, where shock waves (discontinuous solutions) can develop spontaneously even
from smooth initial data. In this case the discrete value Uin is viewed as an approximation to the cell average of u.x; tn / over the grid cell Œxi 1=2 ; xiC1=2 of length h
centered about xi ,
Z
1 xiC1=2
n
Ui 
u.x; tn / dx:
(10.80)
h xi 1=2
According to (E.9) this cell average evolves at a rate given by the difference of fluxes
at the cell edges, and a particular numerical method is obtained by approximating
these fluxes based on the current cell averages. Methods of this form are often called
finite volume methods since the spatial domain is partitioned into volumes of finite
size. Simple finite volume methods often look identical to finite difference methods,
but the change in viewpoint allows the development of more sophisticated methods
that are better suited to solving nonlinear conservation laws. These methods also
have some advantages for linear problems, particularly if they have variable coefficients with jump discontinuities, as often arises in solving wave propagation problems in heterogeneous media. See [66] for a detailed description of such methods.

i

i

i

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 232
i

i

i

i

i

“rjlfdm”
2007/6/1
page 233
i

Chapter 11

Mixed Equations

We have now studied the solution of various types of time-dependent equations: ordinary
differential equations (ODEs), parabolic partial differential equations (PDEs) such as the
heat equation, and hyperbolic PDEs such as the advection equation. In practice several
processes may be happening simultaneously, and the PDE model will not be a pure equation
of any of the types already discussed but rather will be a mixture. In this chapter we discuss
several approaches to handling more complicated equations. We restrict our attention to
time-dependent PDEs of the form
ut D A1 .u/ C A2 .u/ C    C AN .u/;

(11.1)

where each of the Aj .u/ are (possibly nonlinear) functions or differential operators involving only spatial derivatives of u. For simplicity, most of our discussion will be further
restricted to only two terms, which we will write as
ut D A.u/ C B.u/;

(11.2)

but more terms often can be handled by extension or combination of the methods described
here.

11.1

Some examples

We begin with some examples of PDEs involving more than one term. See Appendix E for
more discussion of some of these equations.
 Multidimensional problems, such as the diffusion equation in two dimensions,
ut D .uxx C uyy /;

(11.3)

or the three-dimensional version. This problem has already been discussed in Section 9.7, where we saw that efficient methods can be developed by splitting (11.3)
into two one-dimensional problems. Hyperbolic equations also arise in multidimensional domains, such as the two-dimensional hyperbolic system
ut C Aux C Buy D 0

(11.4)

233

i

i

i

i

i

i

i

234

“rjlfdm”
2007/6/1
page 234
i

Chapter 11. Mixed Equations
or nonlinear hyperbolic conservation laws
ut C f .u/x C g.u/y D 0;

(11.5)

where f .u/ and g.u/ are the flux functions in the two directions.
All the problems discussed below also have multidimensional variants, where even
more terms arise. For simplicity we display only the one-dimensional case.
 Reaction-diffusion equations of the form
ut D uxx C R.u/;

(11.6)

where  is a diffusion coefficient (or diagonal matrix of diffusion coefficients if different components in the system diffuse at different rates) and R.u/ represents chemical reactions, and is typically nonlinear. The reaction terms might or might not be
stiff. If not, then we typically want to handle these terms explicitly (to avoid solving
nonlinear systems of equations in each time step), while the diffusion term is stiff
and requires appropriate methods.
Even if the reaction terms are stiff, they apply locally at a point in space, unlike the
diffusion term that couples different grid points together. Recognizing this fact can
lead to more efficient solution techniques, as discussed further below.
 Advection-diffusion equations of the form
ut C aux D uxx :

(11.7)

The diffusion term is stiff and requires an appropriate solver, while the advection
term can be handled explicitly.
 Nonlinear hyperbolic equations with viscous terms,
ut C f .u/x D uxx :

(11.8)

The advection-diffusion equation (11.7) is one example of this form, but more generally the flux function f .u/ can be nonlinear, modeling fluid dynamics, for example,
in which case the right-hand side represents viscous terms and perhaps heat conduction. The Navier–Stokes equations for compressible gas dynamics have this general
form, for example. A simpler example is the viscous Burgers equation,
ut C uux D uxx ;

(11.9)

where the flux function is f .u/ D 12 u2 . This is a simple scalar model for some of
the effects seen in compressible flow, and it has been widely studied.
 Advection-diffusion-reaction equations or reacting flow problems,
ut C f .u/x D uxx C R.u/:

(11.10)

If chemical reactions are occurring in a fluid flow, then equations of this general
form are obtained. Combustion problems are particularly challenging problems of

i

i

i

i

i

i

i

11.2. Fully coupled method of lines

“rjlfdm”
2007/6/1
page 235
i

235

this type, where exothermic chemical reactions directly influence the fluid dynamics.
Chemotaxis problems are also of this type, which arise in biology when substances
move in response to concentration gradients of other substances, and often give rise
to interesting pattern formation [73].
 The Korteweg–de Vries (KdV) equation,
ut C uux D uxxx :

(11.11)

This is similar to the viscous Burgers equation (11.9), but the term on the righthand side is dispersive rather than dissipative. This leads to very different behavior
and is the simplest example of an equation having soliton solutions. It arises as a
simple model of certain kinds of wave phenomena in fluid dynamics and elsewhere.
The third derivative term is stiffer than a uxx term and would typically require k D
O.h3 / for an explicit method. However, similar to the advective terms considered in
Chapter 10, the eigenvalues of a discretization of uxxx will typically lie on or near
the imaginary axis over an interval stretching distance O.1= h3 / from the origin,
rather than along the negative real axis, influencing the type of time discretization
appropriate for these equations.
Many other equations that couple nonlinearity with dispersion are of importance in
applications, for example, the nonlinear Schrödinger equation
i

t .x; t/ D

xx .x; t/ C V .

/

(11.12)

with a nonlinear potential V . /. (With V D 0 the equation is linear and dispersive,
as shown in Section E.3.8.)
 The Kuramoto–Sivashinsky equation,
1
ut C .ux /2 D uxx uxxxx :
(11.13)
2
The right-hand side gives exponential growth of some low wave numbers, as shown
in Section E.3.7. The nonlinear term transfers energy from low wave numbers to
higher wave numbers, which are damped by the fourth order diffusion. The result
is bounded solutions, but ones that can behave quite chaotically. The fourth order
diffusion term is even more stiff than second order diffusion. Eigenvalues of a discretization of this term typically lie on the negative real axis over an interval of length
O.1= h4 /.
Many approaches can be used for problems that involve two or more different terms,
and a huge number of specialized methods have been developed for particular equations.
The remainder of this chapter contains a brief overview of a few popular approaches, but it
is by no means exhaustive.

11.2

Fully coupled method of lines

One simple approach is to discretize the full right-hand side of (11.1) in space using appropriate spatial discretizations of each term to obtain a semidiscrete method of lines (MOL)

i

i

i

i

i

i

i

236

“rjlfdm”
2007/6/1
page 236
i

Chapter 11. Mixed Equations

system of the form U 0.t/ D F.U.t//, where F represents the full spatial discretization.
This system of ODEs can now be solved using an ODE method in MATLAB or with other
ODE software. This may work well for equations where the terms all have similar character. The problem in general, however, is that the same ODE method is being applied to all
aspects of the spatial discretization, which can be very wasteful for many of the problems
listed above.
Consider a reaction-diffusion equation of the form (11.6), for example. If this represents a system of s equations and we discretize in x on a grid with m points, then we
obtain a coupled system of ms ODEs. Typically the reaction terms R.u/ are nonlinear and
so this will be a nonlinear system. Generally an implicit method is used since the diffusion
terms are stiff, so in every time step a nonlinear system of dimension ms must be solved.
However, if the reaction terms are not stiff, then there is no need to make these terms implicit and it should be possible to solve only linear systems for the diffusion terms, and
s decoupled linear systems of size m each (with tridiagonal matrices) rather than a fully
coupled nonlinear system of size ms. Even if the reaction terms are stiff, the reaction terms
ut D R.u/ are local at a point and by splitting the reaction from the diffusion (using one of
the other approaches discussed below), it is possible to solve decoupled nonlinear equations
of dimension s at each grid point to advance the reaction terms in a stable manner.

11.3

Fully coupled Taylor series methods

A first order accurate explicit method for the equation (11.2) can be obtained by using the
first order term in the Taylor series,
u.x; tn C k/  u.x; tn / C k.A.u.x; tn // C B.u.x; tn ///;

(11.14)

and then replacing the spatial operators A and B with discretizations. A second order
accurate method can sometimes be obtained by adding the next term in the Taylor series,
but this requires determining ut t in terms of spatial derivatives of u. We did this for the
advection equation ut C aux D 0 in Section 10.3 to derive the Lax–Wendroff method, in
which case ut t D a2 uxx . Whether we can do this in general for (11.2) depends on how
complicated the right-hand side is, but in some cases it can be done. For example, for the
two-dimensional hyperbolic equation (11.4) we can compute
ut t D Autx

Buty

D A.Aux C Buy /x C B.Aux C Buy /y
2

(11.15)

2

D A uxx C .AB C BA/uxy C B uyy :
A second order accurate Lax–Wendroff method can then be derived from
1
u.x; y; tn Ck/  u k.Aux CBuy /C k 2 .A2 uxx C.AB CBA/uxy CB 2 uyy / (11.16)
2
(where the terms on the right-hand side are all evaluated at .x; y; tn /) by discretizing in
space using second order accurate centered approximations. This gives the two-dimensional
Lax–Wendroff method.
For some other problems a similar approach works, e.g., for advection-reaction terms
with nonstiff reactions, but this is generally useful only if all terms are nonstiff and can be

i

i

i

i

i

i

i

11.4. Fractional step methods

“rjlfdm”
2007/6/1
page 237
i

237

advanced with explicit methods. Moreover, it is generally difficult to achieve higher than
second order accuracy with this approach.

11.4

Fractional step methods

The idea of a fractional step method (also called a time-split or split-step method, among
other things) is to split up the equation into its constituent pieces and alternate between
advancing simpler equations in time. The simplest splitting for an equation with two terms
of the form (11.2) would be
U  D NA .U n ; k/;
U nC1 D NB .U  ; k/:

(11.17)

Here NA .U n ; k/ represents some one-step numerical method that solves ut D A.u/ over
a time step of length k starting with data U n . Similarly, NB .U  ; k/ solves ut D B.u/ over
a time step of length k starting with the data U  .
Below we will see that this splitting of the equation does often work—the numerical
solution obtained will usually converge to solutions of the original problem as k ! 0
provided the numerical methods used in each step are consistent and stable approximations
to the separate problems they are designed to solve. The approximation obtained often will
be only first order accurate, however, no matter how good each of the constituent numerical
methods is. We will see why below and consider some improvements.
First we note that this fractional step approach has several advantages. It allows us to
use very different methods for each piece ut D A.u/ and ut D B.u/. One can be implicit
and the other explicit, for example. For the reaction-diffusion problem (11.6), if A.u/
represents the diffusion terms, then these can be solved with an implicit method, solving
tridiagonal linear systems for each component. The reaction terms can be solved with an
explicit or implicit method, depending on whether they are stiff. If an implicit method is
used, then a nonlinear system is obtained at each grid point, but each is decoupled from the
nonlinear system at other grid points, typically leading to a much more efficient solution of
these systems.
Another situation in which this type of splitting is often used is in reducing a multidimensional problem to a sequence of one-dimensional problems. In this context the
fractional step approach is often called dimensional splitting. We saw an example of this
in Section 9.8, where the locally one-dimensional (LOD) method for the heat equation
was discussed. By decoupling the space dimensions, one obtains a sequence of tridiagonal
systems to solve instead of a large sparse matrix with more complicated structure.
Another advantage of the fractional step approach is that existing methods for the
simpler subproblems are easily patched together, e.g., ODE methods for the reaction terms
can be applied without worrying about the spatial coupling, or a one-dimensional method
for a PDE can easily be extended to two or three dimensions by repeatedly applying it on
one-dimensional slices.
To see that the fractional step method (11.17) may be only first order accurate, consider a simple linear system of ODEs where the coefficient matrix is split into two matrices
as A C B, so the system is
ut D Au C Bu:
(11.18)

i

i

i

i

i

i

i

238

“rjlfdm”
2007/6/1
page 238
i

Chapter 11. Mixed Equations

Suppose we use the fractional step method (11.17) with the exact solution operator for each
step, so
NA .U; k/ D e Ak U;
NB .U; k/ D e Bk U:
(11.19)
Then (11.17) gives the numerical method
U nC1 D e Bk U  D e Bk e Ak U n ;

(11.20)

whereas the exact solution satisfies
u.tnC1 / D e .ACB/k u.tn /:

(11.21)

By Taylor series expansion of the matrix exponentials (see (D.31)), we find that
1
e .ACB/k D I C k.A C B/ C k 2 .A C B/2 C    ;
2

(11.22)

whereas



1
1
e Bk e Ak D I C kA C k 2 A2 C   
I C kB C k 2 B 2 C   
2
2
1 2 2
D I C k.A C B/ C k .A C 2AB C B 2 / C    :
2

(11.23)

Note that the quadratic term in (11.22) is
.A C B/2 D A2 C AB C BA C B 2 ;
which is not the same as the quadratic term in (11.23) if the matrices A and B do not
commute.
If they do commute, e.g., in the scalar case, then the splitting is exact and all terms in
the Taylor series agree. But in general a one-step error of magnitude O.k 2 / is introduced,
and so the method is only first order accurate even when the exact solution is used for each
piece.
A second order accurate splitting was introduced by Strang [83] in the context of
methods for multidimensional hyperbolic equations and is often called the Strang splitting:
U  D NA .U n ; k=2/;
U  D NB .U  ; k/;
U

nC1

D NA .U



(11.24)

; k=2/:

Working out the product of the Taylor series expansions in this case for the ODE system
(11.18) gives agreement to the quadratic term in (11.22), although there is an error in the
O.k 3 / term unless A and B commute. A similar result can be shown for general PDEs
with smooth solutions split in the form (11.2).
An alternative procedure, which also gives second order accuracy, is to use the splitting (11.17) in every other time step, and in the alternate time steps use a similar splitting
but with the order of NA and NB reversed. Over two time steps this has roughly the same
form as the Strang splitting over a time step of length 2k, although with two applications
of NB with time step k rather than one application with time step 2k.

i

i

i

i

i

i

i

11.5. Implicit-explicit methods

“rjlfdm”
2007/6/1
page 239
i

239

Example 11.1. The LOD method for the heat equation discussed in Section 9.8 uses
a splitting of the form (11.17) but is able to achieve second order accuracy because there
is no splitting error in this case (except near the boundaries, where appropriate treatment
is required). In this case the two-dimensional heat equation is split with A.u/ D uxx and
B.u/ D uyy , and the operators @2x and @2y commute. For a more general variable coefficient
heat equation with A.u/ D ..x; y/ux /x and B.u/ D ..x; y/uy /y , the two operators no
longer commute and the LOD method would be only first order accurate.
Another possible way to improve the accuracy of fractional step methods is to combine them with the spectral deferred correction method of [28]. This method improves the
accuracy of a time-stepping procedure by a deferred correction process. Application to
advection-diffusion-reaction equations in the context of fractional step methods was investigated in [9].
One difficulty with fractional step methods is that boundary conditions may be hard
to apply properly when initial boundary value problems are solved. Each application of
NA or NB typically requires boundary conditions, either physical or artificial, and it is
not always clear how to properly specify the “intermediate boundary conditions” needed in
each stage of the splitting. This has been discussed in relation to the LOD method for the
heat equation in Section 9.8. See [65] for a discussion of intermediate boundary conditions
for hyperbolic equations.
Another potential difficulty is stability. Even if the methods NA and NB are each stable methods for the problems they are designed to solve, it is not always clear that alternating between these methods in every time step will lead to a stable procedure. Example D.3
shows the problem that can arise. Suppose NA .U n ; k/ D A0 U n and NB .U  ; k/ D A1 u ,
where A0 and A1 are given by (D.84). Then each method is stable by itself but the fractional step procedure (11.17) generates exponentially growing solutions. Often stability
of fractional step methods can be easily shown, for example, if kNA .U; k/k  kU k and
kNB .U; k/k  kU k both hold in the same norm, but caution is required.

11.5

Implicit-explicit methods

Suppose we have an equation split as in (11.2), where A.u/ represents stiff terms that we
wish to integrate using an implicit method, whereas B.u/ corresponds to nonstiff terms
that can be handled explicitly with a reasonable time step. We have seen various examples
of this form, such as reaction-diffusion equations with nonstiff reactions, where it may be
much more efficient to avoid an implicit solve for the nonlinear reaction terms.
Implicit-explicit (IMEX) methods are fully coupled methods that are designed to handle some terms implicitly and others explicitly. A simple example is obtained by combining
forward Euler with backward Euler:
U nC1 D U n C k.A.U nC1 / C B.U n //:

(11.25)

Another example is a two-step combination of the second order Adams–Bashforth method
for the explicit term with the trapezoidal method for the implicit term:
U nC1 D U n C

i

i

k
A.U n / C A.U nC1 / C 3B.U n /
2


B.U n 1 / :

(11.26)

i

i

i

i

i

240

“rjlfdm”
2007/6/1
page 240
i

Chapter 11. Mixed Equations

Higher order methods of this type have been derived and widely used. See, for example,
[7] for a number of other multistep methods and [6] for some Runge–Kutta methods of this
type.

11.6

Exponential time differencing methods

Consider a nonlinear ODE u0 D f .u/ (possibly an MOL discretization of a PDE) and
suppose that over the time interval Œtn ; tnC1  we write this as
u0.t/ D An u.t/ C Bn .u.t//;

(11.27)

where we have split the function f .u/ into a linear part and a nonlinear part. The idea of
exponential time differencing (ETD) methods is to use a form of Duhamel’s principle (5.8)
to handle the linear part exactly using the matrix exponential and combine this with an
appropriate numerical method of the desired order for the Bn .u/ term, typically an explicit
method if we assume that the linear term captures the stiff part of the problem.
Two common forms of this type of splitting are as follows:
1. For a general nonlinear function f .u/, let An D f 0.U n /, the Jacobian matrix evaluated at U n (or perhaps some approximate Jacobian), and then
Bn .u/ D f .u/

An u:

(11.28)

2. For problems such as MOL discretizations of reaction-diffusion equations we may
take An to be the matrix representing the diffusion operator for all n and let Bn .u/ be
the reaction terms. In this case An is not the full Jacobian of the nonlinear problem,
but if the reaction terms are not stiff they might be easily approximated with explicit methods, and there are advantages to having A unchanged from one step to the
next—the ETD methods require working with the matrix exponential e kAn , and if A
is constant we may be able to compute this once before beginning the time stepping.
For the system (11.27), Duhamel’s principle (5.8) can be generalized to
u.tnC1 / D e

An k

Z tnC1
u.tn / C

e An.tnC1  / Bn .u.// d:

(11.29)

tn

This expression is exact, but the integral must be approximated since we don’t know
Bn .u.//. Methods of various order can be obtained by different discretizations of this
integral. The simplest approximation is obtained by replacing Bn .u.// with Bn .U n /. We
can then pull this out of the integral and can compute the exact integral of the remaining
integrand by integrating the Taylor series for the matrix exponential (D.31) term by term,
resulting in (5.12),
Z tnC1
tn

i

i

1
1
e An.tnC1  / d D k C k 2An C k 3 A2n C   
2
6


1
An k
D An e
I
(if An is nonsingular):

(11.30)

i

i

i

i

i

11.6. Exponential time differencing methods
Using this, we obtain from (11.29) the numerical method


U nC1 D e An k U n C An 1 e Ank I Bn .U n /:
An U n , we can rewrite this as


U nC1 D U n C An 1 e An k I f .U n /:

“rjlfdm”
2007/6/1
page 241
i

241

(11.31)

Since Bn .U n / D f .U n /

(11.32)

Note that if An D 0, then using the first line of (11.30) we see that (11.32) reduces to
Euler’s method for u0 D f .u/ and is only first order accurate. However, we normally
assume that An is nonsingular and an approximation to the Jacobian matrix. In general we
can compute the local truncation error to be




u.tnC1 / u.tn /
n
 D
k 1 An 1 e An k I u0.tn /
k


1 00
1 2 000
0
D u .tn / C ku .tn / C k u .tn / C   
2
6


1
1 2 2
(11.33)
I C An k C An k C    u0.tn /
2
6
 1

1
D k u00 .tn / An u0.tn / C k 2 u000.tn / An u0.tn / C   
2
6

1
D k f 0.u.tn // An u0.tn / C O.k 2 /:
2
We see that the method is second order accurate if An D f 0.U n /.
Higher order methods can be derived by using better approximations of the integral
in (11.29). This can be done either as a multistep method, approximating Bn .u/ by an
interpolating polynomial through previous values U n j as in the derivation of the Adams–
Bashforth methods, or as multistage generalizations of the Runge–Kutta methods. See, for
example, [8], [19], [48], [53] for more discussion of these methods.
Note that the ETD method is exact on the test problem u0 D u if we take An D .
So the region of absolute stability for this method is exactly the left half-plane. The method
is exact more generally on a linear system of equations, provided of course that we can
compute the matrix exponential accurately, as discussed in the next section.
Many mixed equations involve higher order derivative terms that are linear (and often constant coefficient) and ETD methods may be particularly suitable for handling the
stiffness of spatial discretizations. Note in particular that for dispersive terms, such as the
uxxx term in the KdV equation (11.11), an ETD method that handles this term exactly may
be advantageous over an implicit method. This dispersion is nondissipative (eigenvalues
are on the imaginary axis), but many implicit methods designed for stiff problems have the
imaginary axis in the interior of the stability region, leading to nonphysical dissipation.

11.6.1 Implementing exponential time differencing methods
Computing the matrix exponential is nontrivial—the classic paper [70] presented “19 dubious ways” to do this, and its recent update [71] discusses a 20th way in the appendix,

i

i

i

i

i

i

i

242

“rjlfdm”
2007/6/1
page 242
i

Chapter 11. Mixed Equations

a more recently developed approach based on Krylov space methods. The latter approach
has made the exponential time differencing approach viable for MOL discretizations of
parabolic equations and other linear systems of ODEs involving large but sparse coefficient
matrices and is discussed further below.
One situation in which ETD methods are relatively easy to implement is when the matrix An is diagonal, for then e Ank is just a diagonal matrix of scalar exponential functions.
This arises naturally in some applications, for example, if a problem such as a reactiondiffusion equation is solved with periodic boundary conditions. By Fourier transforming
the problem, the diffusion operator is reduced to a diagonal matrix. For this reason ETD
methods are often particularly attractive in connection with Fourier spectral methods.
Even in the scalar case, however, the evaluation of the exponential factor
.z/ D .e z

1/=z

(11.34)

that appears in (11.32) can be susceptible to numerical cancellation effects in floating point
arithmetic. For higher order ETD methods such as the fourth order method considered by
Cox and Matthews [19], higher order terms of the same nature appear that are even more
sensitive to numerical errors. Kassam and Trefethen [53] suggest an approach to evaluating
these coefficients in the numerical method using contour integration in the complex plane,
numerically approximating the Cauchy integral representation (D.4).
In the nondiagonal case, directly computing the matrix exponential by this sort of
approach can still be very effective if the matrix A involved is of modest size, such as may
arise from a spectral approximation based on polynomials.
For very large sparse matrices, the Krylov space approach often works best. In this
case we do not compute the matrix exponential itself, which is a very large dense matrix,
but rather the application of this matrix to a vector. This is all that is needed in (11.32),
for example. Actually we need to apply A 1 .e Ak I / D k.Ak/ to a vector, which
could be done in two steps, first using a Krylov space method for the exponential and then
a second Krylov space method to solve the linear system, but the Krylov approach can
be applied directly to the function .Ak/. This approach has been briefly outlined at the
end of Section 4.4. In practice it has been found that in some cases, particularly if a good
preconditioner is not available, Krylov space methods may converge faster on the matrix
exponential and related functions than it does for a simple linear system with the same
coefficient matrix. In such cases the ETD methods may be more efficient than using a
traditional implicit method. See, e.g., [32], [48], [77] for more details.

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 243
i

Part III

Appendices

i

i

i

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 244
i

i

i

i

i

“rjlfdm”
2007/6/1
page 245
i

Appendix A

Measuring Errors

To discuss the accuracy of a numerical solution, or the relative virtues of one numerical
method, versus another, it is necessary to choose a manner of measuring that error. It may
seem obvious what is meant by the error, but as we will see there are often many different
ways to measure the error which can sometimes give quite different impressions as to the
accuracy of an approximate solution.

A.1 Errors in a scalar value
First consider a problem in which the answer is a single value zO 2 R. Consider, for example, the scalar ordinary differential equation (ODE)
u0 .t/ D f .u.t//;

u.0/ D ;

and suppose we are trying to compute the solution at some particular time T , so zO D u.T /.
Denote the computed solution by z. Then the error in this computed solution is
EDz

zO :

A.1.1 Absolute error
A natural measure of this error would be the absolute value of E,
jEj D jz

zO j:

This is called the absolute error in the approximation.
As an example, suppose that zO D 2:2, while some numerical method produced a
solution z D 2:20345. Then the absolute error is
jz

zO j D 0:00345 D 3:45  10 3 :

This seems quite reasonable—we have a fairly accurate solution with three correct digits
and the absolute error is fairly small, on the order of 10 3 . We might be very pleased
245

i

i

i

i

i

i

i

246

“rjlfdm”
2007/6/1
page 246
i

Appendix A. Measuring Errors

with an alternative method that produced an error of 10 6 and horrified with a method that
produced an error of 106 .
But note that our notion of what is a large error or a small error might be thrown
off completely if we were to choose a different set of units for measuring zO . For example,
suppose the zO discussed above were measured in meters, so zO D 2:2 meters is the correct
solution. But suppose that instead we expressed the solution (and the approximate solution) in nanometers rather than meters. Then the true solution is zO D 2:2  109 and the
approximate solution is z D 2:20345  109, giving an absolute error of
jz

zO j D 3:45  106:

We have an error that seems huge and yet the solution is just as accurate as before, with
three correct digits.
Conversely, if we measured zO in kilometers, then zO D 2:2  10 3 and z D 2:20345 
3
10 so
jz zO j D 3:45  10 6 :
The error seems much smaller and yet there are still only three correct digits.

A.1.2 Relative error
The above difficulties arise from a poor choice of scaling of the problem. One way to avoid
this is to consider the relative error, defined by
ˇ
ˇ
ˇ z zO ˇ
ˇ
ˇ
ˇ zO ˇ :
The size of the error is scaled by the size of the value being computed. For the above
examples, the relative error in z is equal to
ˇ
ˇ
ˇ ˇ
ˇ 2:20345 2:2 ˇ ˇ 2:20345  109 2:2  109 ˇ
ˇ
ˇ D 1:57  10 3 :
ˇDˇ
ˇ
ˇ
ˇ ˇ
2:2
2:2  109
The value of the relative error is the same no matter what units we use to measure zO , a
very desirable feature. Also note that in general a relative error that is on the order of 10 k
indicates that there are roughly k correct digits in the solution, matching our intuition.
For these reasons the relative error is often a better measure of accuracy than the
absolute error. Of course if we know that our problem is “properly” scaled, so that the
solution zO has magnitude order 1, then it is fine to use the absolute error, which is roughly
the same as the relative error in this case.
In fact it is generally better to ensure that the problem is properly scaled than to
rely on the relative error. Poorly scaled problems can lead to other numerical difficulties,
particularly if several different scales arise in the same problem so that some numbers are
orders of magnitude larger than others for nonphysical reasons. Unless otherwise noted
below, we will assume that the problem is scaled in such a way that the absolute error is
meaningful.

i

i

i

i

i

i

i

A.2. “Big-oh” and “little-oh” notation

“rjlfdm”
2007/6/1
page 247
i

247

A.2 “Big-oh” and “little-oh” notation
In discussing the rate of convergence of a numerical method we use the notation O.hp /,
the so-called big-oh notation. In case this is unfamiliar, here is a brief review of the proper
use of this notation.
If f .h/ and g.h/ are two functions of h, then we say that
f .h/ D O.g.h// as h ! 0
if there is some constant C such that
ˇ
ˇ
ˇ f .h/ ˇ
ˇ
ˇ
ˇ g.h/ ˇ < C for all h sufficiently small
or, equivalently, if we can bound
jf .h/j < C jg.h/j for all h sufficiently small.
Intuitively, this means that f .h/ decays to zero at least as fast as the function g.h/ does.
Usually g.h/ is some monomial hq , but this isn’t necessary.
It is also sometimes convenient to use the “little-oh” notation
f .h/ D o.g.h// as h ! 0:
This means that

ˇ
ˇ
ˇ f .h/ ˇ
ˇ
ˇ
ˇ g.h/ ˇ ! 0 as h ! 0:

This is slightly stronger than the previous statement and means that f .h/ decays to zero
faster than g.h/. If f .h/ D o.g.h//, then f .h/ D O.g.h//, although the converse may
not be true. Saying that f .h/ D o.1/ simply means that the f .h/ ! 0 as h ! 0.
Examples:
2h3 D O.h2 / as h ! 0; since

2h3
1
D 2h < 1 for all h < :
2
h
2

2h3 D o.h2 / as h ! 0; since 2h ! 0 as h ! 0:
sin.h/ D O.h/ as h ! 0; since sin h D h

h3
h5
C
C    < h for all h > 0:
3
5

sin.h/ D h C o.h/ as h ! 0; since .sin h h/= h D O.h2 /:
p
p
p
h D O.1/ as h ! 0; and also h D o.1/; but h is not O.h/.
cos h D o.h/ and 1 cos h D O.h2 / as h ! 0:
p
p
h2 = h C h3 D O.h1:5 / and h2 = h C h3 D o.h/ as h ! 0:
1

e 1= h D o.hq / as h ! 0 for every value of q.
e 1= h
To see this, let x D 1= h then
D e x x q ! 0 as x ! 1:
hq

i

i

i

i

i

i

i

248

“rjlfdm”
2007/6/1
page 248
i

Appendix A. Measuring Errors

Note that saying f .h/ D O.g.h// is a statement about how f behaves in the limit as
h ! 0. This notation is sometimes abused by saying, for example, that if h D 10 3 , then
the number 3  10 6 is O.h2 /. Although it is clear what is meant, this is really meaningless mathematically and may be misleading when analyzing the accuracy of a numerical
method. If the error E.h/ on a grid with h D 10 3 turns out to be 3  10 6 , we cannot
conclude that the method is second order accurate. It could be, for example, that the error
E.h/ has the behavior
E.h/ D 0:003 h;
(A.1)
in which case E.10 3 / D 3  10 6, but it is not true that E.h/ D O.h2 /. In fact the
method is only first order accurate, which would become apparent as we refined the grid.
Conversely, if
E.h/ D 106 h2 ;
(A.2)
then E.10 3 / D 1, which is much larger than h2 , and yet it is still true that
E.h/ D O.h2 / as h ! 0:
Also note that there is more to the choice of a method than its asymptotic rate of
convergence. While in general a second order method outperforms a first order method, if
we are planning to compute on a grid with h D 10 3 , then we would prefer a first order
method with error (A.1) over a second order method with error (A.2).

A.3 Errors in vectors
Now suppose zO 2 Rs , i.e., the true solution to some problem is a vector with s components.
For example, zO may be the solution to a system of s ODEs at some particular fixed time T .
Then z is a vector of approximate values and the error e D z zO is also a vector in Rs . In
this case we can use some vector norm to measure the error.
There are many ways to define a vector norm. In general a vector norm is simply
a mapping from vectors x in Rs to nonnegative real numbers, satisfying the following
conditions (which generalize important properties of the absolute value for scalars):
E
1. kxk  0 for any x 2 Rs , and kxk D 0 if and only if x D 0.
2. If a is any scalar, then kaxk D jaj kxk.
3. If x; y 2 Rm , then kx C yk  kxk C kyk (triangle inequality).
One common choice is the max-norm (or infinity-norm) denoted by k  k1:
kek1 D max jei j:
1is

(A.3)

It is easy to verify that k  k1 satisfies the required properties. A bound on the max-norm
of the error is nice because we know that every component of the error can be no greater
than the max-norm. For some problems, however, there are other norms which are either
more appropriate or easier to bound using our analytical tools.

i

i

i

i

i

i

i

A.3. Errors in vectors

“rjlfdm”
2007/6/1
page 249
i

249

Two other norms that are frequently used are the 1-norm and 2-norm,
s
X

kek1 D

jei j

and

v
u s
uX
kek2 D t
jei j2 :

iD1

(A.4)

iD1

These are special cases of the general family of q-norms, defined by
#1=q

" s
X
kekq D

jei j

q

:

(A.5)

iD1

Note that the max-norm can be obtained as the limit as q ! 1 of the q-norm. (Usually p
is used instead of q in defining these norms, but in this book p is often used for the order
of accuracy, which might be measured in some q-norm.)

A.3.1 Norm equivalence
With so many different norms to choose from, it is natural to ask whether results on convergence of numerical methods will depend on our choice of norm. Suppose e.h/ is the
error obtained with some step size h, and that ke.h/k D O.hp / in some norm, so that the
method is pth order accurate. Is it possible that the rate will be different in some other
norm? The answer is “no,” due to the following result on the “equivalence” of all norms on
Rs . (Note that this result is valid only as long as the dimension s of the vector is fixed as
h ! 0. See Section A.5 for an important case where the length of the vector depends on
h.)
Let k  k˛ and k  kˇ represent two different vector norms on Rs . Then there exist two
constants C1 and C2 such that
C1 kxk˛  kxkˇ  C2 kxk˛

(A.6)

for all vectors x 2 Rm . For example, it is fairly easy to verify that the following relations
hold among the norms mentioned above:
kxk1 kxk1 skxk1;
p
kxk1 kxk2 skxk1;
p
kxk2 kxk1 skxk2:

(A.7a)
(A.7b)
(A.7c)

Now suppose that ke.h/k˛  C hp as h ! 0 in some norm k  k˛ . Then we have
ke.h/kˇ  C2 ke.h/k˛  C2 C hp
and so ke.h/kˇ D O.hp / as well. In particular, if ke.h/k ! 0 in some norm, then the
same is true in any other norm and so the notion of “convergence” is independent of our
choice of norm. This will not be true in Section A.4, where we consider approximating
functions rather than vectors.

i

i

i

i

i

i

i

250

“rjlfdm”
2007/6/1
page 250
i

Appendix A. Measuring Errors

A.3.2 Matrix norms
For any vector norm k  k we can define a corresponding matrix norm. The norm of a matrix
A 2 Rss is denoted by kAk and has the property that C D kAk is the smallest value of
the constant C for which the bound
kAxk  C kxk

(A.8)

holds for every vector x 2 Rs . Hence kAk is defined by
kAk D max
x2Rs
E
x¤0

kAxk
D max kAxk:
kxk
x2Rs

(A.9)

kxkD1

It would be rather difficult to calculate kAk from the above definitions, but for the most
commonly used norms there are simple formulas for computing kAk directly from the
matrix:
s
X
kAk1 D max
jaij j
(maximum column sum),
(A.10a)
1j s

iD1
s
X

kAk1 D max

jaij j

1is

(maximum row sum),

(A.10b)

j D1

q
kAk2 D .AT A/:

(A.10c)

In the definition of the 2-norm, .B/ denotes the spectral radius of the matrix B (the
maximum modulus of an eigenvalue). In particular, if A is a normal matrix, e.g., if A D AT
is symmetric, then kAk2 D .A/.
We also mention the condition number of a matrix in a given norm, defined by
.A/ D kAk kA 1 k;

(A.11)

provided the matrix is nonsingular. If the matrix is normal then the 2-norm condition
number is the ratio of largest to smallest eigenvalue (in modulus). The condition number
plays a role in the convergence rate of many iterative methods for solving a linear system
with the matrix A (see Chapter 4). See, e.g., [35], [91] for more discussion.

A.4 Errors in functions
Now consider a problem in which the solution is a function u.x/ over some interval a 
x  b rather than a single value or vector. Some numerical methods, such as finite element
or collocation methods, produce an approximate solution U.x/ which is also a function.
Then the error is given by a function
e.x/ D U.x/

u.x/:

We can measure the magnitude of this error using standard function space norms, which
are quite analogous to the vector norms described above. For example, the max-norm is
given by
kek1 D max je.x/j:
(A.12)
axb

i

i

i

i

i

i

i

A.5. Errors in grid functions

“rjlfdm”
2007/6/1
page 251
i

251

The 1-norm and 2-norm are given by integrals over Œa; b rather than by sums over the
vector elements:
Z b
kek1 D
je.x/j dx;
(A.13)
a

!1=2

Z b
2

je.x/j dx

kek2 D

:

(A.14)

a

These are again special cases of the general q-norm, defined by
!1=q
Z b
q
kekq D
je.x/j dx
:

(A.15)

a

A.5 Errors in grid functions
Finite difference methods do not produce a function U.x/ as an approximation to u.x/.
Instead they produce a set of values Ui at grid points xi . For example, on a uniform grid
with N equally spaced in some interval .a; b/ and grid spacing h, our approximation to
u.x/ would consist of the N values .U1 ; U2 ; : : : ; UN /. (Note that if h D .b a/=.mC1/
as is often assumed in this book, then generally N D m; m C 1, or m C 2, depending
on whether one or both boundary points are included in the set of unknowns. For our
discussion here this is immaterial—what is important to note is that N D O.1= h/ as
h ! 0.)
How can we measure the error in this approximation? We want to compare a set of
discrete values with a function.
We must first decide what the values Ui are supposed to be approximating. Often
the value Ui is meant to be interpreted as an approximation to the pointwise value of the
function at xi , so Ui  u.xi /. In this case it is natural to define a vector of errors e D
.e1 ; e2 ; : : : ; eN / by
ei D Ui u.xi /:
This is not always the proper interpretation of Ui , however. For example, some numerical
methods are derived using the assumption that Ui approximates the average value of u.x/
over an interval of length h, e.g.,
Z
1 xi
Ui 
u.x/ dx:
h xi 1
In this case it would be more appropriate to compare Ui to this cell average in defining the
error. Clearly the errors will be different depending on what definition we adopt and may
even exhibit different convergence rates, so it is important to make the proper choice for
the method being studied.
Once we have defined the vector of errors .e1 ; : : : ; eN /, we can measure its magnitude using some norm. Since this is simply a vector with N components, it would be
tempting to simply use one of the vector norms discussed above, e.g.,
N
X

kek1 D

jei j:

(A.16)

iD1

i

i

i

i

i

i

i

252

“rjlfdm”
2007/6/1
page 252
i

Appendix A. Measuring Errors

However, this choice would give a very misleading idea of the magnitude of the error. The
quantity in (A.16) can be expected to be roughly N times as large as the error at any single
grid point and here N is not the dimension of some physically relevant space, but rather
the number of points on our grid. If we refine the grid and increase N , then the quantity
(A.16) might well increase even if the error at each grid point decreases, which is clearly
not the correct behavior.
Instead we should define the norm of the error by discretizing the integral in (A.13),
which is motivated by considering the vector .e1 ; : : : ; eN / as a discretization of some
error function e.x/. This suggests defining
N
X

kek1 D h

(A.17)

jei j
iD1

with the factor of h corresponding to the dx in the integral. Note that since h  .b a/=N ,
this scales the sum by 1=N as the number of grid points increases, so that kek1 is the
average value of e over the interval (times the length of the interval), just as in (A.13).
The norm (A.17) will be called a grid function norm and is distinct from the related vector
norm. The set of values .e1 ; : : : ; eN / will sometimes be called a grid function to remind
us that it is a special kind of vector that represents the discretization of a function.
Similarly, the q-norm should be scaled by h1=q , so that the q-norm for grid functions
is
!1=q
N
X
kekq D h
jei jq
:
(A.18)
iD1

Since h

1=q

! 1 as q ! 1, the max-norm remains unchanged,
kek1 D max jei j;
1iN

which makes sense from (A.12).
In two space dimensions we have analogous norms of functions and grid functions,
e.g.,
“
1=q
kekq D
je.x; y/jq dx dy
for functions,
0
kekq D @x y

XX
i

11=q
jeij jq A

for grid functions

j

with the obvious extension to more dimensions.

A.5.1 Norm equivalence
Note that we still have an equivalence of norms in the sense that, for any fixed N (and
hence fixed h), there are constants C1 and C2 such that
C1 kxk˛  kxkˇ  C2 kxk˛

i

i

i

i

i

i

i

A.5. Errors in grid functions

“rjlfdm”
2007/6/1
page 253
i

253

for any vector e 2 RN . For example, translating (A.7a) to the context of grid function
norms gives the bounds
hkek1 kek1 N hkek1 D .b a/ kek1;
p
p
h kek1 kek2 N h kek1 D b a kek1;
p
p
p
h kek2 kek1 N h kek2 D b a kek2:

(A.19a)

p

(A.19b)
(A.19c)

However, since these constants may depend on N and h, this equivalence does not carry
over when we consider the behavior of the error as we refine the grid so that h ! 0 and
N ! 1.
We are particularly interested in the convergence rate of a method. If e.h/ is the
vector of errors obtained on a grid with spacing h, we would like to show that
ke.h/k  O.hq /
for some q. In the last section we saw that the rate is independent of the choice of norm if
e.h/ is a vector in the space Rm with fixed dimension m. But now m D N and grows as
h ! 0, and as a result the rate may be quite different in different norms. This is particularly
noticeable if we approximate a discontinuous function, as the following example shows.
Example 3.1. Set
(
0 x  12 ;
u.x/ D
1 x > 12
and define the grid function approximation as indicated in Figure A.1. Then the error ei .h/
is zero at all grid points but the one at the discontinuity, where it has the value 1=2. Then
no matter how fine the grid is, there is always an error of magnitude 1=2 at one grid point
and hence
1
ke.h/k1 D
for all h:
2
On the other hand, in the 1-norm (A.17) we have
ke.h/k1 D h=2 D O.h/ as h ! 0:
We see that the 1-norm converges to zero as h goes to zero while the max-norm does not.
How should we interpret this? Should we say that U.h/ is a first order accurate approximation to u.x/ or should we say that it does not converge? It depends on what we

Figure A.1. The function u.x/ and the discrete approximation.

i

i

i

i

i

i

i

254

“rjlfdm”
2007/6/1
page 254
i

Appendix A. Measuring Errors

are looking for. If it is important that the maximum error over all grid points be uniformly
small, then the max-norm is the appropriate norm to use and the fact that ke.h/k1 does
not approach zero tells us that we are not achieving our goal. On the other hand, this may
not be required, and in fact this example illustrates that it is unrealistic to expect pointwise convergence in problems where the function is discontinuous. For many purposes the
approximation shown in Figure A.1 would be perfectly acceptable.
This example also illustrates the effect of choosing a different definition of the “error.” If we were to define the error by
ei .h/ D Ui

1
h

Z xi Ch=2
u.x/ dx;
xi h=2

then we would have ei .h/  0 for all i and h and ke.h/k D 0 in every norm, including the
max-norm. With this definition of the error our approximation is not only acceptable, it is
the best possible approximation.
If the function we are approximating is sufficiently smooth, and if we expect the error
to be roughly the same magnitude at all points, then it typically does not matter so much
which norm is chosen. The convergence rate for a given method, as observed on sufficiently
smooth functions, will often be the same in any q-norm. The norm chosen for analysis is
then often determined by the nature of the problem and the availability of mathematical
techniques for estimating different error norms. For example, for linear problems where
Fourier analysis can be applied, the 2-norm is often a natural choice. For conservation laws
where integrals of the solution are studied, the 1-norm is often simplest to use.

A.6 Estimating errors in numerical solutions
When developing a computer program to solve a differential equation, it is generally a
good idea to test the code and ensure that it is producing correct results with the expected
accuracy. How can we do this?
A first step is often to try the code on a problem for which the exact solution is
known, in which case we can compute the error in the numerical solution exactly. Not
only can we then check that the error is small on some grid, we can also refine the grid
and check how the error is behaving asymptotically, to verify that the expected order of
accuracy, and perhaps even error constant, is seen. Of course one must be aware of some
of the issues raised earlier, e.g., that the expected order may appear only for h sufficiently
small.
It is important to test a computer program by doing grid refinement studies even if
the results look quite good on one particular grid. A subtle error in programming (or in
deriving the difference equations or numerical boundary conditions) can lead to a program
that gives reasonable results and may even converge to the correct solution, but at less than
the optimal rate. Consider, for example, the first approach of Section 2.12.
Of course in practice we are usually trying to solve a problem for which we do not
know the exact solution, or we wouldn’t bother with a numerical method in the first place.
However, there are often simplified versions of the problem for which exact solutions are
known, and a good place to start is with these special cases. They may reveal errors in the
code that will affect the solution of the real problem as well.

i

i

i

i

i

i

i

A.6. Estimating errors in numerical solutions

“rjlfdm”
2007/6/1
page 255
i

255

This is generally not sufficient, however, even when it is possible, since in going
from the easy special case to the real problem new errors may be introduced. How do we
estimate the error in a numerical solution if we do not have the exact solution with which
to compare it?
The standard approach, when we can afford to use it, is to compute a numerical
solution on a very fine grid and use this as a “reference solution” (or “fine-grid solution”).
This can be used as a good approximation to the exact solution in estimating the error on
other, much coarser, grids. When the fine grid is fine enough, we can obtain good estimates
not only for the errors but also for the order of accuracy. See Section A.6.2.
Often we cannot afford to take very fine grids, especially in more than one space
dimension. We may then be tempted to use a grid that is only slightly finer than the grid
we are testing in order to generate a reference solution. When done properly this approach
can also yield accurate estimates of the order of accuracy, but more care is required. See
Section A.6.3.

A.6.1 Estimates from the true solution
First suppose we know the true solution. Let E.h/ denote the error in the calculation with
grid spacing h, as computed using the true solution. In this section we suppose that E.h/
is a scalar, typically some norm of the error over the grid, i.e.,
E.h/ D kU.h/

UO .h/k;

where U.h/ is the numerical solution vector (grid function) and UO .h/ is the true solution
evaluated on the same grid.
If the method is pth order accurate, then we expect
E.h/ D C hp C o.hp /

as h ! 0;

and if h is sufficiently small, then
E.h/  C hp :

(A.20)

If we refine the grid by a factor of 2, say, then we expect
E.h=2/  C.h=2/p :
Defining the error ratio
R.h/ D E.h/ = E.h=2/;

(A.21)

R.h/  2p ;

(A.22)

p  log2 .R.h//:

(A.23)

we expect
and hence
Here refinement by a factor of 2 is used only as an example, since this choice is often made
in practice. But more generally if h1 and h2 are any two grid spacings, then we can estimate
p based on calculations on these two grids using
p

i

i

log.E.h1 /=E.h2 //
:
log.h1 = h2/

(A.24)

i

i

i

i

i

256

“rjlfdm”
2007/6/1
page 256
i

Appendix A. Measuring Errors

Hence we can estimate the order p based on any two calculations. (This will be valid only
if h is small enough that (A.20) holds, of course.)
Note that we can also estimate the error constant C by
C  E.h/= hp
once p is known.

A.6.2 Estimates from a fine-grid solution
Now suppose we don’t know the exact solution but that we can afford to run the problem on
N and use this as a reference solution in computing
a very fine grid, say, with grid spacing h,
the errors on some sequence of much coarser grids. To compare U.h/ on the coarser grid
N on the fine grid, we need to make sure that these two grids contain coincident
with U.h/
grid points where we can directly compare the solutions. Typically we choose the grids in
such a way that all grid points on the coarser grid are also fine-grid points. (This is often the
hardest part of doing such grid refinement studies—getting the grids and indexing correct.)
Let UN .h/ be the restriction of the fine-grid solution to the h-grid, so that we can
N
define the approximate error E.h/
 kU.h/ UN .h/k, analogous to the true error E.h/ D
kU.h/ UO .h/k. What is the error in this approximate error? We have
U.h/

UN .h/ D .U.h/

UO .h// C .UO .h/

UN .h//:

If the method is supposed to be pth order accurate and hNp  hp , then the second term on
N
the right-hand side (the true error on the h-grid)
should be negligible compared to the first
N
term (the true error on the h-grid) and E.h/ should give a very accurate estimate of the
error.
Warning: Estimating the error and testing the order of accuracy by this approach only
confirm that the code is converging to some function with the desired rate. It is very possible
that the code is converging very nicely to the wrong function. Consider a second order
accurate method applied to 2-point boundary value problem, for example, and suppose
that we code everything properly except that we mistype the value of one of the boundary
values. Then a grid-refinement study of this type would show that the method is converging
with second order accuracy, as indeed it is. The fact that it is converging to the solution of
the wrong problem would not be revealed by this test. One must use other tests as well, not
least of which is checking that the computed solutions make sense physically, e.g., that the
correct boundary conditions are in fact satisfied.
More generally, a good understanding of the problem being solved, a knowledge of
how the solution should behave, good physical intuition, and common sense are all necessary components in successful scientific computing. Don’t believe the numbers coming out
simply because they are generated by a computer, even if the computer also tells you that
they are second order accurate!

A.6.3 Estimates from coarser solutions
Now suppose that our computation is very expensive even on relatively coarse grids, and
we cannot afford to run a calculation on a much finer grid to test the order of accuracy.

i

i

i

i

i

i

i

A.6. Estimating errors in numerical solutions

“rjlfdm”
2007/6/1
page 257
i

257

Suppose, for example, that we are willing to run the calculation only on grids with spacing
h, h=2, and h=4 and we wish to estimate the order of accuracy from these three calculations,
without using any finer grids. Since we can estimate the order from any two values of the
error, we could define the errors in the two coarser grid calculations by using the h=4
calculation as our reference solution. Will we get a good estimate for the order?
N
In the notation used above, we now have hN D h=4, while h D 4hN and h=2 D 2h.
Assuming the method is pth order accurate and that h is small enough that (A.20) is valid
(a poor assumption, perhaps, if we are using very coarse grids!), we expect
N
N
E.h/
D E.h/ E.h/
p
p
 Ch
C hN
p
D .4
1/C hNp :
Similarly,
N
E.h=2/
 .2p

1/C hN p :

The ratio of approximate errors is thus
N
N
R.h/
D E.h/=
EN

 
h
4p
 p
2
2

1
D 2p C 1:
1

For modest p this differs significantly from (A.22). For a first order accurate method with
N
p D 1, we now have R.h/
 3 and we should expect the apparent error to decrease by
a factor of 3 when we go from h to h=2, not by the factor of 2 that we normally expect.
For a second order method we expect a factor of 5 improvement rather than a factor of 4.
N
This increase in R.h/
results from the fact that we are comparing our numerical solutions
to another approximate solution that has a similar error.
We can obtain a good estimate of p from such calculations (assuming (A.20) is valid),
but to do so we must calculate p by
N
p  log2 .R.h/

1/

rather than by (A.23). The approximation (A.23) would overestimate the order of accuracy.
Again we have used refinement by factors of 2 only as an example. If the calculation
is very expensive we might want to refine the grid more slowly, using, for example, h,
3h=4, and h=2. One can develop appropriate approximations to p based on any three grids.
The tricky part may be to estimate the error at grid points on the coarser grids if these
are not also grid points on the hN grid. Interpolation can be used, but then one must be
careful to ensure that sufficiently accurate interpolation formulas are used that the error in
interpolation does not contaminate the estimate of the error in the numerical method being
studied.
Another approach that is perhaps simpler is to compare the solutions
Q
E.h/
 U.h/

U.h=2/

and

Q
E.h=2/
D U.h=2/

U.h=4/:

In other words, we estimate the error on each grid by using the next finer grid as the reference solution, rather than using the same reference solution for both coarser grids. In this
case we have
 


h
1
Q
E.h/
D E.h/ E
hp
C 1
2
2p

i

i

i

i

i

i

i

258

“rjlfdm”
2007/6/1
page 258
i

Appendix A. Measuring Errors

and
Q
E.h=2/
DE

 
h
2

 

h
E
C 1
4

and so
Q
E.h/=
EQ

1
2p



hp
2p

 
h
 2p :
2

In this case the approximate error decreases by the same factor we would expect if the true
solution were used as the reference solution on each grid.

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 259
i

Appendix B

Polynomial Interpolation
and Orthogonal
Polynomials
B.1 The general interpolation problem
Given a set of discrete points xi for i D 0; 1; : : : ; n and function values Fi , the interpolation problem is to determine a function .x/ of some specified form passing through these
points,
.xi / D Fi for i D 0; 1; : : : ; n:
(B.1)
We use the notation Int.x0 ; : : : ; xn/ to denote the smallest interval containing all these
points (which need not be in increasing order but which are assumed to be distinct).
Interpolation has many uses; for example,
 we may have only discrete data values and want to estimate values in between, x 2
Int.x0 ; : : : ; xn /. This is the origin of the term interpolation. We might also use this
function to extrapolate if we evaluate it outside the interval where data are given.
 we may know the true function F.x/ but want to approximate it by a function .x/
that is cheaper to evaluate, or easier to work with symbolically (to differentiate or
integrate, for example).
 we may use it as a starting point for deriving numerical methods for differential
equations (or for integral equations or numerical integration).
There are infinitely many possible functions . Typically  is chosen to be a linear
combination of some n C 1 given basis functions 0 .x/; : : : ; n .x/,
.x/ D c0 0 .x/ C    C cn n .x/:

(B.2)

Then condition (B.1) gives a linear system of n C 1 equations to solve for the coefficients
c0 ; : : : ; cn ,
2
3 2
3
32
0 .x0 / 1 .x0 /    n .x0 /
F0
c0
6 0 .x1 / 1 .x1 /    n .x1 / 7 6 c1 7 6 F1 7
6
7 6
7
76
(B.3)
6
7 6 :: 7 D 6 :: 7 :
::
::
4
5
4
5
4
:
:
:
: 5
0 .xn / 1 .xn /    n .xn /
cn
Fn
259

i

i

i

i

i

i

i

260

“rjlfdm”
2007/6/1
page 260
i

Appendix B. Polynomial Interpolation and Orthogonal Polynomials

This system can we written as ˆc D F . Different choices of basis functions lead to
different types of interpolation. Using trigonometric functions gives Fourier series, for
example, the basis of Fourier spectral methods.
In this appendix we consider interpolation by polynomials, the basis of many finite
difference and spectral methods.

B.2 Polynomial interpolation
Through any n C 1 points there is a unique interpolating polynomial p.x/ of degree n.
There are many ways to represent this function depending on what basis is chosen for Pn ,
the set of all polynomials of degree n.

B.2.1 Monomial basis
The monomial functions are
0 .x/ D 1;

1 .x/ D x;

2 .x/ D x 2;

:::;

n .x/ D x n :

(B.4)

The matrix ˆ appearing in (B.3) is then the Vandermonde matrix. This matrix may be quite
ill-conditioned for larger values of n.

B.2.2 Lagrange basis
The j th Lagrange basis function (based on a given set of interpolation points xi ) is given by
j .x/ D

n
Y
.x
iD0
i¤j

.xj

xi /
:
xi /

1
0

if i D j ;
if i ¤ j:

(B.5)

This is a polynomial of degree n. Note that

j .xi / D ıij D

Then the matrix in (B.3) is the identity matrix and ci D Fi . The coefficients are easy to
determine in this form but the basis functions are a bit cumbersome.

B.2.3 Newton form
The Newton form of the interpolating polynomial is
p.x/ D c0 Cc1 .x x0/Cc2 .x x0/.x x1 /C  Ccn.x x0 /.x x1 /    .x xn 1 /: (B.6)
For these basis functions the matrix ˆ is lower triangular and the ci may be found by forward substitution. Alternatively they are most easily computed using divided differences,
ci D F Œx0 ; : : : ; xi . These can be computed from a tableau of the form

i

i

i

i

i

i

i

B.2. Polynomial interpolation

“rjlfdm”
2007/6/1
page 261
i

261

x0

F Œx0 

x1

F Œx1 

F Œx0 ; x1
F Œx0 ; x1; x2

(B.7)

F Œx1 ; x2
x2

F Œx2 ;

where
F Œxj  D Fj
and for k > 0,
F Œxj ; : : : ; xj Ck  D

F Œxj C1 ; : : : ; xj Ck  F Œxj ; : : : ; xj Ck 1 
:
xj Ck xj

(B.8)

Then the Newton form can be built up as follows:
p0 .x/ D F Œx0 
is the polynomial of degree 0 interpolating at x0,
p1 .x/ D F Œx0  C F Œx0 ; x1.x x0 /
is the polynomial of degree 1 interpolating at x0; x1 ,
p2 .x/ D F Œx0  C F Œx0 ; x1.x x0 / C F Œx0 ; x1; x2 .x x0 /.x x1 /
is the polynomial of degree 2 interpolating at x0; x1 ; x2,
etc.
In each step we add a term that vanishes at all the preceding interpolation points and makes
the function also interpolate at one new point. Note that the coefficients of previous basis
functions do not change.
Relation to Taylor series. Note that
F Œxj ; xj C1  D

Fj C1
xj C1

Fj
:
xj

(B.9)

Suppose the data values Fi come from some underlying smooth function F.x/, so Fi D
F.xi /. Then (B.9) approximates the derivative F 0 .xj /. Similarly, if xj ; : : : ; xj Ck are close
together, then
1
F Œxj ; : : : ; xj Ck   F .k/ .xj /;
(B.10)
k!
where F .k/ .x/ is the kth derivative. In fact, one can show that for sufficiently smooth F ,
F Œxj ; : : : ; xj Ck  D

1 .k/
F ./
k!

(B.11)

for some  lying in the interval Int.xj ; : : : ; xj Ck /. This is true provided that F is k times
continuously differentiable on this interval. The Newton form (B.6) thus is similar to the
Taylor series
F.x/ D F.x0 / C F 0 .x0 /.x

x0 / C

1 00
F .x0 /.x
2!

x0 / 2 C   

(B.12)

and reduces to this in the limit as xj ! x0 for all j .

i

i

i

i

i

i

i

262

“rjlfdm”
2007/6/1
page 262
i

Appendix B. Polynomial Interpolation and Orthogonal Polynomials

B.2.4 Error in polynomial interpolation
Suppose F.x/ is a smooth function, we evaluate Fi D F.xi / (i D 0; 1; : : : ; n), and
we now fit a polynomial p.x/ of degree n through these points. How well does p.x/
N
approximate F.x/
N at some other point x?
N
Note that we could add xN as another interpolation point and create an interpolating
polynomial p.x/
N
of degree n C 1 that interpolates also at this point,
p.x/
N
D p.x/ C F Œx0 ; : : : ; xn ; x.x
N

x0/    .x

xn /:

x0 /    .xN

xn /:

Then p.
N x/
N D F.x/
N and so
F.x/
N

p.x/
N D F Œx0 ; : : : ; xn ; x.
N xN

Using (B.11), we obtain an error formula similar to the remainder formula for Taylor
series. If p.x/ is given by (B.6), then at any point x,
F.x/

p.x/ D

1 .n/
F ./.x
n!

x0/    .x

xn /;

(B.13)

where  is some point lying in Int.x; x0; : : : ; xn /. How large this is depends on
 how close the point x is to the interpolation points x0 ; : : : ; xn, and
 how small the derivative F .n/ ./ is over this interval, i.e., how smooth the function
is.
For a given x we don’t know exactly what  is in general, but we can often use this expression to obtain an error bound of the form
jp.x/

F.x/j  Kj.x

where
KD

x0 /    .x

xn /j;

(B.14)

1
jF .n/ ./j:
max
n! 2 Int.x0 ;:::;xn /

Q
Note that the bound (B.14) involves values of the polynomial Q.x/  niD1 .x xi /, the
polynomial with roots at the interpolation points xi and with leading coefficient 1 (i.e., a
monic polynomial). If we want to minimize the error over some interval, then we might
want to choose the interpolation points to minimize the maximum value that Q.x/ takes
over that interval. We will return to this in Section B.3.2, where we will see that Chebyshev polynomials satisfy the required optimality condition. These are a particular class of
orthogonal polynomials, as described in the next section.

B.3 Orthogonal polynomials
If w.x/ is a function on an interval Œa; b that is positive everywhere on the interval, then
we can define the inner product of two functions f .x/ and g.x/ on this interval by
Z b
hf; gi D
w.x/f .x/g.x/ dx:
(B.15)
a

i

i

i

i

i

i

i

B.3. Orthogonal polynomials

“rjlfdm”
2007/6/1
page 263
i

263

We say that two functions f .x/ and g.x/ are orthogonal in a given interval Œa; b with
respect to the given weight function w.x/ if the inner product of f and g is equal to zero.
For a given Œa; b and w.x/, one can define a sequence of orthogonal polynomials
P0 .x/, P1 .x/; : : : of increasing degree which have the property that
Pm .x/ 2 Pm

(the set of polynomials of degree m);

hPm ; Pni D 0 for m ¤ n:

(B.16)

The sequence of polynomials is said to be orthonormal if, in addition,
hPm; Pmi D 1 for all m:

(B.17)

Orthogonal polynomials have many interesting and useful properties and arise in numerous branches of numerical analysis. In particular, the Chebyshev polynomials described
in Section B.3.2 are used in several contexts in this book.
The set of polynomials P0 .x/, P1 .x/; : : : ; Pk .x/ forms an orthogonal basis
for Pk . Any polynomial p 2 Pk can be uniquely expressed as a linear combination of
P0 ; : : : ; Pk . Note that each Pm must have an exact degree of m, meaning the coefficient
of x m is nonzero. Otherwise it would be a linear combination of previous polynomials in
the sequence and could not be orthogonal to them all.
Note that if Pm is orthogonal to P0; P1; : : : ; Pm 1 , then in fact Pm is orthogonal
to all polynomials p 2 Pm 1 of degree less than m, since p can be written as p.x/ D
c0 P0.x/ C    C cm 1Pm 1 .x/ and so
hp; Pmi D c0 hP0; Pmi C    cm 1 hPm 1 ; Pmi D 0:
We say that Pm is orthogonal to the space Pm 1 .
Sequences of orthogonal polynomials can be built up by a Gram–Schmidt process,
analogous to the manner in which a sequence of linearly independent vectors is transformed
into a sequence of orthogonal vectors. Suppose P0 ; P1 ; : : : ; Pm are already mutually
orthogonal with Pn having exact degree n. We wish to construct PmC1 .x/, a polynomial
of exact degree m C 1 that is orthogonal to all of these. Start with the polynomial
Q.x/ D ˛m xPm.x/
for some ˛m ¤ 0. This polynomial has exact degree m C 1 and hence is linearly independent from P0 ; P1; : : : ; Pm . Moreover, it is already orthogonal to P0 ; P1 ; : : : ; Pm 2 ,
since
Z b
hQ; Pni D hxPm; Pni D
w.x/xPm.x/Pn .x/ dx D hPm; xPni D 0
a

for n  m 2, since xPn 2 Pm 1 and Pm is orthogonal to this space. We wish to make Q
orthogonal to Pm 1 and Pm and, as in the Gram–Schmidt process for vectors, we do this
by subtracting multiples of Pm 1 and Pm from Q:
PmC1 .x/ D ˛mxPm.x/

i

i

ˇm Pm .x/

m Pm 1 .x/:

(B.18)

i

i

i

i

i

264

“rjlfdm”
2007/6/1
page 264
i

Appendix B. Polynomial Interpolation and Orthogonal Polynomials

Requiring hPmC1 ; Pmi D 0 determines
hPm ; ˛mxPmi
hPm; Pmi

(B.19)

hPm 1; ˛m xPmi ˇm hPm 1; Pm i
:
hPm 1; Pm 1 i

(B.20)

ˇm D
and then hPmC1 ; Pm 1i D 0 gives
m D

The relation (B.18) is a three-term recurrence relation for the sequence of orthogonal polynomials and can be used to generate the entire sequence once P0 and P1 are specified. For
many useful sets of orthogonal polynomials the coefficients ˛m ; ˇm , and m take particularly simple forms.

B.3.1 Legendre polynomials
The sequence of polynomials that are orthogonal on Œ 1; 1 with weight function w.x/ D 1
are called the Legendre polynomials. We must also choose some normalization to uniquely
define this sequence (since multiplying two orthogonal polynomials by arbitrary constants
leaves them orthogonal). This amounts to choosing the nonzero constants ˛m in (B.18).
One might choose the polynomials to be orthonormal (i.e., normalize so that (B.17) is
satisfied), but this leads to messy coefficients. The traditional choice is to require that
Pm.1/ D 1 for all m. The first few Legendre polynomials are then
P0 .x/ D 1;
P1 .x/ D x;
3
P2 .x/ D x 2
2
5 3
P3 .x/ D x
2

1
;
2
3
x:
2

(B.21)

These polynomials satisfy a three-term recurrence relation (B.18) with
˛m D

2m C 1
;
mC1

ˇm D 0;

m D

m
:
mC1

The roots of the Legendre polynomials are of importance in various applications. In particular, they are the nodes for Gaussian quadrature formulas for approximating the integral
of a function; see, e.g., [16], [90]. There is no simple expression for the location of the
roots, but they can be found as the eigenvalues of a tridiagonal matrix in MATLAB by
the following code (adapted from the program gauss.m in [90], which also computes the
weights for the associated Gauss quadrature rules):
Toff = .5./sqrt(1-(2*(1:m-1)).ˆ(-2));
T = diag(Toff,1) + diag(Toff,-1);
xi = sort(eig(T));
These points are also sometimes used as grid points in spectral methods.

i

i

i

i

i

i

i

B.3. Orthogonal polynomials

“rjlfdm”
2007/6/1
page 265
i

265

B.3.2 Chebyshev polynomials
Several topics discussed in this book involve the Chebyshev polynomials Tm.x/. These
are a sequence of polynomials that are orthogonal on the interval Œ 1; 1 with the weight
function
w.x/ D .1 x 2/ 1=2 :
(B.22)
The first few Chebyshev polynomials are
T0 .x/ D 1;
T1 .x/ D x;
T2 .x/ D 2x 2

1;

T3 .x/ D 4x 3

3x:

(B.23)

Again these are normalized so that Tm .1/ D 1 for m D 0; 1; : : :. Chebyshev polynomials
satisfy a particularly simple three-term recurrence,
TmC1 .x/ D 2xTm.x/

Tm 1 .x/:

(B.24)

While the weight function (B.22) may seem to be a less natural choice than w.x/ D 1, the
Chebyshev polynomials have a number of valuable properties, a few of which are listed
below and are used elsewhere in this text.
Property 1. The Chebyshev polynomial Tm .x/ equioscillates m C 1 times in the
interval Œ 1; 1, i.e., jTm .x/j is maximized at m C 1 points x0; x1 ; : : : ; xm in the interval,
points where Tm.x/ takes the values
Tm .xj / D . 1/j :
These Chebyshev extreme points are given by
xj D cos.j =m/;

j D 0; 1; : : : ; m:

(B.25)

(Note that these are labeled in decreasing order from x0 D 1 to xm D 1.) Figure B.1
shows a plot of T7 .x/, for example. This set of extreme points will be useful for spectral
methods as discussed in Section 2.21.
Property 2. For x in the interval Œ 1; 1, the value of Tm .x/ is given by
Tm .x/ D cos.m arccos x/:

(B.26)

This does not look much like a polynomial, but it is since cos.m/ can be written as a
polynomial in cos./ using trigonometric identities, and then set as x D cos./. Note that
from this formulation it is easy to check that (B.25) gives the desired extreme points.
Outside this interval there is an analogous formula in terms of the hyperbolic cosine,
Tm .x/ D cosh.m cosh 1 x/

for jxj  1;

(B.27)

an expression that is used in the analysis of the convergence of conjugate gradients. Note
that outside the unit interval the Chebyshev polynomials grow very rapidly.

i

i

i

i

i

i

i

266

“rjlfdm”
2007/6/1
page 266
i

Appendix B. Polynomial Interpolation and Orthogonal Polynomials
2
1.5
1
0.5
0
−0.5
−1
−1.5
−2

−1

−0.5

0

0.5

1

Figure B.1. The Chebyshev polynomial T7 .x/ of degree 7.
2
1.5

2
1

1

0

0.5

−1

0

−2

−0.5
1

−1
0
−1
−1.5

−1

−0.5

0

0.5

1

1.5

−1.5
−2
−1.5

−1

−0.5

0

0.5

1

1.5

Figure B.2. The Chebyshev polynomial viewed as a function Cm ./ on the unit
disk e i and when projected on the x-axis, i.e., as a function of x D cos./. Shown for
m D 15.
Property 3. Consider the function
Cm ./ D Re.e im / D cos.m/

(B.28)

for 0    . We can view this as a function defined on the upper half of the unit circle in
the complex plane. If we identify x D cos./ or  D arccos x, then this reduces to (B.26),
so we can view the Chebyshev polynomial on the interval Œ 1; 1 as being the projection of
the function (B.28) onto the real axis, as illustrated in Figure B.2. This property is useful
in relating polynomial interpolation at Chebyshev points to trigonometric interpolation at
equally spaced points on the unit circle and allows the use of the Fast Fourier Transform
(FFT) algorithm to efficiently implement Chebyshev spectral methods. Orthogonality of
the Chebyshev polynomials with respect to the weight function (B.22) also can be easily
interpreted in terms of orthogonality of the trigonometric functions cos.m/ and cos.n/.

i

i

i

i

i

i

i

B.3. Orthogonal polynomials

“rjlfdm”
2007/6/1
page 267
i

267

Property 4. The m roots of Tm .x/ all lie in Œ 1; 1, at the points


.j 1=2/
j D cos
for j D 1; 2; : : : ; m:
m

(B.29)

This follows directly from the representation (B.26).
Property 5. The Chebyshev polynomial Tm.x/ solves the mini-max optimization
problem
1
find p 2 Pm
to minimize max jp.x/j;
(B.30)
1x1

1
where Pm
is the set of mth degree polynomials satisfying p.1/ D 1. Recall that Tm.x/
equioscillates with extreme values ˙1 so max 1x1 jTm .x/j D 1.
A slightly different formulation of this property is sometimes useful: the scaled
Chebyshev polynomial 21 m Tm.x/ is the monic polynomial of degree m that minimizes
max 1x1 jp.x/j. A monic polynomial hasQ
leading coefficient 1 on the x m term. The
1 m
scaled Chebyshev polynomial 2
Tm.x/ D m
j / has leading coefficient 1 and
j D1 .x
equioscillates between the values ˙21 m . If we try to reduce the level of any of these
peaks by perturbing the polynomial slightly, at least one of the other peaks will increase in
magnitude.
Note that 21 m decays to zero exponentially fast as we increase the degree. This is
responsible for the spectral accuracy of Chebyshev spectral methods and this optimality
is also used in proving the rapid convergence of the conjugate gradient algorithm (see
Section 4.3.4).
Returning to the formula (B.14) for the error in polynomial interpolation, we see
that if we are interested in approximating the function F.x/ uniformly well on the interval
Œ 1; 1, then we should use the Chebyshev roots (B.29) as interpolation points. Then (B.14)
gives the bound
jp.x/ F.x/j  K 21 n :

On a different interval Œa; b, we can use the shifted Chebyshev polynomial


2x .a C b/
Tn
:
.b a/
The corresponding Chebyshev extreme points and Chebyshev roots are then


aCb
.b a/
j
xj D
C
cos
for j D 0; 1; : : : ; m
2
2
m

(B.31)

(B.32)

and
j D


aCb
.b a/
.j
C
cos
2
2

1=2/
m


for j D 1; 2; : : : ; m;

(B.33)

respectively.

i

i

i

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 268
i

i

i

i

i

“rjlfdm”
2007/6/1
page 269
i

Appendix C

Eigenvalues and
Inner-Product Norms

The analysis of differential equations and of finite difference methods for their solution
relies heavily on “spectral analysis,” based on the eigenvalues and eigenfunctions of differential operators or the eigenvalues and eigenvectors of matrices approximating these
operators. In particular, knowledge of the spectrum of a matrix (the set of eigenvalues)
gives critical information about the behavior of powers or exponentials of the matrix, as
reviewed in Appendix D. An understanding of this is crucial in order to analyze the behavior and stability properties of differential or finite difference equations, as discussed in
Section D.2.1 and at length in the main text.
This appendix contains a review of basic spectral theory and also some additional
results on inner-product norms and the relation between these norms and spectra.
Let A 2 Cmm be an m  m matrix with possibly complex components. We will
mostly be working with real matrices, but many of the results carry over directly to the
complex case or are most easily presented in this generality. Moreover, even real matrices
can have complex eigenvalues and eigenvectors, so we must work in the complex plane.
The matrix A has m eigenvalues 1; 2 ; : : : ; m that are the roots of the characteristic polynomial,
pA .z/ D det.A

zI / D .z

1 /.z

2/    .z

m /:

This polynomial of degree m always has m roots, although some may be multiple roots. If
no two are equal, then we say the roots are distinct. The set of m eigenvalues is called the
spectrum of the matrix, and the spectral radius of A, denoted by .A/, is the maximum
magnitude of any eigenvalue,
.A/ D max jp j:
1pm

If the characteristic polynomial pA .z/ has a factor .z /s , then the eigenvalue  is said
to have algebraic multiplicity ma ./ D s. If  is an eigenvalue, then A I is a singular matrix and the null space of this matrix is the eigenspace of A corresponding to this
eigenvalue,
N .A

I / D fu 2 Cm W .A

I /u D 0g D fu 2 Cm W Au D ug:
269

i

i

i

i

i

i

i

270

“rjlfdm”
2007/6/1
page 270
i

Appendix C. Eigenvalues and Inner-Product Norms

Any vector u in the eigenspace satisfies Au D u. The dimension of this eigenspace is
called the geometric multiplicity mg ./ of the eigenvalue . We always have
1  mg ./  ma ./:

(C.1)

If mg ./ D ma ./, then A has a complete set of eigenvectors for this eigenvalue. Otherwise this eigenvalue is said to be defective. If A has one or more defective eigenvalues,
then A is a defective matrix.
Example C.1. If the eigenvalues of A are all distinct, then mg D ma D 1 for every
eigenvalue and the matrix is not defective.
Example C.2. A diagonal matrix cannot be defective. The eigenvalues are simply
the diagonal elements, and the unit vectors ej (the vector with a 1 in the j th element, zeros
elsewhere) form a complete set of eigenvectors. For example,
2
3
3 0 0
AD4 0 3 0 5
0 0 5
has 1 D 2 D 3 and 3 D 5. The two-dimensional eigenspace for  D 3 is spanned by
e1 D .1; 0; 0/T and e2 D .0; 1; 0/T . The one-dimensional eigenspace for  D 5 is spanned
by e3 D .0; 0; 1/T .
Example C.3. Any upper triangular matrix has eigenvalues equal to its diagonal
elements di since the characteristic polynomial is simply pA .z/ D .z d1 /    .z dm /.
The matrix may be defective if there are repeated roots. For example,
2
3
3 1 0 0
6 0 3 0 0 7
7
AD6
4 0 0 3 0 5
0 0 0 5
has 1 D 2 D 3 D 3 and 4 D 5. The eigenvalue  D 3 has algebraic multiplicity
ma D 3 but there is only a two-dimensional space of eigenvectors associated with  D 3,
spanned by e1 and e3 , so mg D 2.

C.1 Similarity transformations
Let S be any nonsingular matrix and set
B D S 1 AS:

(C.2)

Then B has the same eigenvalues as A. To see this, suppose
Ar D r

(C.3)

for some vector r and scalar . Let w D S 1 r and multiply (C.3) by S 1 to obtain
.S 1 AS/.S 1 r / D .S 1 r /

i

i

H)

Bw D w;

i

i

i

i

i

C.3. The Jordan canonical form

“rjlfdm”
2007/6/1
page 271
i

271

so  is also an eigenvalue of B with eigenvector S 1 r . Conversely, if  is any eigenvalue
of B with eigenvector w, then similar manipulations in reverse show that  is also an
eigenvalue of A with eigenvector Sw.
The transformation (C.2) from A to B is called a similarity transformation and we
say that the matrices A and B are similar if such a relation holds. The fact that similar
matrices have the same eigenvalues is exploited in most numerical methods for computing
eigenvalues of a matrix—a sequence of similarity transformations is performed to approximately reduce A to a simpler form from which it is easy to determine the eigenvalues, such
as a diagonal or upper triangular matrix. See, for example, [35] for introductory discussions
of such algorithms.

C.2 Diagonalizable matrices
If A is not defective (i.e., if every eigenvalue has a complete set of eigenvectors), then
it is diagonalizable. In this case we can choose a set of m linearly independent right
eigenvectors rj spanning all of Cm such that Arj D j rj for j D 1; 2; : : : ; m. Let R
be the matrix of right eigenvectors
R D Œr1 jr2 j    jrm :

(C.4)

AR D Rƒ;

(C.5)

ƒ D diag.1 ; 2; : : : ; m /:

(C.6)

Then
where
This follows by viewing the matrix multiplication columnwise. Since the vectors rj are
linearly independent, the matrix R is invertible and so from (C.5) we obtain
R 1 AR D ƒ;

(C.7)

and hence we can diagonalize A by a similarity transformation. We can also write
A D RƒR 1 ;

(C.8)

which is sometimes called the eigendecomposition of A. This is a special case of the Jordan
canonical form discussed in the next section.
Let `Tj be the j th row of R 1 . We can also write the above expressions as
R 1 A D ƒR 1
and when these multiplications are viewed rowwise we obtain `Tj A D j `Tj , which shows
that the rows of R 1 are the left eigenvectors of A.

C.3 The Jordan canonical form
If A is diagonalizable, we have just seen in (C.8) that we can decompose A as A D
RƒR 1 . If A is defective, then it cannot be written in this form; A is not similar to a

i

i

i

i

i

i

i

272

“rjlfdm”
2007/6/1
page 272
i

Appendix C. Eigenvalues and Inner-Product Norms

diagonal matrix. The closest we can come is to write it in the form A D RJR 1 , where
the matrix J is block diagonal. Each block has nonzeros everywhere except perhaps on
its diagonal and superdiagonal, and is a Jordan block of some order. The Jordan blocks of
orders 1, 2, and 3 are
2
3


 1 0
 1
J.; 1/ D ;
J.; 2/ D
;
J.; 3/ D 4 0  1 5 :
0 
0 0 
In general a Jordan block of order k has the form
J.; k/ D Ik C Sk ;

(C.9)

where Ik is the k  k identity matrix and Sk is the k  k shift matrix
2
3
0 1 0 0  0
6 0 0 1 0  0 7
6
7
6
:: 7
Sk D 6 ::: :::
for k > 1
.with S1 D 0/;
: 7
6
7
4 0 0 0 0  1 5
0 0 0 0  0

(C.10)

so called because Sk .u1 ; u2 ; : : : ; uk 1 ; uk /T D .u2 ; u3 ; : : : ; uk ; 0/T . A Jordan block of
order k has eigenvalues  with algebraic multiplicity ma D k and geometric multiplicity
mg D 1. The unit vector e1 D .1; 0; : : : ; 0/T 2 Ck is a basis for the one-dimensional
eigenspace of this block.
Theorem C.1. Every m  m matrix A 2 Cmm can be transformed into the form
A D RJR 1 ;
where J is a block diagonal matrix of the form
2
J.1; k1 /
6
J.2; k2 /
6
J D6
4

(C.11)
3
::

:

7
7
7:
5

(C.12)

J.s ; ks /
Ps
Each J.i ; ki / is a Jordan block of some order ki and iD1 ki D m. If  is an eigenvalue
of A with algebraic multiplicity ma and geometric multiplicity mg , then  appears in mg
blocks and the sum of the orders of these blocks is ma .
The nonsingular matrix R contains eigenvectors of A. In the defective case, R must
also contain other vectors since there is not a complete set of eigenvectors in this case.
These other vectors are called principal vectors.
Example C.4. For illustration, consider a 3  3 matrix A with a single eigenvalue 
with ma ./ D 3 but mg ./ D 1. Then we wish to find a 3  3 invertible matrix R such
that
2
3
 1 0
AR D RJ D Œr1 jr2jr3  4 0  1 5 :
0 0 

i

i

i

i

i

i

i

C.4. Symmetric and Hermitian matrices

“rjlfdm”
2007/6/1
page 273
i

273

From this we obtain
Ar1 D r1

H)

.A

I /r1 D 0;

Ar2 D r1 C r2

H)

.A

I /r2 D r1

Ar3 D r2 C r3

H)

.A

I /r3 D r2

I /2r2 D 0;

.A

H)

.A

H)

(C.13)

3

I / r3 D 0:

The vector r1 forms a basis for the one-dimensional eigenspace. The vectors r2 and r3 are
principal vectors. They are linearly independent vectors in the null space of .A I /2 and
the null space of .A I /3 that are not in the null space of A I .
The choice of the value 1 on the superdiagonal of the nontrivial Jordan blocks is
the standard convention, but this can be replaced with any nonzero value ı by modifying
the matrix R appropriately. This is easy to verify by applying the following similarity
transformation to a Jordan block J.; k/. Choose ı ¤ 0 and set
2
6
6
6
DD6
6
4

3

1

7
7
7
7;
7
5

ı
ı

2

::

:
ı

2

D

1

6
6
6
D6
6
4

3

1

7
7
7
7:
7
5

ı 1

k 1

ı

2

::

:
ı .k 1/

(C.14)
Then
D 1 J.; k/D D Ik C ıSk :
Note that left multiplying by D 1 multiplies the ith row by ı .i 1/ , while right multiplying
by D multiplies the j th column by ı j 1 . On the diagonal the two effects cancel, while on
the superdiagonal the net effect is to multiply each element by ı.
Similarity transformations of this nature are useful in other contexts as well. If this
transformation is applied to an arbitrary matrix, then all elements on the pth diagonal will
be multiplied by ı p (with p positive for superdiagonals and negative for subdiagonals).
By applying this idea to each block in the Jordan canonical form with ı  1, we
can find a matrix R so that R 1 AR is close to diagonal with the 0 or ı at each location
on the superdiagonal. This is done, for example, in the proof of Theorem C.4. But note
that for ı < 1 the condition number is .D/ D ı 1 k and this blows up as ı ! 0 if
k > 1, so bringing a defective matrix to nearly diagonal form requires an increasingly illconditioned matrix R as the off-diagonals vanish. There is no nonsingular matrix R that
will diagonalize A in the defective case.

C.4 Symmetric and Hermitian matrices
If A 2 Rmm and A D AT , then A is a symmetric matrix. Symmetric matrices arise
naturally in many applications, in particular when discretizing “self-adjoint” differential
equations. The complex analogue of the transpose is the complex conjugate transpose
or adjoint matrix AH D ANT , in which the matrix is transposed and then the complex
conjugate of each element taken. If A is a real matrix, then AH D AT . If A D AH , then
A is said to be Hermitian (so in particular a real symmetric matrix is Hermitian).

i

i

i

i

i

i

i

274

“rjlfdm”
2007/6/1
page 274
i

Appendix C. Eigenvalues and Inner-Product Norms

Hermitian matrices always have real eigenvalues and are always diagonalizable. Moreover, the eigenvectors r1 ; : : : ; rm can be chosen to be mutually orthogonal, and normalized
to have rjH rj D 1, so that the eigenvector matrix R is a unitary matrix, RH R D I , and
hence R 1 D RH . (If R is real and Hermitian, then R 1 D RT and R is called an
orthogonal matrix.)
If R D RT and the eigenvalues of A are all positive, then A is said to be symmetric
positive definite (SPD), or Hermitian positive definite in the complex case, or often simply
“positive definite.” In this case
uH Au > 0
(C.15)
for any vector u ¤ 0.
This concept is generalized to the following: A is
positive definite
positive semidefinite
negative definite
negative semidefinite
indefinite

()
()
()
()
()

uH Au > 0 for all u ¤ 0()
uH Au  0 for all u ¤ 0()
uH Au < 0 for all u ¤ 0()
uH Au  0 for all u ¤ 0()
uH Au indefinite
()

p > 0 for all p,
p  0 for all p,
p < 0 for all p,
p  0 for all p,
 p < 0 < q
for some p; q.

The proofs follow directly from the observation that
uH Au D uH RƒRH u D w H ƒw D

m
X

i jwi j2 ;

iD1

where w D RH u.

C.5 Skew-symmetric and skew-Hermitian matrices
If A D AT , then A is said to be skew-symmetric (or skew-Hermitian in the complex case
if A D AH ). Matrices of this form also arise in discretizing certain types of differential equations (e.g., the advection equation as discussed in Chapter 10). Skew-Hermitian
matrices are diagonalizable and have eigenvalues that are pure imaginary. This is a generalization of the fact that for a scalar , if N D , then  is pure imaginary. As in
the Hermitian case, the eigenvectors of a skew-Hermitian matrix can be chosen so that the
matrix R is unitary, RH R D I .

C.6 Normal matrices
If A commutes with its adjoint, AAH D AH A, then A is said to be a normal matrix.
In particular, Hermitian and skew-Hermitian matrices are normal. Any normal matrix is
diagonalizable and R can be chosen to be unitary. Conversely, if A can be decomposed as
A D RƒRH
with RH D R 1 and ƒ diagonal, then A is normal since ƒƒH D ƒH ƒ for any diagonal
matrix.

i

i

i

i

i

i

i

C.7. Toeplitz and circulant matrices

“rjlfdm”
2007/6/1
page 275
i

275

Eigenvalue analysis is particularly useful for normal matrices, since they can be diagonalized by a unitary matrix. A unitary matrix R satisfies kRk2 D kR 1 k2 D 1, and
hence the behavior of powers of A is very closely related to the powers of the eigenvalues,
for example. Nonnormal matrices can be harder to analyze, and in this case studying only
the eigenvalues of A can be misleading. See Section D.4 for more discussion of this.

C.7 Toeplitz and circulant matrices
A matrix is said to be Toeplitz if the value along each diagonal is constant, e.g.,
2
3
d0
d1
d2 d3
6 d 1 d0
d1 d2 7
7
AD6
4 d 2 d 1 d0 d1 5
d 3 d 2 d 1 d0
is a 4  4 example. Here we use di to denote the constant element along the ith diagonal.
If d1 D d 3 , d2 D d 2 , and d3 D d 1 in the above example, or more generally if
di D di m for i D 1; 2; : : : ; m 1 in the m  m case, then the matrix is said to be
circulant.
Toeplitz matrices naturally arise in the study of finite difference methods (see, e.g.,
Section 2.4) and it is useful to have closed-form expressions for their eigenvalues and eigenvectors. This is often possible because of their simple structure.
First consider a “tridiagonal” circulant matrix (which also has nonzero corner terms)
of the form
2
3
d0
d1
d 1
6 d 1 d0 d1
7
6
7
6
7
d 1 d0
6
7
AD6
(C.16)
7 2 R.mC1/.mC1/ :
:
::
6
7
6
7
4
d1 5
d1
d 1 d0
Alternatively we could use the symbol dm in place of d 1 . We take the dimension to be mC
1 to be consistent with notation used in Chapter 2, since such matrices arise in studying 3point difference equations on the unit interval with periodic boundary conditions. Then h D
1=.m C 1/ is the mesh spacing between grid points and the unknowns are U1 ; : : : ; UmC1 .
The pth eigenvalue of the matrix (C.16) is given by
p D d 1 e 2 iph C d0 C d1 e 2 iph ;

(C.17)

p
where i D

1, and the j th element of the corresponding eigenvector rp is given by
rjp D e 2 ipj h :

(C.18)

This is the .j ; p/ element of the matrix R that diagonalizes A. Once the form of the
eigenvector has been “guessed,” it is easy to compute the corresponding eigenvalue p by
computing the j th component of Arp and using the fact that
e 2 ip.j ˙1/h D e ˙2 iph e 2 ipj h

i

i

(C.19)

i

i

i

i

i

276

“rjlfdm”
2007/6/1
page 276
i

Appendix C. Eigenvalues and Inner-Product Norms

to obtain
.Arp /j D .d 1 e 2 iph C d0 C d1 e 2 iph /rjp :
The circulant structure is needed to verify that this formula also holds for j D 1 and
j D m C 1, using e 2 i.mC1/h D 1.
The same vectors rp with components (C.18) are the eigenvectors of any .m C 1/ 
.m C 1/ circulant matrix with diagonals d0 ; d1 ; : : : ; dm. It can be verified, as in the
computation above, that the corresponding eigenvalue is
m
X

dk e 2 ipkh :

p D

(C.20)

kD0

In the “tridiagonal” example above we used the label d 1 instead of dm, but note that
e 2 ih D e 2 imh , so the expression (C.20) is invariant under this change of notation.
Any constant coefficient difference equation with periodic boundary conditions gives
rise to a circulant matrix of this form and has eigenvectors with components (C.18). Note
that the j th component of rp can be rewritten as
rjp D e 2 ipxj D p .xj /;
where xj D j h is the j th grid point and p .x/ D e 2 ipx . The function p .x/ is the pth
eigenfunction of the differentiation operator @x on the unit interval with periodic boundary
conditions,
@x p .x/ D .2ip/p .x/:
It is also the eigenfunction of any higher order derivative @sx , with eigenvalue .2ip/s .
This is the basis of Fourier analysis of linear differential equations, and the fact that difference equations have eigenvectors that are discretized versions of p .x/ means that discrete
Fourier analysis can be used to analyze finite difference methods for constant coefficient
problems, as is done in von Neumann analysis; see Sections 9.6 and 10.5.
Now consider the symmetric tridiagonal Toeplitz matrix (now truly tridiagonal)
2
3
d0 d1
6 d1 d0 d1
7
6
7
6
7
d1 d0 d1
6
7
AD6
(C.21)
7 2 Rmm:
::
6
7
:
6
7
4
d1 5
d1 d0
Such matrices arise in 3-point discretizations of uxx with Dirichlet boundary conditions,
for example; see Section 2.4. The eigenvalues of A are now
p D d0 C 2d1 cos.ph/;

p D 1; 2; : : : ; m;

(C.22)

where again h D 1=.m C 1/ and now A has dimension m since boundary values are not
included in the solution vector. The eigenvector now has components
rjp D sin.pj h/;

i

i

j D 1; 2; : : : ; m:

(C.23)

i

i

i

i

i

C.8. The Gershgorin theorem

“rjlfdm”
2007/6/1
page 277
i

277

Again it is easy to verify that (C.22) gives the eigenvalue once the form of the eigenvector
is known. In this case we use the fact that, for any p,
sin.pj h/ D 0 for j D 0 and j D m C 1
to verify that .Arp /j D p rjp for j D 0 and j D m C 1 as well as in the interior.
Now consider a nonsymmetric tridiagonal Toeplitz matrix,
2
3
d0
d1
6 d 1 d0 d1
7
6
7
6
7
d 1 d0 d1
6
7
AD6
(C.24)
7 2 Rmm :
::
6
7
:
6
7
4
d1 5
d 1 d0
If d1 D d 1 D 0, then the matrix is diagonal with all eigenvalues equal to d0 . Otherwise,
if one of d1 or d 1 is zero the eigenvalues are all equal to d0 but the matrix is a single
Jordan block, a defective matrix with a one-dimensional eigenspace.
In the general case where both d1 and d 1 are nonzero, the eigenvalues are
p
p D d0 C 2d1 d 1 =d1 cos.ph/;

p D 1; 2; : : : ; m;

(C.25)

and the corresponding eigenvector rp has j th component
rjp D

j
p
d 1 =d1 sin.pj h/;

j D 1; 2; : : : ; m:

(C.26)

These formulas hold also if d 1 =d1 is negative, in which case the eigenvalues are complex.
For example, the skew-symmetric centered difference matrix with d 1 D 1, d0 D 0, and
d1 D 1 has eigenvalues
p D 2i cos.ph/:
(C.27)

C.8 The Gershgorin theorem
If A is diagonal, then its eigenvalues are simply the diagonal elements. If A is “nearly
diagonal,” in the sense that the off-diagonal elements are small compared to the diagonal,
then we might expect the diagonal elements to be good approximations to the eigenvalues.
The Gerschgorin theorem quantifies this and also provides bounds on the eigenvalues in
terms of the diagonal and off-diagonal elements. These bounds are valid in general and
often very useful even when A is far from diagonal.
Theorem C.2. Let A 2 Cmm
P and let Di be the closed disk in the complex plane centered at ai i with radius ri D j ¤i jaij j, the sum of the magnitude of all the off-diagonal
elements in the ith row of A,
Di D fz 2 C W jz

i

i

ai i j  ri g:

i

i

i

i

i

278

“rjlfdm”
2007/6/1
page 278
i

Appendix C. Eigenvalues and Inner-Product Norms

Then,
1. all the eigenvalues of A lie in the union of the disks Di for i D 1; 2; : : : ; m.
2. if some set of k overlapping disks is disjoint from all the other disks, then exactly k
eigenvalues lie in the union of these k disks.
Note the following:
 If a disk Di is disjoint from all other disks, then it contains exactly one eigenvalue of
A.
 If a disk Di overlaps other disks, then it need not contain any eigenvalues (although
the union of the overlapping disks contains the appropriate number).
 If A is real, then AT has the same eigenvalues as A. Then the theorem can also be
applied to AT (or equivalently the disk radii can be defined by summing elements of
columns rather than rows).
For a proof of this theorem see Wilkinson [103], for example.
Example C.5. Let
2
3
5 0:6 0:1
0:1 5 :
AD4 1 6
1
0
2
Applying the Gershgorin theorem to A, we have
D1 D fz W jz

5j  0:7g;

D2 D fz W jz

6j  1:1g;

D3 D fz W jz

2j  1:0g;

as shown in Figure C.1(a). From the theorem we can conclude that there is exactly one
eigenvalue in D3 and two eigenvalues in D1 [ D2 . We can also conclude that all eigenvalues have real parts between 1 and 7.7 (and hence positive real parts, in particular). The
eigenvalue in D3 must be real, since complex eigenvalues must appear in conjugate pairs
(since A is real). The eigenvalues in D1 [ D2 could be real or imaginary, but the imaginary
part must be bounded by 1.1. The actual eigenvalues of A are also shown in Figure C.1(a),
and are
  1:9639; 5:518 ˙ 0:6142i:
Applying the theorem to AT would give
D1 D fz W jz

5j  2:0g;

D2 D fz W jz

6j  0:6g;

D3 D fz W jz

2j  0:2g;

as shown in Figure C.1(b). Note that this gives a tighter bound on the eigenvalue near 2 but
a larger region around the complex pair.
A matrix is said to be reducible if it is possible to reorder the rows and columns in
such a way that the eigenvalue problem is decoupled into simpler problem, specifically if
there exists a permutation matrix P so that


0
A11
PAP 1 D
;
A12 A22

i

i

i

i

i

i

i

C.9. Inner-product norms

279

4

4

3

3

2

2

1

1

0

0

−1

−1

−2

−2

−3

−3

−4

(a) −1

0

1

2

3

4

“rjlfdm”
2007/6/1
page 279
i

5

6

7

8

(b) −4−1

0

1

2

3

4

5

6

7

8

Figure C.1. Gerschgorin circles containing the eigenvalues of A for Example C.5.
where A11 and A22 are square matrices of size at least 1  1. In this case the eigenvalues
of A consist of the eigenvalues of A11 together with those of A22 . If no such P exists, then
A is irreducible. The matrix of Example C.5 is irreducible, for example.
For irreducible matrices, a more refined version of the Gerschgorin theorem states
also that a point on the boundary of a set of Gerschgorin disks can be an eigenvalue only if
it is on the boundary of all disks.
Example C.6. The tridiagonal matrix A of (2.10) arises from discretizing the second
derivative. Since this matrix is symmetric all its eigenvalues are real. By the Gerschgorin
theorem they must lie in the circle of radius 2= h2 centered at 2= h2. In fact, they must
lie in the interior of this disk since the matrix is irreducible and the first and last row of A
give disks with radius 1= h2. Hence 4= h2 < p < 0 for all eigenvalues p . In particular
this shows that all the eigenvalues are negative and hence the matrix A is nonsingular and
negative definite. Showing nonsingularity is one use of the Gerschgorin theorem.
For the tridiagonal matrix (2.10) the eigenvalues can be explicitly computed and are
given by the formula (2.23),
p D

2
.cos.ph/
h2

1/ for p D 1; 2; : : : ; m;

where h D 1=.m C 1/. They are distributed all along the interval 4= h2 < p < 0.
For related matrices that arise from discretizing variable coefficient elliptic equations the
matrices cannot be explicitly computed, but the Gerschgorin theorem can still be used to
show nonsingularity.
Example C.7. Consider the matrix (2.73) with all  > 0. The Gerschgorin disks all
lie in the left half-plane and the disks D1 and Dm are bounded away from the origin. The
matrix is irreducible and hence must be negative definite (and in particular nonsingular).

C.9 Inner-product norms
Some standard vector norms and the corresponding matrix norms were introduced in Section A.3. Here we further investigate the 2-norm and its relation to the spectral radius of

i

i

i

i

i

i

i

280

“rjlfdm”
2007/6/1
page 280
i

Appendix C. Eigenvalues and Inner-Product Norms

a matrix. We will also see how new inner-product norms can be defined that are closely
related to a particular matrix.
Let A 2 Cmm and u 2 Cm . The 2-norm of of u is defined by
kuk22 D uH u D

m
X

jui j2 D hu; ui;

(C.28)

iD1

where h; i is the standard inner product,
hu; vi D uH v D

m
X

uN i vi :

(C.29)

iD1

The 2-norm of the matrix A is defined by the formula (A.9) as

1=2
kAk2 D sup kAuk2 D sup uH AH Au
:
kuk2 D1

kuk2 D1

Note that if we choose u to be an eigenvector of A, with Au D u, then
.uH AH Au/1=2 D jj;
and so kAk2  max1pm jp j D .A/. The 2-norm of A is always at least as large as the
spectral radius. Note that the matrix B D AH A is always Hermitian (B H D B) and so it
is diagonalizable with a unitary eigenvector matrix,
B D RMRH

.RH D R 1 /;

where M is the diagonal matrix of eigenvectors j  0 of B. Any vector u can be written
as u D Rw where w D RH u. Note that kuk2 D kwk2 since
uH u D w H RH Rw D w H w;
i.e., multiplication by a unitary matrix preserves the 2-norm. It follows that
kAk2 D sup .uH Bu/1=2
kuk2 D1

D sup .w H RH BRw/1=2
kwk2 D1

D sup .w H M w/1=2
kwk2 D1

D

max

jp j

1=2

(C.30)

q
D .AH A/:

pD1;2;:::;m

If AH D A, then .AH A/ D ..A// 2 and kAk2 D .A/. More generally this is true for
any normal matrix A (as defined in Section C.6). If A is normal, then A and AH have the
same eigenvector matrix R and so
AH A D .RƒH R/.RƒRH / D RƒH ƒRH :
It follows that .AH A/ D maxpD1;2;:::;m jp j2 .

i

i

i

i

i

i

i

C.10. Other inner-product norms

“rjlfdm”
2007/6/1
page 281
i

281

If A is not normal, then typically kAk2 > .A/. If A is diagonalizable, then an upper
bound on kAk2 can be obtained from
kAk2 D kRƒR 1 k2
 kRk2 kR 1 k2

jp j D 2 .R/.A/;

max

(C.31)

pD1;2;:::;m

where 2 .R/ D kRk2 kR 1 k2 is the 2-norm condition number of the eigenvector matrix
R. We thus have the general relation
.A/  kAk2  2 .R/.A/;

(C.32)

which holds for any diagonalizable matrix A. If A is normal, then R is unitary and
2 .R/ D 1.
This relation between the norm and spectral radius is important in studying iterations
of the form U nC1 D AU n , which leads to U n D An U 0 (where the superscript on U is
an index and the superscript on A is a power). Such iterations arise both in time-stepping
algorithms for solving differential equations and in iterative methods for solving linear
systems. We often wish to investigate the behavior of kU n k as n ! 1, or the related
question of the behavior of powers of the matrix A. For diagonalizable A we have An D
Rƒn R 1 , so that
kAn k2  2 .R/..A//n :
(C.33)
From this we see that kAn k2 ! 0 as n ! 1 if .A/ < 1. In fact this is true for any A,
not just diagonalizable matrices, as can be seen by using the Jordan canonical form. See
Appendix D for more about bounding powers of a matrix.
This spectral analysis is particularly useful when A is normal, in which case 2 .R/ D
1. In this case kAn k2  ..A//n and if .A/ < 1, then we have a strictly decreasing upper
bound on the norm. The asymptotic behavior is still the same if A is not normal, but
convergence is not necessarily monotone and this spectral analysis can be quite misleading
if A is far from normal. This topic is discussed in more detail in Appendix D along with a
discussion of the nondiagonalizable (defective) case.

C.10

Other inner-product norms

If T is any nonsingular matrix, then we can define an inner product based on T in terms of
the standard inner product (C.29) by
hu; viT D hT

1

u; T

1

vi D uH Gv;

(C.34)

where G D T H T 1 . The matrix G is always Hermitian positive definite (SPD if T is
real). We can define a corresponding norm (the T -norm of u) by
kukT D hu; uiT D kT

1

uk2 D .uH Gu/1=2 D hu; Gui:

(C.35)

This satisfies the requirements of a norm summarized in Section A.3.
Inner-product norms of this type naturally arise in the study of conjugate gradient
methods for solving linear system Au D f when A is SPD. In this case G D A is used

i

i

i

i

i

i

i

282

“rjlfdm”
2007/6/1
page 282
i

Appendix C. Eigenvalues and Inner-Product Norms

(see Section 4.3.4) and T could be defined as a “square root” of A, e.g., T D Rƒ1=2 R 1
if A D RƒR 1 .
In studying iterations of the form U nC1 D AU n , and variants such as U nC1 D
n
An U (where the matrix An changes in each iteration), it is often useful to choose norms
that are adapted to the matrix or matrices in question in order to obtain more insight into the
asymptotic behavior of U n . A few results are summarized below that are used elsewhere.
Note that w D T 1 u can be viewed as the vector of coefficients obtained if u is
written as a linear combination of the columns of T , u D T w. Hence kukT D kwk2 can
be viewed as a measure of u based on its representation in the coordinate system defined
by T rather than in the standard basis vectors. A particularly useful coordinate system is
the coordinates defined by the eigenvectors, as we will see below.
We can compute the matrix T -norm of a matrix A using the standard definition of a
matrix norm from (A.9):
kAukT
kAT wkT
D sup
u¤0 kukT
w¤0 kT wkT

kAkT D sup

kT
D sup
w¤0

D kT

1

1

AT wk2
kwk2

(C.36)

AT k2:

Now suppose A is a diagonalizable matrix with R 1 AR D ƒ. Then choosing T D
R yields kAkR D kR 1 ARk2 D .A/. Recall that kAk  .A/ in any matrix norm
subordinate to a vector norm. We have just shown that equality can be achieved by an
appropriate choice of norm in the case when A is diagonalizable. We have proved the
following theorem.
Theorem C.3. Suppose A 2 Cmm is diagonalizable. Then there exists a norm k  k in
which kAk D .A/. The norm is given by the R-norm based on the eigenvector matrix.
This theorem will be generalized to the defective case in Theorem C.4 below.
Note that in general the T -norm, for any nonsingular T , is “equivalent” to the 2-norm
in the sense of Section A.3.1 with the equivalence inequalities
kT k2 1 kuk2  kukT  kT

1

k2 kuk2

(C.37)

2 .T / 1 kAk2  kAkT  2 .T /kAk2

(C.38)

for the vector norm and

for the matrix norm, where 2 .T / is the 2-norm condition number of T . Applying this last
inequality in conjunction with Theorem C.3 gives
.A/ D kAkR  2 .R/kAk2 ;

(C.39)

which agrees with the bound (C.31) obtained earlier.
For general matrices A 2 Cmm that are not necessarily diagonalizable, Theorem C.3 can be generalized to the following.

i

i

i

i

i

i

i

C.10. Other inner-product norms

“rjlfdm”
2007/6/1
page 283
i

283

Theorem C.4. (a) If A 2 Cmm has no defective eigenvalues with modulus .A/, then
there exists a nonsingular matrix T such that
kAkT D .A/:
(b) If A has defective eigenvalue(s) of modulus .A/, then for every  > 0 there exists a
matrix T ./ such that
kAkT ./ < .A/ C :
(C.40)
In the latter case we can find a norm in which kAk is arbitrarily close to .A/, but
T ./ becomes increasingly ill conditioned as  ! 0. The proof of this theorem is based
on a modification of the Jordan canonical form in which the superdiagonal elements are
made sufficiently small by the transformation discussed in Section C.3. Let R 1 AR D J
have the form (C.12), and let Z D fi W ji j D .A/g, the set of indices of the maximal
eigenvalues. To prove part (a), if i 2 Z, then ki D 1 and Ji D i with ji j D .A/. In
this case set Di D 1. If i … Z, let ıi D .A/ ji j > 0 and set
k

Di D diag.1; ıi ; : : : ; ıi i

1

/:

Let D be the block diagonal matrix formed by these blocks. Then JQ D D 1 JD has Jordan
blocks i I C ıi Ski and so
kJQi k2  ji j C ıi  .A/

for i … Z:

It follows that kAkT D kJQ k2 D .A/, where the matrix A is given by T D RD.
To prove part (b), let  > 0 be given and choose


if i 2 Z;
ıi D
.A/ ji j > 0 if i … Z:
Define Di and D as before and we will achieve
kJQi k  .A/ for i … Z;
kJQi k  .A/ C  for i 2 Z;
and so taking T ./ D RD./ gives kAkT ./  .A/ C . Recall that 2 .D.// ! 1 as
 ! 0 and so 2 .T .// ! 1 as  ! 0 in case (b).

i

i

i

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 284
i

i

i

i

i

“rjlfdm”
2007/6/1
page 285
i

Appendix D

Matrix Powers and
Exponentials

The first order scalar linear differential equation u0 .t/ D au.t/ has the solution u.t/ D
e at u.0/ and so
ju.t/j ! 0 as t ! 1 for any u.0/ () Re.a/ < 0;
ju.t/j remains bounded as t ! 1 for any u.0/ () Re.a/  0;
ju.t/j ! 1 as t ! 1 for any u.0/ ¤ 0 () Re.a/ > 0:

(D.1)

The first order scalar linear difference equation U nC1 D bU n has the solution U n D b n U 0
and so
jU n j ! 0 as n ! 1 for any U 0 () jbj < 1;
jU n j remains bounded as n ! 1 for any U 0 () jbj  1;
n

(D.2)

0

jU j ! 1 as n ! 1 for any U ¤ 0 () jbj > 1:
For these first order scalar equations the behavior is very easy to predict. The purpose of this
appendix is to review the extension of these results to the vector case, for linear differential
equations u0.t/ D Au.t/ and difference equations U nC1 D BU n , where u.t/; U n 2 Rm
and A; B 2 Rmm , or more generally could be complex valued. This analysis relies on
a good understanding of the material in Appendix C on eigendecompositions of matrices,
and of the Jordan canonical form for the more interesting defective case.
In the scalar case there is a close relation between the results stated above in (D.1)
and (D.2) for u0 D au and U nC1 D bU n , respectively. If we introduce a time step t D 1
and let U n D u.n/ D e an u.0/ be the solution to the ordinary differential equation (ODE)
at discrete times, then U nC1 D bU n where b D e a. Since the function e z maps the left
half-plane to the unit circle, we have
jbj < 1 () Re.a/ < 0;
jbj D 1 () Re.a/ D 0;
jbj > 1 () Re.a/ > 0:

(D.3)

Similarly, for the system cases we will see that boundedness of the solutions depends on
eigenvalues of B lying inside the unit circle, or eigenvalues of A lying in the left half-plane,
285

i

i

i

i

i

i

i

286

“rjlfdm”
2007/6/1
page 286
i

Appendix D. Matrix Powers and Exponentials

although the possibility of multiple eigenvalues makes the analysis somewhat more complicated. We will also see in Section D.4 that when the matrix involved is nonnormal the
eigenvalues do not tell the full story—rapid transient growth in powers or the exponential
can be observed even if there is eventual decay.
In the rest of this chapter we will use the symbol A as our general symbol in discussing both matrix powers and exponentials, but the reader should keep in mind that results on powers typically involve the unit circle, while results on exponentials concern the
left half-plane.

D.1 The resolvent
Several of the bounds discussed below involve the resolvent of a matrix A, which is the
complex matrix-valued function .zI A/ 1 . The domain of this function is the complex
plane minus the eigenvalues of A, since the matrix zI A is noninvertible at these points.
Bounds on the growth of An or e At can be derived based on the size of k.zI A/ 1 k
over suitably chosen regions of the complex plane. These are generally obtained using the
following representation of a function f .A/, the natural extension of the Cauchy integral
formula to matrices.
Definition D.1. If  is any closed contour in the complex plane that encloses the eigenvalues of A and if f .z/ is an analytic function within , then the matrix f .A/ can be defined
by
Z
1
f .A/ D
f .z/.zI A/ 1 dz:
(D.4)
2i 
The value is independent of the choice of .
For some functions f .z/, such as e z , it may not be clear what f .A/ means for a
matrix, and this can be used as the definition of f .A/. (Some other definitions of e A are
given in Section D.3, which are equivalent.) The representation (D.4) can also be useful
computationally and is one approach to computing the matrix exponential; see, e.g., [53]
and Section 11.6.1.
For the function f .z/ D z n (or any polynomial in z) the meaning of f .A/ is clear
since we know how to compute powers of a matrix, but (D.4) gives a different representation of the function that is sometimes useful in obtaining upper bounds.

D.2 Powers of matrices
Consider the linear difference equation
U nC1 D AU n

(D.5)

for some iteration matrix A. The study of the asymptotic behavior of kU n k is important
both in studying stability of finite difference methods and in studying convergence of iterative method for solving linear systems.
From (D.5) we obtain
kU nC1 k  kAk kU nk

i

i

i

i

i

i

i

D.2. Powers of matrices

“rjlfdm”
2007/6/1
page 287
i

287

in any norm and hence
kU n k  kAkn kU 0k:

(D.6)

Alternatively, we can start with U nC1 D An U 0 to obtain
kU n k  kAn k kU 0k:

(D.7)

Since kAn k  kAkn this again leads to (D.6), but in many cases kAn k is much smaller
than kAkn and the form (D.7) may be more powerful.
If there exists any norm in which kAk < 1, then (D.6) shows that kU n k ! 0 as
n ! 1. This is true not only in this particular norm but also in any other norm. Recall
we are only considering matrix norms that are subordinate to some vector norm and that in
a finite dimensional space of fixed dimension m all such norms are equivalent in the sense
of (A.6). From these inequalities it follows that if kU n k ! 0 in any norm, then it goes to
zero in every equivalent norm, and similarly if kU n k blows up in some norm, then it blows
up in every equivalent norm. Moreover, if kAk D 1 in some norm, then kU n k remains
uniformly bounded as n ! 1 in any equivalent norm.
From Theorem C.4 we thus obtain directly the following results.
Theorem D.1. (a) Suppose .A/ < 1. Then kU n k ! 0 as n ! 1 in any vector norm. (b)
Suppose .A/ D 1 and A has no defective eigenvalues of modulus 1. Then kU n k remains
bounded as n ! 1 in any vector norm, and there exists a norm in which kU n k  kU 0 k.
Note that result (a) holds even if A has defective eigenvalues of modulus .A/ by
choosing  < 1 .A/ in Theorem C.4(b).
If A is diagonalizable, then the results of Theorem D.1 can also be obtained directly
from the eigendecomposition of A. Write A D RƒR 1 , where
ƒ D diag.1 ; 2 ; : : : ; m /
is the diagonal matrix of eigenvalues of A and R is the matrix of right eigenvectors of A.
Then the nth power of A is given by An D Rƒn R 1 and
kU n k  kAn k kU 0k
 .R/.A/n kU 0k;

(D.8)

where .R/ D kRk kR 1 k is the condition number of R in whatever norm we are using. Note that if the value of .R/ is large, then kU n k could grow to large values before
decaying, even in .A/ < 1 (see Example D.2).
If A is a normal matrix and we use the 2-norm, then 2 .R/ D 1 and we have the nice
result that
kU n k2  .A/n kU 0 k2
if A is normal:
(D.9)
For normal matrices, or for those close to normal, .A/ gives a good indication of the
behavior of kU n k2 . For matrices that are far from being normal, the spectral radius may
give a poor indication of how kU n k behaves. The nonnormal case is considered further in
Section D.4.

i

i

i

i

i

i

i

288

“rjlfdm”
2007/6/1
page 288
i

Appendix D. Matrix Powers and Exponentials

More detailed information about the behavior of kU n k can be obtained by decomposing U 0 into eigencomponents. Still assuming A is diagonalizable, we can write
U 0 D W10 r1 C W20 r2 C    C Wm0 rm D RW 0 ;
where r1 ; : : : ; rm are the eigenvectors of A and the vector W 0 is given by W 0 D R 1 U 0 .
Multiplying U 0 by A multiplies each rp by p and so
U n D An U 0 D W10 n1 r1 C W20 n2 r2 C    C Wm0 nm rm D Rƒn W 0 :

(D.10)

For large n this is dominated by the terms corresponding to the largest eigenvalues, and
hence the norm of this vector is proportional to .A/n , at least for generic initial data. This
also shows that if .A/ < 1, then U n ! 0, while if .A/ > 1 we expect U n to blow up.
Note that for certain initial data kU n k may behave differently than .A/n , at least in
exact arithmetic. For example, if U 0 is void in the dominant eigencomponents, then these
terms will be missing from (D.10), and the asymptotic growth or decay rate will be different. In particular, it could happen that .A/ > 1 and yet kU n k ! 0 for special data U 0
if some eigenvalues of A have modulus less than 1 and U 0 contains only these eigencomponents. However, this generally is not relevant for the stability and convergence issues
considered in this book, where arbitrary initial data must be considered. Moreover, even if
U 0 is void of some eigencomponents, rounding errors introduced computationally in each
iteration U nC1 D AU n will typically be random and will contain all eigencomponents.
The growing modes may start out at the level of rounding error, but if .A/ > 1 they will
grow exponentially and eventually dominate the solution so that the asymptotic behavior
will still be governed by .A/n .
If A is defective, then we cannot express an arbitrary initial vector U 0 as a linear
combination of eigenvectors. However, using the Jordan canonical form A D RJR 1 ,
we can still write U 0 D RW 0 , where W 0 D R 1 U 0 . The nonsingular matrix R now
contains principal vectors as well as eigenvectors, as discussed in Section C.3.
Example D.1. Consider the iteration U nC1 D AU n , where A is the 33 matrix from
Example C.4, having a single eigenvalue  with geometric multiplicity 1. If we decompose
U 0 D W10 r1 C W20 r2 C W30 r3 ;
then multiplying by A gives
U 1 D W10 r1 C W20 .r1 C r2/ C W30 .r2 C r3 /
D .W10  C W20 /r1 C .W20  C W30 /r2 C W30 r3:

(D.11)

Repeating this shows that U n has the form
U n D An U 0 D p1 ./r1 C p2 ./r2 C W30 n r3 ;
where p1 ./ and p2 ./ are polynomials in  of degree n. It can be shown that
jp1 ./j  n2 n kW 0 k2 ;

jp2 ./j  nn kW 0 k2 ;

so that there are now algebraic terms (powers of n) in the asymptotic behavior in addition
to the exponential terms (n). More generally, as we will see below, a Jordan block of order
k gives rise to terms of the form nk 1 n .

i

i

i

i

i

i

i

D.2. Powers of matrices

“rjlfdm”
2007/6/1
page 289
i

289

If jj > 1, then the power nk 1 is swamped by the exponential growth and the
algebraic term is unimportant. If jj < 1, then nk 1 grows algebraically but n decays
exponentially and the product decays, nk 1 n ! 0 as n ! 1 for any k > 1.
The borderline case jj D 1 is where this algebraic term makes a difference. In
this case n remains bounded but nk 1 n does not. Note how this relates to the results of
Theorem D.1. If .A/ < 1, then kAn k ! 0 even if A has defective eigenvalues of modulus
.A/, since the exponential decay overpowers the algebraic growth. However, if .A/ D
1, then the boundedness of kAn k depends on whether there are defective eigenvalues of
modulus 1. If so, then kAn k grows algebraically (but not exponentially).
Recall also from Theorem C.4 that in this latter case we can find, for any  > 0, a
norm in which kAk < 1 C . This implies that kAn k < .1 C /n . There may be growth,
but we can make the exponential growth rate arbitrarily slow. This is consistent with the
fact that in this case we have only algebraic growth. Exponential growth at rate .1 C /n
eventually dominates algebraic growth nk 1 no matter how small  is, for any k.
To determine the algebraic growth factors for the general case of a defective matrix,
we can use the fact that if A D RJR 1 then An D RJ n R 1 , and we can compute the nth
power of the Jordan matrix J . Recall that J is a block diagonal matrix with Jordan blocks
on the diagonal, and powers of J simply consist of powers of these blocks. For a single
Jordan block (C.9) of order k,
J.; k/ D Ik C Sk ;
where Sk is the shift matrix (C.10). Powers of J.; k/ can be found using the binomial
expansion and the fact that Ik and Sk commute,
J.; k/n D .Ik C Sk /n





n
n
n Ik C nn 1 Sk C
n 2 Sk2 C
n 3 Sk3
2
3


n
CC
Skn 1 C Skn :
n 1

(D.12)

j

Note that for j < k, Sk is the matrix with 1’s along the j th superdiagonal and zeros
j
everywhere else. For j  k, Sk is the zero matrix. So the series in expression (D.12)
always terminates after at most k terms, and when n > k reduces to

n

n

n 1


n
2


n 2

Sk2 C

J.; k/ D  Ik C n Sk C



n
CC
n kC1 Skk 1 :
k 1


n
3

n 3 Sk3
(D.13)

n 
Since j D O.nj / as n ! 1, we see that J.; k/n D P .n/n , where P .n/ is a
matrix-valued polynomial of degree k 1.

i

i

i

i

i

i

i

290

“rjlfdm”
2007/6/1
page 290
i

Appendix D. Matrix Powers and Exponentials
For example, returning to Example C.4, where k D 3, we have
J.; 3/n D n I3 C nn 1S3 C
2

n


D4 0
0

n 1

n
n
0

n.n
2

1/ n 2 2
 S3

n.n 1/ n 2

2
n 1

n
n

3

(D.14)

5:

This shows that kJ n k  n2 n , the same result obtained in Example D.1,
If A is not normal, i.e., if AH A ¤ AAH , then kAk2 > .A/ and .A/n may not
give a very good indication of the behavior of kAn k. If A is diagonalizable, then we have
kAn k2  kRk2 kƒn k2kR 1 k2  2 .R/.A/n ;

(D.15)

but if 2 .R/ is large, then this may not be useful, particularly for smaller n. This does give
information about the asymptotic behavior as n ! 1, but in practice it may tell us little or
nothing about how kAn k2 is behaving for the finite values of n we care about in a particular
computation. See Section D.4 for more about the nonnormal case.

D.2.1 Solving linear difference equations
Matrix powers arise naturally when iterating with the first order linear difference equation
(D.5). In studying linear multistep methods we need the general solution to an r th order
linear difference equation of the form
a0 V n C a1 V nC1 C a2 V nC2 C    C V nCr D 0 for n  0:

(D.16)

We have normalized the equation by assuming ar D 1. One approach to solving this is
given in Section 6.4.1, and here we give an alternative based on converting this r th order
equation into a first order system of r equations of the form (D.5) and then applying the
results just found for matrix powers.
We can rewrite (D.16) as a system of equations by introducing
U1n D V n ;

U2n D V nC1 ;

:::;

Urn D V nCr 1 :

(D.17)

Then (D.16) takes the form
U nC1 D C U n ;
where

(D.18)

2

3
0

6
6
6
6
C D6
6
6
4
a0

1
0

a1

1
0

a2

1
::
:

::

:
0



1
ar 1

7
7
7
7
7:
7
7
5

(D.19)

This matrix is called the companion matrix for the difference equation. The general solution
to (D.18) is U n D C n U 0 , where
U 0 D ŒV 0 ; V 1 ; : : : ; V r 1 T

i

i

i

i

i

i

i

D.2. Powers of matrices

“rjlfdm”
2007/6/1
page 291
i

291

is the vector consisting of the r initial values required by (D.16). The solution can be
expressed in terms of the eigenvalues of C .
It can be shown that the characteristic polynomial of C is
p./ D det.I

C / D a0 C a1  C    C ar 1r 1 C r :

(D.20)

Call the roots of this polynomial 1 ; : : : ; r , as in Section 6.4.1. Let C D RJR 1 be the
Jordan canonical form of the matrix C . Then U n D RJ n R 1 U 0 and each element of U n
(and in particular V n D U1n ) is a linear combination of elements of J n .
If the roots are distinct, then C is diagonalizable, and so the general solution of (D.16)
is
V n D c1 1n C c22n C    C cr rn ;
(D.21)
where the coefficients cj depend on the initial data.
It can be shown that repeated eigenvalues of a companion matrix always have geometric multiplicity 1, regardless of their algebraic multiplicity. An eigenvalue j has a
one-dimensional space of eigenvectors spanned by Œ1; j ; j2 ; : : : ; jr 1 T . From the
form (D.13) for powers of a Jordan block, we see that if j is a repeated root of algebraic
multiplicity k, then the general solution of the difference equation (D.16) includes terms
of the form jn ; njn ; : : : ; nk 1 jn . We can conclude that the general solution to the r th
order difference equation (D.16) will be bounded as n ! 1 only if the roots of the characteristic polynomial all lie inside the unit circle, with no repeated roots of magnitude 1.
This is known as the root condition and is used in the stability analysis of linear multistep
methods; see Definition 6.2.

D.2.2 Resolvent estimates
In our analysis of powers of A we have used the eigenstructure of A. An alternative
approach is to use the resolvent .zI
A/ 1 and the expression (D.4) for the function
f .z/ D z n . For example, we can obtain an alternative proof of part (a) of Theorem D.1 as
follows. If .A/ < 1, then we choose as our contour  a circle of radius 1 , where  > 0
is chosen small enough that .A/ < 1 . The eigenvalues of A then lie inside  and
zI A is invertible for z on  and so k.zI A/ 1 k is a bounded periodic function of z and
hence attains some maximum value C.A; / on . Now consider (D.4) with f .z/ D z n ,
Z
1
n
A D
z n .zI A/ 1 dz:
(D.22)
2i 
Taking norms and using the fact that jz n j D .1 /n and k.zI A/ 1 k  C.A; / for z
on , and that  has length 2.1 / < 2, we obtain
Z
1
kAn k 
jz n j k.zI A/ 1 k dz
2 
(D.23)
n
< C.A; /.1 / :
Since .1 /n ! 0 as n ! 1 this proves Theorem D.1(a).
Note that we also get a uniform bound on kAn k that holds for all n,
kAn k < C.A; /:

i

i

i

i

i

i

i

292

“rjlfdm”
2007/6/1
page 292
i

Appendix D. Matrix Powers and Exponentials

If A is normal then in the 2-norm we have kAn k2  1 from (D.9), but for nonnormal matrices there can be transient growth in kAn k before it eventually decays, and the resolvent
gives one approach to bounding this potential growth
Note that the value C.A; / depends on the matrix A and may be very large if .A/
is close to 1. The contour  must lie between the spectrum of A and the unit circle for
the argument above, and k.zI A/ 1 k is large near the spectrum of A. In the study of
numerical methods we are often concerned not just with a single matrix A but with a family
of matrices arising from different discretizations, and proving stability results often requires
proving uniform power boundedness of the family, i.e., that there is a single constant C
such that kAn k  C for all matrices in the family. This is more difficult than proving that a
single matrix is power bounded, and it is the subject of the Kreiss matrix theorem discussed
in Section D.6.
Note that this resolvent proof does not extend directly to prove part (b) of Theorem D.1, the case where A has nondefective eigenvalues on the unit circle. In this case the
contour  must lie outside the unit circle, at least near these eigenvalues, for (D.22) to hold.
In this case it is necessary to investigate how the product
.jzj

1/k.zI

A/ 1 k

(D.24)

behaves for jzj > 1. Here the resolvent norm is multiplied by a factor that vanishes as
jzj ! 1 and so there is hope that the product will be bounded even if the resolvent is
blowing up.
In fact it can be shown that if .A/ < 1 or if .A/ D 1 with only nondefective eigenvalues on the unit circle (i.e., if either of the conditions of Theorem D.1 holds), then the
product (D.24) will be uniformly bounded for all z outside the unit circle. The supremum
is called the Kreiss constant for the matrix A, denoted by K.A/,
K.A/ D sup .jzj

1/k.zI

A/ 1 k:

(D.25)

jzj>1

An alternative definition based on the -pseudospectral radius is discussed in Section D.5.
For an idea of how this can be used, again consider the first line of (D.23) but now
take  to be a circle of radius 1 C  with  > 0. We have k.zI A/ 1 k  K.A/= on this
circle, and so
Z
K.A/
1
n
kA k 
j1 C jn
dz
2 

(D.26)
1
D .1 C /nC1 K.A/;

since the circle has radius 2.1 C /. This bound holds for any  > 0. It appears to allows
exponential growth for any fixed , but we are free to choose a different value of  for each
n, and taking  D 1=.n C 1/, for example, gives
kAn k  .n C 1/eK.A/

for all n  0:

Unfortunately, this still allows algebraic growth and so does not prove the theorem.

i

i

i

i

i

i

i

D.3. Matrix exponentials

“rjlfdm”
2007/6/1
page 293
i

293

It fact, it can be shown (by a more complicated argument based on the resolvent that
will not be presented here) that a bound of the required form holds,
kAn k  meK.A/

for all n  0;

(D.27)

where m is the dimension of the matrix. This shows that for a fixed matrix A all powers
remain bounded provided its Kreiss constant is finite (which in turn is true if and only if
.A/  1 with no defective eigenvalues on the unit circle).
The bound in (D.27) is sharp in a sense made precise in Section 18 of [92] and was
the end product of a long sequence of weaker bounds proved over the years (as recounted
in [92]). This result was proved by Spijker [81] as a corollary to a more general result on
the arclength of the image of the unit circle under a rational function.
Resolvent estimates can also be used to obtain a lower bound on the transient growth
of kAn k; see (D.46).
The bound (D.27) is a major part of the proof of the Kreiss matrix theorem (see
Section D.6): a family of matrices is uniformly power bounded if (and only if) there is a
uniform bound on the Kreiss constants K.A/ of all matrices in the family.

D.3 Matrix exponentials
Now consider the linear system of m ordinary differential equations u0 .t/ D Au.t/, where
A 2 Rmm (or more generally A 2 Cmm). The nondiagonalizable (defective) case will be
considered in Section D.4. When A is diagonalizable we can solve this system by changing
variables to v D R 1 u and multiplying both sides of the ODE by R 1 to obtain
R 1 u0 .t/ D R 1 AR  R 1 u.t/
or
v 0 .t/ D ƒv.t/:
This is a decoupled set of m scalar equations vj0 .t/ D j vj .t/ (for j D 1; 2; : : : ; m)
with solutions vj .t/ D e j t vj .0/. Let e ƒt denote the matrix
e ƒt D diag.e 1 t ; e 2 t ; : : : ; e m t /:

(D.28)

Then we have v.t/ D e ƒt v.0/ and hence
u.t/ D Rv.t/ D Re ƒt R 1 u.0/;
so
u.t/ D e At u.0/;

(D.29)

e At D Re ƒt R 1 :

(D.30)

where
At

This gives one way to define the matrix exponential e , at least in the diagonalizable case.
The Cauchy integral of Definition D.1 is another. Yet another way to define it is by the
Taylor series
1

e At D I C At C

X 1
1 2 2
1
A t C A3 t 3 C    D
Aj t j :
2!
3!
j!

(D.31)

j D0

i

i

i

i

i

i

i

294

“rjlfdm”
2007/6/1
page 294
i

Appendix D. Matrix Powers and Exponentials

This is often useful, particularly when t is small. The definitions (D.30) and (D.31) agree
in the diagonalizable case since all powers Aj have the same eigenvector matrix R, and so
(D.31) gives


1
1
e At D R I C ƒt C ƒ2 t 2 C ƒ3 t 3 C    R 1
2!
3!
D Re

ƒt

R

1

(D.32)

;

resulting in (D.30). To go from the first to the second line of (D.32), note that it is easy to
verify that the Taylor series applied to the diagonal matrix ƒ is a diagonal matrix of Taylor
series, each of which converge to the corresponding diagonal element of e ƒt , the value
e j t .
Note that the matrix e At has the same eigenvectors as A and its eigenvalues are e j t .
To investigate the behavior of u.t/ D e At u.0/ as t ! 1, we need only look at the real
part of each eigenvalue j . If none of these are greater than 0, then the solution will remain
bounded as t ! 1 (assuming still that A is diagonalizable) since je j t j  1 for all j .
It is useful to introduce the spectral abscissa ˛.A/, defined by
˛.A/ D max Re.j /:

(D.33)

1j m

Then u.t/ remains bounded provided ˛.A/  0 and u.t/ ! 0 as t ! 1 if ˛.A/ < 0.
Note that for integer values of t D n, we have e An D .e A /n and .e A / D e ˛.A/ , so
this result is consistent with what was found in the last section for matrix powers.
If A is not diagonalizable, then the case ˛.A/ D 0 is more subtle, as is the case
.A/ D 1 for matrix powers. If A has a defective eigenvalue  with Re./ D 0, then
the solution may still grow, although with polynomial growth in t rather than exponential
growth.
When A is not diagonalizable, we can still write the solution to u0 D Au as u.t/ D
At
e u.0/, but we must reconsider the definition of e At . In this case the Jordan canonical
form A D RJR 1 can be used, yielding
e At D ReJ t R 1 :
If J has the block structure (C.12) then e J t is also block diagonal,
2
6
6
eJ t D 6
4

3

e J .1;k1 /t

7
7
7:
5

e J .2;k2 /t
::

:

(D.34)

e J .s ;ks /t
For a single Jordan block the Taylor series expansion (D.31) can be used in conjunction
with the expansion (D.12) for powers of the Jordan block. We find that

i

i

i

i

i

i

i

D.3. Matrix exponentials
e J .;k/t D

“rjlfdm”
2007/6/1
page 295
i

295


 


1 j 
X
t
j j 2 2
j
 Sk C    C
Skj 1 C Skj
j I C j j 1 S k C
2
j 1
j!

j D0

1 j
X
t

D
j D0

j!

j I C t

1
X
j D1

1

tj 1
t2 X tj 2
j 1 S k C
j 2 Sk2 C   
.j 1/!
2!
.j 2/!
j D2

t 2 t 2
t .k 1/ t k 1
e Sk C    C
e Sk
2!
.k 1/!
3
t .k 1/ t
t 2 t
:::
2! e
.k 1/! e
7
t 2 t
7
te t
e
:::
2!
7
2
t
t
t
t
7
e
te
e
7:
2!
7
::
::
7
:
:
7
5

D e t I C te t Sk C
2
6
6
6
6
D6
6
6
6
4

e t

te t
e t

e t
(D.35)
Here we have used the fact that
 
1 j
1
1
D
j! p
p! .j p/!
and the fact that Skq D 0 for q  k. The k k matrix e J .;k/t is an upper triangular Toeplitz
j

matrix with elements d0 D e t on the diagonal and dj D tj ! e t on the j th superdiagonal
for j D 1; 2; : : : ; k 1.
We see that the situation regarding boundedness of e At is exactly analogous to what
we found for matrix powers An . If Re./ < 0, then t j e t ! 0 despite the t j factor, but if
Re./ D 0, then t j e t grows algebraically. We obtain the following theorem, analogous to
Theorem D.1.
Theorem D.2. Let A 2 Cmm be an arbitrary square matrix, and let ˛.A/ D max Re./
be the spectral abscissa of A. Let u.t/ D e At u.0/ solve u0.t/ D Au.t/. Then
(a) if ˛.A/ < 0, then ku.t/k ! 0 as t ! 1 in any vector norm.
(b) if ˛.A/ D 0 and A has no defective eigenvalues with Re./ D 0, then ku.t/k
remains bounded in any norm, and there exists a vector norm in which ku.t/k  ku.0/k
for all t  0.
If A is normal, then
ke At k2 D ke ƒt k2 D e ˛.A/t

(D.36)

since kRk2 D 1. In this case the spectral abscissa gives precise information on the behavior
of e At . If A is nonnormal, then the behavior of e ˛.A/t may not give a good indication of
the behavior of e At (except asymptotically for t sufficiently large), just as .A/n may not
give a good indication of how powers kAn k behave if A is not normal. See Section D.4 for
some discussion of this case.

i

i

i

i

i

i

i

296

“rjlfdm”
2007/6/1
page 296
i

Appendix D. Matrix Powers and Exponentials

D.3.1 Solving linear differential equations
In Section D.2.1 we saw that the r th order linear difference equation (D.16) can be rewritten
as a first order system (D.18) and solved using matrix powers. A similar approach can be
used to convert the homogeneous r th order constant coefficient linear differential equation
a0 v.t/ C a1 v 0 .t/ C a2 v 00 .t/ C    C ar 1v .r 1/ .t/ C v .r /.t/ D 0

(D.37)

to a first order system of r equations and solve this using the matrix exponential. We follow
the approach of Example 5.1 and introduce
u1 .t/ D v.t/;

u2 .t/ D v 0 .t/; : : : ; ur .t/ D v .r 1/ .t/:

(D.38)

The equation (D.37) becomes u0 .t/ D C u.t/, where C is the companion matrix (D.19).
The solution is
u.t/ D e C t u.0/ D ReJ t R 1 u.0/;
where u.0/ is the initial data (consisting of v and its first r 1 derivatives at time t D 0), and
C D RJR 1 is the Jordan canonical form of C . If the roots of the characteristic polynomial (D.20) are distinct, then v.t/ is a linear combination of the exponentials e j t . If there
are repeated roots, then they are defective, and examining the expression (D.35) we see
that a root of algebraic multiplicity k leads to terms of the form e j t ; te j t ; : : : ; t k 1 e j t .
We can thus conclude that in general solutions to the linear differential equations (D.37)
are bounded for all time only if the roots of the characteristic polynomial are in the left
half-plane, with no repeated roots on the imaginary axis.

D.4 Nonnormal matrices
We have seen that if a matrix A has the Jordan form A D RJR 1 (where J may be diagonal), then we can bound powers and the matrix exponential by the following expressions:
kAn k  .R/kJ n k in general and
kAn k  .R/.A/n if A is diagonalizable,
ke At k  .R/ke J t k in general and
ke At k  .R/e ˛.A/t if A is diagonalizable.

(D.39)

(D.40)

Here .A/ and ˛.A/ are the spectral radius and spectral abscissa, respectively. If A is
defective, then J is not diagonal and algebraic growth terms can arise from the kJ n k or
ke J t k factors.
If A is not normal, then even in the nondefective case the above bounds may not
be very useful if .R/ is large. In particular, with nonnormal matrices matrix powers or
exponentials can exhibit exponential growth during an initial transient phase (i.e., for n
or t small enough), even if the bounds guarantee eventual exponential decay. Moreover,
in these cases a small perturbation of the matrix may result in a matrix whose powers or
exponential is not bounded.
This growth can be disastrous in terms of stability, particularly since in practice most
interesting problems are nonlinear and often the matrix problems we consider are obtained

i

i

i

i

i

i

i

D.4. Nonnormal matrices

“rjlfdm”
2007/6/1
page 297
i

297

from a local linearization of the problem. Transient growth or instability of perturbed
problems can easily lead to nonlinear instabilities in the original problem.
A related problem is that in practice we often are dealing with coefficient variable
problems, where the matrix changes in each iteration. This issue is discussed more in
Section D.7. In this section we continue to consider a fixed matrix A and explore some
upper and lower bounds on powers and exponentials in the nonnormal case.

D.4.1 Matrix powers
If A is a normal matrix, AH A D AAH , then A is diagonalizable and the eigenvector matrix
R can be chosen as an unitary matrix, for which RH R D I and .R/ D 1. (We assume
the 2-norm is always used in this section.) In this case kAk D .A/ and kAn k D ..A//n ,
so the eigenvalues of A give precise information about the rate of growth or decay of kAn k,
and similarly for the matrix exponential.
If A is not normal, then kAk > .A/ and ..A//n may not give a very good indication of the behavior of kAn k even in the diagonalizable case. From (D.39) we know that
kAn k eventually decays at worst like ..A//n for large enough n, but if .R/ is huge, then
there can be enormous growth of kAn k before decay sets in. This is easily demonstrated
with a simple example.
Example D.2. Consider the nonnormal matrix


0:8 100
AD
:
(D.41)
0 0:9
This matrix is diagonalizable and the spectral radius is .A/ D 0:9. We expect kAn k 
C.0:9/n for large n, but for smaller n we observe considerable growth before the norm
0
begins to decay. For example, starting with U 0 D 1 and computing U n D An U 0 for
n D 1; 2; : : : we find
 






0
100
170
217
0
1
2
3
U D
; U D
; U D
; U D
; ::::
1
0:9
0:81
0:729
We have the bound kU n k2  2 .R/.0:9/n kU 0 k2 but in this case




1
1
1
1000
RD
;
R 1D
;
0 0:001
0
1000
so 2 .R/ D 2000. Figure D.1 shows kU n k2 for n D 1; : : : ; 30 along with the bound.
Clearly this example could be made much more extreme by replacing a22 D 100
by a larger value. Larger matrices can exhibit similar growth before decay even if all the
elements of the matrix are modest.
To gain some insight into the behavior of kAn k for a general matrix A, recall that the
2-norm of A is
q
kAk D .AH A/;
Hence
kAn k D Œ..An /H An /1=2
D Œ.AH AH    AH AA    A/1=2 :

i

i

(D.42)

i

i

i

i

i

298

“rjlfdm”
2007/6/1
page 298
i

Appendix D. Matrix Powers and Exponentials
2000
1800
1600
1400
1200
1000
800
600
400
200
0
0

5

10

15

20

25

30

Figure D.1. The points show kU n k2 for Example D.2 and the line shows the
upper bound 2000.0:9/n .
If A is normal, then AH A D AAH and the terms in this product of 2n matrices can be
rearranged to give
kAn k D Œ..AH A/n /1=2 D ..AH A//n=2 D ..A//n D kAkn :

(D.43)

If A is not normal, then we cannot rearrange the product, and in general we have
..A//n  kAn k  kAkn :

(D.44)

If .A/ < 1 < kAk, as is often the case for nonnormal matrices of interest, then the lower
bound is decaying exponentially to zero while the upper bound is growing exponentially.
If we expect to see transient growth followed by decay, as illustrated, for example, in Figure D.1, then neither of these bounds tells us anything about how much growth is expected
before the decay sets in. We would like to have lower and upper bounds on
P.A/ D sup kAn k:

(D.45)

n0

The matrix A is said to be power bounded if P.A/ < 1, and of course a necessary condition for this is that the eigenvalues of A lie in the unit disk, with no defective eigenvalues
on the disk.
Lower and upper bounds on P.A/ can be written very concisely in terms of the Kreiss
constant (D.25):
K.A/  P.A/  emK.A/
(D.46)
for any A 2 Cmm . The upper bound has already been discussed in Section D.1. The
lower bound is easier to obtain; it says that if kAn k  C for all n, then
.jzj

i

i

1/k.zI

A/ 1 k  C:

i

i

i

i

i

D.4. Nonnormal matrices

“rjlfdm”
2007/6/1
page 299
i

299

4

10

kAkn
2eK.A/

3

10

K.A/
2

10

.R/.A/n
n

kA k
1

10

0

10

.A/n
−1

10

0

10

20

30

40

50

Figure D.2. The norm of the matrix power, kAn k plotted on a logarithmic scale
for the nonnormal matrix (D.41). Also shown are the lower bound .A/n and the upper
bounds kAkn and .R/.A/n , as well as the value of the Kreiss constant K and 2eK that
give lower and upper bounds on supt 0 kAn k. Note that kAn k initially grows like kAkn
and asymptotically decays like .A/n .
A/ 1 has the series expansion


z 1 A/ 1 D z 1 I C .z 1 A/ C .z 1 A/2 C .z 1 A/3 C    :

To prove this note that .zI
.zI

A/ 1 D z 1 .I

Taking norms and using kAn k  C gives


k.zI A/ 1 k  jz 1 j 1 C jz 1 j C jz 1 j2 C jz 1 j3 C    C D

C
:
jzj 1

Figure D.2 shows the various bounds discussed above along with kAn k for the nonnormal matrix (D.41), this time on a logarithmic scale. For this matrix .A/ D 0:9; kAk 
100, and the Kreiss constant is K.A/ D 171:5.

D.4.2 Matrix exponentials
For the matrix exponential there are similar bounds (Theorem 18.5 in [92]):
Ke .A/  sup ke At k  emKe .A/;

(D.47)

t 0

where the Kreiss constant with respect to the matrix exponential Ke .A/ is defined by
Ke .A/ D

i

i

sup Re.z/k.zI
Re.z/>0

A/ 1 k:

(D.48)

i

i

i

i

i

300

“rjlfdm”
2007/6/1
page 300
i

Appendix D. Matrix Powers and Exponentials

This measures how the resolvent of A behaves near the imaginary axis, which is the stability
boundary for the matrix exponential.
It is also possible to derive upper and lower bounds on the norm of the matrix exponential as functions of t, analogous to (D.44) for powers of the matrix. For an arbitrary
matrix A these take the form
e ˛.A/t  ke At k  e !.A/t

for all t  0;

(D.49)

where ˛.t/ is the spectral abscissa (D.33) and !.A/ is the numerical abscissa defined by
!.A/ D

1
.AH C A/:
2

(D.50)

Note that AH C A is always hermitian and has real eigenvalues, but they may be positive
even if ˛.A/ < 0. In general,
˛.A/  !.A/:
(D.51)
Also note that for any vector u,
1
Re.uH Au/ D .uH Au C uH AH u/
2 

1 H
H
Du
.A C A/ u
2

(D.52)

 !.A/uH u;
since the Rayleigh quotient uH Bu=uH u is always bounded by .B/ for any hermitian
matrix B. Another way to characterize !.A/ is as the maximum value that the real part of
uH Au can take over any unit vector u. The set
W .A/ D fz 2 C W z D uH Au for some u with kuk2 D 1g

(D.53)

is called the numerical range or field of values of the matrix A, and
!.A/ D max Re.z/:
z2W .A/

(D.54)

For a normal matrix W .A/ is the convex hull of the eigenvalues, but for a nonnormal matrix
it may be larger.
The bound (D.52) is of direct interest in studying the matrix exponential and can be
used to prove the upper bound in (D.49) as follows. Suppose u0 .t/ D Au.t/ and consider
d H
.u u/ D .u0 /H u C uH u0
dt
D uH AH u C uH Au

(D.55)

D 2Re.uH Au/
 2!.A/uH u:
In other words, using the 2-norm,
d 
dt


ku.t/k2  2!.A/ku.t/k2 ;

(D.56)

which in turn implies

i

i

i

i

i

i

i

D.4. Nonnormal matrices

“rjlfdm”
2007/6/1
page 301
i

301
d
ku.t/k  !.A/ku.t/k;
dt

(D.57)

d
log.ku.t/k/  !.A/:
dt

(D.58)

ku.t/k  e !.A/t ku.0/k:

(D.59)

ke At u.0/k  e !.A/t ku.0/k

(D.60)

or, dividing by ku.t/k,

Integrating gives
Since u.t/ D e At u.0/, we have

for any vector u.0/ and hence the matrix norm of e At is bounded as in (D.49).
For a normal matrix A, ˛.A/ D !.A/ and (D.49) reduces to
ke At k D e ˛.A/t D e !.A/t

for all t  0:

(D.61)

In the scalar case, for a complex number A, the spectral abscissa and numerical abscissa
are both equal to the real part of A, so each can be viewed as a generalization of the real
part.
Finally, it is sometimes useful to investigate the initial transient growth of ke At k at
t D 0. This can be determined by differentiating ke At k with respect to t and evaluating at
t D 0. The result is
ˇ
d At ˇˇ
ke Ak k 1
D lim
ke kˇ
dt
k!0
k
t D0
(D.62)
kI C Akk 1
D lim
:
k!0
k
We can compute
h 
i1=2
kI C kAk D  .I C AH k/.I C Ak/
h
i1=2
D .I C .A C AH /k C AH Ak 2 /
h
i1=2
D 1 C .A C AH /k C O.k 2 /

(D.63)

1
D 1 C .A C AH /k C O.k 2 /;
2
and so

ˇ
d At ˇˇ
kI C Akk
D lim
ke kˇ
dt
k!0
k
t D0

1
D

1
.A C AH / D !.A/:
2

(D.64)

Hence we expect
ke At k D e !.A/t C o.t/

as t ! 0:

(D.65)

From (D.49) we know that e !.A/t is an upper bound on the norm, but this shows that for
small t we will observe transient growth at this rate.

i

i

i

i

i

i

i

302

“rjlfdm”
2007/6/1
page 302
i

Appendix D. Matrix Powers and Exponentials

2eKe.A/

2

10

e !.A/t
Ke .A/
1

10

.R/e ˛.A/t

ke At k
0

10

e ˛.A/t

−1

10

0

10

20

30

40

50

Figure D.3. The norm of the matrix exponential ke At k plotted on a logarithmic
scale for the nonnormal matrix (D.66). Also shown are the lower bound e ˛.A/t and the
upper bounds e !.A/t and .R/e ˛.A/t , as well as the value of the Kreiss constant Ke and
2eKe that give lower and upper bounds on supt 0 ke At k. Note that ke At k initially grows
like e !.A/t and asymptotically decays like e ˛.A/t .
For example, consider the nonnormal matrix


0:2 10
AD
:
0
0:1

(D.66)

Figure D.3 shows ke At k as a function of t on a logarithmic scale. The initial growth has
slope !.A/ D 5:15 and the eventual decay has slope ˛.A/ D 0:1, which follows from
(D.40). The Kreiss constant for this matrix is Ke .A/ D 17:17.
The quantity !.A/ is sometimes defined as
kI C Akk
k!0
k

!.A/ D lim

1

(D.67)

and in the ODE literature often goes by the name of the logarithmic norm of A. Of course
it is not really a norm since it can be negative, but this terminology is motivated by inequalities such as (D.58).

D.5 Pseudospectra
Various tools have been developed to better understand the behavior of matrix powers or
exponentials in the case of nonnormal matrices. One powerful approach is to investigate
the pseudospectra of the matrix. Roughly speaking, this is the set of eigenvalues of all

i

i

i

i

i

i

i

D.5. Pseudospectra

“rjlfdm”
2007/6/1
page 303
i

303

“nearby” matrices. For a highly nonnormal matrix a small perturbation to the matrix can
give a very large change in the eigenvalues of the matrix. For example, perturbing the
matrix (D.41) to


0:8 100
Q
AD
(D.68)
0:001 0:9
changes the eigenvalues from f0:8; 0:9g to f0:53; 1:17g. A perturbation to A of magnitude
10 3 leads to an eigenvalue that is greater than 1. Since A is so close to a matrix AQ for
which AQn blows up as n ! 1, it is perhaps not so surprising that An exhibits initial growth
before decaying. We say that the value z D 1:17 lies in the -pseudospectrum  of A for
 D 10 3.
The eigenvalues of A are isolated points in the complex plane where .zI A/ is
singular. We know that if any of these points lies outside the unit circle, then An blows up.
The idea of pseudospectral analysis is to expand these isolated points to larger regions, the
pseudospectra  , for some small , and see whether these pseudospectra extend beyond
the unit circle.
There are various equivalent ways to define the -pseudospectrum of a matrix. Here
are three.
Definition D.3. For each   0, the -pseudospectrum  .A/ of A is the set of numbers
z 2 C satisfying any one of the following equivalent conditions:
(a) z is an eigenvalue of A C E for some matrix E with kEk < ,
(b) kAu zuk   for some vector u with kuk D 1, or
(c) k.zI A/ 1 k   1 .
In all these conditions the 2-norm is used (although the ideas can be extended to a
general Banach space). Condition (a) is the easiest to understand and the one already illustrated above by example: z is an -pseudoeigenvalue of A if it is a genuine eigenvalue
of some -sized perturbation of A. Condition (b) says that z is an -pseudoeigenvalue if
there is a unit vector that is almost an eigenvector for this z. Condition (c) relates pseudoeigenvalues to the resolvent .zI A/ 1 , which we have already seen plays a role in
obtaining bounds on the behavior of matrix powers and exponentials. The value z is an
-pseudoeigenvalue of A if the resolvent is sufficiently large at z. This fits with the notion
of expanding the singular points i into regions  where zI A is nearly singular. (Note
that by convention we set k.zI A/ 1 k D 1 if z is an eigenvalue of A.)
The -pseudospectral radius  .A/ and -pseudospectral abscissa ˛ .A/ can be defined in the natural way as the maximum absolute value and maximum real part of any
-pseudoeigenvalue of A, respectively. The Kreiss constants (D.25) and (D.48) can then be
expressed in terms of pseudospectra as
 .A/

>0

K.A/ D sup

1

;

˛ .A/
:

>0

Ke .A/ D sup

(D.69)

Hence the Kreiss constants can be viewed as a measure of how far the pseudospectra of A
extend outside the unit circle or into the right half-plane.

i

i

i

i

i

i

i

304

“rjlfdm”
2007/6/1
page 304
i

Appendix D. Matrix Powers and Exponentials

The MATLAB package eigtool developed by Wright [104] provides tools for
computing and plotting the pseudospectra of matrices and also quantities such as the
-pseudospectral radius and -pseudospectral abscissa. See the book by Trefethen and
Embree [92] for an in-depth discussion of pseudospectra with many examples of their
use.

D.5.1 Nonnormality of a Jordan block
In Section D.2 we found an explicit expression for powers of a Jordan block, and we see
that in addition to terms of the form n , a block of order k has terms of order nk n in its
nth power. This clearly exhibits transient growth even in  < 1. It is interesting to further
investigate the Jordan block as an example of a highly nonnormal matrix.
For this discussion, let
2
3
c 1
6
7
c 1
6
7
6
7
::
J D 6
(D.70)
7 2 Rkk
:
6
7
4
c 1 5

c
with the entries not shown all equal to 0, so that for  D 0, J0 is a Jordan block of the form
(C.9) with all k of its eigenvalues at c.
If  > 0 on the other hand, the characteristic equation is
.

c/k

D0

and the eigenvalues are
p D c C  1= k e 2 ip= k ;

p D 1; 2; : : : ; k:

(D.71)

The eigenvalues are now equally spaced around a circle of radius  1= k centered at z D c in
the complex plane.
Note that if k is large,  1= k will be close to 1 even for very small . For example, if
k D 1000 and  D 10 16, then  1= k  0:96. So although the eigenvalues of J0 are all at
c, a perturbation on the order of the machine round-off will blast them apart to a circle of
radius nearly 1 about this point. For large k the -pseudospectrum of J0 tends to fill up this
circle, even for very small .
A similar matrix arises when studying the upwind method for advection, in which
case k corresponds to the number of grid points and can easily be large. An implication of
this nonnormality in stability analysis is explored in Section 10.12.1.

D.6 Stable families of matrices and the Kreiss matrix
theorem
So far we have studied the behavior of powers of a single matrix A. We have seen that
if the eigenvalues of A are inside the unit circle, then the powers of A are uniformly
bounded,
kAn k  C for all n;
(D.72)

i

i

i

i

i

i

i

D.6. Stable families of matrices and the Kreiss matrix theorem

“rjlfdm”
2007/6/1
page 305
i

305

for some constant C . If A is normal, then it fact kAn k  kAk. Otherwise kAn k may
initially grow, perhaps to a very large value if the deviation from normality is large, but
will eventually decay and hence some bound of the form (D.72) holds.
In studying the stability of discretizations of differential equations, we often need to
consider not just a single matrix but an entire family of matrices. A particular discretization
with mesh width h and/or time step k leads to a particular matrix A, but to study stability
and prove convergence we need to let h; k ! 0 and study the whole family of resulting
matrices. This is quite difficult to study in general because typically the dimensions of the
matrices involved is growing as we refine the grid. However, at least in simple cases we
can use von Neumann analysis to decouple the system into Fourier modes, each of which
leads to a system of fixed dimension (the number of equations in the original differential
equation). As we refine the grid we obtain more modes and the matrices involved may
depend explicitly on h and k as well as on the wave number, but the matrices all have fixed
dimension and it is this case that we consider here. (In Sections 9.6 and 10.5 we consider
von Neumann analysis applied to scalar problems, in which case proving stability only
requires studying powers of the scalar amplification factor g for each wave number, and
powers of a scalar are uniformly bounded if and only if jgj  1. The considerations of
this section come into play if von Neumann analysis is applied to a system of differential
equations.)
Let F represent a family of matrices, say all the amplification matrices for different
wave numbers that arise from discretizing a particular differential equation with different
mesh widths. We say that F is uniformly power bounded if there is a constant C > 0 such
that (D.72) holds for all matrices A 2 F. The bound must be uniform in both A and n, i.e.,
a single constant for all matrices in the family and all powers.
If F consists of only normal matrices and if .A/  1 for all A 2 F, then the family
is uniformly power bounded and (D.72) holds in general with C D 1.
When the matrices are not normal it can be more difficult to establish such a bound.
Obviously a necessary condition is that .A/  1 for all A 2 F and that any eigenvalues of
modulus 1 must be nondefective. If this condition fails for any A 2 F, then that particular
matrix will fail to be power bounded and so the family cannot be. However, this condition
is not sufficient—even if each matrix is power bounded they may not be uniformly so. For
example, the infinite family of matrices



1

A D


0

1
1

(D.73)



for  > 0 are all individually power bounded but not uniformly power bounded. We have

An D

/n

.1
0

n.1 /n 1
.1 /n


;

and the off-diagonal term can be made arbitrarily large for large n by choosing  small
enough.
One fundamental result on power boundedness of matrix families is the Kreiss matrix
theorem.

i

i

i

i

i

i

i

306

“rjlfdm”
2007/6/1
page 306
i

Appendix D. Matrix Powers and Exponentials

Theorem D.4. The following conditions on a family F of matrices are equivalent:
(a) There exists a constant C such that kAn k  C for all n and for all A 2 F. (The
family is power bounded.)
(b) There exists a constant C1 such that, for all A 2 F and all z 2 C with jzj > 1,
the resolvent .zI A/ 1 exists and is bounded by
k.zI

A/ 1 k 

C1
:
jzj 1

(D.74)

In other words, the K.A/  C1 for all A 2 F, where K.A/ is the Kreiss constant (D.25).
(c) There exist constants C2 and C3 such that for each A 2 F a nonsingular matrix
S exists such that
(i) kSk  C2 ; kS 1 k  C2 ,
(ii) B D S 1 AS is upper triangular with off-diagonal elements bounded by
jbij j  C3 min.1

jbi i j; 1

jbjj j/:

(D.75)

(Note that the diagonal elements of b are the eigenvalues of A.)
(d) There exists a constant C4 such that for each A 2 F a positive definite matrix G
exists with
C4 1 I  G  C4 I;
AH GA  G:

(D.76)

In condition (d) we say that two Hermitian matrices A and B satisfy A  B if
uH Au  uH Bu for any vector u. This condition can be rewritten in a more familiar form
as follows:
(d0 ) There exists a constant C5 so that for each A 2 F there is a nonsingular matrix
T such that
kAkT  1 and .T /  C5 :
(D.77)
Here the T -norm of A is defined as in (C.35) in terms of the 2-norm,
kAkT D kT

1

AT k:

Condition (d0) is related to (d) by setting G D T H T 1 , and (d0) states that we can define
a set of norms, one for each A 2 F, for which the norm of A is less than 1 and therefore
kAn kT  1 for all n  0:
From this we can obtain uniform power boundedness by noting that
kAn k  .T /kAn kT  C5 :
To make sense of condition (c), consider the case where all matrices A 2 F are
normal. Then each A can be diagonalized by a unitary similarity transformation and so

i

i

i

i

i

i

i

D.7. Variable coefficient problems

“rjlfdm”
2007/6/1
page 307
i

307

(c) holds with kSk2 D kS 1 k2 D 1 and bij D 0 for i ¤ j . More generally, condition
(c) requires that we can bring all A 2 F to upper triangular form by uniformly wellconditioned similarity transformations, and with a uniform bound on the off-diagonals that
is related to how close the diagonal elements (which are the eigenvalues of A) are to the
unit circle.
Several other equivalent conditions have been identified and are sometimes more
useful in practice; see Richtmyer and Morton [75] or the more recent paper of Strikwerda
and Wade [85].
The equivalence of the conditions in Theorem D.4 can be proved by showing that
(a) H) (b) H) (c) H) (d) H) (d0) H) (a). For a more complete discussion and proofs
see [75] or [85].
The equivalence of (a) and (b) also follows directly from (D.46) and a proof of this
can be found in [92]. It is this equivalence that is the most interesting part of the theorem
and that has received the most attention in subsequent work, to the point where the term
“Kreiss matrix theorem” is often applied to inequalities of the form (D.46).

D.7 Variable coefficient problems
So far we have only considered solving equations of the form U nC1 D AU n or u0 .t/ D
Au.t/, where the matrix A is constant (independent of n or t), and the solution can be
written in terms of powers or matrix exponentials. In most applications, however, the
matrix changes with time. Often A represents the Jacobian matrix for a nonlinear system
and so it certainly varies with time as the solution changes. Adding this complication
makes it considerably more difficult to analyze the behavior of solutions. Often a study of
the “frozen coefficient” problem where A is frozen at a particular value as we solve forward
in time is valuable, however, to gain some information about issues such as boundedness
of the solution, and the theory presented earlier in this appendix will be useful in many
contexts. However, new issues can come into play when the matrices vary, particularly if
they vary rapidly, or more accurately, particularly if the eigenvectors of the matrix vary
rapidly in time. We will not discuss this in detail—just give a brief introduction to this
topic.
We first consider a discrete time iteration of the form
U nC1 D An U n ;

(D.78)

where An may vary with n. The solution is
U j D Aj 1 Aj 2    A1 A0 U 0 :

(D.79)

If An  A for all n, then this reduces to U j D Aj U 0 , but more generally the matrix
product is harder to analyze than powers of a single matrix.
One case is relatively simple: suppose all the matrices An have the same eigenvectors,
although possibly different eigenvalues, so
An D Rƒn R 1

i

i

(D.80)

i

i

i

i

i

308

“rjlfdm”
2007/6/1
page 308
i

Appendix D. Matrix Powers and Exponentials

for some fixed matrix R. In this case we say the An are simultaneously diagonalizable.
Then the product in (D.79) reduces to
U j D Rƒj 1 ƒj 2    ƒ1 ƒ0 R 1 U 0
and ƒj 1 ƒj 2    ƒ1 ƒ0 is a diagonal matrix whose ith diagonal element is the product of
the ith eigenvalue of each of the matrices A0 ; A1 ; : : : ; Aj 1 . Then we clearly have, for
example, that if .An /  1 for all n, then kU n k is uniformly bounded as n ! 1. In fact
we don’t need .An /  1 for all n; it is sufficient to have
.An /  1 C
for some sequence of values

(D.81)

n

n satisfying
1
X
j < 1:

(D.82)

j D0

From (D.81) it follows that .An /  e n and so
.Rƒj 1 ƒj 2    ƒ1 ƒ0 R 1 / 

jY
1

.An /
nD0
j 1

Y

e n


nD0

(D.83)
1

0
j 1

 exp @

X

nA ;

nD0

and hence kU n k is uniformly bounded.
If the eigenvectors vary with n, however, then it happens that kU n k will grow without
bound even if .An / < 1 for all n.
Example D.3. As a simple example, consider




0 0
0:1 2
A0 D
;
A1 D
(D.84)
2 0:1
0 0
and then let An alternate between these two matrices for larger n, so A2i D A0 and
A2iC1 D A1 . Then .An / D 0:1 for all n. However, we see that after an even number of steps
U 2i D .A1 A0 /i U 0
and


A1 A0 D


4
0

0:2
0

;

so kU j k grows like 2j .

i

i

i

i

i

i

i

D.7. Variable coefficient problems

“rjlfdm”
2007/6/1
page 309
i

309

If we iterated with either A0 or A1 alone, then U n would go rapidly to zero. But note
that these matrices are nonnormal and in either case there can be transient growth before
decay sets in. For example,
 
 


1
0
0
0
0
2 0
U D
H) A0 U D
H) A0 U D
0
2
0:2


0
H) A30 U 0 D
H) etc.
0:02
Beyond the first iteration there is exponential decay by a factor 0.1 each iteration since
A0 U 0 is in the eigenspace of A0 corresponding to  D 0:1. But if we apply A0 only once
to U 0 and then apply A1 , we instead obtain
 
 
 
1
0
4
0
0
0
U D
H) A0 U D
H) A1 A0 U D
:
0
2
0
Instead of decay we see amplification by another factor of 2. Moreover, the vector has been
moved by A1 out of the eigenspace it was in and into a vector that again suffers transient
growth when A0 is next applied. This couldn’t happen if A0 and A1 shared the same
eigenspaces.
One way to try to guarantee that the vectors U n generated by the process (D.78) are
uniformly bounded is to look for a single norm k  k in which
kAn k  1 C

n

(D.85)

with (D.82) holding. In the simultaneously diagonalizable case considered above we can
base the norm on the joint eigenvector matrix R using Theorem C.4. Another case in which
we have stability is if the matrices An are all normal and satisfy (D.81), for then we can
use the 2-norm.
But even if the matrices are not normal and the eigenvectors vary, if they do so slowly
enough we may be able to prove boundedness using the following theorem. Here k  kTn
is the Tn -norm defined by (C.35) in terms of some fixed norm k  k, and we may be able
to use the eigenvector matrix Rn of each An for these norms, although the theorem allows
more flexibility.
Theorem D.5. Suppose that for the difference equation (D.78) we can find a sequence of
nonsingular matrices Tn such that
P
1. kAn kTn  1 C n with 1
j D0 j < 1,
2. kTnk  C , a constant independent of n, and
P
3. kTn 1 Tn 1 k  1 C ˇn with 1
j D0 ˇj < 1.
Then kU n k is uniformly bounded for all n.
Note that the third condition is the requirement that the norm vary sufficiently slowly.
The proof can be found in [23].

i

i

i

i

i

i

i

310

“rjlfdm”
2007/6/1
page 310
i

Appendix D. Matrix Powers and Exponentials
Similar considerations apply to solutions to the variable coefficient ODE
u0.t/ D A.t/u.t/:

(D.86)

If the A.t/ are simultaneously diagonalizable for all t, then this reduces to v 0.t/ D ƒ.t/v.t/,
where v.t/ D R 1 u.t/. Then
Z t

vi .t/ D exp
i ./ d
0

Rt

and ku.t/k is uniformly bounded in t if 0 ˛.A.// d is uniformly bounded, where ˛.A.t//
is the spectral abscissa (D.33). (For the constant case A.t/  A, this requires ˛.A/  0.)
If the A.t/ are not simultaneously diagonalizable, then there are examples, similar to
the Example D.3, where ku.t/k may grow without bound even if all the matrices A.t/ have
eigenvalues only in the left half-plane.
For a general function A.t/ we have (D.57),
d
ku.t/k  !.A.t//ku.t/k;
dt
where !.A/ is the numerical abscissa (D.50). Dividing by ku.t/k gives
d
log.ku.t/k/  !.A.t//
dt
and integrating yields
Z t


!.A.t// d

ku.t/k  exp

ku.0/k:

(D.87)

0

Rt
This shows that the solution u.t/ is bounded in norm for all t provided that 0 !.A.t// d
is bounded above uniformly in t.
Recall, however, that !.A/ may be positive even when the eigenvalues of A all have
negative real part (in the nonnormal case). So requiring, for example, !.A.t//  0 for
all t is akin to requiring kAn k  1 in the same norm for all matrices An in the difference
equation (D.78). As in the case of the difference equation this requirement can be relaxed
by introducing the notion of a logarithmic T-norm that varies with time, and a theorem
similar to Theorem D.5 obtained for differential equations if the eigenvector matrix of A.t/
is not varying too rapidly; see [23].
For a hint of what’s involved, suppose the matrices are all diagonalizable, A.t/ D
R.t/ƒ.t/R 1 .t/, and that R.t/ is differentiable. Note that .R 1 /0 .t/ D R 1 .t/R0 .t/R 1 .t/,
obtained by differentiating RR 1 D I . If we set v.t/ D R 1 .t/u.t/ we find that
v 0 D R 1 u0 C .R 1 /0 u
D R 1 ARv C .R 1 /0 Rv
D .ƒ

R

1

(D.88)

0

R /v:

So the boundedness of v.t/ depends on the matrices ƒ.t/ R 1 .t/R0 .t/, and if the eigenvectors vary rapidly, then the latter term can lead to unbounded growth even if the eigenvalues are all in the left half-plane.

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 311
i

Appendix E

Partial Differential
Equations

In this appendix we briefly discuss some of the basic partial differential equations (PDEs)
that are used in this book to illustrate the development of numerical methods, and we review
the manner in which Fourier analysis can be used to gain insight into these problems.

E.1

Classification of differential equations

First we review the classification of differential equations into elliptic, parabolic, and hyperbolic equations. Not all PDEs fall into one of these classes, by any means, but many
important equations that arise in practice do. These classes of equations model different
sorts of phenomena, display different behavior, and require different numerical techniques
for their solution. Standard texts on partial differential equations such as Kevorkian [55]
give further discussion.

E.1.1 Second order equations
In most elementary texts the classification is given for a linear second-order differential
equation in two independent variables of the form
auxx C buxy C cuyy C dux C euy C f u D g:
The classification depends on the sign of the discriminant,
8
H)
elliptic,
< <0
b 2 4ac D 0
H)
parabolic,
:
>0
H)
hyperbolic,
and the names arise by analogy with conic sections. The canonical examples are the Poisson problem uxx C uyy D g for an elliptic problem, the heat equation ut D uxx (with
 > 0) for a parabolic problem, and the wave equation ut t D c 2 uxx for a hyperbolic problem. In the parabolic and hyperbolic case t is used instead of y since these are typically
time-dependent problems. These can all be extended to more space dimensions. These
311

i

i

i

i

i

i

i

312

“rjlfdm”
2007/6/1
page 312
i

Appendix E. Partial Differential Equations

equations describe different types of phenomena and require different techniques for their
solution (both analytically and numerically), and so it is convenient to have names for
classes of equations exhibiting the same general features. Other equations have some of
the same features, and the classification scheme can be extended beyond the second order
linear form given above. Some hint of this is given in the next few sections.

E.1.2 Elliptic equations
The classic example of an elliptic equation is the Poisson problem
r 2 u D f;

(E.1)

where r 2 is the Laplacian operator and f is a given function of xE D .x; y/ in some spatial
domain . We seek a function u.x/
E in  satisfying (E.1) together with some boundary
conditions all along the boundary of . Elliptic equations typically model steady-state or
equilibrium phenomena, and so there is no temporal dependence (however, see Section 2.16
for a counterexample). Elliptic equations may also arise in solving time-dependent problems if we are modeling some phenomena that are always in local equilibrium and equilibrate on time scales that are much faster than the time scale being modeled. For example,
in “incompressible” flow the fast acoustic waves are not modeled and instead the pressure
is computed by solving a Poisson problem at each time step which models the global effect
of these waves.
Elliptic equations give boundary value problems where the solution at all points must
be simultaneously determined based on the boundary conditions all around the domain.
This typically leads to a very large sparse system of linear equations to be solved for the
values of U at each grid point. If an elliptic equation must be solved in every time step of a
time-dependent calculation, as in the examples above, then it is crucial that these systems
be solved as efficiently as possible.
More generally, a linear elliptic equation has the form
Lu D f;

(E.2)

where L is some elliptic operator. For our purposes we will consider only constant coefficient second order operators, which in N space dimensions have the form
N
X

LD

N

Aj k
j ;kD1

X
@2
@
C
Bj
C C;
@xj @xk
@xj

(E.3)

j D1

where the Aj k ; Bj ; C are real numbers. Note that since @2 u=@xj @xk D @2 u=@xk @xj , we
can always choose the N  N matrix A defined by the second order term to be symmetric.
This operator is said to be elliptic if A is positive definite or negative definite, as defined in
Section C.4. This means that v T Av has the same sign for all nonzero vectors v 2 RN and
cannot pass through zero. This can be shown to ensure that the boundary value problem
(E.2) has a unique solution. For an indication of why this is true, see Section E.3.5.
In two space dimensions writing the matrix as


a
b=2
AD
b=2
c

i

i

i

i

i

i

i

E.1. Classification of differential equations

“rjlfdm”
2007/6/1
page 313
i

313

and considering when this matrix is definite, we find that the operator is elliptic if b 2
4ac < 0, as in the classification of the previous section.
For the Laplacian operator r 2 u, A is the N  N identity matrix and so this is an
elliptic operator. Note that in one space dimension r 2 u reduces to u00.x/ and the problem
(E.1) is the 2-point boundary value problem considered in Chapter 2.

E.1.3 Parabolic equations
If L is an elliptic operator with a positive definite A, then the time-dependent equation
ut D Lu

f

(E.4)

is well posed (see Section E.3.5) and is called parabolic. If L D r 2 is the Laplacian, then
(E.4) is known as the heat equation or diffusion equation and models the diffusion of heat
in a material, for example.
Now u.x;
E t/ varies with time and we require initial data u.x;
E 0/ for every xE 2 
as well as boundary conditions around the boundary at each time t > 0. If the boundary
conditions are independent of time, then we might expect the heat distribution to reach a
steady state in which u is independent of t. We could then solve for the steady state directly
by setting ut D 0 in (E.4), which results in the elliptic equation (E.2).
Marching to steady state by solving the time-dependent equation (E.4) numerically
would be one approach to solving the elliptic equation (E.2), but this is typically not the
fastest method if all we require is the steady state.

E.1.4 Hyperbolic equations
Rather than discretizing second order hyperbolic equations such as the wave equation
ut t D c 2 uxx , we will consider a related form of hyperbolic equations known as first order
hyperbolic systems. The linear problem in one space dimension has the form
ut C Aux D 0;

(E.5)

where u.x; t/ 2 Rs and A is an s  s matrix. The problem is called hyperbolic if A has real
eigenvalues and is diagonalizable, i.e., has a complete set of linearly independent eigenvectors. These conditions allow us to view the solution in terms of propagating waves, and
indeed hyperbolic systems typically arise from physical processes that give wave motion
or advective transport. This is explored more in Section 10.10.
The simplest example of a hyperbolic equation is the constant-coefficient advection
equation
ut C aux D 0;
(E.6)
where u is the advection velocity. The solution is simply u.x; t/ D u.x at; 0/, so any u
profile simply advects with the flow at velocity a.
As a simple example of a linear hyperbolic system, the equations of linearized acoustics arising from elasticity or gas dynamics can be written as a first order system of two
equations in one space dimension as

i

i

i

i

i

i

i

314

“rjlfdm”
2007/6/1
page 314
i

Appendix E. Partial Differential Equations



p
u


C

t


0
1=0

0
0


p
u

D0

(E.7)

x

in terms of pressure and velocity perturbations, where 0 is the background density and 0
is the “bulk modulus” of the material. Note that if we differentiate the first equation with
respect to t, the second with respect to x, and then eliminate uxt D utx , we obtain the
second order wave equation for the pressure:
pt t D c 2 pxx ;
where
cD

p
0 =0

is the speed of sound in the material.
Often hyperbolic equations arise most naturally as first order systems, as motivated
in the next section, and we consider only this formulation.

E.2 Derivation of partial differential equations from
conservation principles
Many physically relevant partial differential equations can be derived based on the principle
of conservation. We can view u.x; t/ as a concentration or density function for some
substance or chemical that is in dilute suspension in a liquid, for example. Basic equations
of the same form arise in many other applications, however. The material presented here
is meant to be a brief review, and much more complete discussions are available in many
sources. See, for example, [55], [61], [66], [102].
A reasonable model to consider in one space dimension is the concentration or density of a contaminant in a stream or pipe, where the variable x represents distance along the
pipe. The concentration is assumed to be constant across any cross section, so that its value
varies only with x. The density function u.x; t/ is defined in such a way that integrating
the function u.x; t/ between any two points x1 and x2 gives the total mass of the substance
in this section of the pipe at time t:
Z x2
Total mass between x1 and x2 at time t D
u.x; t/ dx:
x1

The density function in measured in units such as grams/meter. (Note that this u really
represents the integral over the cross section of the pipe of a density function that is properly
measured in grams/meter3 .)
The basic form of differential equation that models many physical processes canR be
x
derived in the following way. Consider a section x1 < x < x2 and the manner in which x12
u.x; t/ dx changes with time. This integral represents the total mass of the substance in
this section, so if we are studying a substance that is neither created nor destroyed within
this section, then the total mass within this section can change only due to the flux or flow
of particles through the endpoints of the section at x1 and x2 . This flux is given by some
function f which, in the simplest case, depends only on the value of u at the corresponding
point.

i

i

i

i

i

i

i

E.2. Derivation of partial differential equations from conservation principles

“rjlfdm”
2007/6/1
page 315
i

315

E.2.1 Advection
If the substance is simply carried along (advected) in a flow at some constant velocity a,
then the flux function is
f .u/ D au:
(E.8)
The local density u.x; t/ (in grams/meter, say) multiplied by the velocity (in meters/sec,
say) gives the flux of material past the point x (in grams/sec).
Since the total mass in Œx1 ; x2 changes only due to the flux at the endpoints, we have
Z x2
d
u.x; t/ dx D f .u.x1 ; t// f .u.x2 ; t//:
(E.9)
dt x1
The minus sign on the last term comes from the fact that f is, by definition, the flux to the
right.
If we assume that u and f are smooth functions, then this equation can be rewritten
as
Z x2
Z x2
d
@
u.x; t/ dx D
f .u.x; t// dx
dt x1
x1 @x
or, with some further modification, as

Z x2 
@
@
u.x; t/ C
f .u.x; t// dx D 0:
@t
@x
x1
Since this integral must be zero for all values of x1 and x2 , it follows that the integrand
must be identically zero. This gives, finally, the differential equation
@
@
u.x; t/ C
f .u.x; t// D 0:
@t
@x

(E.10)

This form of equation is called a conservation law.
For the case considered in Section E.2.1, f .u/ D au with a constant and this equation becomes the advection equation (E.6). This equation requires initial conditions and
possibly also boundary conditions in order to determine a unique solution. The simplest
case is the Cauchy problem on 1 < x < 1 (with no boundary), also called the pure
initial value problem. Then we need only to specify initial data
u.x; 0/ D .x/:

(E.11)

Physically, we would expect the initial profile of  to simply be carried along with the flow
at speed a, so we should find
u.x; t/ D .x at/:
(E.12)
It is easy to verify that this function satisfies the advection equation (E.6) and is the solution
of the PDE.
The curves
x D x0 C at
through each point x0 at time 0 are called the characteristics of the equation. If we set
U.t/ D u.x0 C at; t/

i

i

i

i

i

i

i

316

“rjlfdm”
2007/6/1
page 316
i

Appendix E. Partial Differential Equations

then
U 0.t/ D aux .x0 C at; t/ C ut .x0 C at; t/
D0
using (E.6). Along these curves the PDE reduces to a simple ordinary differential equation
(ODE) U 0 D 0 and the solution must be constant along each such curve, as is also seen
from the solution (E.12).

E.2.2 Diffusion
Now suppose that the fluid in the pipe is not flowing and has zero velocity. Then according
to the above equation, ut D 0 and the initial profile .x/ does not change with time. However, if  is not constant in space then in fact it will tend to slowly change due to molecular
diffusion. The velocity a should really be thought of as a mean velocity, the average velocity that the roughly 1023 molecules in a given drop of fluid have. But individual molecules
are bouncing around in different directions and so molecules of the substance we are tracking will tend to get spread around in the ambient fluid, just as a drop of ink spreads in water.
There will tend to be a net motion from regions where the density is large to regions where
it is smaller, and in fact it can be shown that the flux (in one dimension) is proportional to
ux . The flux at a point x now depends on the value of ux at this point, rather than on the
value of u, so we write
f .ux / D ux ;
(E.13)
where  is the diffusion coefficient. The relation (E.13) is known as Fick’s law. Using this
flux in (E.10) gives
ut D uxx ;
(E.14)
which is known as the diffusion equation.
This equation is also called the heat equation since heat diffuses in much the same
way. In this case u.x; t/ represents the density of thermal energy, which is proportional to
the temperature. The proportionality factor is the heat capacity of the material, which we’ll
take to be the value 1 (with suitable units) so that u can also be viewed as the temperature.
The one-dimensional equation models the conduction of heat in a rod. The heat conduction
coefficient  depends on the material and how well it conducts heat. The relation (E.13)
is known as Fourier’s law of heat conduction, which states more generally that the flux of
thermal energy is proportional to the temperature gradient.
In some problems the diffusion coefficient may vary with x, for example, in a rod
made of a composite of different materials. Then f D .x/ux and the equation becomes
ut D ..x/ux /x :
Returning to the example of fluid flow, more generally there would be both advection
and diffusion occurring simultaneously. Then the flux is f .u; ux / D au ux , giving the
advection-diffusion equation
ut C aux D uxx :
(E.15)
The diffusion and advection-diffusion equations are examples of the general class of
PDEs called parabolic.

i

i

i

i

i

i

i

E.3. Fourier analysis of linear partial differential equations

“rjlfdm”
2007/6/1
page 317
i

317

E.2.3 Source terms

Rx
In some situations x 2 u.x; t/ dx changes due to effects other than flux through the end1
points of the section, if there is some source or sink of the substance within the section.
Denote the density function for such a source by .x; t/. (Negative values of correspond to a sink rather than a source.) Then the equation becomes
Z x2
Z x2
Z x2
d
@
u.x; t/ dx D
.x; t/ dx:
f .u.x; t// dx C
dt x1
x1 @x
x1
This leads to the PDE
ut .x; t/ C f .u.x; t//x D

.x; t/:

(E.16)

For example, if we have heat conduction in a rod together with an external source of heat
energy distributed along the rod with density , then we have
ut D uxx C :
In some cases the strength of the source may depend on the value of u. For example, if the
rod is immersed in a liquid that is held at constant temperature u0 , then the flux of heat into
the rod at the point .x; t/ is proportional to u0 u.x; t/ and the equation becomes
ut .x; t/ D uxx .x; t/ C ˛.u0

u.x; t//:

E.2.4 Reaction-diffusion equations
One common form of source term arises from chemical kinetics. If the components of
u 2 Rs represent concentrations of s different species reacting with one another, then the
kinetics equations have the form ut D R.u/, as described in Section 7.4.1. This assumes
the different species are well mixed at all times and so the concentrations vary only with
time. If there are spatial variations in concentrations, then these equations may be combined
with diffusion of each species. This would lead to a system of reaction-diffusion equations
of the form
ut D uxx C R.u/:
(E.17)
The diffusion coefficient could be different for each species, in which case  would be a
diagonal matrix instead of a scalar. This generalizes to more space dimensions by replacing
uxx by r 2 u, the Laplacian of u.
Advection terms might also be present if the reactions are taking place in a flowing
fluid. More generally the reaction-diffusion equations may be coupled with nonlinear equations of fluid dynamics, which may themselves contain both hyperbolic terms and parabolic
viscous terms.

E.3

Fourier analysis of linear partial differential equations

For linear PDEs, Fourier analysis is often used to obtain solutions or perform theoretical analysis. This is because the functions e ix D cos.x/ C i sin.x/ are essentially1
1 On a periodic domain. For the Cauchy problem these functions are not L2 functions and so strictly speaking
are not called eigenfunctions, but this is unimportant for our purposes.

i

i

i

i

i

i

i

318

“rjlfdm”
2007/6/1
page 318
i

Appendix E. Partial Differential Equations

@
eigenfunctions of the differentiation operator @x D @x
. Differentiating this function gives
a scalar multiple of the function, and hence simple differential equations (linear constant
coefficient ones, at least) are simplified and can be reduced to algebraic equations.
Fourier analysis is equally important in the study of finite difference methods for
linear PDEs for the same reason: these same functions are eigenfunctions of translation
invariant finite difference operators. This is exploited in Sections 9.6 and 10.5, where von
Neumann stability analysis of finite difference methods is discussed. An understanding of
Fourier analysis of PDEs is also required in Section 10.9, where finite difference methods
are analyzed by studying “modified equations.”

E.3.1 Fourier transforms
Recall that a function v.x/ is in the space L2 if it has a finite 2-norm, defined by
Z 1
1=2
kvk2 D
jv.x/j2 dx
:
1
2

If v 2 L , then we can define its Fourier transform v./
O
by
Z 1
1
v./
O
D p
v.x/e ix dx:
2 1

(E.18)

The function v./
O
is also in L2 and in fact it has exactly the same 2-norm as v,
kvk
O 2 D kvk2:

(E.19)

This is known as Parseval’s relation.
We can express the original function v.x/ as a linear combination of the set of functions e ix for different values of , which together form a basis for the infinite dimensional
function space L2 . The Fourier transform v./
O
gives the coefficients in the expression
Z 1
1
ix
v.x/ D p
v./e
O
d;
(E.20)
2 1
which is known as the inverse Fourier transform. This is analogous to writing a vector as a
linear combination of basis vectors.

E.3.2 The advection equation
We already know the solution (E.12) to the advection equation (E.6), but to illustrate the
role of Fourier analysis we will solve the advection equation ut C aux D 0 using Fourier
transforms. We will transform in x only and denote the transform of u.x; t/ (a function of
x at each fixed t) by u.;
O t/:
Z 1
1
u.;
O t/ D p
u.x; t/e ix dx:
(E.21)
2 1
Then
Z 1
1
u.x; t/ D p
u.;
O t/e ix d
(E.22)
2 1
and differentiating this with respect to t and x gives

i

i

i

i

i

i

i

E.3. Fourier analysis of linear partial differential equations

“rjlfdm”
2007/6/1
page 319
i

319

Z 1
1
ut .x; t/ D p
uO t .; t/e ix dx;
2 1
Z 1
1
u.;
O t/ie ix dx:
ux .x; t/ D p
2 1
From this we see that the Fourier transform of ut .x; t/ is uO t .; t/ and the Fourier transform
of ux .x; t/ is i u.;
O t/. Fourier transforming the advection equation by computing
Z 1
1
p
.ut C aux /e ix dx D 0
2 1
thus gives
uO t .; t/ C ai u.;
O t/ D 0
or
uO t D iau:
O
This is a time-dependent ODE for the evolution of u.;
O t/ in time. There are two important
points to notice:
 Since differentiation with respect to x has become multiplication by i after Fourier
transforming, the original PDE involving derivatives with respect to x and t has
become an ODE in t alone.
 The ODEs for different values of  are decoupled from one another. We have to solve
an infinite number of ODEs, one for each value of , but they are decoupled scalar
equations rather than a coupled system.
It is easy to solve these ODEs. We need initial data u.;
O 0/ at time t D 0 for each
value of , but this comes from Fourier transforming the initial data u.x; 0/ D .x/,
Z 1
1
u.;
O 0/ D ./
O
Dp
.x/e ix dx:
2 1
Solving the ODEs then gives
u.;
O t/ D e iat ./:
O

(E.23)

We can now Fourier transform back using (E.22) to get the desired solution u.x; t/:
Z 1
1
ix
u.x; t/ D p
e iat ./e
O
d
2 1
Z 1
1
i .x at /
Dp
./e
O
d
2 1
D .x at/:
This last equality comes from noting that we are simply evaluating the inverse Fourier
transform of O at the point x at. We see that we have recovered the standard solution
(E.12) of the advection equation in this manner.
We can also calculate the “Green’s function” for the advection equation, the solution
to ut C aux D 0 with special initial data .x/ D ı.x x/.
N The solution is clearly

i

i

i

i

i

i

i

320

“rjlfdm”
2007/6/1
page 320
i

Appendix E. Partial Differential Equations
G.x; tI x/
N D ı.x

xN

at/:

(E.24)

The general solution for arbitrary .x/ can be written as a linear combination of these
Green’s functions, weighted by the data:
Z 1
u.x; t/ D
.x/G.x;
N
tI x/
N d xN
1
Z 1
(E.25)
D
.x/ı.x
N
xN at/ d xN
1

D .x

at/:

E.3.3 The heat equation
Now consider the heat equation,
ut D uxx :

(E.26)

2

2

Since the Fourier transform of uxx .x; t/ is .i/ u.;
O t/ D  u.;
O t/, Fourier transforming
(E.26) gives the ODE
uO t .; t/ D  2 u.;
O t/:
(E.27)
Again we have initial data u.;
O 0/ D ./
O
from the given initial data on u. Now solving the
ODE gives
2
u.;
O t/ D e  t ./:
O
(E.28)
Note that this has a very different character than (E.23), the Fourier transform obtained from
the advection equation. For the advection equation, u.;
O t/ D e ia t ./
O
and ju.;
O t/j D
j./j
O
for all t. Each Fourier component maintains its original amplitude and is modified
only in phase, leading to a traveling wave behavior in the solution.
For the heat equation, however, ju.;
O t/j decays in time exponentially fast. The decay
rate depends on , the diffusion coefficient, and also on , the wave number. Highly oscillatory components (with  2 large) decay much faster than those with low wave numbers.
This results in a smoothing of the solution as time evolves. (See Figure 9.3.)
The fact that the solution contains components that decay at very different rates leads
us to expect numerical difficulties with stiffness, similar to those discussed for ODEs in
Chapter 8. In Section 9.4 we will see that this is indeed the case and that implicit methods
must generally be used to efficiently solve the heat equation.
A single Fourier mode decaying exponentially in time is one special solution to the
heat equation. Another class of special solutions that is useful to know about arises from
Gaussian initial data. The Fourier transform of a Gaussian is another Gaussian. Take
.x/ D e ˇx

2

(E.29)

for some ˇ. Then
Z 1
1
2
./
O
D p
e ˇx e ix dx
2 1
1
2
D p e  =4ˇ :
2ˇ

i

i

(E.30)

i

i

i

i

i

E.3. Fourier analysis of linear partial differential equations

“rjlfdm”
2007/6/1
page 321
i

321

Then (E.22) combined with (E.28) gives the solution
Z 1
1
2
ix
u.x; t/ D p
e  t ./e
O
d
2 1
Z 1
1
2
D p
e  .t C1=4ˇ/ e ix d:
2 ˇ 1

(E.31)

2

This is just the inverse Fourier transform of another Gaussian, with e  =4C in place of
2
e  =4ˇ , where C D 1=.4t C 1=ˇ/, and so
s
C C x2
u.x; t/ D
e
ˇ
(E.32)
1
x 2 =.4t C1=ˇ/
D p
:
e
4ˇt C 1
As t increases this Gaussian becomes more spread out and the magnitude decreases, as
we expect from diffusion. You can check that (E.32) solves the heat equation directly by
differentiating.
Note what happens if we shift the initial data to a different location,
2

N
.x/ D e ˇ.x x/
:

(E.33)

1
N 2 =.4t C1=ˇ/
u.x; t/ D p
:
e .x x/
4ˇt C 1

(E.34)

Then the solution simply shifts too,

As a special
p case we can find the Green’s function for the heat equation. Scale the
data (E.33) by ˇ= so that it has integral equal to 1 and represents a smeared out version
of the delta function, setting
r
ˇ ˇ.x x/
N 2
vˇ .x; 0I x/
N D
:
(E.35)
e

The solution to (E.26) with this data is then
1
N 2 =.4t C1=ˇ/
vˇ .x; tI tN/ D p
:
e .x x/
4t C =ˇ

(E.36)

Now let ˇ ! 0 so the initial data approaches a delta function. The solution vˇ .x; tI tN/ then
approaches the Green’s function for (E.26),
G.x; tI x/
N Dp

1
4t

2

N =.4t /
:
e .x x/

(E.37)

Delta function initial data spreads out into a decaying Gaussian. Note that initial data
concentrated at a single point (an idealization of a very tiny drop of ink in water, say)

i

i

i

i

i

i

i

322

“rjlfdm”
2007/6/1
page 322
i

Appendix E. Partial Differential Equations

spreads out immediately to have a nonzero value for all x. More generally if we look at the
solution for general initial data by integrating .x/
N against the Green’s function, we see that
the data at each xN immediately have an effect everywhere. Thus information propagates
infinitely quickly in the heat equation. This is quite different from the advection equation,
where the data at xN affect the solution at only one point xN C at at a later time t.
Of course physically information cannot propagate at infinite speed, and the discrepancy with the behavior of the heat equation simply shows that the heat equation is only a
model of reality, and one that is not exactly correct. But note that away from the point xN
the effect decays very rapidly and so this is often a very accurate model of reality.

E.3.4 The backward heat equation
Note that the diffusion coefficient  is required to be positive (because heat flows from
warm to cool regions, not the other way around). Mathematically we could consider trying
to solve the equation (E.26) with  < 0 but this equation turns out to be ill-posed2 One
way to interpret this physically is to view it as solving the heat equation with coefficient
 > 0 backward in time, starting at some final heat distribution and working backward
to the heat distribution at earlier times. Intuitively this can be seen to be ill-posed because many different sets of initial data can give rise to very similar solutions at later times
since any high-frequency components in initial data for the heat equation are very rapidly
smoothed out. We can formally solve the backward heat equation in Fourier space with the
expression (E.28), but for  < 0 each Fourier mode is growing exponentially in time instead of decaying. Exponential growth in itself doesn’t make the problem ill posed—many
well-posed equations have exponentially growing solutions—but the problem with (E.28)
is that the growth rate depends on the wave number  and increases without bound with .
We can make an infinitesimal high-frequency perturbation to the initial data that will make
an order 1 change in the solution at some fixed time t. Hence the solution to the backward
heat equation does not depend continuously on the data.

E.3.5 More general parabolic equations
Consider a second order parabolic equation ut D Lu in N space dimensions as defined in
Section E.1.3. For simplicity, just consider the second order part of the system, so
N
X

LD

Aj k
j ;kD1

@2
;
@xj @xk

(E.38)

where the N N coefficient matrix A is symmetric positive definite. Let  D .1 ; : : : ; N /
be a wave number vector, one for each space dimension, so that a general Fourier mode has
the form e ix , where x D .x1 ; : : : ; xN /. Let u.;
O t/ be the Fourier transform of u.x; t/
in all space dimensions, defined by
Z
1
u.;
O t/ D p
(E.39)
u.x; t/e ix dx;
2
2 A problem is said to be well posed (in the sense of Hadamard) if it has a unique solution for every valid set
of data and if the solution depends continuously on the data.

i

i

i

i

i

i

i

E.3. Fourier analysis of linear partial differential equations

“rjlfdm”
2007/6/1
page 323
i

323

where the integral is now over all of N -dimensional space. Then it can be verified that the
parabolic equation ut D Lu transforms to
O t/:
uO t .; t/ D  T A u.;

(E.40)

The requirement that A be positive definite is just what is needed to ensure that all Fourier
modes decay, giving a well-posed problem. If  T A < 0 for some vector , then it is also
negative for any scalar multiple ˛ of this wave vector, and there would be exponential
growth of some Fourier modes with arbitrarily large growth rate ˛ 2 T A. As observed for
the backward heat equation, this would give an ill-posed problem. (For the heat equation
with N D 1, the matrix A is just the scalar coefficient .)

E.3.6 Dispersive waves
Now consider the equation
ut D uxxx :

(E.41)

Fourier transforming now leads to the ODE
uO t .; t/ D i 3 u.;
O t/;
so

3

u.;
O t/ D e i t ./:
O
This has a character similar to advection problems in that ju.;
O t/j D j./j
O
for all time and
each Fourier component maintains its original amplitude. However, when we recombine
with the inverse Fourier transform we obtain
Z 1
1
i .x  2 t /
u.x; t/ D p
./e
O
d;
(E.42)
2 1
which shows that the Fourier component with wave number  is propagating with velocity
 2 . In the advection equation all Fourier components propagate with the same speed a, and
hence the shape of the initial data is preserved with time. The solution is the initial data
shifted over a distance at.
With (E.41), the shape of the initial data in general will not be preserved, unless the
data is simply a single Fourier mode. This behavior is called dispersive since the Fourier
components disperse relative to one another. Smooth data typically lead to oscillatory
solutions since the cancellation of high wave number modes that smoothness depends on
will be lost as these modes shift relative to one another. See, for example, Whitham [102]
for an extensive discussion of dispersive waves.
Extending this analysis to an equation of the form
ut C aux C buxxx D 0;

(E.43)

we find that the solution can be written as
Z 1
1
i .x .a b 2 /t /
u.x; t/ D p
./e
O
d;
2 1

i

i

i

i

i

i

i

324

“rjlfdm”
2007/6/1
page 324
i

Appendix E. Partial Differential Equations

where ./
O
is the Fourier transform of the initial data .x/. Each Fourier mode e ix propagates at velocity a b 2, called the phase velocity of this wave number. In general the
initial data .x/ is a linear combination of infinitely many different Fourier modes. For
b ¤ 0 these modes propagate at different speeds relative to one another. Their peaks and
troughs will be shifted relative to other modes and they will no longer add up to a shifted
version of the original data. The waves are called dispersive since the different modes
do not move in tandem. Moreover, we will see below that the “energy” associated with
different wave numbers also disperses.

E.3.7 Even- versus odd-order derivatives
Note that odd-order derivatives @x ; @3x ; : : : (as in the advection equation or the dispersive
equation (E.41)) have pure imaginary eigenvalues i; i 3 ; : : :, which results in Fourier
components that propagate with their magnitude preserved. Even-order derivatives, such
as the @2x in the heat equation, have real eigenvalues (  2 for the heat equation), which
results in exponential decay of the eigencomponents. Another such equation is
ut D uxxxx ;
4t

in which case u.;
O t/ D e
./.
O
Solutions to this equation behave much like solutions
to the heat equation but with even more rapid damping of oscillatory data.
Another interesting example is
ut D

uxx

for which
u.;
O t/ D e .

uxxxx ;
2

 4 /t

./:
O

(E.44)
(E.45)

Note that the uxx term has the “wrong” sign—it looks like a backward heat equation and
there is exponential growth of some wave numbers. But for jj > 1 the fourth order
diffusion dominates and u.;
O t/ ! 0 exponentially fast. For all  we have ju.;
O t/j 
e t =4j./j
O
(since  2  4  1=4 for all ) and the equation is well posed.
The Kuramoto–Sivashinsky equation (11.13) involves terms of this form, and the
exponential growth of some wave numbers leads to chaotic behavior and interesting pattern
formation.

E.3.8 The Schrödinger equation
The discussion of the previous section supposed that u.x; t/ is a real-valued function. The
vacuum Schrödinger equation for a complex wave function .x; t/ has the form (dropping
some physical constants)
i t .x; t/ D
(E.46)
xx .x; t/:
This involves a second derivative, but note the crucial fact that
transforming thus gives
i O t .; t/ D  2 O .; t/;
so

i

i

t is multiplied by i. Fourier

O .; t/ D e i 2t O .; 0/

i

i

i

i

i

E.3. Fourier analysis of linear partial differential equations
and

1
.x; t/ D p
2

“rjlfdm”
2007/6/1
page 325
i

325

Z 1
O .; 0/e i .x .a /t / d:
1

Hence the Schrödinger equation has dispersive wavelike solutions in spite of the even-order
derivative.

E.3.9 The dispersion relation
Consider a general real-valued PDE of the form
ut C a1 ux C a3 uxxx C a5 uxxxxx C    D 0

(E.47)

that contains only odd-order derivative in x. The Fourier transform u.;
O t/ satisfies
uO t .; t/ C a1 i u.;
O t/

a3 i 3 u.;
O t/ C a5 i 5 u.;
O t/ C    D 0;

and hence
u.;
O t/ D e i!t ./;
O
where
a3  3 C a5  5

! D !./ D a1 

 :

(E.48)

i.x !./t /
./e
O
d:

(E.49)

The solution can thus be written as
1
u.x; t/ D p
2

Z 1
1

The relation (E.48) between  and ! is called the dispersion relation for the PDE. Once
we’ve gone through this full Fourier analysis a couple times we realize that since the different wave numbers  decouple, the dispersion relation for a linear PDE can be found simply
by substituting a single Fourier mode of the form
u.x; t/ D e i!t e ix

(E.50)

into the PDE and canceling the common terms to find the relation between ! and . This
is similar to what is done when applying von Neumann analysis for analyzing finite difference methods (see Section 9.6). In fact, there is a close relation between determining
the dispersion relation and doing von Neumann analysis, and the dispersion relation for a
finite difference method can be defined by an approach similar to von Neumann analysis
by setting Ujn D e i! nk e ij h , i.e., using e i!k in place of g.
Note that this same analysis can be done for equations that involve even-order derivatives, such as
ut C a1 ux C a2 uxx C a3 uxxx C a4 uxxxx C    D 0;
but then we find that
!./ D a1  C ia2  2

i

i

a3  3

ia4  4

 :

i

i

i

i

i

326

“rjlfdm”
2007/6/1
page 326
i

Appendix E. Partial Differential Equations

The even-order derivatives give imaginary terms in !./ so that
e i!t D e .a2 

2

a4  4 C /t

3

e i.a1  a3  C /t :

The first term gives exponential growth or decay, as we expect from Section E.3.3, rather
than dispersive behavior. For this reason we call the PDE (purely) dispersive only if !./
is real for all  2 R. Informally we also speak of an equation like ut D uxx C uxxx as
having both a diffusive and a dispersive term.
In the purely dispersive case (E.47) the single Fourier mode (E.50) can be written as
u.x; t/ D e i .x .!=/t /
and so a pure mode of this form propagates at velocity !=. This is called the phase velocity
for this wave number,
!./
cp ./ D
:
(E.51)

Most physical problems have data .x/ that is not simply sinusoidal for all x 2 . 1; 1/
but instead is concentrated in some restricted region, e.g., a Gaussian pulse as in (E.29),
2

.x/ D e ˇx :

(E.52)

The Fourier transform of this function is a Gaussian in , (E.30),
1
2
./
O
D p e  =4ˇ :
2ˇ

(E.53)

Note that for ˇ small, .x/ is a broad and smooth Gaussian with a Fourier transform that
is sharply peaked near  D 0. In this case .x/ consists primarily of low wave number
smooth components. For ˇ large .x/ is sharply peaked while the transform is broad.
More high wave number components are needed to represent the rapid spatial variation of
.x/ in this case.
If we solve the dispersive equation with data of this form, then the different modes
propagate at different phase velocities and will no longer sum to a Gaussian, and the solution evolves as shown in Figure E.1, forming “dispersive ripples.” Note that for large
times it is apparent that the wave length of the ripples is changing through this wave and
that the energy associated with the low wave numbers is apparently moving faster than the
energy associated with larger wave numbers. The propagation velocity of this energy is
not, however, the phase velocity cp ./. Instead it is given by the group velocity
cg ./ D

d!./
:
d

(E.54)

For the advection equation ut Caux D 0 the dispersion relation is !./ D a and the group
velocity agrees with the phase velocity (since all waves propagate at the same velocity a),
but more generally the two do not agree. For the dispersive equation (E.43), !./ D
a b 3 and we find that
cg ./ D a 3b 2;
whereas
cp ./ D a

i

i

b 2:

i

i

i

i

i

E.3. Fourier analysis of linear partial differential equations

“rjlfdm”
2007/6/1
page 327
i

327

1

0.5

0

−0.5

−1
−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

1

0.5

0

−0.5

−1

Figure E.1. Gaussian initial data propagating with dispersion.

E.3.10 Wave packets
The notion and importance of group velocity is easiest to appreciate by considering a “wave
packet” with data of the form
2
.x/ D e i0x e ˇx
(E.55)
or the real part of such a wave,
2

.x/ D cos.0 x/e ˇx :

(E.56)

This is a single Fourier mode modulated by a Gaussian, as shown in Figure E.2.
The Fourier transform of (E.55) is
1
2
./
O
D p e . 0/ =4ˇ ;
2ˇ

(E.57)

a Gaussian centered about  0. If the packet is fairly broad (ˇ small), then the Fourier
transform is concentrated near  D 0 and hence the propagation properties of the wave
packet are well approximated in terms of the phase velocity cp ./ and the group velocity
cg ./. The wave crests propagate at the speed cp .0 /, while the envelope of the packet
propagates at the group velocity cg .0 /.
To get some idea of why the packet propagates at the group velocity, consider the
expression (E.49),
Z 1
1
i.x !./t /
u.x; t/ D p
./e
O
d:
2 1

i

i

i

i

i

i

i

328

“rjlfdm”
2007/6/1
page 328
i

Appendix E. Partial Differential Equations
time = 0

time = 0.4

1

1

0.5

0.5

0

0

−0.5

−0.5

−1

−1
−3

−2

−1

0

1

2

3

−3

−2

−1

time = 0.8

0

1

2

3

1

2

3

time = 1.2

1

1

0.5

0.5

0

0

−0.5

−0.5

−1

−1
−3

−2

−1

0

1

2

3

−3

−2

−1

0

Figure E.2. The oscillatory wave packet satisfies the dispersive equation ut C
aux C buxxx D 0. Also shown is a black dot attached to one wave crest, translating at
the phase velocity cp .0 /, and a Gaussian that is translating at the group velocity cg .0 /.
Shown for a case in which cg .0 / < 0 < cp .0 /.
For a concentrated packet, we expect u.x; t/ to be very close to zero for most x, except
near some point ct, where c is the propagation velocity of the packet. To estimate c we
will ask where this integral could give something nonzero. At each fixed x the integral is a
Gaussian in  (the function ./)
O
multiplied by an oscillatory function of  (the exponential
factor). Integrating this product will given essentially zero at a particular x provided the
oscillatory part is oscillating rapidly enough in  that it averages out to zero, although it is
modulated by the Gaussian ./.
O
This happens provided the function x !./t appearing
as the phase in the exponential is rapidly varying as a function of  at this x. Conversely,
we expect the integral to be significantly different from zero only near points x where this
phase function is stationary, i.e., where
d
.x
d

!./t/ D 0:

This occurs at
x D ! 0 ./t;
showing that the wave packet propagates at the group velocity cg D ! 0 ./. This approach
to studying oscillatory integrals is called the “method of stationary phase” and is useful in
other applications as well. See, for example, [55], [102] for more on dispersive waves.

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 329
i

Bibliography
[1] A. Abdulle. Fourth order Chebyshev methods with recurrence relation. SIAM J. Sci.
Comput., 23:2041–2054, 2002. (cited on 176 )
[2] A. Abdulle and A. A. Medovikov. Second order Chebyshev methods based on orthogonal polynomials. Numer. Math., 90:1–18, 2001. (cited on 176 )
[3] L. M. Adams, R. J. LeVeque, and D. M. Young. Analysis of the SOR iteration for
the 9-point Laplacian. SIAM J. Numer. Anal., 25:1156–1180, 1988. (cited on 77 )
[4] U. Ascher, R. Mattheij, and R. Russell. Numerical Solution of Boundary Value
Problems for Ordinary Differential Equations. Prentice–Hall, Englewood Cliffs,
NJ, 1988. (cited on 38, 52, 55 )
[5] U. M. Ascher and L. R. Petzold. Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations. SIAM, Philadelphia, 1998. (cited on
113, 129 )
[6] U. M. Ascher, S. J. Ruuth, and R. J. Spiteri. Implicit-explicit Runge-Kutta methods
for time-dependent partial differential equations. Appl. Numer. Math., 25:151–167,
1997. (cited on 240 )
[7] U. M. Ascher, S. J. Ruuth, and B. T. R. Wetton. Implicit-explicit methods for timedependent partial differential equations. SIAM J. Numer. Anal., 32:797–823, 1995.
(cited on 240 )
[8] G. Beylkin, J. M. Keiser, and L. Vozovoi. A new class of time discretization schemes
for the solution of nonlinear pdes. J. Comput. Phys., 147:362–387, 1998. (cited on
241 )
[9] A. Bourlioux, A. T. Layton, and M. L. Minion. High-order multi-implicit spectral deferred correction methods for problems of reactive flow. J. Comput. Phys.,
189:651–675, 2003. (cited on 239 )
[10] J. P. Boyd. Chebyshev and Fourier Spectral Methods. Dover, New York, 2001.
(cited on 58 )
[11] W. L. Briggs, V. Emden Henson, and S. F. McCormick. A Multigrid Tutorial,
2nd ed. SIAM, Philadelphia, 2000. (cited on 103 )
329

i

i

i

i

i

i

i

330

“rjlfdm”
2007/6/1
page 330
i

Bibliography

[12] J. C. Butcher, Numerical Methods for Ordinary Differential Equations, John Wiley,
Chichester, UK, 2003. (cited on 113 )
[13] J. C. Butcher. The Numerical Analysis of Ordinary Differential Equations: RungeKutta and General Linear Methods. John Wiley, Chichester, UK, 1987. (cited on
128, 171 )
[14] C. Canuto, M. Y. Hussaini, A. Quarteroni, and T. A. Zang. Spectral Methods in
Fluid Dynamics. Springer, New York, 1988. (cited on 58 )
[15] E. A. Coddington and N. Levinson. Theory of Ordinary Differential Equations.
McGraw–Hill, New York, 1955. (cited on 116 )
[16] S. D. Conte and C. de Boor. Elementary Numerical Analysis. McGraw–Hill, New
York, 1980. (cited on 264 )
[17] R. Courant, K. O. Friedrichs, and H. Lewy. Über die partiellen Differenzengleichungen der mathematischen Physik. Math. Ann., 100:32–74, 1928. (cited on
216 )
[18] R. Courant, K. O. Friedrichs, and H. Lewy. On the partial difference equations of
mathematical physics. IBM Journal, 11:215–234, 1967. (cited on 216 )
[19] S. M. Cox and P. C. Matthews. Exponential time differencing for stiff systems. J.
Comput. Phys., 176:430–455, 2002. (cited on 241, 242 )
[20] C. F. Curtiss and J. O. Hirschfelder. Integration of stiff equations. Proc. Nat. Acad.
Sci. USA, 38:235–243, 1952. (cited on 173 )
[21] G. Dahlquist. A special stability problem for linear multistep methods. BIT, 3:27–
43, 1963. (cited on 171 )
[22] G. Dahlquist, Convergence and stability in the numerical integration of ordinary
differential equations, Math. Scand., 4:33–53, 1956. (cited on 147 )
[23] G. Dahlquist and R. LeVeque. Linear difference equations and matrix theorems.
Lecture Notes, Royal Institute of Technology (KTH), Stockholm, http://www.
amath.washington.edu/rjl/pubs/kth81, (1981). (cited on 309, 310 )
[24] T. A. Davis. Direct Methods for Sparse Linear Systems. SIAM, Philadelphia, 2006.
(cited on 68 )
[25] J. R. Dormand and P. J. Prince. A family of embedded Runge-Kutta formulas. J.
Comput. Appl. Math., 6:19–26, 1980. (cited on 130 )
[26] J. Douglas and H. H. Rachford. On the numerical solution of heat conduction problems in two and three space variables. Trans. AMS, 82:421–439, 1956. (cited on
199 )
[27] D. R. Durran. Numerical Methods for Wave Equations in Geophysical Fluid Dynamics. Springer, New York, 1999. (cited on xv )

i

i

i

i

i

i

i

Bibliography

“rjlfdm”
2007/6/1
page 331
i

331

[28] A. Dutt, L. Greengard, and V. Rokhlin. Spectral deferred correction methods for
ordinary differential equations. BIT, 40:241–266, 2000. (cited on 58, 239 )
[29] B. Fornberg. A Practical Guide to Pseudospectral Methods. Cambridge University
Press, London, 1996. (cited on 58 )
[30] B. Fornberg. Calculation of weights in finite difference formulas. SIAM Rev.,
40:685–691, 1998. (cited on 11 )
[31] F. G. Friedlander and M. Joshi. Introduction to the Theory of Distributions. Cambridge University Press, London, 1998. (cited on 24 )
[32] E. Gallopoulos and Y. Saad. Efficient solution of parabolic equations by Krylov
approximation methods. SIAM J. Sci. Statist. Comput., 13:1236–1264, 1992. (cited
on 242 )
[33] C. W. Gear. Numerical Initial Value Problems in Ordinary Differential Equations.
Prentice–Hall, Englewood Cliffs, NJ, 1971. (cited on 113, 173 )
[34] A. George and J. W. H. Liu. Computer Solution of Large Sparse Positive-Definite
Systems. Prentice–Hall, Englewood Cliffs, NJ, 1981. (cited on 68 )
[35] G. H. Golub and C. F. Van Loan. Matrix Computations, 3rd ed. Johns Hopkins
University Press, Baltimore, 1996. (cited on 67, 250, 271 )
[36] G. H. Golub and D. P. O’Leary. Some history of the conjugate gradient and Lanczos
algorithms: 1948–1976. SIAM Rev., 31:50–102, 1989. (cited on 86 )
[37] G. H. Golub and J. M. Ortega. Scientific Computing and Differential Equations: An
Introduction to Numerical Methods. Academic Press, New York, 1992. (cited on
76 )
[38] D. Gottlieb and S. A. Orszag. Numerical Analysis of Spectral Methods. CBMS-NSF
Regional Conference Series in Applied Mathematics, 26, SIAM, Philadelphia, 1977.
(cited on 58 )
[39] A. Greenbaum. Iterative Methods for Solving Linear Systems. SIAM, Philadelphia,
1997. (cited on 78, 87, 88, 99, 100 )
[40] B. Gustafsson, H.-O. Kreiss, and J. Oliger. Time Dependent Problems and Difference
Methods. John Wiley, New York, 1995. (cited on xv, 191, 214, 228 )
[41] W. Hackbusch. Multigrid Methods and Applications. Springer-Verlag, Berlin, 1985.
(cited on 103 )
[42] L. A. Hageman and D. M. Young. Applied Iterative Methods. Academic Press, New
York, 1981. (cited on 76 )
[43] E. Hairer, S. P. Nørsett, and G. Wanner. Solving Ordinary Differential Equations I.
Nonstiff Problems. Springer-Verlag, Berlin, Heidelberg, 1987. (cited on 113, 128,
129, 148 )

i

i

i

i

i

i

i

332

“rjlfdm”
2007/6/1
page 332
i

Bibliography

[44] E. Hairer, S. P. Nørsett, and G. Wanner. Solving Ordinary Differential Equations II.
Stiff and Differential-Algebraic Problems. Springer-Verlag, New York, 1993. (cited
on 113, 128, 166, 171, 173 )
[45] P. Henrici. Discrete Variable Methods in Ordinary Differential Equations. John
Wiley, New York, 1962. (cited on 113 )
[46] M. R. Hestenes and E. Stiefel. Method of conjugate gradients for solving linear
equations. J. Res. Nat. Bureau Standards, 49:409–436, 1952. (cited on 86 )
[47] D. J. Higham and L. N. Trefethen. Siffness of ODEs. BIT, 33:285–303, 1993. (cited
on 156, 228 )
[48] M. Hochbruck, C. Lubich, and H. Selhofer. Exponential integrators for large systems
of differential equations. SIAM J. Sci. Comput., 19:1552–1574, 1998. (cited on 241,
242 )
[49] A. Iserles. Numerical Analysis of Differential Equations. Cambridge University
Press, Cambridge, UK, 1996. (cited on xv )
[50] A. Iserles. Think globally, act locally: Solving highly-oscillatory ordinary differential equations. Appl. Numer. Math., 43:145–160, 2002. (cited on 169 )
[51] A. Iserles and S. P. Norsett. Order Stars. Chapman and Hall, London, 1991. (cited
on 166, 171 )
[52] D. C. Jesperson. Multigrid methods for partial differential equations. In Studies in
Numerical Analysis, G. H. Golub, ed. MAA Studies in Mathematics, Vol. 24, 1984,
pages 270–317. (cited on 103 )
[53] A.-K. Kassam and L. N. Trefethen. Fourth-order time-stepping for stiff PDEs. SIAM
J. Sci. Comput., 26:1214–1233, 2005. (cited on 241, 242, 286 )
[54] H. B. Keller. Numerical Solution of Two Point Boundary Value Problems. SIAM,
Philadelphia, 1976. (cited on 38, 43, 55 )
[55] J. Kevorkian. Partial Differential Equations. Wadsworth & Brooks/Cole, Pacific
Corove, CA, 1990. (cited on 14, 43, 216, 311, 314, 328 )
[56] J. Kevorkian and J. D. Cole. Perturbation Methods in Applied Mathematics.
Springer, New York, 1981. (cited on 43 )
[57] D. A. Knoll and D. E. Keyes. Jacobian-free Newton-Krylov methods: A survey of
approaches and applications. J. Comput. Phys., 193:357–397, 2004. (cited on 102 )
[58] D. Kröner. Numerical Schemes for Conservation Laws. Wiley-Teubner, New York,
1997. (cited on 201 )
[59] J. D. Lambert. Computational Methods in Ordinary Differential Equations. John
Wiley, New York, 1973. (cited on 113, 173 )

i

i

i

i

i

i

i

Bibliography

“rjlfdm”
2007/6/1
page 333
i

333

[60] J. D. Lambert. Numerical Methods for Ordinary Differential Systems: The initial
value Problem. John Wiley, Chichester, 1991. (cited on 113 )
[61] P. D. Lax. Hyperbolic Systems of Conservation Laws and the Mathematical Theory
of Shock Waves. CBMS-NSF Regional Conference Series in Applied Mathematics,
SIAM, Philadelphia, 11, 1973. (cited on 314 )
[62] R. Lehoucq, K. Maschhoff, D. Sorensen, and C. Yang. ARPACK software. http://
www.caam.rice.edu/software/ARPACK/(1997). (cited on 100 )
[63] S. K. Lele. Compact difference schemes with spectral-like resolution. J. Comput.
Phys., 103:16–42, 1992. (cited on 230 )
[64] R. J. LeVeque. CLAWPACK software, 2006. http://www.amath.washington.edu/claw. (cited on 201 )
[65] R. J. LeVeque. Intermediate boundary conditions for time-split methods applied to
hyperbolic partial differential equations. Math. Comput., 47:37–54, 1986. (cited on
239 )
[66] R. J. LeVeque. Finite Volume Methods for Hyperbolic Problems. Cambridge University Press, London, 2002. (cited on 201, 214, 216, 226, 230, 231, 314 )
[67] R. J. LeVeque and L. N. Trefethen. Fourier analysis of the SOR iteration. IMA J.
Numer. Anal., 8:273–279, 1988. (cited on 76 )
[68] D. Levy and E. Tadmor. From semidiscrete to fully discrete: Stability of Runge–
Kutta schemes by the energy method. SIAM Rev., 40:40–73, 1998. (cited on 191 )
[69] A. A. Medovikov. High order explicit methods for parabolic equations. BIT, 38:372–
390, 1998. (cited on 176 )
[70] C. Moler and C. Van Loan. Nineteen dubious ways to compute the exponential of a
matrix. SIAM Rev., 20:801–836, 1978. (cited on 241 )
[71] C. Moler and C. Van Loan. Nineteen dubious ways to compute the exponential of a
matrix, twenty-five years later. SIAM Rev., 45:3–49, 2003. (cited on 241 )
[72] K. W. Morton and D. F. Mayers. Numerical Solution of Partial Differential Equations. Cambridge University Press, Cambridge, UK, 1994. (cited on xv )
[73] J. D. Murray. Mathematical Biology. Springer-Verlag, Berlin, Heidelberg, 1989.
(cited on 235 )
[74] L. R. Petzold, L. O. Jay, and J. Yen. Numerical solution of highly oscillatory ordinary
differential equations. Acta Numer., 6:437–484, 1997. (cited on 169 )
[75] R. D. Richtmyer and K. W. Morton. Difference Methods for Initial-Value Problems.
Wiley-Interscience, New York, 1967. (cited on 190, 191, 192, 214, 307 )
[76] J. W. Ruge and K. Stüben. Algebraic multigrid, in multigrid methods, S.F. McCormick, ed., SIAM, Philadelphia, 1987, pages 73–103. (cited on 110 )

i

i

i

i

i

i

i

334

“rjlfdm”
2007/6/1
page 334
i

Bibliography

[77] Y. Saad. Analysis of some Krylov subspace approximations to the matrix exponential operator. SIAM J. Numer. Anal., 29:209–228, 1992. (cited on 242 )
[78] L. F. Shampine and M. W. Reichelt. The MATLAB ODE suite. SIAM J. Sci. Comput.,
18:1–22, 1997. (cited on 129, 130, 135 )
[79] J. R. Shewchuk. An Introduction to the Conjugate Gradient Method Without
the Agonizing Pain. Technical report, available from http://www.cs.cmu.
edu/jrs/jrspapers.html (1994). (cited on 78 )
[80] B. P. Sommeijer, L. F. Shampine, and J. G. Verwer. RKC: An explicit solver for
parabolic PDEs. J. Comput. Appl. Math., 88:315–326, 1997. (cited on 176, 178 )
[81] M. N. Spijker. On a conjecture by LeVeque and Trefethen related to the Kreiss
matrix theorem. BIT, 31:551–555, 1991. (cited on 293 )
[82] G. W. Stewart. Introduction to Matrix Computations. Academic Press, New York,
1973. (cited on 67 )
[83] G. Strang. On the construction and comparison of difference schemes. SIAM J.
Numer. Anal., 5:506–517, 1968. (cited on 238 )
[84] J. C. Strikwerda. Finite Difference Schemes and Partial Differential Equations,
2nd ed. SIAM, Philadelphia, 2004. (cited on xv, 191, 192, 228 )
[85] J. C. Strikwerda and B. A. Wade. A survey of the Kreiss matrix theorem for power
bounded families of matrices and its extensions. In Linear Operators, Banach Center
Publ., 38, Polish Acad. Sci., Warsaw, 1997, pages 329–360. (cited on 307 )
[86] K. Stüben. A review of algebraic multigrid. J. Comput. Appl. Math., 128:281–309,
2001. (cited on 110 )
[87] Paul N. Swarztrauber. Fast Poisson Solvers. In Studies in Numerical Analysis,
volume 24, G. H. Golub, ed., Mathematical Association of America, Washington,
D.C., 1984, pp. 319–370. (cited on 68 )
[88] E. F. Toro. Riemann Solvers and Numerical Methods for Fluid Dynamics. SpringerVerlag, Berlin, Heidelberg, 1997. (cited on 201 )
[89] L. N. Trefethen. Group velocity in finite difference schemes. SIAM Rev., 24:113–
136, 1982. (cited on 228 )
[90] L. N. Trefethen. Spectral Methods in MATLAB. SIAM, Philadelphia, 2000. (cited
on 58, 264 )
[91] L. N. Trefethen and D. Bau, III, Numerical Linear Algebra. SIAM, Philadelphia,
1997. (cited on xiv, 67, 78, 88, 94, 100, 250 )
[92] L. N. Trefethen and M. Embree. Spectra and Pseudospectra. Princeton University
Press, Princeton, NJ, 2005. (cited on 74, 228, 231, 293, 299, 304, 307 )

i

i

i

i

i

i

i

Bibliography

“rjlfdm”
2007/6/1
page 335
i

335

[93] A. Tveito and R. Winther. Introduction to Partial Differential Equations: A Computational Approach. Springer, New York, 1998. (cited on xv )
[94] P. J. van der Houwen and B. P. Sommeijer. On the internal stability of explicit mstage Runge–Kutta methods for large values of m. Z. Angew. Math. Mech., 60:479–
485, 1980 (in German). (cited on 178 )
[95] H. A. Van der Vorst. Bi-CGSTAB: A fast and smoothly converging variant of BiCG for the solution of nonsymmetric linear systems. SIAM J. Sci. Statist. Comput.,
13:631–644, 1992. (cited on 100 )
[96] R. S. Varga. Matrix Iterative Analysis. Prentice–Hall, Englewood Cliffs, NJ, 1962.
(cited on 76 )
[97] J. G. Verwer. Explicit Runge-Kutta methods for parabolic differential equations.
Appl. Numer. Math., 22:359, 1996. (cited on 176, 177, 178 )
[98] G. Wanner. Order stars and stability. In The State of the Art in Numerical Analysis,
A. Iserles and M. J. D. Powell, eds., Clarendon Press Oxford, UK, 1987, pp. 451–
471. (cited on 166 )
[99] G. Wanner, E. Hairer, and S. P. Nørsett. Order stars and stability theorems. BIT,
18:475–489, 1978. (cited on 165 )
[100] R. Warming and B. Hyett. The modified equation approach to the stability and
accuracy analysis of finite-difference methods. J. Comput. Phys., 14:159–179, 1974.
(cited on 219 )
[101] P. Wesseling. An Introduction to Multigrid Methods. John Wiley, New York, 1992.
(cited on 103 )
[102] G. Whitham. Linear and Nonlinear Waves. Wiley-Interscience, New York, 1974.
(cited on 314, 323, 328 )
[103] J. H. Wilkinson. The Algebraic Eigenvalue Problem. Oxford University Press, London, 1965. (cited on 278 )
[104] T. Wright. Eigtool, 2002. http://web.comlab.ox.ac.uk/projects/
pseudospectra/eigtool/. (cited on 304 )
[105] D. M. Young. Iterative Methods for Solving Partial Differential Equations of Elliptic
Type. Ph.D. thesis, Harvard University, Cambridge, 1950. (cited on 76 )
[106] D. M. Young. Iterative Solution of Large Linear Systems. Academic Press, New
York, 1971. (cited on 76 )

i

i

i

i

i

i

i

i

i

i

“rjlfdm”
2007/6/1
page 336
i

i

i

i

i

“rjlfdm”
2007/6/1
page 337
i

Index
A-conjugate directions, 83
A-stability, 171
A(˛)-stability, 171
absolute error, 245
absolute stability, 149
for systems of equations, 156
absolute stability region, see stability regions
absorbing boundary conditions, 229
acoustic waves, 201, 313
Adams–Bashforth methods, 131
Adams–Moulton methods, 132
adaptive meshes, 51
advection equation, 201, 313, 315, 318
advection-diffusion equation, 234
advection-diffusion-reaction equation, 234,
239
amplification factor, 194, 212
Arnoldi process, 96
artificial boundary condition, 227

boundary value problems (BVPs), 13, 59
higher order methods, 52
nonlinear, 37
with variable coefficients, 35
Burgers’ equation, 234
BVP, see boundary value problems
Cauchy integral formula, 286
Cauchy problem, 315
centered approximation, 4
CFL (Courant–Friedrich–Lewy) condition,
215
CG, see conjugate-gradient algorithm
characteristic polynomial, 133, 269,
291
characteristic tracing, 214
characteristic variables, 224
Chebyshev
extreme points, 57, 231, 265
polynomials, 57, 265
roots, 267
spectral method, 57
Chebyshev polynomials, 92, 176
chemical kinetics, 157, 167
circulant matrix, 275
CLAWPACK, 201
collocation method, 55
compact methods, 230
companion matrix, 290, 296
condition number, 250
and preconditioners, 93
of discrete Laplacian, 64
of eigenvector matrix, 287
conjugate-gradient (CG) algorithm,
86
conservation laws, 234, 314
consistency

backward differentiation formula (BDF)
method, 121, 173
backward Euler method, 120
backward heat equation, 198, 322
Beam–Warming method, 212, 214
Bi-CGSTAB (bi-conjugate gradient stabilized) algorithm, 100
big-oh notation, 247
boundary conditions, 14
at outflow boundaries, 228
Dirichlet, 14
for fractional step methods, 239
for LOD method, 198
Neumann, 29
boundary layers, 43
boundary locus method, 162
337

i

i

i

i

i

i

i

338
of LMMs, 133
for boundary value problem, 19
for PDEs, 184
continuation method, 52
convergence, 189
for boundary value problem, 19
for initial value problem, 137
for PDEs, 184
Courant number, 216, 225
Crank–Nicolson method, 182
Dahlquist theorem, 147
Dahlquist’s second barrier, 171
defective matrix, 270
deferred corrections, 54, 65
delta function, 23
descent methods, 78
diagonalizable matrix, 271
diffusion, 316
diffusion equation, 181; see also heat equation
Dirichlet boundary conditions, 14, 60
discontinous coefficients, 231
dispersion relation, 221, 325
for leapfrog, 223
dispersive waves, 221, 323
domain of dependence, 216
Duhamel’s principle, 115, 138, 240
discrete form, 139
eigendecomposition, 271
eigenfunctions, 22
eigengridfunction, 192
eigenvalues, 269
of discrete Laplacian, 63
of Toeplitz matrix, 275
of tridiagonal system, 21
elliptic equations, 59, 311, 312
elliptic operator, 312
ETD, see exponential time differencing
method
Euler’s method, 120
convergence, 138
existence and uniqueness, 116
for boundary value problem, 32
for nonlinear problem, 40

i

i

“rjlfdm”
2007/6/1
page 338
i

Index
exponential time differencing method (ETD),
200, 231, 240
extrapolation methods, 53
fast Fourier transform (FFT), 58, 266
fast Poisson solvers, 68
fdcoeffF, 11, 56
fdcoeffV, 11, 53, 230
FFT, see fast Fourier transform
field of values, 300
finite volume methods, 231
Fourier analysis of PDEs, 317
Fourier transform, 318
of Gaussian, 320
fractional step methods, 237
function space norms, 250
fundamental theorem, 20
Gauss–Seidel iteration, 70
Gaussian elimination, 66, 67
Gershgorin theorem, 277
global error, 17, 18, 28
GMRES algorithm, 96
Green’s function
for advection equation, 319
for boundary value problem, 22, 27
for heat equation, 321
grid functions, errors in, 249
group velocity, 221, 228, 326
hat function, 23
heat equation, 13, 24, 181, 316, 320
with Gaussian data, 320
Hermitian matrix, 273
homotopy method, 52
hyperbolic equations, 201, 311, 313
hyperbolic systems, 224, 313
ill-posed problem, 29
ILU (incomplete LU) preconditioner, 96
implicit-explicit (IMEX) methods, 239
incomplete Cholesky, 96
initial boundary value problems, 226
initial value problem (IVP), 113
inner-product norms, 279, 281
interior layers, 46

i

i

i

i

i

Index
internal stability, 179
interpolation, 259
Newton form, 260
irreducible matrix, 279
iterative methods, 66, 69
IVP, see initial value problem
Jacobi iteration, 69, 103
underrelaxed, 106
Jacobian matrix, 39
Jordan block, 272, 304
exponential of, 294
powers of, 289
Jordan canonical form, 271
KdV (Korteweg–deVries) equation, 235
Kinetics, see Chemical kinetics
Kreiss constant, 292
for matrix exponential, 299
Kreiss matrix theorem, 190, 293, 304
Krylov space algorithms, 88, 96, 242
Kuramoto–Sivashinsky equation, 235, 324
L-stability, 171, 200
Lanczos iteration, 100
Laplace’s equation, 60
Laplacian, 313
5-point stencil, 60
9-point stencil, 64
Lax equivalence theorem, 184, 189
Lax–Friedrichs method, 206
Lax–Richtmyer stability, 189, 205
Lax–Wendroff method, 207
in two dimensions, 236
leapfrog method, 121, 205
Legendre polynomial, 231, 264
linear difference equations, 144, 290
linear multistep methods (LMMs), 131,
173
Lipschitz constant, 118
Lipschitz continuity, 116
little-oh notation, 247
LMM, see linear multistep methods
local truncation error (LTE), 5, 17, 63
for advection equation, 206
for Crank–Nicolson, 183

i

i

“rjlfdm”
2007/6/1
page 339
i

339
for initial value problem, 121
for LMMs, 132
for nonlinear problem, 41
locally one-dimensional (LOD) method,
197, 237
LOD, see locally one-dimensional
logarithmic norm, 300
LTE, see local truncation error
matrix exponential, 101, 293
matrix norms, 250
matrix powers, 285, 297
maximum principle, 36
method of lines, 184
for advection equation, 203
for heat equation, 184
for mixed equations, 235
method of undetermined coefficients, 7
midpoint method, 121
mixed equations, 233
modified equations, 218
MOL, see method of lines
multidimensional problems, 233
heat equation, 195
multigrid algorithm, 103
algebraic multigrid, 110
nested dissection, 68
Neumann boundary conditions, 29
Newton’s method, 38
Newton–Krylov methods, 101
nonnormal matrix, 169, 296
nonuniform grids, 49
norm equivalence, 249, 252
normal matrix, 274
norms, 16
numerical abscissa, 300
numerical boundary condition, 227
numerical range, 300
numerical solutions, errors in, 252
Nyström methods, 132
ODEs, see ordinary differential equations
one-sided approximations, 210
one-step error, 122

i

i

i

i

i

340
one-step methods, 121, 138
versus multistep methods, 130
order of accuracy, 4
order stars, 164, 171
ordering equations in linear system, 34,
61
ordinary differential equations (ODEs)
initial value problem, 113
linear, 114
orthogonal matrix, 274
orthogonal polynomials, 262
parabolic equations, 181, 311, 313, 322
multidimensional, 195
Parseval’s relation, 193, 318
partial differential equations (PDEs), 311
pendulum problem, 37
periodic boundary conditions, 203
phase velocity, 324
Poisson problem, 60, 312
polynomial interpolation, 8, 260
power bounded, 298, 305
Powers of matrices, 286
practical choice of step size, 161
preconditioners, 93
predictor-corrector methods, 135
principal root, 148
pseudoeigenvalues, 302
pseudospectra, 227, 302
pseudospectral methods, 55; see also spectral methods
reacting flow, 234
reaction-diffusion equations, 234, 317
red-black ordering, 62
region of absolute stability, see stability
region
region of relative stability, see order stars
relative error, 246
relative stability, 164
resolvent, 286, 291
root condition, 147, 153, 291
rowwise ordering, 62
Runge phenomenon, 56

i

i

“rjlfdm”
2007/6/1
page 340
i

Index
Runge–Kutta methods, 124
Runge–Kutta–Chebyshev methods, 175,
200
Schrödinger equation, 235, 324
self-adjoint problems, 36, 273
semidiscrete method, 184
shock waves, 231
similarity transformation, 270
Simpson’s rule, 126, 132
singular perturbation problems, 43
skew symmetric matrix, 274
SOR, see successive overrelaxtion
source terms, 317
sparse storage, 68
SPD, see symmetric positive definite matrix
spectral abscissa, 294
spectral deferred correction method, 58
spectral methods
for hyperbolic problems, 231
for parabolic equations, 200
for the boundary value problem, 55
Fourier, 242, 260
splitting methods, 237
stability
for advection equation, 203, 212
for boundary value problem, 18, 19
for PDEs versus ODEs, 191
for Poisson problem, 63
of Crank–Nicolson, 186
stability polynomial, 153
stability regions, 152
for Adams methods, 154
for BDF methods, 175
for LMMs, 153
for Runge–Kutta–Chebyshev methods, 176
plotting, 162
starting values
for multistep methods, 134
steady-state problems, 13, 59, 312
steady-state heat conduction, 13, 14, 35,
59
steepest descent algorithm, 79
stencils, 61, 182

i

i

i

i

i

Index
step size selection, 161
stiffness
of ODEs, 167
of the heat equation, 186
stiffness ratio, 169
Strang splitting, 238
strong stability, 190
successive overrelaxtion (SOR), 69
symmetric matrix, 273
symmetric positive definite (SPD) matrix,
273
T -norm, 281, 306
Taylor series methods, 123, 236
test problem, 138, 151
Toeplitz matrix, 275
TR-BDF2 method, 127, 175
trapezoidal method, 121
tridiagonal systems, 16, 276
inverse, 23, 27
symmetric, 36
truncation error, see local truncation error

i

i

“rjlfdm”
2007/6/1
page 341
i

341
underrelaxed Jacobi, 106
unitary matrix, 274
upwind methods, 210
V-cycle, 109
variable coefficients, 307
vector norms, 248
viscosity, 234
von Neumann analysis, 192, 212, 318
W-cycle, 109
wave packets, 327
zero-stability, 137, 153
of LMMs, 143, 147
of one-step methods, 148

i

i

