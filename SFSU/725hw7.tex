\documentclass[11pt]{amsart}

\usepackage{amsthm, amssymb,amsmath}
\usepackage{graphicx}

\theoremstyle{definition}  % Heading is bold, text is roman
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\newcommand{\ojo}[1]{{\sffamily\bfseries\boldmath[#1]}}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\newcommand{\nullspace}{\mathrm{null}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\trace}{\mathrm{tr}}
\newcommand{\TWOXTWO}[4]{\PAREN{ \begin{array}{c c} #1&#2 \\ #3 & #4 \end{array} }}


\oddsidemargin 0pt
\evensidemargin 0pt
\marginparwidth 0pt
\marginparsep 10pt
\topmargin -10pt
\headsep 10pt
\textheight 8.4in
\textwidth 7in

%\input{../header}


\begin{document}

%\homework{}{Homework VII}
\begin{center}
\Large{Math 725 -- Advanced Linear Algebra}\\
\large{Paul Carmody}\\
Assignment \#7 -- Due 10/25/23
\end{center}

\vskip 1.0 cm

\noindent
{\bf 1.} Let $V$ be an $n$-dimensional vector space over the field $F$. What is the characteristic and minimal polynomials of the zero operator on $V$ ?
What is the characteristic and minimal polynomials of the identity operator on $V$? \\
\begin{align*}
	\text{Let } T &\equiv 0  \\
	\det(xI-T) &= \det(xI) = x^n = 0
\end{align*}0 is the only eigenvalue.  The characteristic polynomial is $x^n$ and the minimum polynomial is $x$.
\begin{align*}
	\text{Let } T(v) &= v, \text{ for all } v \in V \\
	[T]_B^B &= I, \text{ for any basis }B\\
	\det(xI-T) &= \det(xI-I) = (x-1)^n \\
	\text{Notice } p(x) &= x-1 \implies p(T) = T-I \equiv 0
\end{align*}1 is the only eigenvalue.  The characteristic polynomial is $(x-1)^n$ and the minimum polynomial is $x-1$.\\

\vskip 0.1cm
\noindent
{\bf 2.}  Let $A, B \in \mathcal{M}_{n \times n}(F)$.  Prove that $AB$ and $BA$ have the same eigenvalues.\\
\\
If $F$ is algebraically closed, then every $A,B \in \mathcal{M}_{n\times n}(F)$ is similar to upper triangular matrices, $A', B'$, respectively. Let $P$ be the change of basis matrix between $A$ and $A'$.  Notice that 
\begin{align*}
	\det(P(xI-A)P^{-1}) &= \det(PxIP^{-1}-PAP^{-1}) \\
		&= \det(xI-PAP^{-1})\\
		&= \det(xI-A')
\end{align*}thus, similar matrices have the same eigenvalues. Then, being upper triangular
\begin{align*}
	\det(xI-A'B') &= \sum_{k=1}^n (x-a'_{kk}b'_{kk})\\
	 &= \sum_{k=1}^n (x-b'_{kk}a'_{kk})\\
	 &=\det(xI-B'A')
\end{align*} hence have the same eigenvalues.

\newpage
\vskip 0.1cm
\noindent
{\bf 3.a)}  Let $N \in \mathcal{M}_{2 \times 2}(F)$ such that $N^2 = 0$. Show that either $N = 0$ or it is similar to $\left( \begin{array}{cc} 0 & 0 \\ 1 & 0 \end{array} \right)$.\\
\\
Let $N'$ be the lower triangular matrix simliuar to $N$.  Thus, 
\begin{align*}
	N' &= \left( \begin{array}{cc}
		a & 0 \\
		b & c
	\end{array}
	\right)\\
	N'^2 &= \left( \begin{array}{cc}
		a^2 & 0 \\
		0 & c^2
	\end{array}
	\right)
\end{align*}which implies that $a=c=0$.  Thus, $b$ maybe any value.  When $b=0, N=0$, otherwise when $b=1$ all other potential values are merely scalar multiples.
\\
{\bf b)}  Suppose $A$ is a $ 2 \times 2$ matrix with complex entries. Prove that $A$ is similar to either $\left( \begin{array}{cc} a & 0 \\ 0  & b \end{array} \right)$ or
$\left( \begin{array}{cc} a & 0 \\ 1 & a \end{array} \right)$.\\
\\
By the corollary, for every $A\in \mathcal{M}_{2\times 2}(F)$ there is a similar matrix $A'$ that is triangular.\\
First: $A'=\left( \begin{array}{cc} a & 0 \\ 0  & b \end{array} \right)$\\
Theorem 6 states that A is diagonalizable if and only if the eigenvalues are distinct, i.e., the multiplicity of each eigenvalue is one.  If the eigenvalues are distinct then the similar matrix must be of this form.  \\
Then $A'=\left( \begin{array}{cc} a & 0 \\ 1 & a \end{array} \right)$.\\
If it is not diagonalizable then it must be triangular and the multiplicity of the single eigenvalue must be greater than 1, namely 2, which means that both elements on the diagonal are the same and that one of the remaining elements is non-zero and the other is zero.\\

\newpage
\vskip 0.1cm
\noindent
{\bf 4.a)} Let $A \, = \, \left( \begin{array}{ccccr} 0 & 0 & 0 & \cdots & -a_0 \\ 1 & 0 & 0 & \cdots & -a_1 \\ 0 & 1 & 0 & \cdots & -a_2 \\ 
\vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 & -a_{n-1} \end{array} \right)$. What is the characteristic polynomial of $A$? [Hint: use induction].
Conclude that any monic polynomial is the characteristic polynomial of some matrix. \\
\\
For $n=2$ we have $xI-A=\left(\begin{array}{cc}
x&-a_0 \\\
1&x-a_1
\end{array} \right)$ and $\det(xI-A) = x(x-a_1)+a_0=x^2-a_1x+a_0$\\
For $n=3$ we have $xI-A=\left(\begin{array}{ccc}
x& 0&-a_0 \\
1& x&-a_1 \\
0&1 &x-a_2
\end{array} \right)$ and $\det(xI-A) = x(x(x-a_2)-a_1)-a_0=x^3-a_2x^2+a_1x-a_0$\\
For $n=4$ we have $xI-A=\left(\begin{array}{cccc}
x&0& 0&-a_0 \\
1&x& 0&-a_1 \\
0&1&x &-a_2 \\
0&0&0&x-a_3
\end{array} \right)$ and 
\begin{align*}
	\det(xI-A) &= x\det\left(\begin{array}{ccc}
		x&0&-a_1\\
		1&x&-a_2\\
		0&1&x-a_3
\end{array}	 \right)+a_0\det\left( \begin{array}{ccc}
	1 & x & 0\\
	0 & 1 & x\\
	0 & 0 & 1
\end{array} \right) \\
	&= x(x^3-a_3x^2+a_2x-a_1)+a_0 \\
	&= x^4-a_3x^3+a_2x^2-a_1x+a_0
\end{align*}In general, $$x^n-a_{n-1}x^{n-1}+a_{n-2}x^{n-2}-\cdots+(-1)^{n-1}a_1x+(-1)^na_0$$
\\
{\bf b)} Show that the minimal polynomial and the characteristic polynomial of $\left( \begin{array}{ccr} 0 & 0 & -c \\ 1 & 0 & -b \\ 0 & 1 & -a \end{array} \right)$ are equal.\\
\\
The characteristic polynomial is $x^3-ax^2+bx-c$


\newpage
\vskip 0.1cm
\noindent
{\bf 5.} Prove that any square matrix $A$ where $A^2 = A$ is diagonalizable. \\
\\
Let $m(x)$ be the minimum polynomial for $A$.  Clearly, the polynomial $p(x)=x^2-x$ we have $p(A)=0$ which has distinct roots and must be divisible by $m$ which implies that it is diagonalizable.
\\

\vskip 0.1cm
\noindent
{\bf 6.} Let $T$ be a linear operator an a finite dimensional vector space $V$. Show that if  every subspace of $V$ is $T$-invariant, then $T$ is a scalar multiple of the identity operator.\\
\\
Let $W$ be a subspace that is $T$-invariant, then 
\begin{align*}
	[T]&= \left(\begin{array}{cc}
		[T_w] & 0 \\
		0 & C
\end{array}	 \right)
\end{align*}for some matrix $C$ and where $T_w: W \to W$.  However, any subspace of $W$ will also be $T$-invariant as will all of $V$.  Extrapolating, downward we can see that the from of $[T]$ will degenerate to placing a single value in first row and first column.  Extrapolating upward to $n$ we can see that eventually th entire diagonal is filled making it diagonalizable and homogenous.\\

\vskip 0.1cm
\noindent
{\bf 7.}  True or false?: if a triangular matrix is similar to a diagonal matrix then it is already a diagonal matrix.\\
\\
True: Let $P$ be our change of basis matrix and $P=[p_{ij}]$ and $P^{-1}=[q_{ij}]$.  Let, $B$ be triangular then $B=[\delta_{ij}b_{ij}]$ and $A$ be similar to $B, A = PBP^{-1}$.  Then $PB = \left [\,\sum_{k=1}^n p_{ik}\delta_{kj}b_{kj}\,\right ]=\left[\,\sum_{k=1}^n p_{kk}b_{kk}\,\right ]$ which is diagonal and similarly when we apply $P^{-1}$.  Thus, $A$ is diagonal.






\vfill
\eject
\noindent {\it Extra Questions}\\
{\bf 1.} Let $V$ be the vector space of all real valued continuous functions on the real line. Let $T$ be the operator defined by $(Tf)(x) \, = \, \int_0^x \, f(t) \, dt$. Prove 
that $T$ does not have an eigenvalue. \\

\vskip 0.1cm
\noindent 
{\bf 2.} Let $D$ be the differentiation operator on $\mathcal{P}^{(n)}$. Compute the characteristic and minimal polynomials of $D$. \\

\vskip 0.1cm
\noindent 
{\bf 3.}  Prove that a square matrix $A$ and its transpose $A^t$ have the same eigenvalues. \\

\vskip 0.1cm
\noindent
{\bf 4. } Let $A$ be an $n\times n$ matrix with complex entries. Suppose $\lambda$ is an eigenvalue of $A$ and $v = (v_1, \ldots, v_n)$ an eigenvector of $A$. Let $k$ be an index
where $|v_k| \geq |v_i|$ for all $i=1, \ldots, n$.  Prove that $|\lambda - a_{kk}| \leq \sum_{j \neq k} |a_{kj}|$. In other words, the eigenvalue $\lambda$ lies in a disk in the complex plane with 
center at $a_{kk}$ and radius $\sum_{j \neq k} |a_{jk}|$. Of course, most of the time we know neither the eigenvalues nor the associated eigenvectors. Therefore, we consider the
region $R(A)$ in the complex plane that is the union of these disks for each row $k=1, \ldots, n$. Moreover, since $A$ and $A^t$ have the same eigenvalues (see the previous exercise),
we have $n$ disks obtained from the columns of $A$. So also consider the region $C(A)$ in the complex plane that is the union of the disks obtained from each column. The region
$G(A) = R(A) \cap C(A)$ is known as the Gersgorin region of $A$ and contains all the eigenvalues of $A$. \\


\vskip 0.1cm
\noindent
{\bf 5.} We call a matrix $A \in \mathcal{M}_{n \times n}(\C)$ {\it strictly row dominant} if $|a_{kk}| > \sum_{j \neq k} |a_{kj}|$ for every $k =1, \ldots, n$. Show that a strictly
row dominant matrix is invertible. [Hint: do the above exercise first]. \\ 


\end{document}